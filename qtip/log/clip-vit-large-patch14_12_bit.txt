I0416 08:14:26.244762 3179531 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:14:26.244861 3179531 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:14:26.244902 3179531 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:14:26.359915 3179531 config.py:58] PyTorch version 2.4.0 available.
W0416 08:14:28.511113 3179531 warnings.py:110] /workspace/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

I0416 08:14:29.884599 3179531 quantize_finetune_clip.py:141] loaded model
I0416 08:14:29.884713 3179531 quantize_finetune_clip.py:143] loaded dataset and devset
I0416 08:14:29.942650 3179531 quantize_finetune_clip.py:151] vision layer 0 gpu 0
I0416 08:14:30.568552 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 0 in 0.51s
I0416 08:14:31.316811 3179531 quantize_finetune_clip.py:151] vision layer 1 gpu 0
I0416 08:14:33.813697 3182083 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:14:33.813854 3182083 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:14:33.813914 3182083 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:14:33.931579 3182083 config.py:58] PyTorch version 2.4.0 available.
W0416 08:14:36.346581 3182083 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-1:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:14:38.621260 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 1 in 0.15s
I0416 08:14:38.741074 3179531 quantize_finetune_clip.py:151] vision layer 2 gpu 0
I0416 08:14:41.004333 3184935 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:14:41.004568 3184935 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:14:41.004635 3184935 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:14:41.133575 3184935 config.py:58] PyTorch version 2.4.0 available.
W0416 08:14:43.678276 3184935 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-2:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:14:47.099320 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 2 in 0.31s
I0416 08:14:47.350796 3179531 quantize_finetune_clip.py:151] vision layer 3 gpu 0
I0416 08:14:49.676570 3186395 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:14:49.676685 3186395 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:14:49.676742 3186395 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:14:49.790810 3186395 config.py:58] PyTorch version 2.4.0 available.
W0416 08:14:52.220631 3186395 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-3:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:14:54.158408 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 3 in 0.17s
I0416 08:14:54.280598 3179531 quantize_finetune_clip.py:151] vision layer 4 gpu 0
I0416 08:14:56.616335 3187836 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:14:56.616554 3187836 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:14:56.616626 3187836 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:14:56.739497 3187836 config.py:58] PyTorch version 2.4.0 available.
W0416 08:14:59.110570 3187836 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-4:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:15:01.130521 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 4 in 0.16s
I0416 08:15:01.247642 3179531 quantize_finetune_clip.py:151] vision layer 5 gpu 0
I0416 08:15:03.544227 3189330 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:03.544342 3189330 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:03.544398 3189330 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:03.657102 3189330 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:05.918025 3189330 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-5:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:15:07.791444 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 5 in 0.15s
I0416 08:15:07.917475 3179531 quantize_finetune_clip.py:151] vision layer 6 gpu 0
I0416 08:15:10.306300 3190682 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:10.306430 3190682 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:10.306520 3190682 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:10.428748 3190682 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:12.741785 3190682 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-6:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:15:14.751370 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 6 in 0.15s
I0416 08:15:14.871795 3179531 quantize_finetune_clip.py:151] vision layer 7 gpu 0
I0416 08:15:17.202136 3192121 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:17.202277 3192121 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:17.202340 3192121 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:17.322548 3192121 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:19.764724 3192121 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-7:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:15:21.890445 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 7 in 0.16s
I0416 08:15:22.057970 3179531 quantize_finetune_clip.py:151] vision layer 8 gpu 0
I0416 08:15:24.425935 3193563 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:24.426264 3193563 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:24.426330 3193563 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:24.550623 3193563 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:27.176261 3193563 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-8:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:15:29.237828 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 8 in 0.17s
I0416 08:15:29.373401 3179531 quantize_finetune_clip.py:151] vision layer 9 gpu 0
I0416 08:15:31.732811 3195834 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:31.732965 3195834 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:31.733029 3195834 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:31.857360 3195834 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:34.225109 3195834 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-9:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:15:36.249300 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 9 in 0.16s
I0416 08:15:36.374131 3179531 quantize_finetune_clip.py:151] vision layer 10 gpu 0
I0416 08:15:38.681791 3197383 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:38.681925 3197383 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:38.681986 3197383 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:38.803120 3197383 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:41.226787 3197383 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-10:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:15:43.130454 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 10 in 0.16s
I0416 08:15:43.253487 3179531 quantize_finetune_clip.py:151] vision layer 11 gpu 0
I0416 08:15:45.650606 3198965 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:45.650732 3198965 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:45.650797 3198965 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:45.769747 3198965 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:48.038853 3198965 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-11:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:15:49.963092 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 11 in 0.17s
I0416 08:15:50.083117 3179531 quantize_finetune_clip.py:151] vision layer 12 gpu 0
I0416 08:15:52.432249 3200774 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:52.432377 3200774 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:52.432441 3200774 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:52.556223 3200774 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:55.109008 3200774 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-12:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:15:57.242013 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 12 in 0.16s
I0416 08:15:57.385311 3179531 quantize_finetune_clip.py:151] vision layer 13 gpu 0
I0416 08:15:59.637070 3202443 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:59.637202 3202443 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:59.637261 3202443 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:59.766609 3202443 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:02.073621 3202443 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-13:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:16:03.987630 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 13 in 0.16s
I0416 08:16:04.120856 3179531 quantize_finetune_clip.py:151] vision layer 14 gpu 0
I0416 08:16:06.509121 3203872 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:06.509330 3203872 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:06.509437 3203872 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:06.629184 3203872 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:09.040273 3203872 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-14:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:16:11.460205 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 14 in 0.19s
I0416 08:16:11.580769 3179531 quantize_finetune_clip.py:151] vision layer 15 gpu 0
I0416 08:16:13.870140 3206667 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:13.870353 3206667 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:13.870419 3206667 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:13.992815 3206667 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:16.402110 3206667 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-15:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:16:19.205521 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 15 in 0.15s
I0416 08:16:19.333998 3179531 quantize_finetune_clip.py:151] vision layer 16 gpu 0
I0416 08:16:21.773813 3208255 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:21.773988 3208255 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:21.774056 3208255 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:21.894296 3208255 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:24.603060 3208255 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-16:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:16:27.215707 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 16 in 0.17s
I0416 08:16:27.346427 3179531 quantize_finetune_clip.py:151] vision layer 17 gpu 0
I0416 08:16:29.916052 3210184 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:29.916182 3210184 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:29.916246 3210184 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:30.040256 3210184 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:32.992535 3210184 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-17:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:16:36.406832 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 17 in 0.18s
I0416 08:16:36.536742 3179531 quantize_finetune_clip.py:151] vision layer 18 gpu 0
I0416 08:16:38.861612 3212314 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:38.861799 3212314 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:38.861886 3212314 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:38.986629 3212314 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:41.798358 3212314 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-18:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:16:43.966842 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 18 in 0.16s
I0416 08:16:44.090943 3179531 quantize_finetune_clip.py:151] vision layer 19 gpu 0
I0416 08:16:46.428868 3213968 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:46.429044 3213968 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:46.429109 3213968 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:46.550497 3213968 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:49.407142 3213968 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-19:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:16:51.826408 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 19 in 0.15s
I0416 08:16:51.955868 3179531 quantize_finetune_clip.py:151] vision layer 20 gpu 0
I0416 08:16:54.329262 3215925 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:54.329387 3215925 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:54.329459 3215925 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:54.455956 3215925 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:56.907053 3215925 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-20:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:16:58.801102 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 20 in 0.15s
I0416 08:16:58.928210 3179531 quantize_finetune_clip.py:151] vision layer 21 gpu 0
I0416 08:17:01.382089 3218350 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:01.382255 3218350 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:01.382319 3218350 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:01.504390 3218350 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:04.487350 3218350 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-21:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:17:06.978892 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 21 in 0.16s
I0416 08:17:07.107248 3179531 quantize_finetune_clip.py:151] vision layer 22 gpu 0
I0416 08:17:09.406926 3220204 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:09.407083 3220204 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:09.407149 3220204 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:09.530732 3220204 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:11.965240 3220204 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-22:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:17:14.010190 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 22 in 0.17s
I0416 08:17:14.141177 3179531 quantize_finetune_clip.py:151] vision layer 23 gpu 0
I0416 08:17:16.560002 3221776 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:16.560153 3221776 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:16.560211 3221776 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:16.677079 3221776 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:19.220311 3221776 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-23:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:17:21.116744 3179531 quantize_finetune_clip.py:168] computed original embedding for vision layer 23 in 0.16s
I0416 08:17:23.588059 3223750 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:23.588238 3223750 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:23.588306 3223750 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:23.707399 3223750 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:26.050837 3223750 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-24:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:17:28.634256 3179531 quantize_finetune_clip.py:151] text layer 0 gpu 0
I0416 08:17:28.939784 3179531 quantize_finetune_clip.py:168] computed original embedding for text layer 0 in 0.17s
I0416 08:17:29.062092 3179531 quantize_finetune_clip.py:151] text layer 1 gpu 0
I0416 08:17:31.442456 3225177 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:31.442578 3225177 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:31.442642 3225177 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:31.563812 3225177 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:33.954810 3225177 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-25:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:17:36.906616 3179531 quantize_finetune_clip.py:168] computed original embedding for text layer 1 in 0.16s
I0416 08:17:37.028166 3179531 quantize_finetune_clip.py:151] text layer 2 gpu 0
I0416 08:17:39.495472 3227284 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:39.495597 3227284 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:39.495660 3227284 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:39.617502 3227284 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:41.988671 3227284 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-26:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:17:44.818157 3179531 quantize_finetune_clip.py:168] computed original embedding for text layer 2 in 0.13s
I0416 08:17:44.935539 3179531 quantize_finetune_clip.py:151] text layer 3 gpu 0
I0416 08:17:47.220382 3229890 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:47.220503 3229890 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:47.220569 3229890 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:47.341009 3229890 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:49.810523 3229890 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-27:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:17:51.798912 3179531 quantize_finetune_clip.py:168] computed original embedding for text layer 3 in 0.17s
I0416 08:17:51.918942 3179531 quantize_finetune_clip.py:151] text layer 4 gpu 0
I0416 08:17:54.325009 3231555 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:54.325126 3231555 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:54.325190 3231555 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:54.440584 3231555 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:56.819121 3231555 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-28:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:17:58.699430 3179531 quantize_finetune_clip.py:168] computed original embedding for text layer 4 in 0.15s
I0416 08:17:58.809994 3179531 quantize_finetune_clip.py:151] text layer 5 gpu 0
I0416 08:18:01.295003 3233068 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:01.295124 3233068 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:01.295183 3233068 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:01.413761 3233068 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:04.204946 3233068 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-29:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:18:06.801189 3179531 quantize_finetune_clip.py:168] computed original embedding for text layer 5 in 0.17s
I0416 08:18:06.913147 3179531 quantize_finetune_clip.py:151] text layer 6 gpu 0
I0416 08:18:09.500645 3235124 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:09.500799 3235124 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:09.500864 3235124 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:09.621680 3235124 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:12.066521 3235124 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-30:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:18:14.184081 3179531 quantize_finetune_clip.py:168] computed original embedding for text layer 6 in 0.14s
I0416 08:18:14.330325 3179531 quantize_finetune_clip.py:151] text layer 7 gpu 0
I0416 08:18:16.557569 3236755 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:16.557696 3236755 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:16.557756 3236755 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:16.674134 3236755 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:19.356733 3236755 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-31:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:18:22.506397 3179531 quantize_finetune_clip.py:168] computed original embedding for text layer 7 in 0.15s
I0416 08:18:22.663513 3179531 quantize_finetune_clip.py:151] text layer 8 gpu 0
I0416 08:18:25.091941 3238717 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:25.092065 3238717 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:25.092129 3238717 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:25.214838 3238717 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:27.949192 3238717 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-32:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:18:30.260703 3179531 quantize_finetune_clip.py:168] computed original embedding for text layer 8 in 0.16s
I0416 08:18:30.374740 3179531 quantize_finetune_clip.py:151] text layer 9 gpu 0
I0416 08:18:32.681409 3241301 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:32.681542 3241301 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:32.681603 3241301 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:32.803022 3241301 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:35.165582 3241301 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-33:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:18:37.198614 3179531 quantize_finetune_clip.py:168] computed original embedding for text layer 9 in 0.16s
I0416 08:18:37.312582 3179531 quantize_finetune_clip.py:151] text layer 10 gpu 0
I0416 08:18:39.790499 3242989 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:39.790628 3242989 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:39.790693 3242989 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:39.909517 3242989 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:42.288461 3242989 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-34:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:18:44.279679 3179531 quantize_finetune_clip.py:168] computed original embedding for text layer 10 in 0.15s
I0416 08:18:44.390700 3179531 quantize_finetune_clip.py:151] text layer 11 gpu 0
I0416 08:18:46.940566 3244562 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:46.940712 3244562 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:46.940775 3244562 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:47.057677 3244562 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:49.495851 3244562 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-35:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:18:51.488448 3179531 quantize_finetune_clip.py:168] computed original embedding for text layer 11 in 0.13s
I0416 08:18:54.015338 3246630 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:54.015483 3246630 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:54.015545 3246630 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:54.132964 3246630 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:56.615568 3246630 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-36:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:19:03.505957 3248575 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:19:03.506067 3248575 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:19:03.506109 3248575 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:19:03.626621 3248575 config.py:58] PyTorch version 2.4.0 available.
W0416 08:19:05.136529 3248575 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0416 08:19:05.137061 3248575 hfize_clip.py:43] CLIPConfig {
  "_name_or_path": "../Wparam_dataset/hf_model/openai--clip-vit-large-patch14",
  "architectures": [
    "CLIPModel"
  ],
  "initializer_factor": 1.0,
  "logit_scale_init_value": 2.6592,
  "model_type": "clip",
  "projection_dim": 768,
  "quip_params": {
    "K": 12,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "text_config": {
    "dropout": 0.0,
    "hidden_size": 768,
    "intermediate_size": 3072,
    "model_type": "clip_text_model",
    "num_attention_heads": 12,
    "projection_dim": 768,
    "torch_dtype": "float32"
  },
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "vision_config": {
    "dropout": 0.0,
    "hidden_size": 1024,
    "intermediate_size": 4096,
    "model_type": "clip_vision_model",
    "num_attention_heads": 16,
    "num_hidden_layers": 24,
    "patch_size": 14,
    "projection_dim": 768,
    "torch_dtype": "float32"
  }
}

Some weights of the model checkpoint at ../Wparam_dataset/hf_model/openai--clip-vit-large-patch14 were not used when initializing CLIPModel: ['text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing CLIPModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of CLIPModel were not initialized from the model checkpoint at ../Wparam_dataset/hf_model/openai--clip-vit-large-patch14 and are newly initialized: ['text_model.encoder.layers.0.mlp.fc1.SU', 'text_model.encoder.layers.0.mlp.fc1.SV', 'text_model.encoder.layers.0.mlp.fc1.rcp', 'text_model.encoder.layers.0.mlp.fc1.tlut', 'text_model.encoder.layers.0.mlp.fc1.tp_rank', 'text_model.encoder.layers.0.mlp.fc1.trellis', 'text_model.encoder.layers.0.mlp.fc2.SU', 'text_model.encoder.layers.0.mlp.fc2.SV', 'text_model.encoder.layers.0.mlp.fc2.rcp', 'text_model.encoder.layers.0.mlp.fc2.tlut', 'text_model.encoder.layers.0.mlp.fc2.tp_rank', 'text_model.encoder.layers.0.mlp.fc2.trellis', 'text_model.encoder.layers.0.self_attn.k_proj.SU', 'text_model.encoder.layers.0.self_attn.k_proj.SV', 'text_model.encoder.layers.0.self_attn.k_proj.rcp', 'text_model.encoder.layers.0.self_attn.k_proj.tlut', 'text_model.encoder.layers.0.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.k_proj.trellis', 'text_model.encoder.layers.0.self_attn.out_proj.SU', 'text_model.encoder.layers.0.self_attn.out_proj.SV', 'text_model.encoder.layers.0.self_attn.out_proj.rcp', 'text_model.encoder.layers.0.self_attn.out_proj.tlut', 'text_model.encoder.layers.0.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.out_proj.trellis', 'text_model.encoder.layers.0.self_attn.q_proj.SU', 'text_model.encoder.layers.0.self_attn.q_proj.SV', 'text_model.encoder.layers.0.self_attn.q_proj.rcp', 'text_model.encoder.layers.0.self_attn.q_proj.tlut', 'text_model.encoder.layers.0.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.q_proj.trellis', 'text_model.encoder.layers.0.self_attn.v_proj.SU', 'text_model.encoder.layers.0.self_attn.v_proj.SV', 'text_model.encoder.layers.0.self_attn.v_proj.rcp', 'text_model.encoder.layers.0.self_attn.v_proj.tlut', 'text_model.encoder.layers.0.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.v_proj.trellis', 'text_model.encoder.layers.1.mlp.fc1.SU', 'text_model.encoder.layers.1.mlp.fc1.SV', 'text_model.encoder.layers.1.mlp.fc1.rcp', 'text_model.encoder.layers.1.mlp.fc1.tlut', 'text_model.encoder.layers.1.mlp.fc1.tp_rank', 'text_model.encoder.layers.1.mlp.fc1.trellis', 'text_model.encoder.layers.1.mlp.fc2.SU', 'text_model.encoder.layers.1.mlp.fc2.SV', 'text_model.encoder.layers.1.mlp.fc2.rcp', 'text_model.encoder.layers.1.mlp.fc2.tlut', 'text_model.encoder.layers.1.mlp.fc2.tp_rank', 'text_model.encoder.layers.1.mlp.fc2.trellis', 'text_model.encoder.layers.1.self_attn.k_proj.SU', 'text_model.encoder.layers.1.self_attn.k_proj.SV', 'text_model.encoder.layers.1.self_attn.k_proj.rcp', 'text_model.encoder.layers.1.self_attn.k_proj.tlut', 'text_model.encoder.layers.1.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.k_proj.trellis', 'text_model.encoder.layers.1.self_attn.out_proj.SU', 'text_model.encoder.layers.1.self_attn.out_proj.SV', 'text_model.encoder.layers.1.self_attn.out_proj.rcp', 'text_model.encoder.layers.1.self_attn.out_proj.tlut', 'text_model.encoder.layers.1.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.out_proj.trellis', 'text_model.encoder.layers.1.self_attn.q_proj.SU', 'text_model.encoder.layers.1.self_attn.q_proj.SV', 'text_model.encoder.layers.1.self_attn.q_proj.rcp', 'text_model.encoder.layers.1.self_attn.q_proj.tlut', 'text_model.encoder.layers.1.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.q_proj.trellis', 'text_model.encoder.layers.1.self_attn.v_proj.SU', 'text_model.encoder.layers.1.self_attn.v_proj.SV', 'text_model.encoder.layers.1.self_attn.v_proj.rcp', 'text_model.encoder.layers.1.self_attn.v_proj.tlut', 'text_model.encoder.layers.1.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.v_proj.trellis', 'text_model.encoder.layers.10.mlp.fc1.SU', 'text_model.encoder.layers.10.mlp.fc1.SV', 'text_model.encoder.layers.10.mlp.fc1.rcp', 'text_model.encoder.layers.10.mlp.fc1.tlut', 'text_model.encoder.layers.10.mlp.fc1.tp_rank', 'text_model.encoder.layers.10.mlp.fc1.trellis', 'text_model.encoder.layers.10.mlp.fc2.SU', 'text_model.encoder.layers.10.mlp.fc2.SV', 'text_model.encoder.layers.10.mlp.fc2.rcp', 'text_model.encoder.layers.10.mlp.fc2.tlut', 'text_model.encoder.layers.10.mlp.fc2.tp_rank', 'text_model.encoder.layers.10.mlp.fc2.trellis', 'text_model.encoder.layers.10.self_attn.k_proj.SU', 'text_model.encoder.layers.10.self_attn.k_proj.SV', 'text_model.encoder.layers.10.self_attn.k_proj.rcp', 'text_model.encoder.layers.10.self_attn.k_proj.tlut', 'text_model.encoder.layers.10.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.k_proj.trellis', 'text_model.encoder.layers.10.self_attn.out_proj.SU', 'text_model.encoder.layers.10.self_attn.out_proj.SV', 'text_model.encoder.layers.10.self_attn.out_proj.rcp', 'text_model.encoder.layers.10.self_attn.out_proj.tlut', 'text_model.encoder.layers.10.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.out_proj.trellis', 'text_model.encoder.layers.10.self_attn.q_proj.SU', 'text_model.encoder.layers.10.self_attn.q_proj.SV', 'text_model.encoder.layers.10.self_attn.q_proj.rcp', 'text_model.encoder.layers.10.self_attn.q_proj.tlut', 'text_model.encoder.layers.10.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.q_proj.trellis', 'text_model.encoder.layers.10.self_attn.v_proj.SU', 'text_model.encoder.layers.10.self_attn.v_proj.SV', 'text_model.encoder.layers.10.self_attn.v_proj.rcp', 'text_model.encoder.layers.10.self_attn.v_proj.tlut', 'text_model.encoder.layers.10.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.v_proj.trellis', 'text_model.encoder.layers.11.mlp.fc1.SU', 'text_model.encoder.layers.11.mlp.fc1.SV', 'text_model.encoder.layers.11.mlp.fc1.rcp', 'text_model.encoder.layers.11.mlp.fc1.tlut', 'text_model.encoder.layers.11.mlp.fc1.tp_rank', 'text_model.encoder.layers.11.mlp.fc1.trellis', 'text_model.encoder.layers.11.mlp.fc2.SU', 'text_model.encoder.layers.11.mlp.fc2.SV', 'text_model.encoder.layers.11.mlp.fc2.rcp', 'text_model.encoder.layers.11.mlp.fc2.tlut', 'text_model.encoder.layers.11.mlp.fc2.tp_rank', 'text_model.encoder.layers.11.mlp.fc2.trellis', 'text_model.encoder.layers.11.self_attn.k_proj.SU', 'text_model.encoder.layers.11.self_attn.k_proj.SV', 'text_model.encoder.layers.11.self_attn.k_proj.rcp', 'text_model.encoder.layers.11.self_attn.k_proj.tlut', 'text_model.encoder.layers.11.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.k_proj.trellis', 'text_model.encoder.layers.11.self_attn.out_proj.SU', 'text_model.encoder.layers.11.self_attn.out_proj.SV', 'text_model.encoder.layers.11.self_attn.out_proj.rcp', 'text_model.encoder.layers.11.self_attn.out_proj.tlut', 'text_model.encoder.layers.11.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.out_proj.trellis', 'text_model.encoder.layers.11.self_attn.q_proj.SU', 'text_model.encoder.layers.11.self_attn.q_proj.SV', 'text_model.encoder.layers.11.self_attn.q_proj.rcp', 'text_model.encoder.layers.11.self_attn.q_proj.tlut', 'text_model.encoder.layers.11.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.q_proj.trellis', 'text_model.encoder.layers.11.self_attn.v_proj.SU', 'text_model.encoder.layers.11.self_attn.v_proj.SV', 'text_model.encoder.layers.11.self_attn.v_proj.rcp', 'text_model.encoder.layers.11.self_attn.v_proj.tlut', 'text_model.encoder.layers.11.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.v_proj.trellis', 'text_model.encoder.layers.2.mlp.fc1.SU', 'text_model.encoder.layers.2.mlp.fc1.SV', 'text_model.encoder.layers.2.mlp.fc1.rcp', 'text_model.encoder.layers.2.mlp.fc1.tlut', 'text_model.encoder.layers.2.mlp.fc1.tp_rank', 'text_model.encoder.layers.2.mlp.fc1.trellis', 'text_model.encoder.layers.2.mlp.fc2.SU', 'text_model.encoder.layers.2.mlp.fc2.SV', 'text_model.encoder.layers.2.mlp.fc2.rcp', 'text_model.encoder.layers.2.mlp.fc2.tlut', 'text_model.encoder.layers.2.mlp.fc2.tp_rank', 'text_model.encoder.layers.2.mlp.fc2.trellis', 'text_model.encoder.layers.2.self_attn.k_proj.SU', 'text_model.encoder.layers.2.self_attn.k_proj.SV', 'text_model.encoder.layers.2.self_attn.k_proj.rcp', 'text_model.encoder.layers.2.self_attn.k_proj.tlut', 'text_model.encoder.layers.2.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.k_proj.trellis', 'text_model.encoder.layers.2.self_attn.out_proj.SU', 'text_model.encoder.layers.2.self_attn.out_proj.SV', 'text_model.encoder.layers.2.self_attn.out_proj.rcp', 'text_model.encoder.layers.2.self_attn.out_proj.tlut', 'text_model.encoder.layers.2.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.out_proj.trellis', 'text_model.encoder.layers.2.self_attn.q_proj.SU', 'text_model.encoder.layers.2.self_attn.q_proj.SV', 'text_model.encoder.layers.2.self_attn.q_proj.rcp', 'text_model.encoder.layers.2.self_attn.q_proj.tlut', 'text_model.encoder.layers.2.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.q_proj.trellis', 'text_model.encoder.layers.2.self_attn.v_proj.SU', 'text_model.encoder.layers.2.self_attn.v_proj.SV', 'text_model.encoder.layers.2.self_attn.v_proj.rcp', 'text_model.encoder.layers.2.self_attn.v_proj.tlut', 'text_model.encoder.layers.2.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.v_proj.trellis', 'text_model.encoder.layers.3.mlp.fc1.SU', 'text_model.encoder.layers.3.mlp.fc1.SV', 'text_model.encoder.layers.3.mlp.fc1.rcp', 'text_model.encoder.layers.3.mlp.fc1.tlut', 'text_model.encoder.layers.3.mlp.fc1.tp_rank', 'text_model.encoder.layers.3.mlp.fc1.trellis', 'text_model.encoder.layers.3.mlp.fc2.SU', 'text_model.encoder.layers.3.mlp.fc2.SV', 'text_model.encoder.layers.3.mlp.fc2.rcp', 'text_model.encoder.layers.3.mlp.fc2.tlut', 'text_model.encoder.layers.3.mlp.fc2.tp_rank', 'text_model.encoder.layers.3.mlp.fc2.trellis', 'text_model.encoder.layers.3.self_attn.k_proj.SU', 'text_model.encoder.layers.3.self_attn.k_proj.SV', 'text_model.encoder.layers.3.self_attn.k_proj.rcp', 'text_model.encoder.layers.3.self_attn.k_proj.tlut', 'text_model.encoder.layers.3.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.k_proj.trellis', 'text_model.encoder.layers.3.self_attn.out_proj.SU', 'text_model.encoder.layers.3.self_attn.out_proj.SV', 'text_model.encoder.layers.3.self_attn.out_proj.rcp', 'text_model.encoder.layers.3.self_attn.out_proj.tlut', 'text_model.encoder.layers.3.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.out_proj.trellis', 'text_model.encoder.layers.3.self_attn.q_proj.SU', 'text_model.encoder.layers.3.self_attn.q_proj.SV', 'text_model.encoder.layers.3.self_attn.q_proj.rcp', 'text_model.encoder.layers.3.self_attn.q_proj.tlut', 'text_model.encoder.layers.3.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.q_proj.trellis', 'text_model.encoder.layers.3.self_attn.v_proj.SU', 'text_model.encoder.layers.3.self_attn.v_proj.SV', 'text_model.encoder.layers.3.self_attn.v_proj.rcp', 'text_model.encoder.layers.3.self_attn.v_proj.tlut', 'text_model.encoder.layers.3.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.v_proj.trellis', 'text_model.encoder.layers.4.mlp.fc1.SU', 'text_model.encoder.layers.4.mlp.fc1.SV', 'text_model.encoder.layers.4.mlp.fc1.rcp', 'text_model.encoder.layers.4.mlp.fc1.tlut', 'text_model.encoder.layers.4.mlp.fc1.tp_rank', 'text_model.encoder.layers.4.mlp.fc1.trellis', 'text_model.encoder.layers.4.mlp.fc2.SU', 'text_model.encoder.layers.4.mlp.fc2.SV', 'text_model.encoder.layers.4.mlp.fc2.rcp', 'text_model.encoder.layers.4.mlp.fc2.tlut', 'text_model.encoder.layers.4.mlp.fc2.tp_rank', 'text_model.encoder.layers.4.mlp.fc2.trellis', 'text_model.encoder.layers.4.self_attn.k_proj.SU', 'text_model.encoder.layers.4.self_attn.k_proj.SV', 'text_model.encoder.layers.4.self_attn.k_proj.rcp', 'text_model.encoder.layers.4.self_attn.k_proj.tlut', 'text_model.encoder.layers.4.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.k_proj.trellis', 'text_model.encoder.layers.4.self_attn.out_proj.SU', 'text_model.encoder.layers.4.self_attn.out_proj.SV', 'text_model.encoder.layers.4.self_attn.out_proj.rcp', 'text_model.encoder.layers.4.self_attn.out_proj.tlut', 'text_model.encoder.layers.4.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.out_proj.trellis', 'text_model.encoder.layers.4.self_attn.q_proj.SU', 'text_model.encoder.layers.4.self_attn.q_proj.SV', 'text_model.encoder.layers.4.self_attn.q_proj.rcp', 'text_model.encoder.layers.4.self_attn.q_proj.tlut', 'text_model.encoder.layers.4.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.q_proj.trellis', 'text_model.encoder.layers.4.self_attn.v_proj.SU', 'text_model.encoder.layers.4.self_attn.v_proj.SV', 'text_model.encoder.layers.4.self_attn.v_proj.rcp', 'text_model.encoder.layers.4.self_attn.v_proj.tlut', 'text_model.encoder.layers.4.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.v_proj.trellis', 'text_model.encoder.layers.5.mlp.fc1.SU', 'text_model.encoder.layers.5.mlp.fc1.SV', 'text_model.encoder.layers.5.mlp.fc1.rcp', 'text_model.encoder.layers.5.mlp.fc1.tlut', 'text_model.encoder.layers.5.mlp.fc1.tp_rank', 'text_model.encoder.layers.5.mlp.fc1.trellis', 'text_model.encoder.layers.5.mlp.fc2.SU', 'text_model.encoder.layers.5.mlp.fc2.SV', 'text_model.encoder.layers.5.mlp.fc2.rcp', 'text_model.encoder.layers.5.mlp.fc2.tlut', 'text_model.encoder.layers.5.mlp.fc2.tp_rank', 'text_model.encoder.layers.5.mlp.fc2.trellis', 'text_model.encoder.layers.5.self_attn.k_proj.SU', 'text_model.encoder.layers.5.self_attn.k_proj.SV', 'text_model.encoder.layers.5.self_attn.k_proj.rcp', 'text_model.encoder.layers.5.self_attn.k_proj.tlut', 'text_model.encoder.layers.5.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.k_proj.trellis', 'text_model.encoder.layers.5.self_attn.out_proj.SU', 'text_model.encoder.layers.5.self_attn.out_proj.SV', 'text_model.encoder.layers.5.self_attn.out_proj.rcp', 'text_model.encoder.layers.5.self_attn.out_proj.tlut', 'text_model.encoder.layers.5.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.out_proj.trellis', 'text_model.encoder.layers.5.self_attn.q_proj.SU', 'text_model.encoder.layers.5.self_attn.q_proj.SV', 'text_model.encoder.layers.5.self_attn.q_proj.rcp', 'text_model.encoder.layers.5.self_attn.q_proj.tlut', 'text_model.encoder.layers.5.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.q_proj.trellis', 'text_model.encoder.layers.5.self_attn.v_proj.SU', 'text_model.encoder.layers.5.self_attn.v_proj.SV', 'text_model.encoder.layers.5.self_attn.v_proj.rcp', 'text_model.encoder.layers.5.self_attn.v_proj.tlut', 'text_model.encoder.layers.5.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.v_proj.trellis', 'text_model.encoder.layers.6.mlp.fc1.SU', 'text_model.encoder.layers.6.mlp.fc1.SV', 'text_model.encoder.layers.6.mlp.fc1.rcp', 'text_model.encoder.layers.6.mlp.fc1.tlut', 'text_model.encoder.layers.6.mlp.fc1.tp_rank', 'text_model.encoder.layers.6.mlp.fc1.trellis', 'text_model.encoder.layers.6.mlp.fc2.SU', 'text_model.encoder.layers.6.mlp.fc2.SV', 'text_model.encoder.layers.6.mlp.fc2.rcp', 'text_model.encoder.layers.6.mlp.fc2.tlut', 'text_model.encoder.layers.6.mlp.fc2.tp_rank', 'text_model.encoder.layers.6.mlp.fc2.trellis', 'text_model.encoder.layers.6.self_attn.k_proj.SU', 'text_model.encoder.layers.6.self_attn.k_proj.SV', 'text_model.encoder.layers.6.self_attn.k_proj.rcp', 'text_model.encoder.layers.6.self_attn.k_proj.tlut', 'text_model.encoder.layers.6.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.k_proj.trellis', 'text_model.encoder.layers.6.self_attn.out_proj.SU', 'text_model.encoder.layers.6.self_attn.out_proj.SV', 'text_model.encoder.layers.6.self_attn.out_proj.rcp', 'text_model.encoder.layers.6.self_attn.out_proj.tlut', 'text_model.encoder.layers.6.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.out_proj.trellis', 'text_model.encoder.layers.6.self_attn.q_proj.SU', 'text_model.encoder.layers.6.self_attn.q_proj.SV', 'text_model.encoder.layers.6.self_attn.q_proj.rcp', 'text_model.encoder.layers.6.self_attn.q_proj.tlut', 'text_model.encoder.layers.6.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.q_proj.trellis', 'text_model.encoder.layers.6.self_attn.v_proj.SU', 'text_model.encoder.layers.6.self_attn.v_proj.SV', 'text_model.encoder.layers.6.self_attn.v_proj.rcp', 'text_model.encoder.layers.6.self_attn.v_proj.tlut', 'text_model.encoder.layers.6.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.v_proj.trellis', 'text_model.encoder.layers.7.mlp.fc1.SU', 'text_model.encoder.layers.7.mlp.fc1.SV', 'text_model.encoder.layers.7.mlp.fc1.rcp', 'text_model.encoder.layers.7.mlp.fc1.tlut', 'text_model.encoder.layers.7.mlp.fc1.tp_rank', 'text_model.encoder.layers.7.mlp.fc1.trellis', 'text_model.encoder.layers.7.mlp.fc2.SU', 'text_model.encoder.layers.7.mlp.fc2.SV', 'text_model.encoder.layers.7.mlp.fc2.rcp', 'text_model.encoder.layers.7.mlp.fc2.tlut', 'text_model.encoder.layers.7.mlp.fc2.tp_rank', 'text_model.encoder.layers.7.mlp.fc2.trellis', 'text_model.encoder.layers.7.self_attn.k_proj.SU', 'text_model.encoder.layers.7.self_attn.k_proj.SV', 'text_model.encoder.layers.7.self_attn.k_proj.rcp', 'text_model.encoder.layers.7.self_attn.k_proj.tlut', 'text_model.encoder.layers.7.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.k_proj.trellis', 'text_model.encoder.layers.7.self_attn.out_proj.SU', 'text_model.encoder.layers.7.self_attn.out_proj.SV', 'text_model.encoder.layers.7.self_attn.out_proj.rcp', 'text_model.encoder.layers.7.self_attn.out_proj.tlut', 'text_model.encoder.layers.7.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.out_proj.trellis', 'text_model.encoder.layers.7.self_attn.q_proj.SU', 'text_model.encoder.layers.7.self_attn.q_proj.SV', 'text_model.encoder.layers.7.self_attn.q_proj.rcp', 'text_model.encoder.layers.7.self_attn.q_proj.tlut', 'text_model.encoder.layers.7.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.q_proj.trellis', 'text_model.encoder.layers.7.self_attn.v_proj.SU', 'text_model.encoder.layers.7.self_attn.v_proj.SV', 'text_model.encoder.layers.7.self_attn.v_proj.rcp', 'text_model.encoder.layers.7.self_attn.v_proj.tlut', 'text_model.encoder.layers.7.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.v_proj.trellis', 'text_model.encoder.layers.8.mlp.fc1.SU', 'text_model.encoder.layers.8.mlp.fc1.SV', 'text_model.encoder.layers.8.mlp.fc1.rcp', 'text_model.encoder.layers.8.mlp.fc1.tlut', 'text_model.encoder.layers.8.mlp.fc1.tp_rank', 'text_model.encoder.layers.8.mlp.fc1.trellis', 'text_model.encoder.layers.8.mlp.fc2.SU', 'text_model.encoder.layers.8.mlp.fc2.SV', 'text_model.encoder.layers.8.mlp.fc2.rcp', 'text_model.encoder.layers.8.mlp.fc2.tlut', 'text_model.encoder.layers.8.mlp.fc2.tp_rank', 'text_model.encoder.layers.8.mlp.fc2.trellis', 'text_model.encoder.layers.8.self_attn.k_proj.SU', 'text_model.encoder.layers.8.self_attn.k_proj.SV', 'text_model.encoder.layers.8.self_attn.k_proj.rcp', 'text_model.encoder.layers.8.self_attn.k_proj.tlut', 'text_model.encoder.layers.8.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.k_proj.trellis', 'text_model.encoder.layers.8.self_attn.out_proj.SU', 'text_model.encoder.layers.8.self_attn.out_proj.SV', 'text_model.encoder.layers.8.self_attn.out_proj.rcp', 'text_model.encoder.layers.8.self_attn.out_proj.tlut', 'text_model.encoder.layers.8.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.out_proj.trellis', 'text_model.encoder.layers.8.self_attn.q_proj.SU', 'text_model.encoder.layers.8.self_attn.q_proj.SV', 'text_model.encoder.layers.8.self_attn.q_proj.rcp', 'text_model.encoder.layers.8.self_attn.q_proj.tlut', 'text_model.encoder.layers.8.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.q_proj.trellis', 'text_model.encoder.layers.8.self_attn.v_proj.SU', 'text_model.encoder.layers.8.self_attn.v_proj.SV', 'text_model.encoder.layers.8.self_attn.v_proj.rcp', 'text_model.encoder.layers.8.self_attn.v_proj.tlut', 'text_model.encoder.layers.8.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.v_proj.trellis', 'text_model.encoder.layers.9.mlp.fc1.SU', 'text_model.encoder.layers.9.mlp.fc1.SV', 'text_model.encoder.layers.9.mlp.fc1.rcp', 'text_model.encoder.layers.9.mlp.fc1.tlut', 'text_model.encoder.layers.9.mlp.fc1.tp_rank', 'text_model.encoder.layers.9.mlp.fc1.trellis', 'text_model.encoder.layers.9.mlp.fc2.SU', 'text_model.encoder.layers.9.mlp.fc2.SV', 'text_model.encoder.layers.9.mlp.fc2.rcp', 'text_model.encoder.layers.9.mlp.fc2.tlut', 'text_model.encoder.layers.9.mlp.fc2.tp_rank', 'text_model.encoder.layers.9.mlp.fc2.trellis', 'text_model.encoder.layers.9.self_attn.k_proj.SU', 'text_model.encoder.layers.9.self_attn.k_proj.SV', 'text_model.encoder.layers.9.self_attn.k_proj.rcp', 'text_model.encoder.layers.9.self_attn.k_proj.tlut', 'text_model.encoder.layers.9.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.k_proj.trellis', 'text_model.encoder.layers.9.self_attn.out_proj.SU', 'text_model.encoder.layers.9.self_attn.out_proj.SV', 'text_model.encoder.layers.9.self_attn.out_proj.rcp', 'text_model.encoder.layers.9.self_attn.out_proj.tlut', 'text_model.encoder.layers.9.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.out_proj.trellis', 'text_model.encoder.layers.9.self_attn.q_proj.SU', 'text_model.encoder.layers.9.self_attn.q_proj.SV', 'text_model.encoder.layers.9.self_attn.q_proj.rcp', 'text_model.encoder.layers.9.self_attn.q_proj.tlut', 'text_model.encoder.layers.9.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.q_proj.trellis', 'text_model.encoder.layers.9.self_attn.v_proj.SU', 'text_model.encoder.layers.9.self_attn.v_proj.SV', 'text_model.encoder.layers.9.self_attn.v_proj.rcp', 'text_model.encoder.layers.9.self_attn.v_proj.tlut', 'text_model.encoder.layers.9.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.v_proj.trellis', 'vision_model.encoder.layers.0.mlp.fc1.SU', 'vision_model.encoder.layers.0.mlp.fc1.SV', 'vision_model.encoder.layers.0.mlp.fc1.rcp', 'vision_model.encoder.layers.0.mlp.fc1.tlut', 'vision_model.encoder.layers.0.mlp.fc1.tp_rank', 'vision_model.encoder.layers.0.mlp.fc1.trellis', 'vision_model.encoder.layers.0.mlp.fc2.SU', 'vision_model.encoder.layers.0.mlp.fc2.SV', 'vision_model.encoder.layers.0.mlp.fc2.rcp', 'vision_model.encoder.layers.0.mlp.fc2.tlut', 'vision_model.encoder.layers.0.mlp.fc2.tp_rank', 'vision_model.encoder.layers.0.mlp.fc2.trellis', 'vision_model.encoder.layers.0.self_attn.k_proj.SU', 'vision_model.encoder.layers.0.self_attn.k_proj.SV', 'vision_model.encoder.layers.0.self_attn.k_proj.rcp', 'vision_model.encoder.layers.0.self_attn.k_proj.tlut', 'vision_model.encoder.layers.0.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.k_proj.trellis', 'vision_model.encoder.layers.0.self_attn.out_proj.SU', 'vision_model.encoder.layers.0.self_attn.out_proj.SV', 'vision_model.encoder.layers.0.self_attn.out_proj.rcp', 'vision_model.encoder.layers.0.self_attn.out_proj.tlut', 'vision_model.encoder.layers.0.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.out_proj.trellis', 'vision_model.encoder.layers.0.self_attn.q_proj.SU', 'vision_model.encoder.layers.0.self_attn.q_proj.SV', 'vision_model.encoder.layers.0.self_attn.q_proj.rcp', 'vision_model.encoder.layers.0.self_attn.q_proj.tlut', 'vision_model.encoder.layers.0.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.q_proj.trellis', 'vision_model.encoder.layers.0.self_attn.v_proj.SU', 'vision_model.encoder.layers.0.self_attn.v_proj.SV', 'vision_model.encoder.layers.0.self_attn.v_proj.rcp', 'vision_model.encoder.layers.0.self_attn.v_proj.tlut', 'vision_model.encoder.layers.0.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.v_proj.trellis', 'vision_model.encoder.layers.1.mlp.fc1.SU', 'vision_model.encoder.layers.1.mlp.fc1.SV', 'vision_model.encoder.layers.1.mlp.fc1.rcp', 'vision_model.encoder.layers.1.mlp.fc1.tlut', 'vision_model.encoder.layers.1.mlp.fc1.tp_rank', 'vision_model.encoder.layers.1.mlp.fc1.trellis', 'vision_model.encoder.layers.1.mlp.fc2.SU', 'vision_model.encoder.layers.1.mlp.fc2.SV', 'vision_model.encoder.layers.1.mlp.fc2.rcp', 'vision_model.encoder.layers.1.mlp.fc2.tlut', 'vision_model.encoder.layers.1.mlp.fc2.tp_rank', 'vision_model.encoder.layers.1.mlp.fc2.trellis', 'vision_model.encoder.layers.1.self_attn.k_proj.SU', 'vision_model.encoder.layers.1.self_attn.k_proj.SV', 'vision_model.encoder.layers.1.self_attn.k_proj.rcp', 'vision_model.encoder.layers.1.self_attn.k_proj.tlut', 'vision_model.encoder.layers.1.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.k_proj.trellis', 'vision_model.encoder.layers.1.self_attn.out_proj.SU', 'vision_model.encoder.layers.1.self_attn.out_proj.SV', 'vision_model.encoder.layers.1.self_attn.out_proj.rcp', 'vision_model.encoder.layers.1.self_attn.out_proj.tlut', 'vision_model.encoder.layers.1.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.out_proj.trellis', 'vision_model.encoder.layers.1.self_attn.q_proj.SU', 'vision_model.encoder.layers.1.self_attn.q_proj.SV', 'vision_model.encoder.layers.1.self_attn.q_proj.rcp', 'vision_model.encoder.layers.1.self_attn.q_proj.tlut', 'vision_model.encoder.layers.1.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.q_proj.trellis', 'vision_model.encoder.layers.1.self_attn.v_proj.SU', 'vision_model.encoder.layers.1.self_attn.v_proj.SV', 'vision_model.encoder.layers.1.self_attn.v_proj.rcp', 'vision_model.encoder.layers.1.self_attn.v_proj.tlut', 'vision_model.encoder.layers.1.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.v_proj.trellis', 'vision_model.encoder.layers.10.mlp.fc1.SU', 'vision_model.encoder.layers.10.mlp.fc1.SV', 'vision_model.encoder.layers.10.mlp.fc1.rcp', 'vision_model.encoder.layers.10.mlp.fc1.tlut', 'vision_model.encoder.layers.10.mlp.fc1.tp_rank', 'vision_model.encoder.layers.10.mlp.fc1.trellis', 'vision_model.encoder.layers.10.mlp.fc2.SU', 'vision_model.encoder.layers.10.mlp.fc2.SV', 'vision_model.encoder.layers.10.mlp.fc2.rcp', 'vision_model.encoder.layers.10.mlp.fc2.tlut', 'vision_model.encoder.layers.10.mlp.fc2.tp_rank', 'vision_model.encoder.layers.10.mlp.fc2.trellis', 'vision_model.encoder.layers.10.self_attn.k_proj.SU', 'vision_model.encoder.layers.10.self_attn.k_proj.SV', 'vision_model.encoder.layers.10.self_attn.k_proj.rcp', 'vision_model.encoder.layers.10.self_attn.k_proj.tlut', 'vision_model.encoder.layers.10.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.k_proj.trellis', 'vision_model.encoder.layers.10.self_attn.out_proj.SU', 'vision_model.encoder.layers.10.self_attn.out_proj.SV', 'vision_model.encoder.layers.10.self_attn.out_proj.rcp', 'vision_model.encoder.layers.10.self_attn.out_proj.tlut', 'vision_model.encoder.layers.10.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.out_proj.trellis', 'vision_model.encoder.layers.10.self_attn.q_proj.SU', 'vision_model.encoder.layers.10.self_attn.q_proj.SV', 'vision_model.encoder.layers.10.self_attn.q_proj.rcp', 'vision_model.encoder.layers.10.self_attn.q_proj.tlut', 'vision_model.encoder.layers.10.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.q_proj.trellis', 'vision_model.encoder.layers.10.self_attn.v_proj.SU', 'vision_model.encoder.layers.10.self_attn.v_proj.SV', 'vision_model.encoder.layers.10.self_attn.v_proj.rcp', 'vision_model.encoder.layers.10.self_attn.v_proj.tlut', 'vision_model.encoder.layers.10.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.v_proj.trellis', 'vision_model.encoder.layers.11.mlp.fc1.SU', 'vision_model.encoder.layers.11.mlp.fc1.SV', 'vision_model.encoder.layers.11.mlp.fc1.rcp', 'vision_model.encoder.layers.11.mlp.fc1.tlut', 'vision_model.encoder.layers.11.mlp.fc1.tp_rank', 'vision_model.encoder.layers.11.mlp.fc1.trellis', 'vision_model.encoder.layers.11.mlp.fc2.SU', 'vision_model.encoder.layers.11.mlp.fc2.SV', 'vision_model.encoder.layers.11.mlp.fc2.rcp', 'vision_model.encoder.layers.11.mlp.fc2.tlut', 'vision_model.encoder.layers.11.mlp.fc2.tp_rank', 'vision_model.encoder.layers.11.mlp.fc2.trellis', 'vision_model.encoder.layers.11.self_attn.k_proj.SU', 'vision_model.encoder.layers.11.self_attn.k_proj.SV', 'vision_model.encoder.layers.11.self_attn.k_proj.rcp', 'vision_model.encoder.layers.11.self_attn.k_proj.tlut', 'vision_model.encoder.layers.11.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.k_proj.trellis', 'vision_model.encoder.layers.11.self_attn.out_proj.SU', 'vision_model.encoder.layers.11.self_attn.out_proj.SV', 'vision_model.encoder.layers.11.self_attn.out_proj.rcp', 'vision_model.encoder.layers.11.self_attn.out_proj.tlut', 'vision_model.encoder.layers.11.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.out_proj.trellis', 'vision_model.encoder.layers.11.self_attn.q_proj.SU', 'vision_model.encoder.layers.11.self_attn.q_proj.SV', 'vision_model.encoder.layers.11.self_attn.q_proj.rcp', 'vision_model.encoder.layers.11.self_attn.q_proj.tlut', 'vision_model.encoder.layers.11.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.q_proj.trellis', 'vision_model.encoder.layers.11.self_attn.v_proj.SU', 'vision_model.encoder.layers.11.self_attn.v_proj.SV', 'vision_model.encoder.layers.11.self_attn.v_proj.rcp', 'vision_model.encoder.layers.11.self_attn.v_proj.tlut', 'vision_model.encoder.layers.11.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.v_proj.trellis', 'vision_model.encoder.layers.12.mlp.fc1.SU', 'vision_model.encoder.layers.12.mlp.fc1.SV', 'vision_model.encoder.layers.12.mlp.fc1.rcp', 'vision_model.encoder.layers.12.mlp.fc1.tlut', 'vision_model.encoder.layers.12.mlp.fc1.tp_rank', 'vision_model.encoder.layers.12.mlp.fc1.trellis', 'vision_model.encoder.layers.12.mlp.fc2.SU', 'vision_model.encoder.layers.12.mlp.fc2.SV', 'vision_model.encoder.layers.12.mlp.fc2.rcp', 'vision_model.encoder.layers.12.mlp.fc2.tlut', 'vision_model.encoder.layers.12.mlp.fc2.tp_rank', 'vision_model.encoder.layers.12.mlp.fc2.trellis', 'vision_model.encoder.layers.12.self_attn.k_proj.SU', 'vision_model.encoder.layers.12.self_attn.k_proj.SV', 'vision_model.encoder.layers.12.self_attn.k_proj.rcp', 'vision_model.encoder.layers.12.self_attn.k_proj.tlut', 'vision_model.encoder.layers.12.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.k_proj.trellis', 'vision_model.encoder.layers.12.self_attn.out_proj.SU', 'vision_model.encoder.layers.12.self_attn.out_proj.SV', 'vision_model.encoder.layers.12.self_attn.out_proj.rcp', 'vision_model.encoder.layers.12.self_attn.out_proj.tlut', 'vision_model.encoder.layers.12.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.out_proj.trellis', 'vision_model.encoder.layers.12.self_attn.q_proj.SU', 'vision_model.encoder.layers.12.self_attn.q_proj.SV', 'vision_model.encoder.layers.12.self_attn.q_proj.rcp', 'vision_model.encoder.layers.12.self_attn.q_proj.tlut', 'vision_model.encoder.layers.12.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.q_proj.trellis', 'vision_model.encoder.layers.12.self_attn.v_proj.SU', 'vision_model.encoder.layers.12.self_attn.v_proj.SV', 'vision_model.encoder.layers.12.self_attn.v_proj.rcp', 'vision_model.encoder.layers.12.self_attn.v_proj.tlut', 'vision_model.encoder.layers.12.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.v_proj.trellis', 'vision_model.encoder.layers.13.mlp.fc1.SU', 'vision_model.encoder.layers.13.mlp.fc1.SV', 'vision_model.encoder.layers.13.mlp.fc1.rcp', 'vision_model.encoder.layers.13.mlp.fc1.tlut', 'vision_model.encoder.layers.13.mlp.fc1.tp_rank', 'vision_model.encoder.layers.13.mlp.fc1.trellis', 'vision_model.encoder.layers.13.mlp.fc2.SU', 'vision_model.encoder.layers.13.mlp.fc2.SV', 'vision_model.encoder.layers.13.mlp.fc2.rcp', 'vision_model.encoder.layers.13.mlp.fc2.tlut', 'vision_model.encoder.layers.13.mlp.fc2.tp_rank', 'vision_model.encoder.layers.13.mlp.fc2.trellis', 'vision_model.encoder.layers.13.self_attn.k_proj.SU', 'vision_model.encoder.layers.13.self_attn.k_proj.SV', 'vision_model.encoder.layers.13.self_attn.k_proj.rcp', 'vision_model.encoder.layers.13.self_attn.k_proj.tlut', 'vision_model.encoder.layers.13.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.k_proj.trellis', 'vision_model.encoder.layers.13.self_attn.out_proj.SU', 'vision_model.encoder.layers.13.self_attn.out_proj.SV', 'vision_model.encoder.layers.13.self_attn.out_proj.rcp', 'vision_model.encoder.layers.13.self_attn.out_proj.tlut', 'vision_model.encoder.layers.13.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.out_proj.trellis', 'vision_model.encoder.layers.13.self_attn.q_proj.SU', 'vision_model.encoder.layers.13.self_attn.q_proj.SV', 'vision_model.encoder.layers.13.self_attn.q_proj.rcp', 'vision_model.encoder.layers.13.self_attn.q_proj.tlut', 'vision_model.encoder.layers.13.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.q_proj.trellis', 'vision_model.encoder.layers.13.self_attn.v_proj.SU', 'vision_model.encoder.layers.13.self_attn.v_proj.SV', 'vision_model.encoder.layers.13.self_attn.v_proj.rcp', 'vision_model.encoder.layers.13.self_attn.v_proj.tlut', 'vision_model.encoder.layers.13.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.v_proj.trellis', 'vision_model.encoder.layers.14.mlp.fc1.SU', 'vision_model.encoder.layers.14.mlp.fc1.SV', 'vision_model.encoder.layers.14.mlp.fc1.rcp', 'vision_model.encoder.layers.14.mlp.fc1.tlut', 'vision_model.encoder.layers.14.mlp.fc1.tp_rank', 'vision_model.encoder.layers.14.mlp.fc1.trellis', 'vision_model.encoder.layers.14.mlp.fc2.SU', 'vision_model.encoder.layers.14.mlp.fc2.SV', 'vision_model.encoder.layers.14.mlp.fc2.rcp', 'vision_model.encoder.layers.14.mlp.fc2.tlut', 'vision_model.encoder.layers.14.mlp.fc2.tp_rank', 'vision_model.encoder.layers.14.mlp.fc2.trellis', 'vision_model.encoder.layers.14.self_attn.k_proj.SU', 'vision_model.encoder.layers.14.self_attn.k_proj.SV', 'vision_model.encoder.layers.14.self_attn.k_proj.rcp', 'vision_model.encoder.layers.14.self_attn.k_proj.tlut', 'vision_model.encoder.layers.14.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.k_proj.trellis', 'vision_model.encoder.layers.14.self_attn.out_proj.SU', 'vision_model.encoder.layers.14.self_attn.out_proj.SV', 'vision_model.encoder.layers.14.self_attn.out_proj.rcp', 'vision_model.encoder.layers.14.self_attn.out_proj.tlut', 'vision_model.encoder.layers.14.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.out_proj.trellis', 'vision_model.encoder.layers.14.self_attn.q_proj.SU', 'vision_model.encoder.layers.14.self_attn.q_proj.SV', 'vision_model.encoder.layers.14.self_attn.q_proj.rcp', 'vision_model.encoder.layers.14.self_attn.q_proj.tlut', 'vision_model.encoder.layers.14.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.q_proj.trellis', 'vision_model.encoder.layers.14.self_attn.v_proj.SU', 'vision_model.encoder.layers.14.self_attn.v_proj.SV', 'vision_model.encoder.layers.14.self_attn.v_proj.rcp', 'vision_model.encoder.layers.14.self_attn.v_proj.tlut', 'vision_model.encoder.layers.14.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.v_proj.trellis', 'vision_model.encoder.layers.15.mlp.fc1.SU', 'vision_model.encoder.layers.15.mlp.fc1.SV', 'vision_model.encoder.layers.15.mlp.fc1.rcp', 'vision_model.encoder.layers.15.mlp.fc1.tlut', 'vision_model.encoder.layers.15.mlp.fc1.tp_rank', 'vision_model.encoder.layers.15.mlp.fc1.trellis', 'vision_model.encoder.layers.15.mlp.fc2.SU', 'vision_model.encoder.layers.15.mlp.fc2.SV', 'vision_model.encoder.layers.15.mlp.fc2.rcp', 'vision_model.encoder.layers.15.mlp.fc2.tlut', 'vision_model.encoder.layers.15.mlp.fc2.tp_rank', 'vision_model.encoder.layers.15.mlp.fc2.trellis', 'vision_model.encoder.layers.15.self_attn.k_proj.SU', 'vision_model.encoder.layers.15.self_attn.k_proj.SV', 'vision_model.encoder.layers.15.self_attn.k_proj.rcp', 'vision_model.encoder.layers.15.self_attn.k_proj.tlut', 'vision_model.encoder.layers.15.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.k_proj.trellis', 'vision_model.encoder.layers.15.self_attn.out_proj.SU', 'vision_model.encoder.layers.15.self_attn.out_proj.SV', 'vision_model.encoder.layers.15.self_attn.out_proj.rcp', 'vision_model.encoder.layers.15.self_attn.out_proj.tlut', 'vision_model.encoder.layers.15.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.out_proj.trellis', 'vision_model.encoder.layers.15.self_attn.q_proj.SU', 'vision_model.encoder.layers.15.self_attn.q_proj.SV', 'vision_model.encoder.layers.15.self_attn.q_proj.rcp', 'vision_model.encoder.layers.15.self_attn.q_proj.tlut', 'vision_model.encoder.layers.15.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.q_proj.trellis', 'vision_model.encoder.layers.15.self_attn.v_proj.SU', 'vision_model.encoder.layers.15.self_attn.v_proj.SV', 'vision_model.encoder.layers.15.self_attn.v_proj.rcp', 'vision_model.encoder.layers.15.self_attn.v_proj.tlut', 'vision_model.encoder.layers.15.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.v_proj.trellis', 'vision_model.encoder.layers.16.mlp.fc1.SU', 'vision_model.encoder.layers.16.mlp.fc1.SV', 'vision_model.encoder.layers.16.mlp.fc1.rcp', 'vision_model.encoder.layers.16.mlp.fc1.tlut', 'vision_model.encoder.layers.16.mlp.fc1.tp_rank', 'vision_model.encoder.layers.16.mlp.fc1.trellis', 'vision_model.encoder.layers.16.mlp.fc2.SU', 'vision_model.encoder.layers.16.mlp.fc2.SV', 'vision_model.encoder.layers.16.mlp.fc2.rcp', 'vision_model.encoder.layers.16.mlp.fc2.tlut', 'vision_model.encoder.layers.16.mlp.fc2.tp_rank', 'vision_model.encoder.layers.16.mlp.fc2.trellis', 'vision_model.encoder.layers.16.self_attn.k_proj.SU', 'vision_model.encoder.layers.16.self_attn.k_proj.SV', 'vision_model.encoder.layers.16.self_attn.k_proj.rcp', 'vision_model.encoder.layers.16.self_attn.k_proj.tlut', 'vision_model.encoder.layers.16.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.k_proj.trellis', 'vision_model.encoder.layers.16.self_attn.out_proj.SU', 'vision_model.encoder.layers.16.self_attn.out_proj.SV', 'vision_model.encoder.layers.16.self_attn.out_proj.rcp', 'vision_model.encoder.layers.16.self_attn.out_proj.tlut', 'vision_model.encoder.layers.16.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.out_proj.trellis', 'vision_model.encoder.layers.16.self_attn.q_proj.SU', 'vision_model.encoder.layers.16.self_attn.q_proj.SV', 'vision_model.encoder.layers.16.self_attn.q_proj.rcp', 'vision_model.encoder.layers.16.self_attn.q_proj.tlut', 'vision_model.encoder.layers.16.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.q_proj.trellis', 'vision_model.encoder.layers.16.self_attn.v_proj.SU', 'vision_model.encoder.layers.16.self_attn.v_proj.SV', 'vision_model.encoder.layers.16.self_attn.v_proj.rcp', 'vision_model.encoder.layers.16.self_attn.v_proj.tlut', 'vision_model.encoder.layers.16.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.v_proj.trellis', 'vision_model.encoder.layers.17.mlp.fc1.SU', 'vision_model.encoder.layers.17.mlp.fc1.SV', 'vision_model.encoder.layers.17.mlp.fc1.rcp', 'vision_model.encoder.layers.17.mlp.fc1.tlut', 'vision_model.encoder.layers.17.mlp.fc1.tp_rank', 'vision_model.encoder.layers.17.mlp.fc1.trellis', 'vision_model.encoder.layers.17.mlp.fc2.SU', 'vision_model.encoder.layers.17.mlp.fc2.SV', 'vision_model.encoder.layers.17.mlp.fc2.rcp', 'vision_model.encoder.layers.17.mlp.fc2.tlut', 'vision_model.encoder.layers.17.mlp.fc2.tp_rank', 'vision_model.encoder.layers.17.mlp.fc2.trellis', 'vision_model.encoder.layers.17.self_attn.k_proj.SU', 'vision_model.encoder.layers.17.self_attn.k_proj.SV', 'vision_model.encoder.layers.17.self_attn.k_proj.rcp', 'vision_model.encoder.layers.17.self_attn.k_proj.tlut', 'vision_model.encoder.layers.17.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.k_proj.trellis', 'vision_model.encoder.layers.17.self_attn.out_proj.SU', 'vision_model.encoder.layers.17.self_attn.out_proj.SV', 'vision_model.encoder.layers.17.self_attn.out_proj.rcp', 'vision_model.encoder.layers.17.self_attn.out_proj.tlut', 'vision_model.encoder.layers.17.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.out_proj.trellis', 'vision_model.encoder.layers.17.self_attn.q_proj.SU', 'vision_model.encoder.layers.17.self_attn.q_proj.SV', 'vision_model.encoder.layers.17.self_attn.q_proj.rcp', 'vision_model.encoder.layers.17.self_attn.q_proj.tlut', 'vision_model.encoder.layers.17.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.q_proj.trellis', 'vision_model.encoder.layers.17.self_attn.v_proj.SU', 'vision_model.encoder.layers.17.self_attn.v_proj.SV', 'vision_model.encoder.layers.17.self_attn.v_proj.rcp', 'vision_model.encoder.layers.17.self_attn.v_proj.tlut', 'vision_model.encoder.layers.17.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.v_proj.trellis', 'vision_model.encoder.layers.18.mlp.fc1.SU', 'vision_model.encoder.layers.18.mlp.fc1.SV', 'vision_model.encoder.layers.18.mlp.fc1.rcp', 'vision_model.encoder.layers.18.mlp.fc1.tlut', 'vision_model.encoder.layers.18.mlp.fc1.tp_rank', 'vision_model.encoder.layers.18.mlp.fc1.trellis', 'vision_model.encoder.layers.18.mlp.fc2.SU', 'vision_model.encoder.layers.18.mlp.fc2.SV', 'vision_model.encoder.layers.18.mlp.fc2.rcp', 'vision_model.encoder.layers.18.mlp.fc2.tlut', 'vision_model.encoder.layers.18.mlp.fc2.tp_rank', 'vision_model.encoder.layers.18.mlp.fc2.trellis', 'vision_model.encoder.layers.18.self_attn.k_proj.SU', 'vision_model.encoder.layers.18.self_attn.k_proj.SV', 'vision_model.encoder.layers.18.self_attn.k_proj.rcp', 'vision_model.encoder.layers.18.self_attn.k_proj.tlut', 'vision_model.encoder.layers.18.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.k_proj.trellis', 'vision_model.encoder.layers.18.self_attn.out_proj.SU', 'vision_model.encoder.layers.18.self_attn.out_proj.SV', 'vision_model.encoder.layers.18.self_attn.out_proj.rcp', 'vision_model.encoder.layers.18.self_attn.out_proj.tlut', 'vision_model.encoder.layers.18.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.out_proj.trellis', 'vision_model.encoder.layers.18.self_attn.q_proj.SU', 'vision_model.encoder.layers.18.self_attn.q_proj.SV', 'vision_model.encoder.layers.18.self_attn.q_proj.rcp', 'vision_model.encoder.layers.18.self_attn.q_proj.tlut', 'vision_model.encoder.layers.18.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.q_proj.trellis', 'vision_model.encoder.layers.18.self_attn.v_proj.SU', 'vision_model.encoder.layers.18.self_attn.v_proj.SV', 'vision_model.encoder.layers.18.self_attn.v_proj.rcp', 'vision_model.encoder.layers.18.self_attn.v_proj.tlut', 'vision_model.encoder.layers.18.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.v_proj.trellis', 'vision_model.encoder.layers.19.mlp.fc1.SU', 'vision_model.encoder.layers.19.mlp.fc1.SV', 'vision_model.encoder.layers.19.mlp.fc1.rcp', 'vision_model.encoder.layers.19.mlp.fc1.tlut', 'vision_model.encoder.layers.19.mlp.fc1.tp_rank', 'vision_model.encoder.layers.19.mlp.fc1.trellis', 'vision_model.encoder.layers.19.mlp.fc2.SU', 'vision_model.encoder.layers.19.mlp.fc2.SV', 'vision_model.encoder.layers.19.mlp.fc2.rcp', 'vision_model.encoder.layers.19.mlp.fc2.tlut', 'vision_model.encoder.layers.19.mlp.fc2.tp_rank', 'vision_model.encoder.layers.19.mlp.fc2.trellis', 'vision_model.encoder.layers.19.self_attn.k_proj.SU', 'vision_model.encoder.layers.19.self_attn.k_proj.SV', 'vision_model.encoder.layers.19.self_attn.k_proj.rcp', 'vision_model.encoder.layers.19.self_attn.k_proj.tlut', 'vision_model.encoder.layers.19.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.k_proj.trellis', 'vision_model.encoder.layers.19.self_attn.out_proj.SU', 'vision_model.encoder.layers.19.self_attn.out_proj.SV', 'vision_model.encoder.layers.19.self_attn.out_proj.rcp', 'vision_model.encoder.layers.19.self_attn.out_proj.tlut', 'vision_model.encoder.layers.19.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.out_proj.trellis', 'vision_model.encoder.layers.19.self_attn.q_proj.SU', 'vision_model.encoder.layers.19.self_attn.q_proj.SV', 'vision_model.encoder.layers.19.self_attn.q_proj.rcp', 'vision_model.encoder.layers.19.self_attn.q_proj.tlut', 'vision_model.encoder.layers.19.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.q_proj.trellis', 'vision_model.encoder.layers.19.self_attn.v_proj.SU', 'vision_model.encoder.layers.19.self_attn.v_proj.SV', 'vision_model.encoder.layers.19.self_attn.v_proj.rcp', 'vision_model.encoder.layers.19.self_attn.v_proj.tlut', 'vision_model.encoder.layers.19.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.v_proj.trellis', 'vision_model.encoder.layers.2.mlp.fc1.SU', 'vision_model.encoder.layers.2.mlp.fc1.SV', 'vision_model.encoder.layers.2.mlp.fc1.rcp', 'vision_model.encoder.layers.2.mlp.fc1.tlut', 'vision_model.encoder.layers.2.mlp.fc1.tp_rank', 'vision_model.encoder.layers.2.mlp.fc1.trellis', 'vision_model.encoder.layers.2.mlp.fc2.SU', 'vision_model.encoder.layers.2.mlp.fc2.SV', 'vision_model.encoder.layers.2.mlp.fc2.rcp', 'vision_model.encoder.layers.2.mlp.fc2.tlut', 'vision_model.encoder.layers.2.mlp.fc2.tp_rank', 'vision_model.encoder.layers.2.mlp.fc2.trellis', 'vision_model.encoder.layers.2.self_attn.k_proj.SU', 'vision_model.encoder.layers.2.self_attn.k_proj.SV', 'vision_model.encoder.layers.2.self_attn.k_proj.rcp', 'vision_model.encoder.layers.2.self_attn.k_proj.tlut', 'vision_model.encoder.layers.2.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.k_proj.trellis', 'vision_model.encoder.layers.2.self_attn.out_proj.SU', 'vision_model.encoder.layers.2.self_attn.out_proj.SV', 'vision_model.encoder.layers.2.self_attn.out_proj.rcp', 'vision_model.encoder.layers.2.self_attn.out_proj.tlut', 'vision_model.encoder.layers.2.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.out_proj.trellis', 'vision_model.encoder.layers.2.self_attn.q_proj.SU', 'vision_model.encoder.layers.2.self_attn.q_proj.SV', 'vision_model.encoder.layers.2.self_attn.q_proj.rcp', 'vision_model.encoder.layers.2.self_attn.q_proj.tlut', 'vision_model.encoder.layers.2.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.q_proj.trellis', 'vision_model.encoder.layers.2.self_attn.v_proj.SU', 'vision_model.encoder.layers.2.self_attn.v_proj.SV', 'vision_model.encoder.layers.2.self_attn.v_proj.rcp', 'vision_model.encoder.layers.2.self_attn.v_proj.tlut', 'vision_model.encoder.layers.2.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.v_proj.trellis', 'vision_model.encoder.layers.20.mlp.fc1.SU', 'vision_model.encoder.layers.20.mlp.fc1.SV', 'vision_model.encoder.layers.20.mlp.fc1.rcp', 'vision_model.encoder.layers.20.mlp.fc1.tlut', 'vision_model.encoder.layers.20.mlp.fc1.tp_rank', 'vision_model.encoder.layers.20.mlp.fc1.trellis', 'vision_model.encoder.layers.20.mlp.fc2.SU', 'vision_model.encoder.layers.20.mlp.fc2.SV', 'vision_model.encoder.layers.20.mlp.fc2.rcp', 'vision_model.encoder.layers.20.mlp.fc2.tlut', 'vision_model.encoder.layers.20.mlp.fc2.tp_rank', 'vision_model.encoder.layers.20.mlp.fc2.trellis', 'vision_model.encoder.layers.20.self_attn.k_proj.SU', 'vision_model.encoder.layers.20.self_attn.k_proj.SV', 'vision_model.encoder.layers.20.self_attn.k_proj.rcp', 'vision_model.encoder.layers.20.self_attn.k_proj.tlut', 'vision_model.encoder.layers.20.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.k_proj.trellis', 'vision_model.encoder.layers.20.self_attn.out_proj.SU', 'vision_model.encoder.layers.20.self_attn.out_proj.SV', 'vision_model.encoder.layers.20.self_attn.out_proj.rcp', 'vision_model.encoder.layers.20.self_attn.out_proj.tlut', 'vision_model.encoder.layers.20.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.out_proj.trellis', 'vision_model.encoder.layers.20.self_attn.q_proj.SU', 'vision_model.encoder.layers.20.self_attn.q_proj.SV', 'vision_model.encoder.layers.20.self_attn.q_proj.rcp', 'vision_model.encoder.layers.20.self_attn.q_proj.tlut', 'vision_model.encoder.layers.20.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.q_proj.trellis', 'vision_model.encoder.layers.20.self_attn.v_proj.SU', 'vision_model.encoder.layers.20.self_attn.v_proj.SV', 'vision_model.encoder.layers.20.self_attn.v_proj.rcp', 'vision_model.encoder.layers.20.self_attn.v_proj.tlut', 'vision_model.encoder.layers.20.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.v_proj.trellis', 'vision_model.encoder.layers.21.mlp.fc1.SU', 'vision_model.encoder.layers.21.mlp.fc1.SV', 'vision_model.encoder.layers.21.mlp.fc1.rcp', 'vision_model.encoder.layers.21.mlp.fc1.tlut', 'vision_model.encoder.layers.21.mlp.fc1.tp_rank', 'vision_model.encoder.layers.21.mlp.fc1.trellis', 'vision_model.encoder.layers.21.mlp.fc2.SU', 'vision_model.encoder.layers.21.mlp.fc2.SV', 'vision_model.encoder.layers.21.mlp.fc2.rcp', 'vision_model.encoder.layers.21.mlp.fc2.tlut', 'vision_model.encoder.layers.21.mlp.fc2.tp_rank', 'vision_model.encoder.layers.21.mlp.fc2.trellis', 'vision_model.encoder.layers.21.self_attn.k_proj.SU', 'vision_model.encoder.layers.21.self_attn.k_proj.SV', 'vision_model.encoder.layers.21.self_attn.k_proj.rcp', 'vision_model.encoder.layers.21.self_attn.k_proj.tlut', 'vision_model.encoder.layers.21.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.k_proj.trellis', 'vision_model.encoder.layers.21.self_attn.out_proj.SU', 'vision_model.encoder.layers.21.self_attn.out_proj.SV', 'vision_model.encoder.layers.21.self_attn.out_proj.rcp', 'vision_model.encoder.layers.21.self_attn.out_proj.tlut', 'vision_model.encoder.layers.21.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.out_proj.trellis', 'vision_model.encoder.layers.21.self_attn.q_proj.SU', 'vision_model.encoder.layers.21.self_attn.q_proj.SV', 'vision_model.encoder.layers.21.self_attn.q_proj.rcp', 'vision_model.encoder.layers.21.self_attn.q_proj.tlut', 'vision_model.encoder.layers.21.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.q_proj.trellis', 'vision_model.encoder.layers.21.self_attn.v_proj.SU', 'vision_model.encoder.layers.21.self_attn.v_proj.SV', 'vision_model.encoder.layers.21.self_attn.v_proj.rcp', 'vision_model.encoder.layers.21.self_attn.v_proj.tlut', 'vision_model.encoder.layers.21.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.v_proj.trellis', 'vision_model.encoder.layers.22.mlp.fc1.SU', 'vision_model.encoder.layers.22.mlp.fc1.SV', 'vision_model.encoder.layers.22.mlp.fc1.rcp', 'vision_model.encoder.layers.22.mlp.fc1.tlut', 'vision_model.encoder.layers.22.mlp.fc1.tp_rank', 'vision_model.encoder.layers.22.mlp.fc1.trellis', 'vision_model.encoder.layers.22.mlp.fc2.SU', 'vision_model.encoder.layers.22.mlp.fc2.SV', 'vision_model.encoder.layers.22.mlp.fc2.rcp', 'vision_model.encoder.layers.22.mlp.fc2.tlut', 'vision_model.encoder.layers.22.mlp.fc2.tp_rank', 'vision_model.encoder.layers.22.mlp.fc2.trellis', 'vision_model.encoder.layers.22.self_attn.k_proj.SU', 'vision_model.encoder.layers.22.self_attn.k_proj.SV', 'vision_model.encoder.layers.22.self_attn.k_proj.rcp', 'vision_model.encoder.layers.22.self_attn.k_proj.tlut', 'vision_model.encoder.layers.22.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.k_proj.trellis', 'vision_model.encoder.layers.22.self_attn.out_proj.SU', 'vision_model.encoder.layers.22.self_attn.out_proj.SV', 'vision_model.encoder.layers.22.self_attn.out_proj.rcp', 'vision_model.encoder.layers.22.self_attn.out_proj.tlut', 'vision_model.encoder.layers.22.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.out_proj.trellis', 'vision_model.encoder.layers.22.self_attn.q_proj.SU', 'vision_model.encoder.layers.22.self_attn.q_proj.SV', 'vision_model.encoder.layers.22.self_attn.q_proj.rcp', 'vision_model.encoder.layers.22.self_attn.q_proj.tlut', 'vision_model.encoder.layers.22.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.q_proj.trellis', 'vision_model.encoder.layers.22.self_attn.v_proj.SU', 'vision_model.encoder.layers.22.self_attn.v_proj.SV', 'vision_model.encoder.layers.22.self_attn.v_proj.rcp', 'vision_model.encoder.layers.22.self_attn.v_proj.tlut', 'vision_model.encoder.layers.22.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.v_proj.trellis', 'vision_model.encoder.layers.23.mlp.fc1.SU', 'vision_model.encoder.layers.23.mlp.fc1.SV', 'vision_model.encoder.layers.23.mlp.fc1.rcp', 'vision_model.encoder.layers.23.mlp.fc1.tlut', 'vision_model.encoder.layers.23.mlp.fc1.tp_rank', 'vision_model.encoder.layers.23.mlp.fc1.trellis', 'vision_model.encoder.layers.23.mlp.fc2.SU', 'vision_model.encoder.layers.23.mlp.fc2.SV', 'vision_model.encoder.layers.23.mlp.fc2.rcp', 'vision_model.encoder.layers.23.mlp.fc2.tlut', 'vision_model.encoder.layers.23.mlp.fc2.tp_rank', 'vision_model.encoder.layers.23.mlp.fc2.trellis', 'vision_model.encoder.layers.23.self_attn.k_proj.SU', 'vision_model.encoder.layers.23.self_attn.k_proj.SV', 'vision_model.encoder.layers.23.self_attn.k_proj.rcp', 'vision_model.encoder.layers.23.self_attn.k_proj.tlut', 'vision_model.encoder.layers.23.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.k_proj.trellis', 'vision_model.encoder.layers.23.self_attn.out_proj.SU', 'vision_model.encoder.layers.23.self_attn.out_proj.SV', 'vision_model.encoder.layers.23.self_attn.out_proj.rcp', 'vision_model.encoder.layers.23.self_attn.out_proj.tlut', 'vision_model.encoder.layers.23.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.out_proj.trellis', 'vision_model.encoder.layers.23.self_attn.q_proj.SU', 'vision_model.encoder.layers.23.self_attn.q_proj.SV', 'vision_model.encoder.layers.23.self_attn.q_proj.rcp', 'vision_model.encoder.layers.23.self_attn.q_proj.tlut', 'vision_model.encoder.layers.23.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.q_proj.trellis', 'vision_model.encoder.layers.23.self_attn.v_proj.SU', 'vision_model.encoder.layers.23.self_attn.v_proj.SV', 'vision_model.encoder.layers.23.self_attn.v_proj.rcp', 'vision_model.encoder.layers.23.self_attn.v_proj.tlut', 'vision_model.encoder.layers.23.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.v_proj.trellis', 'vision_model.encoder.layers.3.mlp.fc1.SU', 'vision_model.encoder.layers.3.mlp.fc1.SV', 'vision_model.encoder.layers.3.mlp.fc1.rcp', 'vision_model.encoder.layers.3.mlp.fc1.tlut', 'vision_model.encoder.layers.3.mlp.fc1.tp_rank', 'vision_model.encoder.layers.3.mlp.fc1.trellis', 'vision_model.encoder.layers.3.mlp.fc2.SU', 'vision_model.encoder.layers.3.mlp.fc2.SV', 'vision_model.encoder.layers.3.mlp.fc2.rcp', 'vision_model.encoder.layers.3.mlp.fc2.tlut', 'vision_model.encoder.layers.3.mlp.fc2.tp_rank', 'vision_model.encoder.layers.3.mlp.fc2.trellis', 'vision_model.encoder.layers.3.self_attn.k_proj.SU', 'vision_model.encoder.layers.3.self_attn.k_proj.SV', 'vision_model.encoder.layers.3.self_attn.k_proj.rcp', 'vision_model.encoder.layers.3.self_attn.k_proj.tlut', 'vision_model.encoder.layers.3.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.k_proj.trellis', 'vision_model.encoder.layers.3.self_attn.out_proj.SU', 'vision_model.encoder.layers.3.self_attn.out_proj.SV', 'vision_model.encoder.layers.3.self_attn.out_proj.rcp', 'vision_model.encoder.layers.3.self_attn.out_proj.tlut', 'vision_model.encoder.layers.3.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.out_proj.trellis', 'vision_model.encoder.layers.3.self_attn.q_proj.SU', 'vision_model.encoder.layers.3.self_attn.q_proj.SV', 'vision_model.encoder.layers.3.self_attn.q_proj.rcp', 'vision_model.encoder.layers.3.self_attn.q_proj.tlut', 'vision_model.encoder.layers.3.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.q_proj.trellis', 'vision_model.encoder.layers.3.self_attn.v_proj.SU', 'vision_model.encoder.layers.3.self_attn.v_proj.SV', 'vision_model.encoder.layers.3.self_attn.v_proj.rcp', 'vision_model.encoder.layers.3.self_attn.v_proj.tlut', 'vision_model.encoder.layers.3.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.v_proj.trellis', 'vision_model.encoder.layers.4.mlp.fc1.SU', 'vision_model.encoder.layers.4.mlp.fc1.SV', 'vision_model.encoder.layers.4.mlp.fc1.rcp', 'vision_model.encoder.layers.4.mlp.fc1.tlut', 'vision_model.encoder.layers.4.mlp.fc1.tp_rank', 'vision_model.encoder.layers.4.mlp.fc1.trellis', 'vision_model.encoder.layers.4.mlp.fc2.SU', 'vision_model.encoder.layers.4.mlp.fc2.SV', 'vision_model.encoder.layers.4.mlp.fc2.rcp', 'vision_model.encoder.layers.4.mlp.fc2.tlut', 'vision_model.encoder.layers.4.mlp.fc2.tp_rank', 'vision_model.encoder.layers.4.mlp.fc2.trellis', 'vision_model.encoder.layers.4.self_attn.k_proj.SU', 'vision_model.encoder.layers.4.self_attn.k_proj.SV', 'vision_model.encoder.layers.4.self_attn.k_proj.rcp', 'vision_model.encoder.layers.4.self_attn.k_proj.tlut', 'vision_model.encoder.layers.4.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.k_proj.trellis', 'vision_model.encoder.layers.4.self_attn.out_proj.SU', 'vision_model.encoder.layers.4.self_attn.out_proj.SV', 'vision_model.encoder.layers.4.self_attn.out_proj.rcp', 'vision_model.encoder.layers.4.self_attn.out_proj.tlut', 'vision_model.encoder.layers.4.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.out_proj.trellis', 'vision_model.encoder.layers.4.self_attn.q_proj.SU', 'vision_model.encoder.layers.4.self_attn.q_proj.SV', 'vision_model.encoder.layers.4.self_attn.q_proj.rcp', 'vision_model.encoder.layers.4.self_attn.q_proj.tlut', 'vision_model.encoder.layers.4.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.q_proj.trellis', 'vision_model.encoder.layers.4.self_attn.v_proj.SU', 'vision_model.encoder.layers.4.self_attn.v_proj.SV', 'vision_model.encoder.layers.4.self_attn.v_proj.rcp', 'vision_model.encoder.layers.4.self_attn.v_proj.tlut', 'vision_model.encoder.layers.4.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.v_proj.trellis', 'vision_model.encoder.layers.5.mlp.fc1.SU', 'vision_model.encoder.layers.5.mlp.fc1.SV', 'vision_model.encoder.layers.5.mlp.fc1.rcp', 'vision_model.encoder.layers.5.mlp.fc1.tlut', 'vision_model.encoder.layers.5.mlp.fc1.tp_rank', 'vision_model.encoder.layers.5.mlp.fc1.trellis', 'vision_model.encoder.layers.5.mlp.fc2.SU', 'vision_model.encoder.layers.5.mlp.fc2.SV', 'vision_model.encoder.layers.5.mlp.fc2.rcp', 'vision_model.encoder.layers.5.mlp.fc2.tlut', 'vision_model.encoder.layers.5.mlp.fc2.tp_rank', 'vision_model.encoder.layers.5.mlp.fc2.trellis', 'vision_model.encoder.layers.5.self_attn.k_proj.SU', 'vision_model.encoder.layers.5.self_attn.k_proj.SV', 'vision_model.encoder.layers.5.self_attn.k_proj.rcp', 'vision_model.encoder.layers.5.self_attn.k_proj.tlut', 'vision_model.encoder.layers.5.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.k_proj.trellis', 'vision_model.encoder.layers.5.self_attn.out_proj.SU', 'vision_model.encoder.layers.5.self_attn.out_proj.SV', 'vision_model.encoder.layers.5.self_attn.out_proj.rcp', 'vision_model.encoder.layers.5.self_attn.out_proj.tlut', 'vision_model.encoder.layers.5.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.out_proj.trellis', 'vision_model.encoder.layers.5.self_attn.q_proj.SU', 'vision_model.encoder.layers.5.self_attn.q_proj.SV', 'vision_model.encoder.layers.5.self_attn.q_proj.rcp', 'vision_model.encoder.layers.5.self_attn.q_proj.tlut', 'vision_model.encoder.layers.5.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.q_proj.trellis', 'vision_model.encoder.layers.5.self_attn.v_proj.SU', 'vision_model.encoder.layers.5.self_attn.v_proj.SV', 'vision_model.encoder.layers.5.self_attn.v_proj.rcp', 'vision_model.encoder.layers.5.self_attn.v_proj.tlut', 'vision_model.encoder.layers.5.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.v_proj.trellis', 'vision_model.encoder.layers.6.mlp.fc1.SU', 'vision_model.encoder.layers.6.mlp.fc1.SV', 'vision_model.encoder.layers.6.mlp.fc1.rcp', 'vision_model.encoder.layers.6.mlp.fc1.tlut', 'vision_model.encoder.layers.6.mlp.fc1.tp_rank', 'vision_model.encoder.layers.6.mlp.fc1.trellis', 'vision_model.encoder.layers.6.mlp.fc2.SU', 'vision_model.encoder.layers.6.mlp.fc2.SV', 'vision_model.encoder.layers.6.mlp.fc2.rcp', 'vision_model.encoder.layers.6.mlp.fc2.tlut', 'vision_model.encoder.layers.6.mlp.fc2.tp_rank', 'vision_model.encoder.layers.6.mlp.fc2.trellis', 'vision_model.encoder.layers.6.self_attn.k_proj.SU', 'vision_model.encoder.layers.6.self_attn.k_proj.SV', 'vision_model.encoder.layers.6.self_attn.k_proj.rcp', 'vision_model.encoder.layers.6.self_attn.k_proj.tlut', 'vision_model.encoder.layers.6.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.k_proj.trellis', 'vision_model.encoder.layers.6.self_attn.out_proj.SU', 'vision_model.encoder.layers.6.self_attn.out_proj.SV', 'vision_model.encoder.layers.6.self_attn.out_proj.rcp', 'vision_model.encoder.layers.6.self_attn.out_proj.tlut', 'vision_model.encoder.layers.6.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.out_proj.trellis', 'vision_model.encoder.layers.6.self_attn.q_proj.SU', 'vision_model.encoder.layers.6.self_attn.q_proj.SV', 'vision_model.encoder.layers.6.self_attn.q_proj.rcp', 'vision_model.encoder.layers.6.self_attn.q_proj.tlut', 'vision_model.encoder.layers.6.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.q_proj.trellis', 'vision_model.encoder.layers.6.self_attn.v_proj.SU', 'vision_model.encoder.layers.6.self_attn.v_proj.SV', 'vision_model.encoder.layers.6.self_attn.v_proj.rcp', 'vision_model.encoder.layers.6.self_attn.v_proj.tlut', 'vision_model.encoder.layers.6.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.v_proj.trellis', 'vision_model.encoder.layers.7.mlp.fc1.SU', 'vision_model.encoder.layers.7.mlp.fc1.SV', 'vision_model.encoder.layers.7.mlp.fc1.rcp', 'vision_model.encoder.layers.7.mlp.fc1.tlut', 'vision_model.encoder.layers.7.mlp.fc1.tp_rank', 'vision_model.encoder.layers.7.mlp.fc1.trellis', 'vision_model.encoder.layers.7.mlp.fc2.SU', 'vision_model.encoder.layers.7.mlp.fc2.SV', 'vision_model.encoder.layers.7.mlp.fc2.rcp', 'vision_model.encoder.layers.7.mlp.fc2.tlut', 'vision_model.encoder.layers.7.mlp.fc2.tp_rank', 'vision_model.encoder.layers.7.mlp.fc2.trellis', 'vision_model.encoder.layers.7.self_attn.k_proj.SU', 'vision_model.encoder.layers.7.self_attn.k_proj.SV', 'vision_model.encoder.layers.7.self_attn.k_proj.rcp', 'vision_model.encoder.layers.7.self_attn.k_proj.tlut', 'vision_model.encoder.layers.7.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.k_proj.trellis', 'vision_model.encoder.layers.7.self_attn.out_proj.SU', 'vision_model.encoder.layers.7.self_attn.out_proj.SV', 'vision_model.encoder.layers.7.self_attn.out_proj.rcp', 'vision_model.encoder.layers.7.self_attn.out_proj.tlut', 'vision_model.encoder.layers.7.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.out_proj.trellis', 'vision_model.encoder.layers.7.self_attn.q_proj.SU', 'vision_model.encoder.layers.7.self_attn.q_proj.SV', 'vision_model.encoder.layers.7.self_attn.q_proj.rcp', 'vision_model.encoder.layers.7.self_attn.q_proj.tlut', 'vision_model.encoder.layers.7.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.q_proj.trellis', 'vision_model.encoder.layers.7.self_attn.v_proj.SU', 'vision_model.encoder.layers.7.self_attn.v_proj.SV', 'vision_model.encoder.layers.7.self_attn.v_proj.rcp', 'vision_model.encoder.layers.7.self_attn.v_proj.tlut', 'vision_model.encoder.layers.7.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.v_proj.trellis', 'vision_model.encoder.layers.8.mlp.fc1.SU', 'vision_model.encoder.layers.8.mlp.fc1.SV', 'vision_model.encoder.layers.8.mlp.fc1.rcp', 'vision_model.encoder.layers.8.mlp.fc1.tlut', 'vision_model.encoder.layers.8.mlp.fc1.tp_rank', 'vision_model.encoder.layers.8.mlp.fc1.trellis', 'vision_model.encoder.layers.8.mlp.fc2.SU', 'vision_model.encoder.layers.8.mlp.fc2.SV', 'vision_model.encoder.layers.8.mlp.fc2.rcp', 'vision_model.encoder.layers.8.mlp.fc2.tlut', 'vision_model.encoder.layers.8.mlp.fc2.tp_rank', 'vision_model.encoder.layers.8.mlp.fc2.trellis', 'vision_model.encoder.layers.8.self_attn.k_proj.SU', 'vision_model.encoder.layers.8.self_attn.k_proj.SV', 'vision_model.encoder.layers.8.self_attn.k_proj.rcp', 'vision_model.encoder.layers.8.self_attn.k_proj.tlut', 'vision_model.encoder.layers.8.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.k_proj.trellis', 'vision_model.encoder.layers.8.self_attn.out_proj.SU', 'vision_model.encoder.layers.8.self_attn.out_proj.SV', 'vision_model.encoder.layers.8.self_attn.out_proj.rcp', 'vision_model.encoder.layers.8.self_attn.out_proj.tlut', 'vision_model.encoder.layers.8.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.out_proj.trellis', 'vision_model.encoder.layers.8.self_attn.q_proj.SU', 'vision_model.encoder.layers.8.self_attn.q_proj.SV', 'vision_model.encoder.layers.8.self_attn.q_proj.rcp', 'vision_model.encoder.layers.8.self_attn.q_proj.tlut', 'vision_model.encoder.layers.8.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.q_proj.trellis', 'vision_model.encoder.layers.8.self_attn.v_proj.SU', 'vision_model.encoder.layers.8.self_attn.v_proj.SV', 'vision_model.encoder.layers.8.self_attn.v_proj.rcp', 'vision_model.encoder.layers.8.self_attn.v_proj.tlut', 'vision_model.encoder.layers.8.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.v_proj.trellis', 'vision_model.encoder.layers.9.mlp.fc1.SU', 'vision_model.encoder.layers.9.mlp.fc1.SV', 'vision_model.encoder.layers.9.mlp.fc1.rcp', 'vision_model.encoder.layers.9.mlp.fc1.tlut', 'vision_model.encoder.layers.9.mlp.fc1.tp_rank', 'vision_model.encoder.layers.9.mlp.fc1.trellis', 'vision_model.encoder.layers.9.mlp.fc2.SU', 'vision_model.encoder.layers.9.mlp.fc2.SV', 'vision_model.encoder.layers.9.mlp.fc2.rcp', 'vision_model.encoder.layers.9.mlp.fc2.tlut', 'vision_model.encoder.layers.9.mlp.fc2.tp_rank', 'vision_model.encoder.layers.9.mlp.fc2.trellis', 'vision_model.encoder.layers.9.self_attn.k_proj.SU', 'vision_model.encoder.layers.9.self_attn.k_proj.SV', 'vision_model.encoder.layers.9.self_attn.k_proj.rcp', 'vision_model.encoder.layers.9.self_attn.k_proj.tlut', 'vision_model.encoder.layers.9.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.k_proj.trellis', 'vision_model.encoder.layers.9.self_attn.out_proj.SU', 'vision_model.encoder.layers.9.self_attn.out_proj.SV', 'vision_model.encoder.layers.9.self_attn.out_proj.rcp', 'vision_model.encoder.layers.9.self_attn.out_proj.tlut', 'vision_model.encoder.layers.9.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.out_proj.trellis', 'vision_model.encoder.layers.9.self_attn.q_proj.SU', 'vision_model.encoder.layers.9.self_attn.q_proj.SV', 'vision_model.encoder.layers.9.self_attn.q_proj.rcp', 'vision_model.encoder.layers.9.self_attn.q_proj.tlut', 'vision_model.encoder.layers.9.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.q_proj.trellis', 'vision_model.encoder.layers.9.self_attn.v_proj.SU', 'vision_model.encoder.layers.9.self_attn.v_proj.SV', 'vision_model.encoder.layers.9.self_attn.v_proj.rcp', 'vision_model.encoder.layers.9.self_attn.v_proj.tlut', 'vision_model.encoder.layers.9.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
I0416 08:19:08.877553 3248575 hfize_clip.py:65] Loading text layer 0
W0416 08:19:08.877814 3248575 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved = torch.load(f'{path_prefix}/{full_key}.pt', map_location='cpu')

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py", line 133, in <module>
    main(args)
  File "/workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py", line 80, in main
    load_clip_block('text', model.text_model.encoder.layers, orig_model.text_model.encoder.layers)
  File "/workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py", line 71, in load_clip_block
    load_proj_or_restore(layer.self_attn, 'q_proj', f'{prefix}_{i}', 'q', orig.self_attn, args.quantized_path, skip_list)
  File "/workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py", line 32, in load_proj_or_restore
    saved = torch.load(f'{path_prefix}/{full_key}.pt', map_location='cpu')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 1065, in load
    with _open_file_like(f, 'rb') as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 468, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 449, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '../hf_model_comp/qtip/ckpt/clip-vit-large-patch14_12bit/text_0_q.pt'
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../hf_model_comp/qtip/hf/clip-vit-large-patch14_12bit'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/qtip/eval/eval_clip_imagenet.py", line 267, in <module>
    main(args)
  File "/workspace/Weight_compression/qtip/eval/eval_clip_imagenet.py", line 176, in main
    model = model_from_hf_path_clip(args.hf_path).to('cuda')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/eval/eval_clip_imagenet.py", line 156, in model_from_hf_path_clip
    bad_config = transformers.AutoConfig.from_pretrained(path)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1006, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 570, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 629, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 469, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: '../hf_model_comp/qtip/hf/clip-vit-large-patch14_12bit'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
