I0318 06:28:31.069966 1091386 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:28:31.070103 1091386 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:28:31.070144 1091386 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:28:31.253944 1091386 config.py:58] PyTorch version 2.4.0 available.
W0318 06:28:33.414166 1091386 warnings.py:110] /workspace/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:01,  3.79it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  4.56it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  4.64it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  4.58it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:01<00:00,  4.54it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  4.58it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  4.53it/s]
I0318 06:28:35.180785 1091386 quantize_finetune_llama.py:135] loaded model
I0318 06:28:50.748515 1091386 quantize_finetune_llama.py:139] loaded dataset and devset
I0318 06:28:58.655384 1091386 quantize_finetune_llama.py:159] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0318 06:29:23.989260 1092738 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:29:23.989418 1092738 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:29:23.989461 1092738 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:29:24.173873 1092738 config.py:58] PyTorch version 2.4.0 available.
W0318 06:29:26.342919 1092738 warnings.py:110] /workspace/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:01,  4.13it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  4.49it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  4.65it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  4.80it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:01<00:00,  4.96it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  5.12it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  4.89it/s]
I0318 06:29:28.002656 1092738 quantize_finetune_llama.py:135] loaded model
I0318 06:29:43.671507 1092738 quantize_finetune_llama.py:139] loaded dataset and devset
I0318 06:29:49.743157 1092738 quantize_finetune_llama.py:159] layer 0 gpu 0
I0318 06:29:53.361104 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 0 in 3.4685873985290527s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0318 06:30:10.634495 1092738 quantize_finetune_llama.py:159] layer 1 gpu 1
I0318 06:30:12.564424 1093970 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:30:12.564536 1093970 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:30:12.564596 1093970 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:30:12.741083 1093970 config.py:58] PyTorch version 2.4.0 available.
I0318 06:30:14.007677 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 1 in 3.1672518253326416s
I0318 06:30:14.474979 1092738 quantize_finetune_llama.py:159] layer 2 gpu 2
I0318 06:30:14.912447 1093970 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0318 06:30:15.370347 1093970 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]I0318 06:30:16.414669 1094231 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:30:16.414771 1094231 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:30:16.414829 1094231 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:30:16.597688 1094231 config.py:58] PyTorch version 2.4.0 available.
I0318 06:30:17.946363 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 2 in 3.3326680660247803s
I0318 06:30:18.381063 1092738 quantize_finetune_llama.py:159] layer 3 gpu 3
  3%|▎         | 1/32 [00:02<01:06,  2.14s/it]I0318 06:30:18.723739 1094231 data_utils.py:336] using 256 training seqs, 128 validation seqs
  6%|▋         | 2/32 [00:02<00:32,  1.09s/it]W0318 06:30:19.109332 1094231 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:02<00:21,  1.34it/s] 12%|█▎        | 4/32 [00:03<00:16,  1.71it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.05it/s]  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s]I0318 06:30:20.295730 1094818 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:30:20.295840 1094818 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:30:20.295906 1094818 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:30:20.479115 1094818 config.py:58] PyTorch version 2.4.0 available.
 22%|██▏       | 7/32 [00:04<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:04<00:08,  2.68it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.80it/s]I0318 06:30:21.348324 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 3 in 2.82143497467041s
 31%|███▏      | 10/32 [00:05<00:07,  2.84it/s]I0318 06:30:21.816577 1092738 quantize_finetune_llama.py:159] layer 4 gpu 0
 34%|███▍      | 11/32 [00:05<00:07,  2.83it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.86it/s] 41%|████      | 13/32 [00:06<00:06,  2.90it/s]  3%|▎         | 1/32 [00:02<01:18,  2.54s/it]I0318 06:30:22.684935 1094818 data_utils.py:336] using 256 training seqs, 128 validation seqs
 44%|████▍     | 14/32 [00:06<00:06,  2.87it/s]  6%|▋         | 2/32 [00:02<00:37,  1.25s/it]W0318 06:30:23.076452 1094818 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 15/32 [00:06<00:05,  2.90it/s]  9%|▉         | 3/32 [00:03<00:24,  1.19it/s] 50%|█████     | 16/32 [00:07<00:05,  2.91it/s] 12%|█▎        | 4/32 [00:03<00:18,  1.55it/s]I0318 06:30:23.843517 1095379 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:30:23.843630 1095379 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:30:23.843689 1095379 utils.py:162] NumExpr defaulting to 16 threads.
 53%|█████▎    | 17/32 [00:07<00:05,  2.96it/s] 16%|█▌        | 5/32 [00:03<00:14,  1.88it/s]I0318 06:30:24.028088 1095379 config.py:58] PyTorch version 2.4.0 available.
  0%|          | 0/32 [00:00<?, ?it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.99it/s] 19%|█▉        | 6/32 [00:04<00:12,  2.15it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.96it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.34it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.95it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.48it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.00it/s] 28%|██▊       | 9/32 [00:05<00:08,  2.60it/s] 69%|██████▉   | 22/32 [00:09<00:03,  3.00it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.70it/s] 72%|███████▏  | 23/32 [00:09<00:02,  3.02it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.75it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.98it/s]  3%|▎         | 1/32 [00:02<01:12,  2.34s/it] 38%|███▊      | 12/32 [00:06<00:07,  2.80it/s] 78%|███████▊  | 25/32 [00:10<00:02,  3.00it/s]I0318 06:30:26.546087 1095379 data_utils.py:336] using 256 training seqs, 128 validation seqs
  6%|▋         | 2/32 [00:02<00:34,  1.16s/it] 41%|████      | 13/32 [00:06<00:06,  2.84it/s] 81%|████████▏ | 26/32 [00:10<00:01,  3.01it/s]W0318 06:30:27.055272 1095379 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:03<00:22,  1.26it/s] 44%|████▍     | 14/32 [00:07<00:06,  2.83it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.00it/s] 12%|█▎        | 4/32 [00:03<00:17,  1.62it/s] 47%|████▋     | 15/32 [00:07<00:06,  2.81it/s] 88%|████████▊ | 28/32 [00:11<00:01,  3.01it/s] 16%|█▌        | 5/32 [00:03<00:13,  1.93it/s] 50%|█████     | 16/32 [00:07<00:05,  2.86it/s] 91%|█████████ | 29/32 [00:11<00:00,  3.04it/s] 19%|█▉        | 6/32 [00:04<00:11,  2.18it/s] 53%|█████▎    | 17/32 [00:08<00:05,  2.88it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.05it/s]  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.39it/s] 97%|█████████▋| 31/32 [00:12<00:00,  3.08it/s] 56%|█████▋    | 18/32 [00:08<00:04,  2.90it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.54it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.93it/s]100%|██████████| 32/32 [00:12<00:00,  3.02it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
 28%|██▊       | 9/32 [00:05<00:08,  2.63it/s] 62%|██████▎   | 20/32 [00:09<00:04,  2.91it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.67it/s] 66%|██████▌   | 21/32 [00:09<00:03,  2.86it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.74it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.88it/s] 38%|███▊      | 12/32 [00:06<00:07,  2.78it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.90it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.93it/s] 41%|████      | 13/32 [00:06<00:06,  2.82it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.96it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.83it/s]  3%|▎         | 1/32 [00:02<01:22,  2.68s/it] 81%|████████▏ | 26/32 [00:11<00:02,  2.98it/s] 47%|████▋     | 15/32 [00:07<00:05,  2.85it/s]  6%|▋         | 2/32 [00:03<00:39,  1.30s/it] 84%|████████▍ | 27/32 [00:11<00:01,  2.98it/s] 50%|█████     | 16/32 [00:07<00:05,  2.87it/s]  9%|▉         | 3/32 [00:03<00:25,  1.16it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.98it/s]W0318 06:30:31.899000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:31.899000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:30:31.899000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:31.899000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:31.899000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:31.899000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:30:31.899000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:31.925000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:31.925000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:31.925000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:31.925000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:31.925000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:31.941000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:31.941000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:31.941000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:31.941000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:31.941000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:07<00:05,  2.88it/s] 12%|█▎        | 4/32 [00:03<00:18,  1.52it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.99it/s]W0318 06:30:32.249000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:32.249000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:32.249000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:32.249000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:32.249000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:08<00:04,  2.89it/s] 16%|█▌        | 5/32 [00:04<00:14,  1.84it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.99it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.89it/s] 19%|█▉        | 6/32 [00:04<00:12,  2.08it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.98it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.88it/s]W0318 06:30:33.097000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:33.097000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:30:33.097000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:33.097000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:33.097000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:33.097000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:33.097000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:30:33.114000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:33.114000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:33.114000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:33.114000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:33.114000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:04<00:10,  2.28it/s]100%|██████████| 32/32 [00:13<00:00,  2.98it/s]100%|██████████| 32/32 [00:13<00:00,  2.44it/s]
 66%|██████▌   | 21/32 [00:09<00:03,  2.88it/s]W0318 06:30:33.344000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:33.344000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:33.344000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:33.344000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:33.344000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:05<00:09,  2.44it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.80it/s] 28%|██▊       | 9/32 [00:05<00:08,  2.57it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.80it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.68it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.81it/s] 34%|███▍      | 11/32 [00:06<00:07,  2.76it/s]W0318 06:30:34.524000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:34.525000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:34.525000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:34.525000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:34.525000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:30:34.525000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:30:34.525000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:34.543000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:34.543000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:34.543000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:34.543000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:30:34.543000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:10<00:02,  2.83it/s] 38%|███▊      | 12/32 [00:06<00:07,  2.82it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.87it/s] 41%|████      | 13/32 [00:06<00:06,  2.82it/s]W0318 06:30:35.411000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:35.411000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:35.411000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:35.411000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:30:35.411000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:11<00:01,  2.90it/s] 44%|████▍     | 14/32 [00:07<00:06,  2.83it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.89it/s] 47%|████▋     | 15/32 [00:07<00:05,  2.84it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.90it/s] 50%|█████     | 16/32 [00:07<00:05,  2.86it/s]W0318 06:30:36.374000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.374000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.374000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.375000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.375000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0318 06:30:36.375000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.375000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.400000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.401000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.401000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.401000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.401000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.416000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.417000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.417000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.417000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.417000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:12<00:00,  2.92it/s] 53%|█████▎    | 17/32 [00:08<00:05,  2.89it/s]W0318 06:30:36.737000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.737000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.737000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.737000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:36.737000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:12<00:00,  2.94it/s] 56%|█████▋    | 18/32 [00:08<00:04,  2.90it/s]100%|██████████| 32/32 [00:13<00:00,  2.89it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
 59%|█████▉    | 19/32 [00:08<00:04,  2.86it/s] 62%|██████▎   | 20/32 [00:09<00:04,  2.82it/s]W0318 06:30:37.661000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:37.661000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:30:37.661000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:37.661000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:37.662000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:30:37.662000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:37.662000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:37.680000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:37.680000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:37.680000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:37.680000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:37.681000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:37.933000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:37.933000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:37.933000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:37.933000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:37.933000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:09<00:03,  2.81it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.80it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.79it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.79it/s]W0318 06:30:39.163000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:30:39.163000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:39.163000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:39.163000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:30:39.163000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:39.163000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:39.163000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:39.182000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:39.182000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:30:39.182000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:39.182000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:39.182000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:11<00:02,  2.79it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.78it/s]W0318 06:30:40.132000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.132000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.132000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.132000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.132000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:11<00:01,  2.78it/s]W0318 06:30:40.478000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.478000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.478000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.478000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.478000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.479000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.479000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.506000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.506000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.506000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.506000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.506000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:12<00:01,  2.78it/s]W0318 06:30:40.523000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.523000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.523000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.523000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.523000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0318 06:30:40.858000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.858000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.858000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.858000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:40.858000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:12<00:01,  2.76it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.77it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.76it/s]I0318 06:30:41.752755 1093970 finetune.py:45] layer 0_v initial loss 0.000506094831507653
W0318 06:30:41.753115 1093970 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:30:41.776000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:41.776000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:30:41.777000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:41.777000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:41.777000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:41.777000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:41.777000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:30:41.795000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:41.795000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:41.796000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:41.796000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:41.796000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:13<00:00,  2.77it/s]100%|██████████| 32/32 [00:13<00:00,  2.35it/s]
W0318 06:30:42.045000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:42.045000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:42.045000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:42.045000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:42.046000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:42.713764 1093970 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:30:43.267000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:30:43.267000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:43.267000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:43.267000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:43.267000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:43.267000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:43.267000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:30:43.285000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:43.285000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:43.286000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:43.286000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:43.286000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
0_v proxy err 0.0011872358154505491 tr(WHW.T) 4.225186347961426
  0%|          | 0/32 [00:00<?, ?it/s]W0318 06:30:44.420000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:44.420000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:44.420000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:44.420000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:44.420000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:25,  1.22it/s]  6%|▋         | 2/32 [00:01<00:16,  1.85it/s]  9%|▉         | 3/32 [00:01<00:13,  2.21it/s]W0318 06:30:45.471000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.471000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.471000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.471000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.471000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.471000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.471000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.498000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.498000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.498000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.499000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.499000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.516000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.516000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.516000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.516000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.516000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:01<00:11,  2.44it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0318 06:30:45.855000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.855000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.855000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.855000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:45.855000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:10,  2.59it/s]I0318 06:30:46.055939 1094231 finetune.py:45] layer 1_v initial loss 0.0016079636989161372
W0318 06:30:46.056291 1094231 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:02<00:09,  2.70it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.76it/s]W0318 06:30:46.772000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:46.772000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:46.772000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:46.772000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:30:46.772000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:46.772000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:46.772000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:30:46.791000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:46.791000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:46.791000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:46.791000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:46.791000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:47.040000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:47.040000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:47.041000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:47.041000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:47.041000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:08,  2.80it/s]W0318 06:30:47.138047 1094231 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:03<00:08,  2.82it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.85it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.86it/s]1_v proxy err 0.003002993995323777 tr(WHW.T) 16.465883255004883
  0%|          | 0/32 [00:00<?, ?it/s]W0318 06:30:48.253000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:48.253000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:30:48.253000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:48.253000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:30:48.253000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:48.253000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:30:48.253000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:30:48.272000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:48.272000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:48.272000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:48.272000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:30:48.272000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:04<00:06,  2.87it/s] 41%|████      | 13/32 [00:04<00:06,  2.87it/s]  3%|▎         | 1/32 [00:00<00:19,  1.55it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.88it/s]W0318 06:30:49.216000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:30:49.217000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:30:49.217000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:30:49.217000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:30:49.217000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:00<00:14,  2.11it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.89it/s]  9%|▉         | 3/32 [00:01<00:12,  2.38it/s] 50%|█████     | 16/32 [00:05<00:05,  2.90it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.52it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.90it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.62it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.90it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 19%|█▉        | 6/32 [00:02<00:09,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.90it/s]I0318 06:30:50.968177 1094818 finetune.py:45] layer 2_v initial loss 0.0025653247721493244
W0318 06:30:50.969164 1094818 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:02<00:09,  2.72it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.92it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.92it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.74it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.92it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.75it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.92it/s]W0318 06:30:52.349181 1094818 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:04<00:07,  2.76it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.92it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.76it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.91it/s] 41%|████      | 13/32 [00:04<00:06,  2.77it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.89it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.78it/s]2_v proxy err 0.004947145003825426 tr(WHW.T) 136.67333984375
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.89it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.78it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s]  3%|▎         | 1/32 [00:00<00:19,  1.58it/s] 50%|█████     | 16/32 [00:06<00:05,  2.80it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.89it/s]  6%|▋         | 2/32 [00:00<00:14,  2.14it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.80it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.89it/s]  9%|▉         | 3/32 [00:01<00:11,  2.42it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.81it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.89it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.58it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.81it/s]100%|██████████| 32/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.78it/s]
 16%|█▌        | 5/32 [00:02<00:10,  2.69it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.80it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.76it/s]I0318 06:30:55.926476 1095379 finetune.py:45] layer 3_v initial loss 0.006608595140278339
W0318 06:30:55.926835 1095379 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 66%|██████▌   | 21/32 [00:07<00:03,  2.80it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.82it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.81it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.85it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.85it/s]W0318 06:30:56.956995 1095379 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:08<00:02,  2.82it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.86it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.89it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.81it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.89it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.82it/s] 41%|████      | 13/32 [00:04<00:06,  2.89it/s]3_v proxy err 0.00750023452565074 tr(WHW.T) 284.77557373046875
  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.82it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.90it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.82it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.89it/s]  3%|▎         | 1/32 [00:00<00:19,  1.57it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.84it/s] 50%|█████     | 16/32 [00:05<00:05,  2.89it/s]  6%|▋         | 2/32 [00:00<00:14,  2.12it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.84it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.90it/s]  9%|▉         | 3/32 [00:01<00:12,  2.41it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
 56%|█████▋    | 18/32 [00:06<00:04,  2.88it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.59it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.88it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.70it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.89it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.78it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.90it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.83it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.90it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.83it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.92it/s]I0318 06:31:01.833615 1093970 finetune.py:45] layer 0_q initial loss 0.0005060890689492226
W0318 06:31:01.833963 1093970 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:03<00:08,  2.82it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.93it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.87it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.93it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.90it/s]W0318 06:31:02.706102 1093970 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 81%|████████▏ | 26/32 [00:09<00:02,  2.92it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.94it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.93it/s] 41%|████      | 13/32 [00:04<00:06,  2.96it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.91it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.94it/s]0_q proxy err 1.3956185284769163e-05 tr(WHW.T) 2710.363037109375
  0%|          | 0/32 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.92it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.95it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.92it/s] 50%|█████     | 16/32 [00:05<00:05,  2.91it/s]  3%|▎         | 1/32 [00:00<00:18,  1.68it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.91it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s]  6%|▋         | 2/32 [00:00<00:13,  2.27it/s]100%|██████████| 32/32 [00:11<00:00,  2.92it/s]100%|██████████| 32/32 [00:11<00:00,  2.83it/s]
 56%|█████▋    | 18/32 [00:06<00:04,  2.93it/s]  9%|▉         | 3/32 [00:01<00:11,  2.55it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.95it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.68it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.93it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.76it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.93it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.83it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.93it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.85it/s]I0318 06:31:06.611121 1094231 finetune.py:45] layer 1_q initial loss 0.001607012702152133
W0318 06:31:06.611422 1094231 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 72%|███████▏  | 23/32 [00:08<00:03,  2.93it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.90it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.94it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.96it/s] 31%|███▏      | 10/32 [00:03<00:07,  3.00it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.89it/s]W0318 06:31:07.542762 1094231 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:03<00:06,  3.02it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.88it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.02it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.87it/s] 41%|████      | 13/32 [00:04<00:06,  3.06it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.88it/s]1_q proxy err 7.281870057340711e-05 tr(WHW.T) 4778.43994140625
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.10it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.89it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.11it/s]  3%|▎         | 1/32 [00:00<00:17,  1.74it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.92it/s] 50%|█████     | 16/32 [00:05<00:05,  3.13it/s]  6%|▋         | 2/32 [00:00<00:12,  2.33it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.93it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.14it/s]  9%|▉         | 3/32 [00:01<00:11,  2.59it/s]100%|██████████| 32/32 [00:11<00:00,  2.90it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]
 56%|█████▋    | 18/32 [00:06<00:04,  3.14it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.75it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.14it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.79it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.09it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.81it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.08it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.88it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.11it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.93it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.13it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.95it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.12it/s]I0318 06:31:12.004221 1094818 finetune.py:45] layer 2_q initial loss 0.0025615564081817865
W0318 06:31:12.004449 1094818 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:03<00:07,  2.91it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.08it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.93it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.10it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.12it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.95it/s]W0318 06:31:13.100410 1094818 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 88%|████████▊ | 28/32 [00:09<00:01,  3.10it/s] 41%|████      | 13/32 [00:04<00:06,  2.94it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.05it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.90it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.02it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.89it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.01it/s]2_q proxy err 0.0003024728794116527 tr(WHW.T) 7752.8515625
  0%|          | 0/32 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:05<00:05,  2.87it/s]100%|██████████| 32/32 [00:10<00:00,  3.01it/s]100%|██████████| 32/32 [00:10<00:00,  2.98it/s]
 53%|█████▎    | 17/32 [00:06<00:05,  2.87it/s]  3%|▎         | 1/32 [00:00<00:18,  1.67it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.88it/s]  6%|▋         | 2/32 [00:00<00:13,  2.23it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.85it/s]  9%|▉         | 3/32 [00:01<00:11,  2.48it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.88it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.61it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.88it/s] 16%|█▌        | 5/32 [00:01<00:10,  2.69it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.91it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.73it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.91it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.75it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.88it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.79it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.86it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.82it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.85it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.84it/s]I0318 06:31:17.999204 1095379 finetune.py:45] layer 3_q initial loss 0.006590630859136581
W0318 06:31:17.999686 1095379 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 84%|████████▍ | 27/32 [00:09<00:01,  2.84it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.87it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.84it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.87it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.82it/s] 41%|████      | 13/32 [00:04<00:06,  2.87it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.84it/s]W0318 06:31:19.150541 1095379 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:05<00:06,  2.87it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.83it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]
 50%|█████     | 16/32 [00:05<00:05,  2.89it/s]3_q proxy err 0.0010207174345850945 tr(WHW.T) 7217.63720703125
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.89it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.91it/s]  3%|▎         | 1/32 [00:00<00:18,  1.70it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.89it/s]  6%|▋         | 2/32 [00:00<00:13,  2.24it/s]I0318 06:31:21.205589 1093970 finetune.py:45] layer 0_k initial loss 0.0005061077536083758
W0318 06:31:21.205933 1093970 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 20/32 [00:07<00:04,  2.91it/s]  9%|▉         | 3/32 [00:01<00:11,  2.52it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.91it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.65it/s]W0318 06:31:22.057715 1093970 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:07<00:03,  2.90it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.73it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.90it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.78it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.91it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.82it/s]0_k proxy err 6.482106982730329e-06 tr(WHW.T) 1698.7349853515625
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.89it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.83it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.91it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.86it/s]  3%|▎         | 1/32 [00:00<00:18,  1.71it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.92it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.88it/s]  6%|▋         | 2/32 [00:00<00:13,  2.27it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.93it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.89it/s]  9%|▉         | 3/32 [00:01<00:11,  2.53it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.93it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.91it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.69it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.92it/s] 41%|████      | 13/32 [00:04<00:06,  2.92it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.77it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.89it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.92it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.82it/s]100%|██████████| 32/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.82it/s]
 47%|████▋     | 15/32 [00:05<00:05,  2.93it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.86it/s] 50%|█████     | 16/32 [00:05<00:05,  2.92it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.88it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.89it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.89it/s]I0318 06:31:26.583091 1094231 finetune.py:45] layer 1_k initial loss 0.0016048494726419449
W0318 06:31:26.583338 1094231 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▋    | 18/32 [00:06<00:04,  2.89it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.93it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.87it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.96it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.88it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.01it/s]W0318 06:31:27.557624 1094231 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 66%|██████▌   | 21/32 [00:07<00:03,  2.86it/s] 41%|████      | 13/32 [00:04<00:06,  3.03it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.86it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.04it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.07it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.81it/s]1_k proxy err 6.989316898398101e-05 tr(WHW.T) 4995.3916015625
  0%|          | 0/32 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:05<00:05,  3.09it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.80it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.10it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.81it/s]  3%|▎         | 1/32 [00:00<00:17,  1.76it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.12it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.83it/s]  6%|▋         | 2/32 [00:00<00:12,  2.34it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.13it/s]  9%|▉         | 3/32 [00:01<00:11,  2.62it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.84it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.14it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.76it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.83it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.15it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.85it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.83it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.15it/s] 19%|█▉        | 6/32 [00:02<00:08,  2.92it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.83it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.16it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.96it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.17it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.84it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.99it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.17it/s]100%|██████████| 32/32 [00:11<00:00,  2.83it/s]100%|██████████| 32/32 [00:11<00:00,  2.81it/s]
 28%|██▊       | 9/32 [00:03<00:07,  3.00it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.17it/s] 31%|███▏      | 10/32 [00:03<00:07,  3.00it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.15it/s]I0318 06:31:32.387524 1094818 finetune.py:45] layer 2_k initial loss 0.0025589114520698786
W0318 06:31:32.387719 1094818 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:03<00:06,  3.01it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.15it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.02it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.17it/s] 41%|████      | 13/32 [00:04<00:06,  3.01it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.15it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.02it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.16it/s]W0318 06:31:33.453946 1094818 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:10<00:00,  3.16it/s]100%|██████████| 32/32 [00:10<00:00,  3.01it/s]
 47%|████▋     | 15/32 [00:05<00:05,  3.01it/s] 50%|█████     | 16/32 [00:05<00:05,  2.99it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.95it/s]2_k proxy err 0.00025534862652421 tr(WHW.T) 10205.837890625
  0%|          | 0/32 [00:00<?, ?it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.93it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.93it/s]  3%|▎         | 1/32 [00:00<00:19,  1.61it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.95it/s]  6%|▋         | 2/32 [00:00<00:13,  2.15it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.92it/s]  9%|▉         | 3/32 [00:01<00:12,  2.40it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.89it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.54it/s] 72%|███████▏  | 23/32 [00:07<00:03,  2.87it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.62it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.87it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.67it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.86it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.71it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.87it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.85it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.76it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.85it/s]I0318 06:31:38.294935 1095379 finetune.py:45] layer 3_k initial loss 0.006584216840565205
W0318 06:31:38.295466 1095379 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:03<00:07,  2.79it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.84it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.85it/s]W0318 06:31:39.157890 1095379 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 38%|███▊      | 12/32 [00:04<00:06,  2.86it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.84it/s] 41%|████      | 13/32 [00:04<00:06,  2.85it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.88it/s]
 44%|████▍     | 14/32 [00:05<00:06,  2.87it/s]3_k proxy err 0.0007924610981717706 tr(WHW.T) 10074.73828125
  0%|          | 0/32 [00:00<?, ?it/s]I0318 06:31:40.192341 1093970 finetune.py:45] layer 0_o initial loss 0.0005059360410086811
W0318 06:31:40.192690 1093970 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 47%|████▋     | 15/32 [00:05<00:05,  2.88it/s] 50%|█████     | 16/32 [00:05<00:05,  2.89it/s]  3%|▎         | 1/32 [00:00<00:18,  1.70it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.90it/s]W0318 06:31:41.001703 1093970 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:00<00:13,  2.26it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.91it/s]  9%|▉         | 3/32 [00:01<00:11,  2.52it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.93it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.66it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.92it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.75it/s]0_o proxy err 3.563765494618565e-05 tr(WHW.T) 0.9668056964874268
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.93it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.81it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.91it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.84it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.92it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.88it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.93it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.91it/s]  3%|▎         | 1/32 [00:01<00:40,  1.29s/it] 78%|███████▊  | 25/32 [00:08<00:02,  2.94it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.93it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.95it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.94it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.96it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.95it/s]  6%|▋         | 2/32 [00:02<00:33,  1.13s/it] 88%|████████▊ | 28/32 [00:09<00:01,  2.95it/s] 41%|████      | 13/32 [00:04<00:06,  2.93it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.93it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.94it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.94it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.94it/s]  9%|▉         | 3/32 [00:03<00:31,  1.07s/it] 97%|█████████▋| 31/32 [00:11<00:00,  2.94it/s] 50%|█████     | 16/32 [00:05<00:05,  2.94it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.82it/s]
 53%|█████▎    | 17/32 [00:06<00:05,  2.93it/s]I0318 06:31:46.315131 1094231 finetune.py:45] layer 1_o initial loss 0.0015978500014171004
W0318 06:31:46.315438 1094231 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▋    | 18/32 [00:06<00:04,  2.91it/s] 12%|█▎        | 4/32 [00:04<00:29,  1.04s/it] 59%|█████▉    | 19/32 [00:06<00:04,  2.88it/s]W0318 06:31:47.209500 1094231 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 62%|██████▎   | 20/32 [00:07<00:04,  2.87it/s] 16%|█▌        | 5/32 [00:05<00:27,  1.02s/it] 66%|██████▌   | 21/32 [00:07<00:03,  2.89it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.89it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.87it/s] 19%|█▉        | 6/32 [00:06<00:25,  1.00it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.86it/s]1_o proxy err 0.0007702442817389965 tr(WHW.T) 1.111581563949585
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.83it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.83it/s] 22%|██▏       | 7/32 [00:07<00:24,  1.02it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.84it/s]  3%|▎         | 1/32 [00:01<00:38,  1.25s/it] 88%|████████▊ | 28/32 [00:09<00:01,  2.85it/s] 25%|██▌       | 8/32 [00:08<00:23,  1.03it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.85it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.85it/s]  6%|▋         | 2/32 [00:02<00:32,  1.10s/it] 97%|█████████▋| 31/32 [00:10<00:00,  2.84it/s] 28%|██▊       | 9/32 [00:09<00:22,  1.04it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.83it/s]
  9%|▉         | 3/32 [00:03<00:30,  1.05s/it] 31%|███▏      | 10/32 [00:10<00:20,  1.05it/s] 12%|█▎        | 4/32 [00:04<00:28,  1.02s/it]I0318 06:31:52.913261 1094818 finetune.py:45] layer 2_o initial loss 0.0025798145215958357
W0318 06:31:52.913490 1094818 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:10<00:19,  1.05it/s] 16%|█▌        | 5/32 [00:05<00:27,  1.00s/it]W0318 06:31:54.033567 1094818 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 38%|███▊      | 12/32 [00:11<00:18,  1.06it/s] 19%|█▉        | 6/32 [00:06<00:25,  1.01it/s] 41%|████      | 13/32 [00:12<00:17,  1.06it/s]2_o proxy err 0.00535249151289463 tr(WHW.T) 1.4603197574615479
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:07<00:24,  1.01it/s] 44%|████▍     | 14/32 [00:13<00:16,  1.06it/s] 25%|██▌       | 8/32 [00:08<00:23,  1.02it/s]  3%|▎         | 1/32 [00:01<00:42,  1.37s/it] 47%|████▋     | 15/32 [00:14<00:16,  1.06it/s] 28%|██▊       | 9/32 [00:09<00:22,  1.02it/s] 50%|█████     | 16/32 [00:15<00:15,  1.06it/s]  6%|▋         | 2/32 [00:02<00:35,  1.19s/it]I0318 06:31:57.994025 1095379 finetune.py:45] layer 3_o initial loss 0.006517550442367792
W0318 06:31:57.994421 1095379 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:10<00:21,  1.02it/s] 53%|█████▎    | 17/32 [00:16<00:14,  1.06it/s]W0318 06:31:58.862016 1095379 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:03<00:32,  1.12s/it] 34%|███▍      | 11/32 [00:11<00:20,  1.02it/s] 56%|█████▋    | 18/32 [00:17<00:13,  1.06it/s] 12%|█▎        | 4/32 [00:04<00:30,  1.09s/it]3_o proxy err 0.004967038985341787 tr(WHW.T) 3.3527450561523438
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:12<00:19,  1.02it/s] 59%|█████▉    | 19/32 [00:18<00:12,  1.06it/s] 16%|█▌        | 5/32 [00:05<00:28,  1.07s/it]  3%|▎         | 1/32 [00:01<00:40,  1.30s/it] 41%|████      | 13/32 [00:13<00:18,  1.01it/s] 62%|██████▎   | 20/32 [00:19<00:11,  1.05it/s] 19%|█▉        | 6/32 [00:06<00:27,  1.05s/it]  6%|▋         | 2/32 [00:02<00:34,  1.14s/it] 66%|██████▌   | 21/32 [00:20<00:10,  1.05it/s] 44%|████▍     | 14/32 [00:14<00:17,  1.01it/s] 22%|██▏       | 7/32 [00:07<00:26,  1.05s/it]  9%|▉         | 3/32 [00:03<00:31,  1.09s/it] 69%|██████▉   | 22/32 [00:21<00:09,  1.05it/s] 47%|████▋     | 15/32 [00:15<00:16,  1.01it/s] 25%|██▌       | 8/32 [00:08<00:24,  1.04s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.07s/it] 72%|███████▏  | 23/32 [00:22<00:08,  1.05it/s] 50%|█████     | 16/32 [00:16<00:15,  1.01it/s] 28%|██▊       | 9/32 [00:09<00:23,  1.04s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.05s/it] 75%|███████▌  | 24/32 [00:23<00:07,  1.05it/s] 53%|█████▎    | 17/32 [00:16<00:14,  1.01it/s] 31%|███▏      | 10/32 [00:10<00:22,  1.04s/it] 78%|███████▊  | 25/32 [00:24<00:06,  1.05it/s] 19%|█▉        | 6/32 [00:06<00:27,  1.04s/it] 56%|█████▋    | 18/32 [00:17<00:13,  1.01it/s] 34%|███▍      | 11/32 [00:11<00:21,  1.04s/it] 81%|████████▏ | 26/32 [00:25<00:05,  1.05it/s] 22%|██▏       | 7/32 [00:07<00:26,  1.04s/it] 59%|█████▉    | 19/32 [00:18<00:12,  1.01it/s] 38%|███▊      | 12/32 [00:12<00:20,  1.04s/it] 84%|████████▍ | 27/32 [00:26<00:04,  1.05it/s] 25%|██▌       | 8/32 [00:08<00:24,  1.04s/it] 62%|██████▎   | 20/32 [00:19<00:11,  1.01it/s] 41%|████      | 13/32 [00:13<00:19,  1.04s/it] 88%|████████▊ | 28/32 [00:27<00:03,  1.05it/s] 66%|██████▌   | 21/32 [00:20<00:10,  1.01it/s] 28%|██▊       | 9/32 [00:09<00:23,  1.04s/it] 91%|█████████ | 29/32 [00:28<00:02,  1.05it/s] 44%|████▍     | 14/32 [00:14<00:18,  1.03s/it] 69%|██████▉   | 22/32 [00:21<00:09,  1.01it/s] 31%|███▏      | 10/32 [00:10<00:22,  1.04s/it] 94%|█████████▍| 30/32 [00:28<00:01,  1.05it/s] 47%|████▋     | 15/32 [00:15<00:17,  1.04s/it] 72%|███████▏  | 23/32 [00:22<00:08,  1.01it/s] 34%|███▍      | 11/32 [00:11<00:21,  1.04s/it] 97%|█████████▋| 31/32 [00:29<00:00,  1.05it/s] 50%|█████     | 16/32 [00:16<00:16,  1.04s/it] 75%|███████▌  | 24/32 [00:23<00:07,  1.01it/s] 38%|███▊      | 12/32 [00:12<00:20,  1.04s/it]100%|██████████| 32/32 [00:30<00:00,  1.05it/s]100%|██████████| 32/32 [00:30<00:00,  1.04it/s]
 53%|█████▎    | 17/32 [00:17<00:15,  1.04s/it] 78%|███████▊  | 25/32 [00:24<00:07,  1.00s/it] 41%|████      | 13/32 [00:13<00:19,  1.04s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.04s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.01s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.04s/it] 59%|█████▉    | 19/32 [00:20<00:13,  1.04s/it] 84%|████████▍ | 27/32 [00:27<00:05,  1.01s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.04s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.04s/it] 88%|████████▊ | 28/32 [00:28<00:04,  1.01s/it] 50%|█████     | 16/32 [00:16<00:16,  1.03s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.04s/it] 91%|█████████ | 29/32 [00:29<00:03,  1.00s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.03s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.04s/it] 94%|█████████▍| 30/32 [00:30<00:01,  1.00it/s] 56%|█████▋    | 18/32 [00:18<00:14,  1.03s/it]W0318 06:32:19.503000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.503000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.503000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.503000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.503000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.504000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.504000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.531000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.531000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.531000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.532000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.532000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.546000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.546000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.547000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.547000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.547000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:31<00:00,  1.00it/s]W0318 06:32:19.695000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.695000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.695000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.695000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.695000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:24<00:09,  1.05s/it]W0318 06:32:19.908000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.908000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.908000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.908000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.908000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.908000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.908000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.929000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.929000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.929000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.929000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.929000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.990000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.991000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.991000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.991000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:19.991000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 19/32 [00:19<00:13,  1.04s/it]100%|██████████| 32/32 [00:31<00:00,  1.00it/s]100%|██████████| 32/32 [00:31<00:00,  1.00it/s]
 75%|███████▌  | 24/32 [00:25<00:08,  1.05s/it]W0318 06:32:20.812000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:20<00:12,  1.04s/it]W0318 06:32:21.129000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:21.130000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:21.130000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:32:21.130000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:21.130000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:21.130000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:21.130000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:21.153000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:21.153000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:21.153000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:21.153000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:21.153000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:21.416000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:21.416000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:21.417000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:21.417000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:21.417000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:21.690000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:26<00:07,  1.05s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.04s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.04s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.04s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.04s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.04s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.04s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.03s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.04s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.03s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.03s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.03s/it]W0318 06:32:27.414000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.414000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.414000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.414000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.414000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.414000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.415000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.445000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.445000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.445000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.445000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.445000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.461000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.461000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.461000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.461000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.461000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.623000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.623000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.624000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.624000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.624000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.856000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.856000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.856000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.856000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.856000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.856000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.856000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.877000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.877000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.877000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.877000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.877000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.945000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.945000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.945000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.945000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:27.945000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:32<00:01,  1.04s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.04s/it]I0318 06:32:28.702729 1093970 finetune.py:45] layer 0_up initial loss 0.0005039157113060355
W0318 06:32:28.702916 1093970 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:32:28.827000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:33<00:00,  1.04s/it]100%|██████████| 32/32 [00:33<00:00,  1.05s/it]
W0318 06:32:29.129000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:32:29.129000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:29.130000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:29.130000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:29.130000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:29.130000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:29.130000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:29.150000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:29.150000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:29.150000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:29.150000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:29.150000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:29<00:04,  1.05s/it]W0318 06:32:29.403000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:29.403000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:29.403000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:29.403000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:29.403000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:29.447870 1093970 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:32:29.671000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:30<00:03,  1.05s/it]0_up proxy err 0.0036143483594059944 tr(WHW.T) 43.27138137817383
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:31<00:02,  1.05s/it]  3%|▎         | 1/32 [00:01<00:38,  1.25s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.04s/it]  6%|▋         | 2/32 [00:02<00:33,  1.11s/it]100%|██████████| 32/32 [00:33<00:00,  1.03s/it]100%|██████████| 32/32 [00:33<00:00,  1.04s/it]
  9%|▉         | 3/32 [00:03<00:30,  1.07s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.05s/it]W0318 06:32:35.512000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.513000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.513000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.513000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.513000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.513000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.513000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.541000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.541000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.541000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.541000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.541000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.557000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.557000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.557000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.557000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.557000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.711000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.711000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.711000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.711000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.712000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.940000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.940000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.940000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.940000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.940000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.940000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.941000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.961000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.961000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.961000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.961000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:35.961000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:36.024000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:36.024000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:36.025000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:36.025000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:36.025000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:05<00:28,  1.04s/it]I0318 06:32:36.596100 1094231 finetune.py:45] layer 1_up initial loss 0.001566653954796493
W0318 06:32:36.596378 1094231 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:32:36.854000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:06<00:26,  1.03s/it]W0318 06:32:37.164000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:32:37.164000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:37.164000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:37.164000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:37.164000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:37.164000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:37.164000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:37.185000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:37.185000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:37.185000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:37.185000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:37.185000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:37.363942 1094231 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:32:37.429000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:37.430000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:37.430000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:37.430000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:37.430000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:37.682000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:07<00:25,  1.02s/it]1_up proxy err 0.0056134979240596294 tr(WHW.T) 109.663818359375
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:08<00:23,  1.01it/s]W0318 06:32:39.841000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:39.841000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:39.841000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:39.841000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:39.842000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:39.842000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:39.842000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:32:39.872000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:39.872000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:39.872000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:39.872000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:39.872000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:39.888000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:39.888000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:39.888000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:39.888000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:39.888000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:09<00:22,  1.02it/s]  3%|▎         | 1/32 [00:01<00:39,  1.26s/it]W0318 06:32:40.050000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.050000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.050000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.050000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.050000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.289000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.289000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.289000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.289000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.289000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.289000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.289000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.310000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.310000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.310000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.310000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.310000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.377000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.378000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.378000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.378000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:40.378000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:10<00:21,  1.03it/s]  6%|▋         | 2/32 [00:02<00:33,  1.10s/it]W0318 06:32:41.272000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:41.595000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:41.595000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:32:41.595000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:41.596000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:32:41.596000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:41.596000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:41.596000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:32:41.619000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:41.619000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:41.619000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:41.619000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:41.619000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:11<00:20,  1.04it/s]W0318 06:32:41.883000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:32:41.883000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:32:41.883000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:32:41.883000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:32:41.883000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:03<00:30,  1.05s/it]W0318 06:32:42.151000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:12<00:19,  1.04it/s] 12%|█▎        | 4/32 [00:04<00:28,  1.03s/it] 41%|████      | 13/32 [00:13<00:18,  1.05it/s] 16%|█▌        | 5/32 [00:05<00:27,  1.01s/it] 44%|████▍     | 14/32 [00:13<00:17,  1.05it/s]I0318 06:32:44.861025 1094818 finetune.py:45] layer 2_up initial loss 0.00256232637912035
W0318 06:32:44.861381 1094818 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:06<00:25,  1.00it/s] 47%|████▋     | 15/32 [00:14<00:16,  1.06it/s]W0318 06:32:45.714266 1094818 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:07<00:24,  1.01it/s] 50%|█████     | 16/32 [00:15<00:15,  1.06it/s] 25%|██▌       | 8/32 [00:08<00:23,  1.02it/s]2_up proxy err 0.008300596848130226 tr(WHW.T) 193.43600463867188
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:16<00:14,  1.06it/s] 28%|██▊       | 9/32 [00:09<00:22,  1.02it/s]  3%|▎         | 1/32 [00:01<00:42,  1.38s/it] 56%|█████▋    | 18/32 [00:17<00:13,  1.06it/s] 31%|███▏      | 10/32 [00:10<00:21,  1.02it/s]I0318 06:32:49.160671 1095379 finetune.py:45] layer 3_up initial loss 0.006479298230260611
W0318 06:32:49.160925 1095379 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 59%|█████▉    | 19/32 [00:18<00:12,  1.06it/s]  6%|▋         | 2/32 [00:02<00:35,  1.20s/it] 34%|███▍      | 11/32 [00:11<00:20,  1.02it/s]W0318 06:32:49.977437 1095379 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 62%|██████▎   | 20/32 [00:19<00:11,  1.06it/s]  9%|▉         | 3/32 [00:03<00:32,  1.14s/it] 38%|███▊      | 12/32 [00:12<00:19,  1.01it/s]3_up proxy err 0.010523847304284573 tr(WHW.T) 284.7950439453125
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:20<00:10,  1.05it/s] 12%|█▎        | 4/32 [00:04<00:30,  1.10s/it] 41%|████      | 13/32 [00:13<00:18,  1.01it/s] 69%|██████▉   | 22/32 [00:21<00:09,  1.05it/s]  3%|▎         | 1/32 [00:01<00:40,  1.31s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.07s/it] 44%|████▍     | 14/32 [00:14<00:17,  1.01it/s] 72%|███████▏  | 23/32 [00:22<00:08,  1.05it/s]  6%|▋         | 2/32 [00:02<00:34,  1.14s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.05s/it] 47%|████▋     | 15/32 [00:15<00:16,  1.01it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.05it/s]  9%|▉         | 3/32 [00:03<00:31,  1.09s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.05s/it] 50%|█████     | 16/32 [00:16<00:15,  1.00it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.05it/s] 12%|█▎        | 4/32 [00:04<00:30,  1.08s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.05s/it] 53%|█████▎    | 17/32 [00:17<00:14,  1.00it/s] 81%|████████▏ | 26/32 [00:25<00:05,  1.04it/s] 16%|█▌        | 5/32 [00:05<00:28,  1.06s/it] 28%|██▊       | 9/32 [00:09<00:24,  1.05s/it] 56%|█████▋    | 18/32 [00:18<00:13,  1.00it/s] 84%|████████▍ | 27/32 [00:26<00:04,  1.04it/s] 19%|█▉        | 6/32 [00:06<00:27,  1.06s/it] 59%|█████▉    | 19/32 [00:19<00:12,  1.00it/s] 31%|███▏      | 10/32 [00:10<00:23,  1.05s/it] 88%|████████▊ | 28/32 [00:27<00:03,  1.04it/s] 22%|██▏       | 7/32 [00:07<00:26,  1.05s/it] 62%|██████▎   | 20/32 [00:20<00:11,  1.00it/s] 34%|███▍      | 11/32 [00:11<00:21,  1.05s/it] 91%|█████████ | 29/32 [00:28<00:02,  1.05it/s] 66%|██████▌   | 21/32 [00:21<00:10,  1.00it/s] 25%|██▌       | 8/32 [00:08<00:25,  1.04s/it] 94%|█████████▍| 30/32 [00:29<00:01,  1.05it/s] 38%|███▊      | 12/32 [00:12<00:20,  1.05s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.00s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.04s/it] 97%|█████████▋| 31/32 [00:30<00:00,  1.05it/s] 41%|████      | 13/32 [00:13<00:19,  1.04s/it] 72%|███████▏  | 23/32 [00:23<00:08,  1.00it/s]100%|██████████| 32/32 [00:31<00:00,  1.05it/s]100%|██████████| 32/32 [00:31<00:00,  1.03it/s]
 31%|███▏      | 10/32 [00:10<00:22,  1.04s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.05s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.01s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.03s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.04s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.02s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.04s/it] 50%|█████     | 16/32 [00:17<00:16,  1.04s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.02s/it] 41%|████      | 13/32 [00:13<00:19,  1.03s/it] 53%|█████▎    | 17/32 [00:18<00:15,  1.04s/it] 84%|████████▍ | 27/32 [00:27<00:05,  1.03s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.03s/it] 56%|█████▋    | 18/32 [00:19<00:14,  1.04s/it] 88%|████████▊ | 28/32 [00:28<00:04,  1.03s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.03s/it] 59%|█████▉    | 19/32 [00:20<00:13,  1.04s/it] 91%|█████████ | 29/32 [00:29<00:03,  1.04s/it] 50%|█████     | 16/32 [00:16<00:16,  1.03s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.04s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.02s/it] 94%|█████████▍| 30/32 [00:30<00:02,  1.04s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.04s/it]I0318 06:33:09.564334 1093970 finetune.py:45] layer 0_gate initial loss 0.0005018446827307343
W0318 06:33:09.564543 1093970 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it] 97%|█████████▋| 31/32 [00:31<00:01,  1.03s/it]W0318 06:33:10.237523 1093970 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:23<00:10,  1.04s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.02s/it]100%|██████████| 32/32 [00:32<00:00,  1.03s/it]100%|██████████| 32/32 [00:32<00:00,  1.01s/it]
 72%|███████▏  | 23/32 [00:24<00:09,  1.04s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.03s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.04s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.03s/it]0_gate proxy err 0.002551678568124771 tr(WHW.T) 63.47429656982422
  0%|          | 0/86 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:26<00:07,  1.04s/it]  1%|          | 1/86 [00:00<00:49,  1.71it/s] 69%|██████▉   | 22/32 [00:22<00:10,  1.02s/it]  2%|▏         | 2/86 [00:00<00:36,  2.28it/s] 81%|████████▏ | 26/32 [00:27<00:06,  1.04s/it]  3%|▎         | 3/86 [00:01<00:32,  2.53it/s]  5%|▍         | 4/86 [00:01<00:30,  2.66it/s] 72%|███████▏  | 23/32 [00:23<00:09,  1.02s/it]  6%|▌         | 5/86 [00:01<00:29,  2.75it/s] 84%|████████▍ | 27/32 [00:28<00:05,  1.04s/it]  7%|▋         | 6/86 [00:02<00:28,  2.78it/s]  8%|▊         | 7/86 [00:02<00:27,  2.83it/s] 75%|███████▌  | 24/32 [00:24<00:08,  1.02s/it]  9%|▉         | 8/86 [00:02<00:27,  2.86it/s] 88%|████████▊ | 28/32 [00:29<00:04,  1.04s/it] 10%|█         | 9/86 [00:03<00:26,  2.89it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.89it/s] 78%|███████▊  | 25/32 [00:26<00:07,  1.03s/it] 13%|█▎        | 11/86 [00:04<00:25,  2.90it/s] 91%|█████████ | 29/32 [00:30<00:03,  1.04s/it] 14%|█▍        | 12/86 [00:04<00:25,  2.90it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.90it/s] 81%|████████▏ | 26/32 [00:27<00:06,  1.03s/it] 16%|█▋        | 14/86 [00:05<00:24,  2.91it/s] 94%|█████████▍| 30/32 [00:31<00:02,  1.04s/it]I0318 06:33:18.744792 1094231 finetune.py:45] layer 1_gate initial loss 0.0015464442549273372
W0318 06:33:18.745114 1094231 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 17%|█▋        | 15/86 [00:05<00:24,  2.92it/s] 19%|█▊        | 16/86 [00:05<00:23,  2.93it/s] 84%|████████▍ | 27/32 [00:28<00:05,  1.03s/it] 20%|█▉        | 17/86 [00:06<00:23,  2.98it/s]W0318 06:33:19.518757 1094231 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:32<00:01,  1.05s/it] 21%|██        | 18/86 [00:06<00:22,  3.00it/s] 22%|██▏       | 19/86 [00:06<00:22,  2.99it/s] 88%|████████▊ | 28/32 [00:29<00:04,  1.05s/it] 23%|██▎       | 20/86 [00:07<00:22,  2.95it/s]100%|██████████| 32/32 [00:33<00:00,  1.05s/it]100%|██████████| 32/32 [00:33<00:00,  1.05s/it]
 24%|██▍       | 21/86 [00:07<00:21,  2.97it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.96it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.96it/s] 91%|█████████ | 29/32 [00:30<00:03,  1.05s/it] 28%|██▊       | 24/86 [00:08<00:20,  2.96it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.94it/s] 30%|███       | 26/86 [00:09<00:20,  2.99it/s]1_gate proxy err 0.0031518014147877693 tr(WHW.T) 221.3038330078125
  0%|          | 0/86 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:31<00:02,  1.05s/it] 31%|███▏      | 27/86 [00:09<00:19,  3.02it/s]  1%|          | 1/86 [00:00<00:50,  1.68it/s] 33%|███▎      | 28/86 [00:09<00:19,  3.03it/s]  2%|▏         | 2/86 [00:00<00:37,  2.26it/s] 34%|███▎      | 29/86 [00:10<00:18,  3.05it/s] 97%|█████████▋| 31/32 [00:32<00:01,  1.06s/it] 35%|███▍      | 30/86 [00:10<00:18,  3.07it/s]  3%|▎         | 3/86 [00:01<00:32,  2.53it/s] 36%|███▌      | 31/86 [00:10<00:17,  3.09it/s]  5%|▍         | 4/86 [00:01<00:30,  2.69it/s] 37%|███▋      | 32/86 [00:11<00:17,  3.11it/s]  6%|▌         | 5/86 [00:01<00:29,  2.79it/s]100%|██████████| 32/32 [00:33<00:00,  1.06s/it]100%|██████████| 32/32 [00:33<00:00,  1.04s/it]
 38%|███▊      | 33/86 [00:11<00:16,  3.13it/s]  7%|▋         | 6/86 [00:02<00:28,  2.86it/s] 40%|███▉      | 34/86 [00:11<00:16,  3.14it/s]  8%|▊         | 7/86 [00:02<00:27,  2.90it/s] 41%|████      | 35/86 [00:11<00:16,  3.15it/s]  9%|▉         | 8/86 [00:02<00:26,  2.93it/s] 42%|████▏     | 36/86 [00:12<00:15,  3.16it/s] 10%|█         | 9/86 [00:03<00:26,  2.95it/s] 43%|████▎     | 37/86 [00:12<00:15,  3.15it/s] 12%|█▏        | 10/86 [00:03<00:25,  2.97it/s] 44%|████▍     | 38/86 [00:12<00:15,  3.15it/s] 13%|█▎        | 11/86 [00:03<00:25,  2.98it/s] 45%|████▌     | 39/86 [00:13<00:14,  3.17it/s] 14%|█▍        | 12/86 [00:04<00:24,  2.99it/s] 47%|████▋     | 40/86 [00:13<00:14,  3.15it/s] 15%|█▌        | 13/86 [00:04<00:24,  2.99it/s] 48%|████▊     | 41/86 [00:13<00:14,  3.15it/s] 16%|█▋        | 14/86 [00:04<00:24,  3.00it/s] 49%|████▉     | 42/86 [00:14<00:13,  3.15it/s] 17%|█▋        | 15/86 [00:05<00:23,  3.01it/s] 50%|█████     | 43/86 [00:14<00:13,  3.15it/s]I0318 06:33:28.074587 1094818 finetune.py:45] layer 2_gate initial loss 0.002541840309277177
W0318 06:33:28.075087 1094818 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▊        | 16/86 [00:05<00:23,  3.00it/s] 51%|█████     | 44/86 [00:14<00:13,  3.15it/s] 20%|█▉        | 17/86 [00:05<00:23,  3.00it/s] 52%|█████▏    | 45/86 [00:15<00:13,  3.15it/s] 21%|██        | 18/86 [00:06<00:22,  2.99it/s] 53%|█████▎    | 46/86 [00:15<00:12,  3.15it/s]W0318 06:33:28.912259 1094818 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 19/86 [00:06<00:22,  2.99it/s] 55%|█████▍    | 47/86 [00:15<00:12,  3.15it/s] 23%|██▎       | 20/86 [00:06<00:22,  2.99it/s] 56%|█████▌    | 48/86 [00:16<00:12,  3.16it/s] 24%|██▍       | 21/86 [00:07<00:21,  2.98it/s] 57%|█████▋    | 49/86 [00:16<00:11,  3.15it/s] 58%|█████▊    | 50/86 [00:16<00:11,  3.14it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.98it/s] 59%|█████▉    | 51/86 [00:17<00:11,  3.14it/s] 27%|██▋       | 23/86 [00:07<00:21,  2.98it/s] 60%|██████    | 52/86 [00:17<00:10,  3.14it/s] 28%|██▊       | 24/86 [00:08<00:20,  2.99it/s] 62%|██████▏   | 53/86 [00:17<00:10,  3.15it/s] 29%|██▉       | 25/86 [00:08<00:20,  3.01it/s] 63%|██████▎   | 54/86 [00:17<00:10,  3.15it/s] 30%|███       | 26/86 [00:08<00:19,  3.01it/s] 64%|██████▍   | 55/86 [00:18<00:09,  3.16it/s] 31%|███▏      | 27/86 [00:09<00:19,  3.01it/s] 65%|██████▌   | 56/86 [00:18<00:09,  3.15it/s]2_gate proxy err 0.00603704247623682 tr(WHW.T) 306.6622009277344
  0%|          | 0/86 [00:00<?, ?it/s] 33%|███▎      | 28/86 [00:09<00:19,  3.00it/s]I0318 06:33:32.156931 1095379 finetune.py:45] layer 3_gate initial loss 0.006412074435502291
W0318 06:33:32.157134 1095379 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 66%|██████▋   | 57/86 [00:18<00:09,  3.13it/s] 34%|███▎      | 29/86 [00:09<00:19,  2.97it/s]  1%|          | 1/86 [00:00<00:51,  1.67it/s] 67%|██████▋   | 58/86 [00:19<00:08,  3.12it/s] 35%|███▍      | 30/86 [00:10<00:18,  2.96it/s]W0318 06:33:32.866989 1095379 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▊   | 59/86 [00:19<00:08,  3.12it/s]  2%|▏         | 2/86 [00:00<00:37,  2.23it/s] 36%|███▌      | 31/86 [00:10<00:18,  2.96it/s] 70%|██████▉   | 60/86 [00:19<00:08,  3.12it/s]  3%|▎         | 3/86 [00:01<00:33,  2.47it/s] 37%|███▋      | 32/86 [00:10<00:18,  2.95it/s] 71%|███████   | 61/86 [00:20<00:08,  3.11it/s]  5%|▍         | 4/86 [00:01<00:32,  2.55it/s] 38%|███▊      | 33/86 [00:11<00:17,  2.95it/s] 72%|███████▏  | 62/86 [00:20<00:07,  3.12it/s]  6%|▌         | 5/86 [00:02<00:30,  2.63it/s] 40%|███▉      | 34/86 [00:11<00:17,  2.95it/s] 73%|███████▎  | 63/86 [00:20<00:07,  3.12it/s]  7%|▋         | 6/86 [00:02<00:29,  2.69it/s] 41%|████      | 35/86 [00:11<00:17,  2.96it/s] 74%|███████▍  | 64/86 [00:21<00:07,  3.13it/s]  8%|▊         | 7/86 [00:02<00:29,  2.72it/s] 42%|████▏     | 36/86 [00:12<00:16,  2.96it/s] 76%|███████▌  | 65/86 [00:21<00:06,  3.13it/s]  9%|▉         | 8/86 [00:03<00:28,  2.74it/s] 43%|████▎     | 37/86 [00:12<00:16,  2.96it/s] 77%|███████▋  | 66/86 [00:21<00:06,  3.12it/s] 10%|█         | 9/86 [00:03<00:27,  2.78it/s] 44%|████▍     | 38/86 [00:12<00:16,  2.97it/s] 78%|███████▊  | 67/86 [00:22<00:06,  3.12it/s]3_gate proxy err 0.007328884210437536 tr(WHW.T) 478.1370544433594
  0%|          | 0/86 [00:00<?, ?it/s] 45%|████▌     | 39/86 [00:13<00:15,  2.97it/s] 12%|█▏        | 10/86 [00:03<00:27,  2.80it/s] 79%|███████▉  | 68/86 [00:22<00:05,  3.12it/s] 47%|████▋     | 40/86 [00:13<00:15,  2.97it/s] 80%|████████  | 69/86 [00:22<00:05,  3.11it/s]  1%|          | 1/86 [00:00<00:50,  1.68it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.81it/s] 81%|████████▏ | 70/86 [00:23<00:05,  3.10it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.96it/s]  2%|▏         | 2/86 [00:00<00:37,  2.26it/s] 14%|█▍        | 12/86 [00:04<00:26,  2.84it/s] 83%|████████▎ | 71/86 [00:23<00:04,  3.09it/s] 49%|████▉     | 42/86 [00:14<00:14,  2.96it/s]  3%|▎         | 3/86 [00:01<00:33,  2.51it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.85it/s] 84%|████████▎ | 72/86 [00:23<00:04,  3.09it/s] 50%|█████     | 43/86 [00:14<00:14,  2.94it/s]  5%|▍         | 4/86 [00:01<00:31,  2.64it/s] 16%|█▋        | 14/86 [00:05<00:25,  2.84it/s] 85%|████████▍ | 73/86 [00:24<00:04,  3.09it/s] 51%|█████     | 44/86 [00:15<00:14,  2.94it/s]  6%|▌         | 5/86 [00:01<00:29,  2.72it/s] 17%|█▋        | 15/86 [00:05<00:25,  2.83it/s] 86%|████████▌ | 74/86 [00:24<00:03,  3.10it/s] 52%|█████▏    | 45/86 [00:15<00:13,  2.94it/s]  7%|▋         | 6/86 [00:02<00:28,  2.78it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.83it/s] 87%|████████▋ | 75/86 [00:24<00:03,  3.06it/s] 53%|█████▎    | 46/86 [00:15<00:13,  2.92it/s]  8%|▊         | 7/86 [00:02<00:28,  2.79it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.82it/s] 88%|████████▊ | 76/86 [00:25<00:03,  3.07it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.93it/s]  9%|▉         | 8/86 [00:03<00:27,  2.83it/s] 21%|██        | 18/86 [00:06<00:24,  2.82it/s] 90%|████████▉ | 77/86 [00:25<00:02,  3.08it/s] 56%|█████▌    | 48/86 [00:16<00:12,  2.95it/s] 10%|█         | 9/86 [00:03<00:26,  2.88it/s] 22%|██▏       | 19/86 [00:06<00:23,  2.85it/s] 91%|█████████ | 78/86 [00:25<00:02,  3.09it/s] 57%|█████▋    | 49/86 [00:16<00:12,  2.95it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.90it/s] 23%|██▎       | 20/86 [00:07<00:23,  2.86it/s] 92%|█████████▏| 79/86 [00:26<00:02,  3.09it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.94it/s] 13%|█▎        | 11/86 [00:04<00:25,  2.92it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.87it/s] 93%|█████████▎| 80/86 [00:26<00:01,  3.09it/s] 59%|█████▉    | 51/86 [00:17<00:11,  2.95it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.94it/s] 26%|██▌       | 22/86 [00:08<00:22,  2.87it/s] 94%|█████████▍| 81/86 [00:26<00:01,  3.09it/s] 60%|██████    | 52/86 [00:17<00:11,  2.94it/s] 15%|█▌        | 13/86 [00:04<00:24,  2.94it/s] 27%|██▋       | 23/86 [00:08<00:22,  2.86it/s] 95%|█████████▌| 82/86 [00:27<00:01,  3.08it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.94it/s] 16%|█▋        | 14/86 [00:05<00:24,  2.92it/s] 97%|█████████▋| 83/86 [00:27<00:00,  3.07it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.84it/s] 63%|██████▎   | 54/86 [00:18<00:10,  2.94it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.91it/s] 98%|█████████▊| 84/86 [00:27<00:00,  3.08it/s] 29%|██▉       | 25/86 [00:09<00:21,  2.84it/s] 64%|██████▍   | 55/86 [00:18<00:10,  2.94it/s] 19%|█▊        | 16/86 [00:05<00:23,  2.92it/s] 99%|█████████▉| 85/86 [00:27<00:00,  3.08it/s] 30%|███       | 26/86 [00:09<00:21,  2.84it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.93it/s] 20%|█▉        | 17/86 [00:06<00:23,  2.92it/s]100%|██████████| 86/86 [00:28<00:00,  3.09it/s]100%|██████████| 86/86 [00:28<00:00,  3.04it/s]
 31%|███▏      | 27/86 [00:09<00:20,  2.82it/s] 66%|██████▋   | 57/86 [00:19<00:09,  2.93it/s] 21%|██        | 18/86 [00:06<00:23,  2.90it/s] 33%|███▎      | 28/86 [00:10<00:20,  2.84it/s] 67%|██████▋   | 58/86 [00:19<00:09,  2.89it/s] 22%|██▏       | 19/86 [00:06<00:23,  2.90it/s] 34%|███▎      | 29/86 [00:10<00:20,  2.83it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.85it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.92it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.83it/s] 70%|██████▉   | 60/86 [00:20<00:09,  2.84it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.92it/s] 36%|███▌      | 31/86 [00:11<00:19,  2.85it/s] 71%|███████   | 61/86 [00:20<00:08,  2.86it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.95it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.87it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.96it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.85it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.87it/s] 28%|██▊       | 24/86 [00:08<00:20,  2.95it/s] 73%|███████▎  | 63/86 [00:21<00:07,  2.88it/s] 40%|███▉      | 34/86 [00:12<00:18,  2.88it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.94it/s] 74%|███████▍  | 64/86 [00:21<00:07,  2.90it/s] 41%|████      | 35/86 [00:12<00:17,  2.88it/s] 30%|███       | 26/86 [00:09<00:20,  2.94it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.91it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.89it/s] 31%|███▏      | 27/86 [00:09<00:19,  2.95it/s] 77%|███████▋  | 66/86 [00:22<00:06,  2.92it/s] 43%|████▎     | 37/86 [00:13<00:16,  2.88it/s] 33%|███▎      | 28/86 [00:09<00:19,  2.95it/s] 78%|███████▊  | 67/86 [00:22<00:06,  2.93it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.90it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.94it/s] 79%|███████▉  | 68/86 [00:23<00:06,  2.94it/s] 45%|████▌     | 39/86 [00:13<00:16,  2.89it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.94it/s] 80%|████████  | 69/86 [00:23<00:05,  2.94it/s] 47%|████▋     | 40/86 [00:14<00:15,  2.89it/s] 36%|███▌      | 31/86 [00:10<00:18,  2.96it/s] 81%|████████▏ | 70/86 [00:23<00:05,  2.95it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.88it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.95it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.96it/s] 49%|████▉     | 42/86 [00:14<00:15,  2.88it/s] 38%|███▊      | 33/86 [00:11<00:17,  2.96it/s] 84%|████████▎ | 72/86 [00:24<00:04,  2.96it/s] 50%|█████     | 43/86 [00:15<00:14,  2.89it/s] 40%|███▉      | 34/86 [00:11<00:17,  2.98it/s] 85%|████████▍ | 73/86 [00:24<00:04,  2.96it/s] 51%|█████     | 44/86 [00:15<00:14,  2.86it/s] 41%|████      | 35/86 [00:12<00:17,  2.96it/s] 86%|████████▌ | 74/86 [00:25<00:04,  2.96it/s] 52%|█████▏    | 45/86 [00:16<00:14,  2.87it/s] 42%|████▏     | 36/86 [00:12<00:16,  2.97it/s] 87%|████████▋ | 75/86 [00:25<00:03,  2.95it/s]W0318 06:33:48.149000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.149000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.149000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.149000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.149000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.149000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.149000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.190000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.190000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.190000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.190000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.190000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.205000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.205000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.205000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.205000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.205000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.364000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.364000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.364000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.364000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.364000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 46/86 [00:16<00:13,  2.88it/s] 43%|████▎     | 37/86 [00:12<00:16,  2.98it/s] 88%|████████▊ | 76/86 [00:25<00:03,  2.95it/s]W0318 06:33:48.666000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.666000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.666000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.666000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.667000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.667000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.667000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.698000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.699000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.699000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.699000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.699000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.765000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.765000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.765000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.766000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:33:48.766000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 55%|█████▍    | 47/86 [00:16<00:13,  2.87it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.97it/s] 90%|████████▉ | 77/86 [00:26<00:03,  2.95it/s] 45%|████▌     | 39/86 [00:13<00:15,  2.97it/s] 56%|█████▌    | 48/86 [00:17<00:13,  2.86it/s] 91%|█████████ | 78/86 [00:26<00:02,  2.95it/s] 47%|████▋     | 40/86 [00:13<00:15,  2.98it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.88it/s] 92%|█████████▏| 79/86 [00:26<00:02,  2.95it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.97it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.88it/s] 93%|█████████▎| 80/86 [00:27<00:02,  2.95it/s]W0318 06:33:49.879000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:33:49.891000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:33:49.899000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:33:49.899000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 49%|████▉     | 42/86 [00:14<00:14,  2.95it/s] 94%|█████████▍| 81/86 [00:27<00:01,  2.95it/s] 59%|█████▉    | 51/86 [00:18<00:12,  2.86it/s]W0318 06:33:50.320000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.320000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.320000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.320000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.320000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.320000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.321000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.350000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.350000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.351000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.351000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.351000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 43/86 [00:14<00:14,  2.97it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.95it/s] 60%|██████    | 52/86 [00:18<00:11,  2.87it/s]W0318 06:33:50.677000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.677000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.677000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.677000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.677000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.677000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.678000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.678000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 51%|█████     | 44/86 [00:15<00:14,  2.96it/s] 97%|█████████▋| 83/86 [00:28<00:01,  2.95it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.86it/s]W0318 06:33:50.958000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.958000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.958000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.958000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:33:50.958000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 52%|█████▏    | 45/86 [00:15<00:13,  2.97it/s] 98%|█████████▊| 84/86 [00:28<00:00,  2.95it/s] 63%|██████▎   | 54/86 [00:19<00:11,  2.88it/s]W0318 06:33:51.276000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:33:51.281000 139968865007424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 46/86 [00:15<00:13,  2.98it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.93it/s] 64%|██████▍   | 55/86 [00:19<00:10,  2.87it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.97it/s]100%|██████████| 86/86 [00:29<00:00,  2.93it/s]100%|██████████| 86/86 [00:29<00:00,  2.93it/s]
 65%|██████▌   | 56/86 [00:19<00:10,  2.87it/s] 56%|█████▌    | 48/86 [00:16<00:12,  2.97it/s] 66%|██████▋   | 57/86 [00:20<00:10,  2.86it/s] 57%|█████▋    | 49/86 [00:16<00:12,  2.97it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.87it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.96it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.85it/s] 59%|█████▉    | 51/86 [00:17<00:11,  2.96it/s] 70%|██████▉   | 60/86 [00:21<00:09,  2.88it/s] 60%|██████    | 52/86 [00:17<00:11,  2.98it/s] 71%|███████   | 61/86 [00:21<00:08,  2.90it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.98it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.90it/s] 63%|██████▎   | 54/86 [00:18<00:10,  3.00it/s] 73%|███████▎  | 63/86 [00:22<00:07,  2.90it/s] 64%|██████▍   | 55/86 [00:18<00:10,  3.01it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.88it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.99it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.89it/s] 66%|██████▋   | 57/86 [00:19<00:09,  2.98it/s] 77%|███████▋  | 66/86 [00:23<00:06,  2.88it/s] 67%|██████▋   | 58/86 [00:19<00:09,  2.98it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.87it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.96it/s] 79%|███████▉  | 68/86 [00:24<00:06,  2.87it/s] 70%|██████▉   | 60/86 [00:20<00:08,  2.97it/s] 80%|████████  | 69/86 [00:24<00:05,  2.89it/s] 71%|███████   | 61/86 [00:20<00:08,  2.98it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.88it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.98it/s] 83%|████████▎ | 71/86 [00:25<00:05,  2.88it/s] 73%|███████▎  | 63/86 [00:21<00:07,  2.97it/s] 84%|████████▎ | 72/86 [00:25<00:04,  2.89it/s] 74%|███████▍  | 64/86 [00:21<00:07,  2.99it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.90it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.99it/s] 86%|████████▌ | 74/86 [00:26<00:04,  2.90it/s] 77%|███████▋  | 66/86 [00:22<00:06,  2.99it/s]I0318 06:33:58.346586 1093970 finetune.py:45] layer 0_down initial loss 0.0005012679612264037
W0318 06:33:58.346872 1093970 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 87%|████████▋ | 75/86 [00:26<00:03,  2.91it/s] 78%|███████▊  | 67/86 [00:22<00:06,  2.99it/s]W0318 06:33:58.785000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:33:58.786000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:33:58.786000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:33:58.786000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:33:58.786000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:33:58.786000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:33:58.786000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:33:58.827000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:33:58.827000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:33:58.827000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:33:58.827000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:33:58.827000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 76/86 [00:26<00:03,  2.90it/s]W0318 06:33:58.830914 1093970 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0318 06:33:58.842000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:33:58.842000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:33:58.842000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:33:58.842000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:33:58.842000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 79%|███████▉  | 68/86 [00:23<00:06,  2.99it/s]W0318 06:33:59.005000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.005000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.005000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.005000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.005000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
0_down proxy err 0.0020672378595918417 tr(WHW.T) 0.656814455986023
 90%|████████▉ | 77/86 [00:27<00:03,  2.89it/s] 80%|████████  | 69/86 [00:23<00:05,  2.99it/s]W0318 06:33:59.316000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.316000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.317000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.317000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.317000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.317000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.317000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.348000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.348000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.348000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.348000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.348000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.415000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.416000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.416000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.416000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:33:59.416000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 78/86 [00:27<00:02,  2.88it/s] 81%|████████▏ | 70/86 [00:23<00:05,  2.96it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.95it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.87it/s] 84%|████████▎ | 72/86 [00:24<00:04,  2.95it/s] 93%|█████████▎| 80/86 [00:28<00:02,  2.87it/s] 85%|████████▍ | 73/86 [00:24<00:04,  2.95it/s]W0318 06:34:00.560000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 81/86 [00:28<00:01,  2.88it/s]W0318 06:34:00.574000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:00.582000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:00.582000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 86%|████████▌ | 74/86 [00:25<00:04,  2.93it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.87it/s]W0318 06:34:01.037000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.037000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.037000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.037000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.037000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.037000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.037000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.067000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.067000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.067000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.068000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.068000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 87%|████████▋ | 75/86 [00:25<00:03,  2.93it/s] 97%|█████████▋| 83/86 [00:29<00:01,  2.87it/s]W0318 06:34:01.395000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.395000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.395000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.395000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.395000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.396000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.396000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.396000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 76/86 [00:25<00:03,  2.95it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.89it/s]W0318 06:34:01.683000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.684000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.684000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.684000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:01.684000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 90%|████████▉ | 77/86 [00:26<00:03,  2.93it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.87it/s]W0318 06:34:02.006000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:02.011000 140606322931520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 78/86 [00:26<00:02,  2.93it/s]100%|██████████| 86/86 [00:30<00:00,  2.88it/s]100%|██████████| 86/86 [00:30<00:00,  2.84it/s]
 92%|█████████▏| 79/86 [00:27<00:02,  2.95it/s] 93%|█████████▎| 80/86 [00:27<00:02,  2.91it/s] 94%|█████████▍| 81/86 [00:27<00:01,  2.87it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.90it/s] 97%|█████████▋| 83/86 [00:28<00:01,  2.90it/s] 98%|█████████▊| 84/86 [00:28<00:00,  2.90it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.95it/s]100%|██████████| 86/86 [00:29<00:00,  2.98it/s]100%|██████████| 86/86 [00:29<00:00,  2.93it/s]
W0318 06:34:08.833000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:08.833000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:08.833000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:08.833000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:08.833000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:08.833000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:08.833000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:08.875000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:08.875000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:08.876000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:08.876000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:08.876000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
I0318 06:34:08.890503 1094231 finetune.py:45] layer 1_down initial loss 0.0015227681724354625
W0318 06:34:08.890717 1094231 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:34:08.891000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:08.892000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:08.892000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:08.892000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:08.892000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.056000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.056000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.056000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.056000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.056000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.328742 1094231 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0318 06:34:09.377000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.377000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.377000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.377000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.377000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.378000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.378000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.409000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.409000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.409000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.409000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.409000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.477000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.477000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.477000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.477000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:09.477000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
1_down proxy err 8.144474122673273e-05 tr(WHW.T) 2041.4736328125
W0318 06:34:10.613000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:10.626000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:10.634000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:10.634000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.075000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.075000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.075000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.075000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.075000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.075000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.075000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.104000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.104000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.105000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.105000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.105000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.347000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.347000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.347000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.347000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.347000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.347000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.347000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.389000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.389000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.389000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.390000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.390000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.405000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.405000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.405000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.405000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.405000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.445000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.445000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.445000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.445000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.445000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.445000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.445000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.446000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.572000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.572000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.572000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.572000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.573000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.734000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.734000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.734000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.734000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.735000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.886000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.886000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.886000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.886000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.886000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.886000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.886000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.917000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.917000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.917000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.917000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.917000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.984000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.984000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.984000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.984000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:11.984000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:12.058000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:12.063000 140304414144320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.125000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.138000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.146000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.146000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
I0318 06:34:13.160873 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 4 in 1.427342176437378s
W0318 06:34:13.598000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.598000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.598000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.599000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.599000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.599000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.599000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
I0318 06:34:13.620848 1092738 quantize_finetune_llama.py:159] layer 5 gpu 1
W0318 06:34:13.634000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.634000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.634000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.634000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.634000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.991000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.991000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.991000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.991000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.992000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.992000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.992000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:13.992000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:14.299000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:14.299000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:14.299000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:14.300000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:14.300000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:14.649000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:14.654000 140053938194240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0318 06:34:15.504619 1099388 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:34:15.504720 1099388 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:34:15.504776 1099388 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:34:15.681097 1099388 config.py:58] PyTorch version 2.4.0 available.
I0318 06:34:17.744404 1099388 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0318 06:34:18.086775 1099388 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:34:18.933334 1094818 finetune.py:45] layer 2_down initial loss 0.002530819270759821
W0318 06:34:18.933518 1094818 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  0%|          | 0/32 [00:00<?, ?it/s]W0318 06:34:19.383432 1094818 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

2_down proxy err 0.01128390058875084 tr(WHW.T) 3.0107386112213135
  3%|▎         | 1/32 [00:01<00:40,  1.31s/it]  6%|▋         | 2/32 [00:01<00:21,  1.38it/s]  9%|▉         | 3/32 [00:01<00:15,  1.86it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s]I0318 06:34:21.537569 1095379 finetune.py:45] layer 3_down initial loss 0.006377645302563906
W0318 06:34:21.537843 1095379 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.64it/s]W0318 06:34:22.060217 1095379 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 22%|██▏       | 7/32 [00:03<00:08,  2.80it/s]3_down proxy err 0.011645298451185226 tr(WHW.T) 6.133227348327637
 25%|██▌       | 8/32 [00:03<00:08,  2.92it/s]I0318 06:34:22.724440 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 5 in 1.1081433296203613s
 28%|██▊       | 9/32 [00:03<00:07,  2.94it/s]I0318 06:34:23.185549 1092738 quantize_finetune_llama.py:159] layer 6 gpu 2
 31%|███▏      | 10/32 [00:04<00:07,  2.93it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.97it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.96it/s] 41%|████      | 13/32 [00:05<00:06,  2.96it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.02it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.06it/s]I0318 06:34:25.148804 1100007 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:34:25.148907 1100007 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:34:25.148967 1100007 utils.py:162] NumExpr defaulting to 16 threads.
 50%|█████     | 16/32 [00:06<00:05,  3.10it/s]I0318 06:34:25.330697 1100007 config.py:58] PyTorch version 2.4.0 available.
I0318 06:34:25.467128 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 6 in 1.1020698547363281s
 53%|█████▎    | 17/32 [00:06<00:04,  3.10it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.04it/s]I0318 06:34:25.945708 1092738 quantize_finetune_llama.py:159] layer 7 gpu 3
 59%|█████▉    | 19/32 [00:07<00:04,  3.01it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.03it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.00it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.05it/s]I0318 06:34:27.523390 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 7 in 1.1111748218536377s
 72%|███████▏  | 23/32 [00:08<00:02,  3.02it/s]I0318 06:34:27.731629 1100007 data_utils.py:336] using 256 training seqs, 128 validation seqs
 75%|███████▌  | 24/32 [00:08<00:02,  2.98it/s]I0318 06:34:27.923782 1100176 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:34:27.923916 1100176 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:34:27.924077 1100176 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:34:27.925531 1092738 quantize_finetune_llama.py:159] layer 8 gpu 0
I0318 06:34:28.133018 1100176 config.py:58] PyTorch version 2.4.0 available.
W0318 06:34:28.141579 1100007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 78%|███████▊  | 25/32 [00:09<00:02,  2.96it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.95it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.93it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.95it/s]  0%|          | 0/32 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.94it/s]I0318 06:34:29.892968 1100601 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:34:29.893079 1100601 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:34:29.893137 1100601 utils.py:162] NumExpr defaulting to 16 threads.
 94%|█████████▍| 30/32 [00:10<00:00,  2.93it/s]I0318 06:34:30.083011 1100601 config.py:58] PyTorch version 2.4.0 available.
I0318 06:34:30.250432 1100176 data_utils.py:336] using 256 training seqs, 128 validation seqs
 97%|█████████▋| 31/32 [00:11<00:00,  2.92it/s]W0318 06:34:30.592215 1100176 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:11<00:00,  2.94it/s]100%|██████████| 32/32 [00:11<00:00,  2.77it/s]
  3%|▎         | 1/32 [00:01<00:51,  1.66s/it]  6%|▋         | 2/32 [00:02<00:26,  1.12it/s]  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:02<00:18,  1.55it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.87it/s]I0318 06:34:32.253772 1100601 data_utils.py:336] using 256 training seqs, 128 validation seqs
 16%|█▌        | 5/32 [00:03<00:12,  2.12it/s]W0318 06:34:32.599669 1100601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s]  3%|▎         | 1/32 [00:01<00:47,  1.53s/it] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s]  6%|▋         | 2/32 [00:01<00:24,  1.21it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.62it/s]  0%|          | 0/32 [00:00<?, ?it/s]W0318 06:34:33.714000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:33.714000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:33.714000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:33.714000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:33.714000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:33.714000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:33.714000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:33.740000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:33.740000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:33.740000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:33.740000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:33.740000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:33.757000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:33.757000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:33.757000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:33.757000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:33.757000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:02<00:17,  1.66it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.71it/s]W0318 06:34:34.080000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:34.080000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:34.080000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:34.080000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:34.080000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:02<00:14,  2.00it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.75it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.26it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.80it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.44it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.83it/s]W0318 06:34:34.949000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:34.949000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:34.949000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:34.949000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:34.949000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:34.949000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:34.949000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:34.967000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:34.967000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:34.967000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:34.967000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:34.967000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:44,  1.42s/it] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 41%|████      | 13/32 [00:05<00:06,  2.87it/s]W0318 06:34:35.202000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:35.202000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:35.202000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:35.202000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:35.202000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:23,  1.27it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.70it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.90it/s]  9%|▉         | 3/32 [00:02<00:16,  1.72it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.78it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.92it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.06it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.83it/s] 50%|█████     | 16/32 [00:06<00:05,  2.94it/s]W0318 06:34:36.326000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:36.327000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:36.327000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:36.327000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:36.327000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:36.327000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:36.327000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:36.344000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:36.344000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:36.344000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:36.344000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:36.344000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.87it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.94it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.51it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.89it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.94it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s] 41%|████      | 13/32 [00:05<00:06,  2.90it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.94it/s]W0318 06:34:37.224000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:37.224000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:37.224000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:37.224000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:37.225000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:08,  2.72it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.91it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.93it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 28%|██▊       | 9/32 [00:04<00:08,  2.77it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.92it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.96it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.83it/s] 50%|█████     | 16/32 [00:06<00:05,  2.94it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.97it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.89it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.92it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.81it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.85it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.89it/s] 41%|████      | 13/32 [00:05<00:06,  2.79it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.82it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.85it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.78it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.81it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.82it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.78it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.80it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 50%|█████     | 16/32 [00:06<00:05,  2.77it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.82it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.77it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.84it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.78it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.76it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.77it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.87it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
 81%|████████▏ | 26/32 [00:10<00:02,  2.77it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.74it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.74it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.77it/s]I0318 06:34:42.948354 1099388 finetune.py:45] layer 4_v initial loss 0.00830015167593956
W0318 06:34:42.948712 1099388 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.82it/s]W0318 06:34:43.831436 1099388 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 81%|████████▏ | 26/32 [00:10<00:02,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
 84%|████████▍ | 27/32 [00:10<00:01,  2.80it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.78it/s]4_v proxy err 0.00792490690946579 tr(WHW.T) 274.61309814453125
  0%|          | 0/32 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.80it/s]W0318 06:34:45.162000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.162000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.162000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.162000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.163000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.163000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.163000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.189000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.190000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.190000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.190000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.190000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.207000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.207000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.207000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.207000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.207000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.79it/s]  3%|▎         | 1/32 [00:00<00:19,  1.61it/s]W0318 06:34:45.519000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.523000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.523000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.523000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:45.523000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:12<00:00,  2.80it/s]  6%|▋         | 2/32 [00:00<00:13,  2.24it/s]100%|██████████| 32/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
  9%|▉         | 3/32 [00:01<00:11,  2.56it/s]W0318 06:34:46.408000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:46.408000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:46.408000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:46.408000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:46.408000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:46.408000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:46.408000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:46.426000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:46.426000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:46.426000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:46.426000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:46.426000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:01<00:10,  2.67it/s]W0318 06:34:46.664000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:46.664000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:46.664000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:46.664000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:46.665000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:01<00:09,  2.73it/s]W0318 06:34:47.035000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.035000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.036000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.036000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.036000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.036000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.036000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.063000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.063000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.063000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.063000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.064000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.081000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.081000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.081000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.081000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.081000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:02<00:09,  2.78it/s]W0318 06:34:47.412000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.412000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.412000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.412000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.412000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:02<00:08,  2.83it/s]W0318 06:34:47.823000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.824000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.824000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.824000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.824000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.824000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.824000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:02<00:08,  2.90it/s]W0318 06:34:47.841000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.841000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.841000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.841000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:47.842000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:03<00:07,  2.93it/s]W0318 06:34:48.267000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.267000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.267000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.267000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.267000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.267000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.267000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.284000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.284000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.284000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.284000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.284000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:03<00:07,  2.96it/s]W0318 06:34:48.514000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.514000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.514000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.515000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.515000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.719000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.719000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.720000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.720000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:48.720000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:03<00:07,  2.96it/s]W0318 06:34:49.099000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.100000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.100000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.100000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.100000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.100000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.100000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.125000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.125000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.125000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.125000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.125000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.141000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.141000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.141000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.141000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.141000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:04<00:06,  2.99it/s]W0318 06:34:49.451000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.451000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.451000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.451000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.451000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:04<00:06,  3.02it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0318 06:34:49.637000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.637000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.637000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.637000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.637000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.637000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.637000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.655000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.655000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.655000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.655000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:49.655000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:04<00:05,  3.04it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.05it/s]W0318 06:34:50.309000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.309000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.310000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.310000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.310000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.310000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.310000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.328000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.328000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.328000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.329000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.329000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:05<00:05,  3.00it/s]W0318 06:34:50.552000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.552000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.552000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.552000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.552000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.578000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.578000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.578000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.578000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:50.578000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:05<00:05,  2.97it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 56%|█████▋    | 18/32 [00:06<00:04,  2.95it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.93it/s]W0318 06:34:51.798000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:34:51.798000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:51.798000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:51.798000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:51.798000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:34:51.798000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:51.798000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:34:51.816000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:51.817000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:51.817000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:51.817000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:51.817000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:07<00:04,  2.91it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.89it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.86it/s]W0318 06:34:52.758000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:34:52.759000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:34:52.759000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:34:52.759000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:34:52.759000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:08<00:03,  2.87it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.88it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 78%|███████▊  | 25/32 [00:08<00:02,  2.87it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.87it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.88it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.89it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.89it/s]I0318 06:34:55.416287 1100007 finetune.py:45] layer 5_v initial loss 0.009398195892572403
W0318 06:34:55.417377 1100007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 31/32 [00:10<00:00,  2.85it/s]100%|██████████| 32/32 [00:11<00:00,  2.86it/s]100%|██████████| 32/32 [00:11<00:00,  2.86it/s]
I0318 06:34:56.332695 1100176 finetune.py:45] layer 6_v initial loss 0.01061431784182787
W0318 06:34:56.332868 1100176 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:34:56.980389 1100007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:34:57.311941 1100176 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

5_v proxy err 0.008942842483520508 tr(WHW.T) 298.47540283203125
  0%|          | 0/32 [00:00<?, ?it/s]6_v proxy err 0.008558052591979504 tr(WHW.T) 443.5465087890625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.64it/s]  6%|▋         | 2/32 [00:00<00:13,  2.20it/s]  3%|▎         | 1/32 [00:00<00:20,  1.55it/s]I0318 06:34:59.294170 1100601 finetune.py:45] layer 7_v initial loss 0.011286858469247818
W0318 06:34:59.295840 1100601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:01<00:11,  2.45it/s]  6%|▋         | 2/32 [00:01<00:14,  2.10it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.58it/s]  9%|▉         | 3/32 [00:01<00:12,  2.36it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.68it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.51it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.74it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.60it/s]W0318 06:35:00.621030 1100601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:02<00:08,  2.79it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.65it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.82it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.69it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.84it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s]7_v proxy err 0.008753767237067223 tr(WHW.T) 489.935791015625
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.85it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.86it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.74it/s]  3%|▎         | 1/32 [00:00<00:19,  1.60it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.86it/s]  6%|▋         | 2/32 [00:00<00:13,  2.16it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.75it/s]I0318 06:35:02.740542 1099388 finetune.py:45] layer 4_q initial loss 0.00830780528485775
W0318 06:35:02.740906 1099388 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:04<00:06,  2.85it/s]  9%|▉         | 3/32 [00:01<00:11,  2.44it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.78it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.86it/s] 41%|████      | 13/32 [00:04<00:06,  2.81it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.57it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.81it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.85it/s]W0318 06:35:03.621127 1099388 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:02<00:10,  2.65it/s] 50%|█████     | 16/32 [00:05<00:05,  2.84it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.84it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.72it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.85it/s] 50%|█████     | 16/32 [00:05<00:05,  2.88it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.76it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.84it/s]4_q proxy err 0.0010609655873849988 tr(WHW.T) 6914.9892578125
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.91it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.79it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.85it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.90it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.81it/s]  3%|▎         | 1/32 [00:00<00:17,  1.77it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.85it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.92it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.82it/s]  6%|▋         | 2/32 [00:00<00:12,  2.37it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.85it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.93it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s]  9%|▉         | 3/32 [00:01<00:10,  2.64it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.85it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.94it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.83it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.79it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.85it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.93it/s] 41%|████      | 13/32 [00:04<00:06,  2.80it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.87it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.83it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.94it/s] 19%|█▉        | 6/32 [00:02<00:08,  2.93it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.82it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.83it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.94it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.96it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.83it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.94it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.81it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.98it/s] 50%|█████     | 16/32 [00:05<00:05,  2.83it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.94it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.81it/s] 28%|██▊       | 9/32 [00:03<00:07,  3.01it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.85it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.94it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.83it/s] 31%|███▏      | 10/32 [00:03<00:07,  3.01it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.86it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.95it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.84it/s] 34%|███▍      | 11/32 [00:03<00:06,  3.02it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.87it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.96it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.03it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.85it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.86it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.97it/s] 41%|████      | 13/32 [00:04<00:06,  3.04it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.85it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.85it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.98it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.05it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.78it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s]100%|██████████| 32/32 [00:11<00:00,  2.97it/s]100%|██████████| 32/32 [00:11<00:00,  2.81it/s]
 47%|████▋     | 15/32 [00:05<00:05,  3.06it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.82it/s] 50%|█████     | 16/32 [00:05<00:05,  3.01it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.84it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.96it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.00it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.86it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.98it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.88it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.03it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.01it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.88it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.96it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.89it/s] 72%|███████▏  | 23/32 [00:07<00:03,  2.94it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.90it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.92it/s]100%|██████████| 32/32 [00:11<00:00,  2.90it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]
 78%|███████▊  | 25/32 [00:08<00:02,  2.90it/s] 81%|████████▏ | 26/32 [00:08<00:02,  2.87it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:09<00:01,  2.87it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.88it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.88it/s]100%|██████████| 32/32 [00:11<00:00,  2.88it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]
I0318 06:35:15.938216 1100176 finetune.py:45] layer 6_q initial loss 0.010605521500110626
W0318 06:35:15.938399 1100176 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0318 06:35:16.638335 1100007 finetune.py:45] layer 5_q initial loss 0.009401346556842327
W0318 06:35:16.638525 1100007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:35:16.906301 1100176 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:35:17.927034 1100007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_q proxy err 0.0016773708630353212 tr(WHW.T) 7576.5390625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.62it/s]5_q proxy err 0.0012722049141302705 tr(WHW.T) 6770.97509765625
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:00<00:13,  2.14it/s]  9%|▉         | 3/32 [00:01<00:12,  2.39it/s]  3%|▎         | 1/32 [00:00<00:17,  1.72it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.52it/s]  6%|▋         | 2/32 [00:00<00:13,  2.28it/s]I0318 06:35:20.068943 1100601 finetune.py:45] layer 7_q initial loss 0.01127153541892767
W0318 06:35:20.069411 1100601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:02<00:10,  2.59it/s]  9%|▉         | 3/32 [00:01<00:11,  2.53it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.66it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.67it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.70it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.75it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.70it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.79it/s]W0318 06:35:21.272174 1100601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:03<00:08,  2.72it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.82it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.73it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.82it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.74it/s]I0318 06:35:22.257651 1099388 finetune.py:45] layer 4_k initial loss 0.008308744058012962
W0318 06:35:22.257982 1099388 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:03<00:08,  2.83it/s]7_q proxy err 0.00181950768455863 tr(WHW.T) 7672.17919921875
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.79it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.86it/s]  3%|▎         | 1/32 [00:00<00:17,  1.73it/s] 41%|████      | 13/32 [00:04<00:06,  2.81it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.84it/s]W0318 06:35:23.127729 1099388 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:00<00:13,  2.26it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.83it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s]  9%|▉         | 3/32 [00:01<00:11,  2.50it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.84it/s] 41%|████      | 13/32 [00:04<00:06,  2.84it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.63it/s] 50%|█████     | 16/32 [00:05<00:05,  2.87it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.85it/s]4_k proxy err 0.000734912056941539 tr(WHW.T) 10415.33203125
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.70it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.89it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.84it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.75it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.90it/s]  3%|▎         | 1/32 [00:00<00:17,  1.79it/s] 50%|█████     | 16/32 [00:05<00:05,  2.84it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.78it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.92it/s]  6%|▋         | 2/32 [00:00<00:12,  2.38it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.86it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.81it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.91it/s]  9%|▉         | 3/32 [00:01<00:10,  2.66it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.86it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.81it/s] 12%|█▎        | 4/32 [00:01<00:09,  2.80it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.92it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.87it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.90it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.81it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.93it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.87it/s] 19%|█▉        | 6/32 [00:02<00:08,  2.95it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.93it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.87it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.99it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.93it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.83it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.86it/s] 25%|██▌       | 8/32 [00:02<00:07,  3.01it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.93it/s] 41%|████      | 13/32 [00:04<00:06,  2.82it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.86it/s] 28%|██▊       | 9/32 [00:03<00:07,  3.03it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.94it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.83it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.87it/s] 31%|███▏      | 10/32 [00:03<00:07,  3.05it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.95it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.83it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.87it/s] 34%|███▍      | 11/32 [00:03<00:06,  3.06it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.95it/s] 50%|█████     | 16/32 [00:05<00:05,  2.84it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.87it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.07it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.95it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.86it/s] 41%|████      | 13/32 [00:04<00:06,  3.07it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.94it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.85it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.86it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.06it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.94it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.84it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.07it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.85it/s]100%|██████████| 32/32 [00:11<00:00,  2.95it/s]100%|██████████| 32/32 [00:11<00:00,  2.80it/s]
 62%|██████▎   | 20/32 [00:07<00:04,  2.83it/s] 50%|█████     | 16/32 [00:05<00:05,  3.08it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.85it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.85it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.03it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.87it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.86it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.00it/s]100%|██████████| 32/32 [00:11<00:00,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  2.81it/s]
 72%|███████▏  | 23/32 [00:08<00:03,  2.87it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.99it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.88it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.96it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.89it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.96it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.89it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.92it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.88it/s] 72%|███████▏  | 23/32 [00:07<00:03,  2.91it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.91it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.89it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.90it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.90it/s] 81%|████████▏ | 26/32 [00:08<00:02,  2.89it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.91it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.92it/s]100%|██████████| 32/32 [00:11<00:00,  2.81it/s]
 88%|████████▊ | 28/32 [00:09<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:09<00:01,  2.89it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.89it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.89it/s]100%|██████████| 32/32 [00:10<00:00,  2.89it/s]100%|██████████| 32/32 [00:10<00:00,  2.92it/s]
I0318 06:35:35.752942 1100176 finetune.py:45] layer 6_k initial loss 0.010604899376630783
W0318 06:35:35.753316 1100176 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:35:36.681731 1100176 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:35:37.203775 1100007 finetune.py:45] layer 5_k initial loss 0.009393044747412205
W0318 06:35:37.204013 1100007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

6_k proxy err 0.001269217929802835 tr(WHW.T) 10409.40234375
  0%|          | 0/32 [00:00<?, ?it/s]W0318 06:35:38.128223 1100007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:00<00:18,  1.64it/s]  6%|▋         | 2/32 [00:00<00:13,  2.16it/s]5_k proxy err 0.000862354354467243 tr(WHW.T) 10841.955078125
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:01<00:12,  2.40it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.53it/s]  3%|▎         | 1/32 [00:00<00:18,  1.68it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.61it/s]  6%|▋         | 2/32 [00:00<00:13,  2.22it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.66it/s]  9%|▉         | 3/32 [00:01<00:11,  2.50it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.69it/s]I0318 06:35:40.743087 1100601 finetune.py:45] layer 7_k initial loss 0.011271513998508453
W0318 06:35:40.743394 1100601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 4/32 [00:01<00:10,  2.65it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.73it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.79it/s]W0318 06:35:41.671627 1100601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:03<00:08,  2.75it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.79it/s]I0318 06:35:41.838873 1099388 finetune.py:45] layer 4_o initial loss 0.008156020194292068
W0318 06:35:41.839177 1099388 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:04<00:07,  2.76it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.83it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.82it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.83it/s]W0318 06:35:42.603948 1099388 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

7_k proxy err 0.0013822944601997733 tr(WHW.T) 10198.3701171875
  0%|          | 0/32 [00:00<?, ?it/s] 41%|████      | 13/32 [00:04<00:06,  2.83it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.84it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.82it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.81it/s]  3%|▎         | 1/32 [00:00<00:18,  1.68it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.85it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.83it/s]  6%|▋         | 2/32 [00:00<00:13,  2.23it/s]4_o proxy err 0.00665702298283577 tr(WHW.T) 5.139806747436523
  0%|          | 0/32 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:05<00:05,  2.89it/s] 41%|████      | 13/32 [00:04<00:06,  2.83it/s]  9%|▉         | 3/32 [00:01<00:11,  2.49it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.90it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.85it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.63it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.92it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.84it/s] 16%|█▌        | 5/32 [00:01<00:10,  2.69it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.92it/s]  3%|▎         | 1/32 [00:01<00:37,  1.22s/it] 50%|█████     | 16/32 [00:05<00:05,  2.83it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.74it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.93it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.77it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.94it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.84it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.79it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.94it/s]  6%|▋         | 2/32 [00:02<00:32,  1.08s/it] 59%|█████▉    | 19/32 [00:06<00:04,  2.85it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.80it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.94it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.86it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.83it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.94it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.86it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.94it/s]  9%|▉         | 3/32 [00:03<00:29,  1.02s/it] 69%|██████▉   | 22/32 [00:07<00:03,  2.84it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.82it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.93it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.85it/s] 41%|████      | 13/32 [00:04<00:06,  2.82it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.93it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.86it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.83it/s] 12%|█▎        | 4/32 [00:04<00:27,  1.00it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.94it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.86it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.85it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.94it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.87it/s] 50%|█████     | 16/32 [00:05<00:05,  2.84it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.94it/s] 16%|█▌        | 5/32 [00:05<00:26,  1.02it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.87it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.85it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.94it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.87it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.94it/s]100%|██████████| 32/32 [00:11<00:00,  2.81it/s]
 91%|█████████ | 29/32 [00:10<00:01,  2.86it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.83it/s] 19%|█▉        | 6/32 [00:06<00:25,  1.02it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.86it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.83it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.87it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.85it/s]100%|██████████| 32/32 [00:11<00:00,  2.88it/s]100%|██████████| 32/32 [00:11<00:00,  2.80it/s]
 69%|██████▉   | 22/32 [00:07<00:03,  2.85it/s] 22%|██▏       | 7/32 [00:07<00:24,  1.02it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.83it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.82it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.83it/s] 25%|██▌       | 8/32 [00:08<00:23,  1.01it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.82it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.85it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.83it/s] 28%|██▊       | 9/32 [00:09<00:22,  1.00it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.80it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.78it/s] 31%|███▏      | 10/32 [00:10<00:22,  1.01s/it] 97%|█████████▋| 31/32 [00:11<00:00,  2.76it/s]100%|██████████| 32/32 [00:11<00:00,  2.75it/s]100%|██████████| 32/32 [00:11<00:00,  2.77it/s]
 34%|███▍      | 11/32 [00:11<00:21,  1.01s/it]I0318 06:35:55.640825 1100176 finetune.py:45] layer 6_o initial loss 0.010217837058007717
W0318 06:35:55.641006 1100176 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:12<00:20,  1.01s/it]W0318 06:35:56.441480 1100176 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 13/32 [00:13<00:18,  1.00it/s]I0318 06:35:57.147152 1100007 finetune.py:45] layer 5_o initial loss 0.009214398451149464
W0318 06:35:57.147364 1100007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

6_o proxy err 0.007579140830785036 tr(WHW.T) 11.564380645751953
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:14<00:17,  1.01it/s]W0318 06:35:58.138056 1100007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 15/32 [00:14<00:16,  1.02it/s]  3%|▎         | 1/32 [00:01<00:39,  1.27s/it]5_o proxy err 0.007066984660923481 tr(WHW.T) 7.947142601013184
  0%|          | 0/32 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:15<00:15,  1.03it/s]  6%|▋         | 2/32 [00:02<00:33,  1.12s/it] 53%|█████▎    | 17/32 [00:16<00:14,  1.04it/s]  3%|▎         | 1/32 [00:01<00:41,  1.33s/it]  9%|▉         | 3/32 [00:03<00:30,  1.07s/it]I0318 06:36:01.020457 1100601 finetune.py:45] layer 7_o initial loss 0.011230729520320892
W0318 06:36:01.020791 1100601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▋    | 18/32 [00:17<00:13,  1.04it/s]  6%|▋         | 2/32 [00:02<00:35,  1.17s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.05s/it]W0318 06:36:02.034703 1100601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 59%|█████▉    | 19/32 [00:18<00:12,  1.04it/s]  9%|▉         | 3/32 [00:03<00:32,  1.12s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.04s/it]7_o proxy err 0.008399316109716892 tr(WHW.T) 15.11335563659668
  0%|          | 0/32 [00:00<?, ?it/s] 62%|██████▎   | 20/32 [00:19<00:11,  1.04it/s] 12%|█▎        | 4/32 [00:04<00:30,  1.08s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.03s/it] 66%|██████▌   | 21/32 [00:20<00:10,  1.04it/s]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.06s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.03s/it] 69%|██████▉   | 22/32 [00:21<00:09,  1.04it/s]  6%|▋         | 2/32 [00:02<00:34,  1.14s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.05s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.03s/it] 72%|███████▏  | 23/32 [00:22<00:08,  1.04it/s]  9%|▉         | 3/32 [00:03<00:31,  1.09s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.05s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.02s/it] 75%|███████▌  | 24/32 [00:23<00:07,  1.04it/s] 12%|█▎        | 4/32 [00:04<00:29,  1.07s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.05s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.02s/it] 78%|███████▊  | 25/32 [00:24<00:06,  1.04it/s] 16%|█▌        | 5/32 [00:05<00:28,  1.06s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.04s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.02s/it] 81%|████████▏ | 26/32 [00:25<00:05,  1.03it/s] 19%|█▉        | 6/32 [00:06<00:27,  1.05s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.04s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.02s/it] 84%|████████▍ | 27/32 [00:26<00:04,  1.03it/s] 22%|██▏       | 7/32 [00:07<00:26,  1.05s/it] 41%|████      | 13/32 [00:13<00:19,  1.02s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.04s/it] 88%|████████▊ | 28/32 [00:27<00:03,  1.03it/s] 25%|██▌       | 8/32 [00:08<00:25,  1.05s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.02s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.04s/it] 91%|█████████ | 29/32 [00:28<00:02,  1.04it/s] 28%|██▊       | 9/32 [00:09<00:24,  1.05s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.02s/it] 41%|████      | 13/32 [00:13<00:19,  1.04s/it] 94%|█████████▍| 30/32 [00:29<00:01,  1.03it/s] 31%|███▏      | 10/32 [00:10<00:23,  1.05s/it] 50%|█████     | 16/32 [00:16<00:16,  1.02s/it] 97%|█████████▋| 31/32 [00:30<00:00,  1.04it/s] 44%|████▍     | 14/32 [00:14<00:18,  1.04s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.04s/it]100%|██████████| 32/32 [00:31<00:00,  1.04it/s]100%|██████████| 32/32 [00:31<00:00,  1.02it/s]
 53%|█████▎    | 17/32 [00:17<00:15,  1.02s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.04s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.05s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.03s/it] 50%|█████     | 16/32 [00:16<00:16,  1.04s/it] 41%|████      | 13/32 [00:13<00:19,  1.05s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.03s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.03s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.05s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.03s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.04s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.04s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.03s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.03s/it] 50%|█████     | 16/32 [00:16<00:16,  1.04s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.02s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.03s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.04s/it]W0318 06:36:21.245000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.245000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.245000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.245000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.246000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.246000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.246000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.275000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.275000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.275000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.275000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.275000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.290000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.290000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.290000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.290000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.290000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:23<00:09,  1.02s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.03s/it]W0318 06:36:21.439000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.439000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.439000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.439000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.439000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.654000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.654000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.654000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.654000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.654000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.654000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.655000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.675000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.675000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.675000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.675000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.675000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.738000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.738000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.738000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.738000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:21.738000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:18<00:14,  1.04s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.02s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.03s/it]W0318 06:36:22.570000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:22.867000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:22.867000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:22.867000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:22.867000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:22.867000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:22.867000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:22.867000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:36:22.888000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:22.888000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:22.888000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:22.888000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:22.888000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:23.136000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:23.137000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:23.137000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:23.137000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:23.137000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 19/32 [00:20<00:13,  1.04s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.02s/it]W0318 06:36:23.387000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:24<00:09,  1.03s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.04s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.02s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.03s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.03s/it] 84%|████████▍ | 27/32 [00:27<00:05,  1.04s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.03s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.03s/it] 88%|████████▊ | 28/32 [00:28<00:04,  1.05s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.02s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.03s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.02s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.06s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.03s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.02s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.07s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.03s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.02s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.07s/it]I0318 06:36:29.838814 1099388 finetune.py:45] layer 4_up initial loss 0.008101164363324642
W0318 06:36:29.839010 1099388 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:27<00:06,  1.02s/it]W0318 06:36:30.613174 1099388 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:31<00:02,  1.02s/it]100%|██████████| 32/32 [00:33<00:00,  1.06s/it]100%|██████████| 32/32 [00:33<00:00,  1.04s/it]
 84%|████████▍ | 27/32 [00:28<00:05,  1.02s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.02s/it]4_up proxy err 0.010256903246045113 tr(WHW.T) 397.6959533691406
  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:29<00:04,  1.02s/it]100%|██████████| 32/32 [00:33<00:00,  1.02s/it]100%|██████████| 32/32 [00:33<00:00,  1.04s/it]
  3%|▎         | 1/32 [00:01<00:38,  1.25s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.03s/it]  6%|▋         | 2/32 [00:02<00:32,  1.08s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.03s/it]  9%|▉         | 3/32 [00:03<00:29,  1.03s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.03s/it] 12%|█▎        | 4/32 [00:04<00:28,  1.00s/it]100%|██████████| 32/32 [00:33<00:00,  1.03s/it]100%|██████████| 32/32 [00:33<00:00,  1.04s/it]
 16%|█▌        | 5/32 [00:05<00:26,  1.01it/s]W0318 06:36:37.054000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.054000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.055000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.055000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.055000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.055000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.055000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.083000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.083000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.083000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.083000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.083000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.098000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.098000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.098000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.098000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.098000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.247000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.247000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.247000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.247000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.248000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.466000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.466000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.466000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.466000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.466000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.466000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.466000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.486000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.486000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.486000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.486000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.486000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.548000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.548000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.548000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.548000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:37.548000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:06<00:25,  1.02it/s]W0318 06:36:38.382000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:38.681000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:38.681000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:36:38.681000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:38.681000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:38.681000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:38.681000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:38.681000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:38.701000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:38.701000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:38.701000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:38.701000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:38.701000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:38.946000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:38.946000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:38.946000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:38.946000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:38.946000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:07<00:24,  1.03it/s]W0318 06:36:39.074000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.074000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.074000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.074000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.074000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.075000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.075000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.103000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.103000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.103000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.103000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.103000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.118000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.118000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.118000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.118000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.118000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.196000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.271000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.271000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.271000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.271000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.271000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.487000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.487000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.488000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.488000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.488000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.488000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.488000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.507000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.507000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.507000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.507000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.507000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.570000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.570000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.570000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.570000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:39.570000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:08<00:23,  1.03it/s]W0318 06:36:40.420000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:40.718000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:40.718000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:40.718000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:40.718000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:36:40.718000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:40.718000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:40.718000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:40.740000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:40.740000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:40.740000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:40.740000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:40.740000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:09<00:22,  1.02it/s]W0318 06:36:40.992000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:40.992000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:40.992000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:40.992000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:40.992000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:41.253000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:10<00:21,  1.01it/s] 34%|███▍      | 11/32 [00:11<00:21,  1.00s/it]W0318 06:36:43.027000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.027000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.027000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.027000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.027000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.027000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.027000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.062000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.062000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.062000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.062000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.062000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.078000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.078000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.078000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.079000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.079000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.243000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.243000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.243000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.243000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.243000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.481000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.481000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.481000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.481000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.482000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.482000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.482000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.505000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.505000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.505000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.505000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.505000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.573000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.573000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.573000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.573000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:43.573000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:12<00:20,  1.01s/it]W0318 06:36:44.491000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:44.825000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:44.825000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:44.825000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:44.826000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:44.826000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:36:44.826000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:36:44.826000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:36:44.852000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:44.852000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:44.853000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:44.853000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:44.853000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:13<00:19,  1.01s/it]W0318 06:36:45.124000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:36:45.124000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:36:45.124000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:36:45.124000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:36:45.124000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
I0318 06:36:45.351866 1100176 finetune.py:45] layer 6_up initial loss 0.010097838938236237
W0318 06:36:45.352091 1100176 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:36:45.400000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:14<00:18,  1.01s/it]W0318 06:36:46.140272 1100176 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 15/32 [00:15<00:16,  1.00it/s]6_up proxy err 0.01021210104227066 tr(WHW.T) 617.2607421875
  0%|          | 0/32 [00:00<?, ?it/s]I0318 06:36:47.524453 1100007 finetune.py:45] layer 5_up initial loss 0.009088506922125816
W0318 06:36:47.524662 1100007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 50%|█████     | 16/32 [00:16<00:15,  1.01it/s]W0318 06:36:48.354183 1100007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:39,  1.29s/it] 53%|█████▎    | 17/32 [00:16<00:14,  1.02it/s]5_up proxy err 0.01009354367852211 tr(WHW.T) 506.6407775878906
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:02<00:33,  1.12s/it] 56%|█████▋    | 18/32 [00:17<00:13,  1.03it/s]  9%|▉         | 3/32 [00:03<00:31,  1.07s/it] 59%|█████▉    | 19/32 [00:18<00:12,  1.03it/s]  3%|▎         | 1/32 [00:01<00:42,  1.38s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.05s/it] 62%|██████▎   | 20/32 [00:19<00:11,  1.04it/s]I0318 06:36:51.878261 1100601 finetune.py:45] layer 7_up initial loss 0.011018112301826477
W0318 06:36:51.878493 1100601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:02<00:36,  1.20s/it] 66%|██████▌   | 21/32 [00:20<00:10,  1.03it/s] 16%|█▌        | 5/32 [00:05<00:28,  1.04s/it]W0318 06:36:52.774023 1100601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:03<00:32,  1.13s/it] 69%|██████▉   | 22/32 [00:21<00:09,  1.04it/s] 19%|█▉        | 6/32 [00:06<00:26,  1.03s/it]7_up proxy err 0.01000197883695364 tr(WHW.T) 735.853759765625
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:04<00:30,  1.10s/it] 72%|███████▏  | 23/32 [00:22<00:08,  1.04it/s] 22%|██▏       | 7/32 [00:07<00:25,  1.03s/it] 16%|█▌        | 5/32 [00:05<00:29,  1.08s/it]  3%|▎         | 1/32 [00:01<00:40,  1.30s/it] 75%|███████▌  | 24/32 [00:23<00:07,  1.04it/s] 25%|██▌       | 8/32 [00:08<00:24,  1.03s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.07s/it]  6%|▋         | 2/32 [00:02<00:34,  1.14s/it] 78%|███████▊  | 25/32 [00:24<00:06,  1.04it/s] 28%|██▊       | 9/32 [00:09<00:23,  1.03s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.06s/it]  9%|▉         | 3/32 [00:03<00:31,  1.10s/it] 81%|████████▏ | 26/32 [00:25<00:05,  1.04it/s] 31%|███▏      | 10/32 [00:10<00:22,  1.02s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.06s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.07s/it] 84%|████████▍ | 27/32 [00:26<00:04,  1.03it/s] 34%|███▍      | 11/32 [00:11<00:21,  1.02s/it] 28%|██▊       | 9/32 [00:09<00:24,  1.05s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.05s/it] 88%|████████▊ | 28/32 [00:27<00:03,  1.03it/s] 38%|███▊      | 12/32 [00:12<00:20,  1.02s/it] 91%|█████████ | 29/32 [00:28<00:02,  1.03it/s] 31%|███▏      | 10/32 [00:10<00:23,  1.05s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.05s/it] 41%|████      | 13/32 [00:13<00:19,  1.02s/it] 94%|█████████▍| 30/32 [00:29<00:01,  1.04it/s] 34%|███▍      | 11/32 [00:11<00:21,  1.05s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.04s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.02s/it] 97%|█████████▋| 31/32 [00:30<00:00,  1.03it/s] 38%|███▊      | 12/32 [00:12<00:20,  1.05s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.04s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.02s/it]100%|██████████| 32/32 [00:31<00:00,  1.03it/s]100%|██████████| 32/32 [00:31<00:00,  1.02it/s]
 41%|████      | 13/32 [00:13<00:19,  1.05s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.04s/it] 50%|█████     | 16/32 [00:16<00:16,  1.03s/it] 44%|████▍     | 14/32 [00:15<00:18,  1.04s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.04s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.04s/it] 47%|████▋     | 15/32 [00:16<00:17,  1.04s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.03s/it] 56%|█████▋    | 18/32 [00:18<00:15,  1.09s/it] 38%|███▊      | 12/32 [00:12<00:21,  1.08s/it] 50%|█████     | 16/32 [00:17<00:17,  1.09s/it] 59%|█████▉    | 19/32 [00:19<00:14,  1.09s/it] 41%|████      | 13/32 [00:13<00:20,  1.07s/it] 53%|█████▎    | 17/32 [00:18<00:16,  1.08s/it] 62%|██████▎   | 20/32 [00:21<00:13,  1.09s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.05s/it] 56%|█████▋    | 18/32 [00:19<00:14,  1.06s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.08s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.04s/it] 59%|█████▉    | 19/32 [00:20<00:13,  1.05s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.08s/it]I0318 06:37:10.952952 1099388 finetune.py:45] layer 4_gate initial loss 0.007947387173771858
W0318 06:37:10.953197 1099388 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 50%|█████     | 16/32 [00:16<00:16,  1.04s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.05s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.07s/it]W0318 06:37:11.759073 1099388 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 17/32 [00:17<00:15,  1.04s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.05s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.07s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.04s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.04s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.06s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.03s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.04s/it]4_gate proxy err 0.006142045371234417 tr(WHW.T) 821.185546875
  0%|          | 0/86 [00:00<?, ?it/s] 81%|████████▏ | 26/32 [00:27<00:06,  1.05s/it]  1%|          | 1/86 [00:00<00:48,  1.77it/s] 62%|██████▎   | 20/32 [00:21<00:12,  1.03s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.03s/it]  2%|▏         | 2/86 [00:00<00:35,  2.36it/s]  3%|▎         | 3/86 [00:01<00:31,  2.64it/s] 84%|████████▍ | 27/32 [00:28<00:05,  1.04s/it]  5%|▍         | 4/86 [00:01<00:29,  2.78it/s] 66%|██████▌   | 21/32 [00:22<00:11,  1.03s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.04s/it]  6%|▌         | 5/86 [00:01<00:28,  2.87it/s]  7%|▋         | 6/86 [00:02<00:27,  2.94it/s] 88%|████████▊ | 28/32 [00:29<00:04,  1.03s/it]  8%|▊         | 7/86 [00:02<00:26,  2.98it/s] 69%|██████▉   | 22/32 [00:23<00:10,  1.03s/it]  9%|▉         | 8/86 [00:02<00:26,  3.00it/s] 81%|████████▏ | 26/32 [00:27<00:06,  1.04s/it] 10%|█         | 9/86 [00:03<00:25,  3.02it/s] 91%|█████████ | 29/32 [00:30<00:03,  1.03s/it] 12%|█▏        | 10/86 [00:03<00:25,  3.02it/s] 72%|███████▏  | 23/32 [00:24<00:09,  1.03s/it] 13%|█▎        | 11/86 [00:03<00:24,  3.02it/s] 84%|████████▍ | 27/32 [00:28<00:05,  1.04s/it] 14%|█▍        | 12/86 [00:04<00:24,  3.02it/s] 15%|█▌        | 13/86 [00:04<00:24,  3.03it/s] 94%|█████████▍| 30/32 [00:31<00:02,  1.03s/it] 16%|█▋        | 14/86 [00:04<00:23,  3.04it/s] 75%|███████▌  | 24/32 [00:25<00:08,  1.03s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.04s/it] 17%|█▋        | 15/86 [00:05<00:23,  3.04it/s] 19%|█▊        | 16/86 [00:05<00:23,  3.04it/s] 97%|█████████▋| 31/32 [00:32<00:01,  1.03s/it] 20%|█▉        | 17/86 [00:05<00:22,  3.04it/s] 78%|███████▊  | 25/32 [00:26<00:07,  1.03s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.03s/it] 21%|██        | 18/86 [00:06<00:22,  3.04it/s] 22%|██▏       | 19/86 [00:06<00:22,  3.04it/s]100%|██████████| 32/32 [00:33<00:00,  1.03s/it]100%|██████████| 32/32 [00:33<00:00,  1.05s/it]
 23%|██▎       | 20/86 [00:06<00:21,  3.04it/s] 81%|████████▏ | 26/32 [00:27<00:06,  1.03s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.04s/it] 24%|██▍       | 21/86 [00:07<00:21,  2.99it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.94it/s] 27%|██▋       | 23/86 [00:07<00:21,  2.96it/s] 84%|████████▍ | 27/32 [00:28<00:05,  1.03s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.04s/it] 28%|██▊       | 24/86 [00:08<00:20,  2.96it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.98it/s] 30%|███       | 26/86 [00:08<00:20,  2.98it/s] 88%|████████▊ | 28/32 [00:29<00:04,  1.03s/it]100%|██████████| 32/32 [00:33<00:00,  1.03s/it]100%|██████████| 32/32 [00:33<00:00,  1.06s/it]
 31%|███▏      | 27/86 [00:09<00:20,  2.95it/s] 33%|███▎      | 28/86 [00:09<00:19,  2.93it/s] 34%|███▎      | 29/86 [00:09<00:19,  2.92it/s] 91%|█████████ | 29/32 [00:30<00:03,  1.04s/it] 35%|███▍      | 30/86 [00:10<00:19,  2.91it/s] 36%|███▌      | 31/86 [00:10<00:19,  2.89it/s] 37%|███▋      | 32/86 [00:10<00:18,  2.88it/s] 94%|█████████▍| 30/32 [00:31<00:02,  1.04s/it] 38%|███▊      | 33/86 [00:11<00:18,  2.88it/s] 40%|███▉      | 34/86 [00:11<00:18,  2.88it/s] 41%|████      | 35/86 [00:11<00:17,  2.89it/s] 97%|█████████▋| 31/32 [00:32<00:01,  1.05s/it] 42%|████▏     | 36/86 [00:12<00:17,  2.90it/s] 43%|████▎     | 37/86 [00:12<00:17,  2.88it/s] 44%|████▍     | 38/86 [00:12<00:16,  2.89it/s]100%|██████████| 32/32 [00:33<00:00,  1.07s/it]100%|██████████| 32/32 [00:33<00:00,  1.05s/it]
 45%|████▌     | 39/86 [00:13<00:16,  2.88it/s] 47%|████▋     | 40/86 [00:13<00:15,  2.88it/s]I0318 06:37:28.100360 1100176 finetune.py:45] layer 6_gate initial loss 0.009908990934491158
W0318 06:37:28.100571 1100176 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 48%|████▊     | 41/86 [00:14<00:15,  2.92it/s] 49%|████▉     | 42/86 [00:14<00:14,  2.98it/s]W0318 06:37:28.811424 1100176 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 50%|█████     | 43/86 [00:14<00:14,  3.00it/s] 51%|█████     | 44/86 [00:15<00:14,  2.99it/s] 52%|█████▏    | 45/86 [00:15<00:13,  2.97it/s] 53%|█████▎    | 46/86 [00:15<00:13,  2.99it/s] 55%|█████▍    | 47/86 [00:16<00:12,  3.01it/s]I0318 06:37:30.664608 1100007 finetune.py:45] layer 5_gate initial loss 0.008959372527897358
W0318 06:37:30.664994 1100007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▌    | 48/86 [00:16<00:12,  3.02it/s] 57%|█████▋    | 49/86 [00:16<00:12,  3.00it/s] 58%|█████▊    | 50/86 [00:17<00:11,  3.02it/s]6_gate proxy err 0.005285555496811867 tr(WHW.T) 1554.726806640625
  0%|          | 0/86 [00:00<?, ?it/s]W0318 06:37:31.430569 1100007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 59%|█████▉    | 51/86 [00:17<00:11,  3.05it/s]  1%|          | 1/86 [00:00<00:49,  1.73it/s] 60%|██████    | 52/86 [00:17<00:11,  3.05it/s]  2%|▏         | 2/86 [00:00<00:36,  2.29it/s] 62%|██████▏   | 53/86 [00:17<00:10,  3.06it/s]  3%|▎         | 3/86 [00:01<00:32,  2.54it/s] 63%|██████▎   | 54/86 [00:18<00:10,  3.07it/s]  5%|▍         | 4/86 [00:01<00:30,  2.69it/s] 64%|██████▍   | 55/86 [00:18<00:10,  3.08it/s] 65%|██████▌   | 56/86 [00:18<00:09,  3.08it/s]  6%|▌         | 5/86 [00:01<00:29,  2.78it/s] 66%|██████▋   | 57/86 [00:19<00:09,  3.09it/s]  7%|▋         | 6/86 [00:02<00:28,  2.83it/s] 67%|██████▋   | 58/86 [00:19<00:09,  3.09it/s]  8%|▊         | 7/86 [00:02<00:27,  2.86it/s] 69%|██████▊   | 59/86 [00:19<00:08,  3.09it/s]  9%|▉         | 8/86 [00:02<00:27,  2.88it/s]5_gate proxy err 0.005791390314698219 tr(WHW.T) 1104.8677978515625
  0%|          | 0/86 [00:00<?, ?it/s] 70%|██████▉   | 60/86 [00:20<00:08,  3.09it/s] 10%|█         | 9/86 [00:03<00:26,  2.89it/s] 71%|███████   | 61/86 [00:20<00:08,  3.11it/s]  1%|          | 1/86 [00:00<00:53,  1.60it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.92it/s] 72%|███████▏  | 62/86 [00:20<00:07,  3.13it/s]I0318 06:37:35.269823 1100601 finetune.py:45] layer 7_gate initial loss 0.010794145986437798
W0318 06:37:35.270256 1100601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  2%|▏         | 2/86 [00:00<00:39,  2.14it/s] 13%|█▎        | 11/86 [00:03<00:25,  2.93it/s] 73%|███████▎  | 63/86 [00:21<00:07,  3.11it/s]  3%|▎         | 3/86 [00:01<00:34,  2.41it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.91it/s] 74%|███████▍  | 64/86 [00:21<00:07,  3.09it/s]W0318 06:37:36.049352 1100601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 15%|█▌        | 13/86 [00:04<00:25,  2.91it/s]  5%|▍         | 4/86 [00:01<00:32,  2.55it/s] 76%|███████▌  | 65/86 [00:21<00:06,  3.10it/s] 16%|█▋        | 14/86 [00:05<00:24,  2.90it/s]  6%|▌         | 5/86 [00:02<00:30,  2.63it/s] 77%|███████▋  | 66/86 [00:22<00:06,  3.09it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.90it/s]  7%|▋         | 6/86 [00:02<00:30,  2.65it/s] 78%|███████▊  | 67/86 [00:22<00:06,  3.09it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.89it/s]  8%|▊         | 7/86 [00:02<00:29,  2.65it/s] 79%|███████▉  | 68/86 [00:22<00:05,  3.09it/s] 20%|█▉        | 17/86 [00:06<00:23,  2.90it/s] 80%|████████  | 69/86 [00:23<00:05,  3.09it/s]  9%|▉         | 8/86 [00:03<00:28,  2.69it/s] 21%|██        | 18/86 [00:06<00:23,  2.91it/s] 81%|████████▏ | 70/86 [00:23<00:05,  3.09it/s] 10%|█         | 9/86 [00:03<00:28,  2.72it/s] 22%|██▏       | 19/86 [00:06<00:23,  2.91it/s] 83%|████████▎ | 71/86 [00:23<00:04,  3.07it/s] 12%|█▏        | 10/86 [00:03<00:27,  2.74it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.91it/s] 84%|████████▎ | 72/86 [00:24<00:04,  3.06it/s] 13%|█▎        | 11/86 [00:04<00:27,  2.75it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.91it/s] 85%|████████▍ | 73/86 [00:24<00:04,  3.06it/s]7_gate proxy err 0.0050729745998978615 tr(WHW.T) 1876.038818359375
  0%|          | 0/86 [00:00<?, ?it/s] 14%|█▍        | 12/86 [00:04<00:26,  2.78it/s] 86%|████████▌ | 74/86 [00:24<00:03,  3.07it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.91it/s] 15%|█▌        | 13/86 [00:04<00:26,  2.80it/s] 87%|████████▋ | 75/86 [00:25<00:03,  3.07it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.90it/s]  1%|          | 1/86 [00:00<00:52,  1.62it/s] 16%|█▋        | 14/86 [00:05<00:25,  2.80it/s] 88%|████████▊ | 76/86 [00:25<00:03,  3.07it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.89it/s]  2%|▏         | 2/86 [00:00<00:39,  2.15it/s] 17%|█▋        | 15/86 [00:05<00:25,  2.78it/s] 90%|████████▉ | 77/86 [00:25<00:02,  3.07it/s] 29%|██▉       | 25/86 [00:08<00:21,  2.90it/s]  3%|▎         | 3/86 [00:01<00:34,  2.40it/s] 19%|█▊        | 16/86 [00:06<00:25,  2.79it/s] 91%|█████████ | 78/86 [00:26<00:02,  3.06it/s] 30%|███       | 26/86 [00:09<00:20,  2.88it/s]  5%|▍         | 4/86 [00:01<00:32,  2.54it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.79it/s] 92%|█████████▏| 79/86 [00:26<00:02,  3.07it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.89it/s]  6%|▌         | 5/86 [00:02<00:31,  2.61it/s] 21%|██        | 18/86 [00:06<00:24,  2.80it/s] 93%|█████████▎| 80/86 [00:26<00:01,  3.05it/s] 33%|███▎      | 28/86 [00:09<00:20,  2.89it/s]  7%|▋         | 6/86 [00:02<00:29,  2.68it/s] 94%|█████████▍| 81/86 [00:27<00:01,  3.05it/s] 22%|██▏       | 19/86 [00:07<00:23,  2.82it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.88it/s]  8%|▊         | 7/86 [00:02<00:29,  2.72it/s] 95%|█████████▌| 82/86 [00:27<00:01,  3.05it/s] 23%|██▎       | 20/86 [00:07<00:23,  2.82it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.88it/s]  9%|▉         | 8/86 [00:03<00:28,  2.75it/s] 97%|█████████▋| 83/86 [00:27<00:00,  3.05it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.83it/s] 36%|███▌      | 31/86 [00:10<00:19,  2.88it/s] 10%|█         | 9/86 [00:03<00:27,  2.77it/s] 98%|█████████▊| 84/86 [00:28<00:00,  3.05it/s] 26%|██▌       | 22/86 [00:08<00:22,  2.81it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.87it/s] 99%|█████████▉| 85/86 [00:28<00:00,  3.04it/s] 12%|█▏        | 10/86 [00:03<00:27,  2.78it/s] 27%|██▋       | 23/86 [00:08<00:22,  2.82it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.86it/s]100%|██████████| 86/86 [00:28<00:00,  3.04it/s]100%|██████████| 86/86 [00:28<00:00,  3.00it/s]
 13%|█▎        | 11/86 [00:04<00:26,  2.79it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.83it/s] 40%|███▉      | 34/86 [00:11<00:18,  2.87it/s] 14%|█▍        | 12/86 [00:04<00:26,  2.79it/s] 29%|██▉       | 25/86 [00:09<00:21,  2.83it/s] 41%|████      | 35/86 [00:12<00:18,  2.82it/s] 15%|█▌        | 13/86 [00:04<00:26,  2.76it/s] 30%|███       | 26/86 [00:09<00:21,  2.83it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.78it/s] 16%|█▋        | 14/86 [00:05<00:25,  2.79it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.83it/s] 43%|████▎     | 37/86 [00:13<00:17,  2.81it/s] 17%|█▋        | 15/86 [00:05<00:25,  2.79it/s] 33%|███▎      | 28/86 [00:10<00:20,  2.84it/s] 44%|████▍     | 38/86 [00:13<00:17,  2.80it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.81it/s] 34%|███▎      | 29/86 [00:10<00:20,  2.81it/s] 45%|████▌     | 39/86 [00:13<00:16,  2.82it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.80it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.82it/s] 47%|████▋     | 40/86 [00:14<00:16,  2.84it/s] 21%|██        | 18/86 [00:06<00:24,  2.82it/s] 36%|███▌      | 31/86 [00:11<00:19,  2.84it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.85it/s] 22%|██▏       | 19/86 [00:07<00:23,  2.82it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.84it/s] 49%|████▉     | 42/86 [00:14<00:15,  2.86it/s] 23%|██▎       | 20/86 [00:07<00:23,  2.84it/s] 38%|███▊      | 33/86 [00:12<00:18,  2.86it/s] 50%|█████     | 43/86 [00:15<00:14,  2.87it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.84it/s] 40%|███▉      | 34/86 [00:12<00:18,  2.86it/s] 51%|█████     | 44/86 [00:15<00:14,  2.87it/s] 26%|██▌       | 22/86 [00:08<00:22,  2.84it/s] 41%|████      | 35/86 [00:12<00:17,  2.85it/s] 52%|█████▏    | 45/86 [00:15<00:14,  2.87it/s] 27%|██▋       | 23/86 [00:08<00:22,  2.82it/s] 42%|████▏     | 36/86 [00:13<00:17,  2.84it/s] 53%|█████▎    | 46/86 [00:16<00:13,  2.86it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.83it/s] 43%|████▎     | 37/86 [00:13<00:17,  2.83it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.86it/s] 29%|██▉       | 25/86 [00:09<00:21,  2.82it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.83it/s] 56%|█████▌    | 48/86 [00:16<00:13,  2.87it/s] 30%|███       | 26/86 [00:09<00:21,  2.81it/s] 45%|████▌     | 39/86 [00:14<00:16,  2.83it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.87it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.82it/s] 47%|████▋     | 40/86 [00:14<00:16,  2.81it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.87it/s] 33%|███▎      | 28/86 [00:10<00:20,  2.82it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.83it/s] 59%|█████▉    | 51/86 [00:17<00:12,  2.88it/s]W0318 06:37:49.371000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.371000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.371000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.371000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.371000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.371000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.372000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.411000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.411000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.412000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.412000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.412000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.426000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.427000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.427000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.427000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.427000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 34%|███▎      | 29/86 [00:10<00:20,  2.81it/s] 49%|████▉     | 42/86 [00:15<00:15,  2.83it/s]W0318 06:37:49.594000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.594000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.594000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.594000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.594000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 60%|██████    | 52/86 [00:18<00:11,  2.89it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.84it/s]W0318 06:37:49.906000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.906000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.906000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.906000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.906000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.906000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.906000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 43/86 [00:15<00:15,  2.83it/s]W0318 06:37:49.937000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.937000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.937000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.937000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:37:49.937000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 62%|██████▏   | 53/86 [00:18<00:11,  2.89it/s]W0318 06:37:50.004000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:37:50.004000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:37:50.004000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:37:50.004000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:37:50.004000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 36%|███▌      | 31/86 [00:11<00:19,  2.85it/s] 51%|█████     | 44/86 [00:15<00:14,  2.83it/s] 63%|██████▎   | 54/86 [00:18<00:11,  2.89it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.85it/s] 52%|█████▏    | 45/86 [00:16<00:14,  2.83it/s] 64%|██████▍   | 55/86 [00:19<00:10,  2.89it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.86it/s] 53%|█████▎    | 46/86 [00:16<00:14,  2.83it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.89it/s]W0318 06:37:51.147000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.159000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.167000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.167000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 40%|███▉      | 34/86 [00:12<00:18,  2.87it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.84it/s] 66%|██████▋   | 57/86 [00:19<00:10,  2.89it/s] 41%|████      | 35/86 [00:12<00:17,  2.88it/s]W0318 06:37:51.595000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.595000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.595000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.595000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.596000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.596000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.596000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.629000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.629000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.629000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.629000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.629000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 56%|█████▌    | 48/86 [00:17<00:13,  2.85it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.89it/s] 42%|████▏     | 36/86 [00:13<00:17,  2.87it/s]W0318 06:37:51.965000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.965000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.965000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.965000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.965000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.965000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.965000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:37:51.965000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
 57%|█████▋    | 49/86 [00:17<00:13,  2.84it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.88it/s]W0318 06:37:52.248000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:37:52.248000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:37:52.248000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:37:52.249000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:37:52.249000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 43%|████▎     | 37/86 [00:13<00:17,  2.88it/s] 58%|█████▊    | 50/86 [00:18<00:12,  2.84it/s] 70%|██████▉   | 60/86 [00:21<00:09,  2.88it/s]W0318 06:37:52.570000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:37:52.575000 140592675292992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 38/86 [00:13<00:16,  2.87it/s] 59%|█████▉    | 51/86 [00:18<00:12,  2.84it/s] 71%|███████   | 61/86 [00:21<00:08,  2.87it/s] 45%|████▌     | 39/86 [00:14<00:16,  2.86it/s] 60%|██████    | 52/86 [00:18<00:12,  2.83it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.87it/s] 47%|████▋     | 40/86 [00:14<00:16,  2.83it/s] 73%|███████▎  | 63/86 [00:22<00:07,  2.88it/s] 62%|██████▏   | 53/86 [00:19<00:11,  2.80it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.86it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.88it/s] 63%|██████▎   | 54/86 [00:19<00:11,  2.83it/s] 49%|████▉     | 42/86 [00:15<00:15,  2.85it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.86it/s] 64%|██████▍   | 55/86 [00:19<00:11,  2.81it/s] 50%|█████     | 43/86 [00:15<00:14,  2.87it/s] 65%|██████▌   | 56/86 [00:20<00:10,  2.82it/s] 77%|███████▋  | 66/86 [00:23<00:07,  2.83it/s] 51%|█████     | 44/86 [00:15<00:14,  2.88it/s] 66%|██████▋   | 57/86 [00:20<00:10,  2.84it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.81it/s] 52%|█████▏    | 45/86 [00:16<00:14,  2.89it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.84it/s] 79%|███████▉  | 68/86 [00:23<00:06,  2.78it/s] 53%|█████▎    | 46/86 [00:16<00:13,  2.87it/s] 69%|██████▊   | 59/86 [00:21<00:09,  2.83it/s] 80%|████████  | 69/86 [00:24<00:06,  2.77it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.86it/s] 70%|██████▉   | 60/86 [00:21<00:09,  2.82it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.78it/s] 56%|█████▌    | 48/86 [00:17<00:13,  2.85it/s] 71%|███████   | 61/86 [00:21<00:08,  2.83it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.77it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.87it/s] 72%|███████▏  | 62/86 [00:22<00:08,  2.86it/s] 84%|████████▎ | 72/86 [00:25<00:05,  2.77it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.90it/s] 73%|███████▎  | 63/86 [00:22<00:08,  2.85it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.77it/s] 59%|█████▉    | 51/86 [00:18<00:12,  2.89it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.85it/s] 86%|████████▌ | 74/86 [00:26<00:04,  2.76it/s] 60%|██████    | 52/86 [00:18<00:11,  2.88it/s] 76%|███████▌  | 65/86 [00:23<00:07,  2.86it/s] 87%|████████▋ | 75/86 [00:26<00:03,  2.75it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.89it/s] 77%|███████▋  | 66/86 [00:23<00:07,  2.85it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.76it/s] 63%|██████▎   | 54/86 [00:19<00:11,  2.89it/s] 78%|███████▊  | 67/86 [00:24<00:06,  2.85it/s] 90%|████████▉ | 77/86 [00:27<00:03,  2.76it/s] 64%|██████▍   | 55/86 [00:19<00:10,  2.87it/s] 79%|███████▉  | 68/86 [00:24<00:06,  2.84it/s] 91%|█████████ | 78/86 [00:27<00:02,  2.76it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.87it/s] 80%|████████  | 69/86 [00:24<00:05,  2.84it/s] 66%|██████▋   | 57/86 [00:20<00:10,  2.86it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.75it/s] 81%|████████▏ | 70/86 [00:25<00:05,  2.81it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.87it/s] 93%|█████████▎| 80/86 [00:28<00:02,  2.75it/s]I0318 06:37:59.647863 1099388 finetune.py:45] layer 4_down initial loss 0.00783990416675806
W0318 06:37:59.648059 1099388 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 83%|████████▎ | 71/86 [00:25<00:05,  2.84it/s] 69%|██████▊   | 59/86 [00:21<00:09,  2.87it/s] 94%|█████████▍| 81/86 [00:28<00:01,  2.80it/s]W0318 06:38:00.110613 1099388 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 84%|████████▎ | 72/86 [00:25<00:04,  2.84it/s] 70%|██████▉   | 60/86 [00:21<00:09,  2.88it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.83it/s]4_down proxy err 0.011222125962376595 tr(WHW.T) 11.562734603881836
 85%|████████▍ | 73/86 [00:26<00:04,  2.84it/s] 71%|███████   | 61/86 [00:21<00:08,  2.87it/s] 97%|█████████▋| 83/86 [00:29<00:01,  2.86it/s] 86%|████████▌ | 74/86 [00:26<00:04,  2.86it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.88it/s] 72%|███████▏  | 62/86 [00:22<00:08,  2.87it/s] 87%|████████▋ | 75/86 [00:26<00:03,  2.86it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.88it/s] 73%|███████▎  | 63/86 [00:22<00:08,  2.87it/s] 88%|████████▊ | 76/86 [00:27<00:03,  2.85it/s]100%|██████████| 86/86 [00:30<00:00,  2.91it/s]100%|██████████| 86/86 [00:30<00:00,  2.84it/s]
 74%|███████▍  | 64/86 [00:22<00:07,  2.87it/s] 90%|████████▉ | 77/86 [00:27<00:03,  2.84it/s] 76%|███████▌  | 65/86 [00:23<00:07,  2.86it/s] 91%|█████████ | 78/86 [00:27<00:02,  2.85it/s] 77%|███████▋  | 66/86 [00:23<00:06,  2.87it/s] 92%|█████████▏| 79/86 [00:28<00:02,  2.84it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.87it/s] 93%|█████████▎| 80/86 [00:28<00:02,  2.85it/s] 79%|███████▉  | 68/86 [00:24<00:06,  2.87it/s] 94%|█████████▍| 81/86 [00:28<00:01,  2.85it/s] 80%|████████  | 69/86 [00:24<00:05,  2.88it/s] 95%|█████████▌| 82/86 [00:29<00:01,  2.85it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.88it/s] 97%|█████████▋| 83/86 [00:29<00:01,  2.85it/s] 83%|████████▎ | 71/86 [00:25<00:05,  2.87it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.81it/s] 84%|████████▎ | 72/86 [00:25<00:04,  2.84it/s] 99%|█████████▉| 85/86 [00:30<00:00,  2.82it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.84it/s]100%|██████████| 86/86 [00:30<00:00,  2.84it/s]100%|██████████| 86/86 [00:30<00:00,  2.80it/s]
 86%|████████▌ | 74/86 [00:26<00:04,  2.85it/s] 87%|████████▋ | 75/86 [00:26<00:03,  2.82it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.79it/s] 90%|████████▉ | 77/86 [00:27<00:03,  2.78it/s] 91%|█████████ | 78/86 [00:27<00:02,  2.79it/s] 92%|█████████▏| 79/86 [00:28<00:02,  2.79it/s] 93%|█████████▎| 80/86 [00:28<00:02,  2.81it/s] 94%|█████████▍| 81/86 [00:28<00:01,  2.84it/s] 95%|█████████▌| 82/86 [00:29<00:01,  2.85it/s]W0318 06:38:08.028000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.028000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.028000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.029000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.029000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.029000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.029000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.070000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.070000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.070000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.070000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.070000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.085000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.085000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.085000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.085000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.085000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.246000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.246000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.246000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.246000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.246000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 83/86 [00:29<00:01,  2.87it/s]W0318 06:38:08.553000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.553000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.553000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.553000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.554000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.554000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.554000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.583000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.583000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.583000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.583000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.583000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.650000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.650000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.650000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.650000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:08.650000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 98%|█████████▊| 84/86 [00:29<00:00,  2.86it/s] 99%|█████████▉| 85/86 [00:30<00:00,  2.87it/s]100%|██████████| 86/86 [00:30<00:00,  2.86it/s]100%|██████████| 86/86 [00:30<00:00,  2.82it/s]
W0318 06:38:09.788000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:09.800000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:09.807000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:09.807000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.235000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.235000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.235000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.235000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.236000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.236000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.236000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.267000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.267000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.267000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.267000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.267000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.596000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.597000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.597000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.597000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.597000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.597000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.597000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.597000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.882000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.882000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.883000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.883000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:10.883000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.207000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.212000 140274101987136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.619000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.620000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.620000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.620000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.620000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.620000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.620000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.661000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.661000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.662000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.662000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.662000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.677000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.677000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.677000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.677000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.678000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.849000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.849000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.849000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.850000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:11.850000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:12.166000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:12.166000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:12.166000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:12.166000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:12.166000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:12.166000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:12.166000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:12.197000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:12.197000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:12.197000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:12.197000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:12.197000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:12.270000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:12.270000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:12.270000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:12.271000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:12.271000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:13.435000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:13.441000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:13.447000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:13.447000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:13.889000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:13.889000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:13.889000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:13.889000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:13.889000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:13.889000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:13.889000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:13.920000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:13.920000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:13.920000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:13.920000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:13.920000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:14.258000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:14.258000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:14.258000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:14.258000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:14.258000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:14.258000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:14.258000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:14.259000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:14.556000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:14.556000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:14.556000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:14.556000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:14.556000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:14.893000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:14.899000 139886483699520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:15.820000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:15.820000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:15.821000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:15.821000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:15.821000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:15.821000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:15.821000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:15.861000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:15.861000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:15.861000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:15.861000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:15.862000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:15.877000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:15.877000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:15.877000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:15.877000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:15.877000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.039000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.039000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.040000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.040000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.040000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.344000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.345000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.345000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.345000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.345000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.345000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.345000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.374000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.374000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.374000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.374000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.374000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.442000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.442000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.442000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.442000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:16.442000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:17.639000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:17.653000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:17.662000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:17.662000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
I0318 06:38:17.984525 1100176 finetune.py:45] layer 6_down initial loss 0.009806075133383274
W0318 06:38:17.984815 1100176 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:38:18.118000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.118000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.118000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.118000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.118000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.118000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.118000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.148000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.148000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.148000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.148000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.148000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.439515 1100176 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0318 06:38:18.500000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.500000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.500000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.500000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.500000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.500000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.500000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.500000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
6_down proxy err 0.012357343919575214 tr(WHW.T) 22.988161087036133
W0318 06:38:18.804000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.804000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.804000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.804000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:18.804000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:19.147000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:19.152000 139692865460032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0318 06:38:21.754686 1100007 finetune.py:45] layer 5_down initial loss 0.008859717287123203
W0318 06:38:21.755236 1100007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:38:22.339957 1100007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

5_down proxy err 0.012383677996695042 tr(WHW.T) 15.649473190307617
I0318 06:38:26.011110 1100601 finetune.py:45] layer 7_down initial loss 0.010679646395146847
W0318 06:38:26.011427 1100601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0318 06:38:26.091437 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 8 in 1.2225866317749023s
W0318 06:38:26.548384 1100601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

I0318 06:38:26.559135 1092738 quantize_finetune_llama.py:159] layer 9 gpu 1
7_down proxy err 0.012385725043714046 tr(WHW.T) 30.586715698242188
I0318 06:38:28.150277 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 9 in 1.1650266647338867s
I0318 06:38:28.506210 1104620 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:38:28.506331 1104620 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:38:28.506399 1104620 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:38:28.623311 1092738 quantize_finetune_llama.py:159] layer 10 gpu 2
I0318 06:38:28.714397 1104620 config.py:58] PyTorch version 2.4.0 available.
I0318 06:38:30.218789 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 10 in 1.1802287101745605s
I0318 06:38:30.544564 1104801 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:38:30.544690 1104801 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:38:30.544749 1104801 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:38:30.748647 1092738 quantize_finetune_llama.py:159] layer 11 gpu 3
I0318 06:38:30.753187 1104801 config.py:58] PyTorch version 2.4.0 available.
I0318 06:38:30.878953 1104620 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0318 06:38:31.236370 1104620 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:38:32.594397 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 11 in 1.4181029796600342s
  0%|          | 0/32 [00:00<?, ?it/s]I0318 06:38:32.727446 1105152 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:38:32.727640 1105152 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:38:32.727750 1105152 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:38:33.000795 1104801 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0318 06:38:33.014144 1105152 config.py:58] PyTorch version 2.4.0 available.
I0318 06:38:33.055866 1092738 quantize_finetune_llama.py:159] layer 12 gpu 0
W0318 06:38:33.381363 1104801 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:44,  1.45s/it]  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:01<00:23,  1.27it/s]  9%|▉         | 3/32 [00:02<00:17,  1.68it/s]I0318 06:38:34.933695 1105740 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:38:34.933848 1105740 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:38:34.933913 1105740 utils.py:162] NumExpr defaulting to 16 threads.
 12%|█▎        | 4/32 [00:02<00:13,  2.04it/s]I0318 06:38:35.258657 1105740 config.py:58] PyTorch version 2.4.0 available.
I0318 06:38:35.485528 1105152 data_utils.py:336] using 256 training seqs, 128 validation seqs
 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.48it/s]W0318 06:38:35.898708 1105152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:47,  1.54s/it] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s]  6%|▋         | 2/32 [00:01<00:25,  1.19it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.72it/s]  9%|▉         | 3/32 [00:02<00:17,  1.65it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.84it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.00it/s]  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.85it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.23it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s]I0318 06:38:37.568634 1105740 data_utils.py:336] using 256 training seqs, 128 validation seqs
 19%|█▉        | 6/32 [00:03<00:10,  2.41it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.90it/s]W0318 06:38:37.910344 1105740 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s] 41%|████      | 13/32 [00:05<00:06,  2.96it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.00it/s]  3%|▎         | 1/32 [00:01<00:48,  1.56s/it] 28%|██▊       | 9/32 [00:04<00:08,  2.78it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.02it/s]  6%|▋         | 2/32 [00:01<00:25,  1.19it/s]  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.84it/s] 50%|█████     | 16/32 [00:06<00:05,  2.97it/s]  9%|▉         | 3/32 [00:02<00:17,  1.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.88it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.95it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.99it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.91it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.00it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.26it/s] 41%|████      | 13/32 [00:05<00:06,  2.93it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.04it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.47it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.96it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.07it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.62it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.97it/s]  3%|▎         | 1/32 [00:01<00:55,  1.80s/it] 66%|██████▌   | 21/32 [00:08<00:03,  3.07it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.72it/s] 50%|█████     | 16/32 [00:06<00:05,  2.97it/s]  6%|▋         | 2/32 [00:02<00:28,  1.06it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.07it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.80it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.99it/s]  9%|▉         | 3/32 [00:02<00:19,  1.51it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.09it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.86it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.99it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.86it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.10it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.88it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.98it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.09it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.15it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.91it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.98it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.09it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.38it/s] 41%|████      | 13/32 [00:05<00:06,  2.94it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.99it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.10it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.96it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.98it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.10it/s] 25%|██▌       | 8/32 [00:04<00:08,  2.67it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.97it/s] 72%|███████▏  | 23/32 [00:08<00:03,  3.00it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.11it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.75it/s] 50%|█████     | 16/32 [00:06<00:05,  2.98it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.01it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.10it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.82it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.99it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.02it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.11it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.86it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.99it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.01it/s]100%|██████████| 32/32 [00:11<00:00,  3.12it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
 38%|███▊      | 12/32 [00:05<00:06,  2.89it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.99it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.99it/s] 41%|████      | 13/32 [00:05<00:06,  2.90it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.92it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.92it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.92it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.93it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.93it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.95it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.89it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.90it/s] 50%|█████     | 16/32 [00:06<00:05,  2.96it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.92it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.93it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.97it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.94it/s]100%|██████████| 32/32 [00:11<00:00,  2.95it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
 56%|█████▋    | 18/32 [00:07<00:04,  2.98it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.96it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.97it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.90it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.98it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.92it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.95it/s]W0318 06:38:47.490000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.490000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.490000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.490000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.490000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.490000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.490000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.518000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.518000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.518000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.518000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.518000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.535000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.535000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.535000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.535000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.536000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.96it/s]W0318 06:38:47.863000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.863000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.863000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.863000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:47.863000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.91it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.97it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.94it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.99it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.96it/s]W0318 06:38:48.720000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:48.720000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:48.720000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:48.720000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:48.720000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:48.720000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:48.720000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:48.737000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:48.737000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:48.737000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:48.737000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:48.738000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:09<00:02,  3.00it/s]W0318 06:38:48.968000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:48.968000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:48.968000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:48.968000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:48.968000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.97it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
 81%|████████▏ | 26/32 [00:10<00:01,  3.00it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.00it/s]W0318 06:38:49.513000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.513000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.513000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.513000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.513000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.513000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.513000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.541000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.541000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.541000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.541000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.541000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.558000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.559000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.559000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.559000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.559000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  2.99it/s]W0318 06:38:49.888000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.888000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.888000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.888000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:49.888000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.140000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.140000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.140000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.140000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.141000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.141000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.141000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.159000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.159000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.159000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.159000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.159000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.97it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.98it/s]W0318 06:38:50.772000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.773000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.773000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.773000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.773000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.773000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.773000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.790000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.790000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.790000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.790000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:50.790000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  2.99it/s]W0318 06:38:51.020000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:51.020000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:51.020000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:51.021000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:51.021000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:51.050000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:51.050000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:51.050000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:51.050000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:51.050000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.99it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0318 06:38:52.143000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.143000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.143000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.144000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.144000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.144000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.144000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.162000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.162000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.162000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.162000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.162000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.195000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.195000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.195000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.195000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.195000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.195000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.195000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.222000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.222000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.222000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.223000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.223000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.240000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.240000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.240000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.240000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.240000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.569000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.569000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.569000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.569000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:52.569000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.097000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.097000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.097000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.097000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.097000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.490000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.490000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.490000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.490000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.490000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.490000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.490000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.508000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.509000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.509000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.509000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.509000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0318 06:38:53.760000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.760000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.760000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.760000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:53.760000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.227000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.228000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.228000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.228000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.228000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.228000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.228000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.254000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.254000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.254000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.254000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.254000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.270000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.270000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.270000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.270000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.270000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.581000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.581000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.582000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.582000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.582000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.976000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.977000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.977000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.977000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.977000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.977000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.977000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.995000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.995000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.996000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.996000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:54.996000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.439000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.439000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.440000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.440000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.440000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.440000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.440000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.456000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.456000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.457000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.457000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.457000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.691000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.691000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.691000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.691000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.691000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.934000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.934000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.934000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.934000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:55.934000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0318 06:38:56.825000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:56.825000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:56.825000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:38:56.825000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:56.825000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:56.825000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:38:56.825000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:56.842000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:56.842000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:56.843000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:56.843000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:56.843000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
I0318 06:38:56.942954 1104620 finetune.py:45] layer 8_v initial loss 0.013124086894094944
W0318 06:38:56.943134 1104620 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:38:57.728000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:38:57.728000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:38:57.728000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:38:57.728000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:38:57.728000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:38:58.106534 1104620 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0318 06:38:58.906548 1104801 finetune.py:45] layer 9_v initial loss 0.014329146593809128
W0318 06:38:58.906777 1104801 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

8_v proxy err 0.008304744958877563 tr(WHW.T) 530.9967651367188
  0%|          | 0/32 [00:00<?, ?it/s]W0318 06:38:59.905715 1104801 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:00<00:20,  1.55it/s]  6%|▋         | 2/32 [00:00<00:13,  2.14it/s]  9%|▉         | 3/32 [00:01<00:11,  2.45it/s]9_v proxy err 0.008648363873362541 tr(WHW.T) 565.0662841796875
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.62it/s] 16%|█▌        | 5/32 [00:02<00:09,  2.72it/s]  3%|▎         | 1/32 [00:00<00:19,  1.57it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.79it/s]  6%|▋         | 2/32 [00:00<00:14,  2.12it/s]I0318 06:39:01.973905 1105152 finetune.py:45] layer 10_v initial loss 0.024287894368171692
W0318 06:39:01.974210 1105152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:02<00:08,  2.85it/s]  9%|▉         | 3/32 [00:01<00:11,  2.45it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.93it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.63it/s] 28%|██▊       | 9/32 [00:03<00:07,  3.00it/s]W0318 06:39:02.833734 1105152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:01<00:09,  2.75it/s] 31%|███▏      | 10/32 [00:03<00:07,  3.02it/s]I0318 06:39:03.164596 1105740 finetune.py:45] layer 11_v initial loss 0.018155736848711967
W0318 06:39:03.164896 1105740 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:02<00:09,  2.80it/s] 34%|███▍      | 11/32 [00:03<00:06,  3.02it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.86it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.06it/s]10_v proxy err 0.008614793419837952 tr(WHW.T) 578.807373046875
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.90it/s] 41%|████      | 13/32 [00:04<00:06,  3.08it/s]W0318 06:39:04.101967 1105740 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:03<00:07,  2.91it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.09it/s]  3%|▎         | 1/32 [00:00<00:18,  1.64it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.93it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.09it/s]  6%|▋         | 2/32 [00:00<00:13,  2.25it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.94it/s] 50%|█████     | 16/32 [00:05<00:05,  3.10it/s]  9%|▉         | 3/32 [00:01<00:11,  2.55it/s]11_v proxy err 0.009309221059083939 tr(WHW.T) 723.1956176757812
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.11it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.95it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.70it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.12it/s] 41%|████      | 13/32 [00:04<00:06,  2.96it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.81it/s]  3%|▎         | 1/32 [00:00<00:19,  1.62it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.11it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.96it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.88it/s]  6%|▋         | 2/32 [00:00<00:13,  2.19it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.12it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.97it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.94it/s]  9%|▉         | 3/32 [00:01<00:11,  2.49it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.11it/s] 50%|█████     | 16/32 [00:05<00:05,  2.96it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.97it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.63it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.13it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.96it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.97it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.73it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.12it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.96it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.99it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.80it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.13it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.96it/s] 34%|███▍      | 11/32 [00:03<00:06,  3.00it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.84it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.13it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.95it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.99it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.87it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.13it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.96it/s] 41%|████      | 13/32 [00:04<00:06,  3.01it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.15it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.89it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.96it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.01it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.16it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.90it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.97it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.02it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.15it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.90it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.97it/s] 50%|█████     | 16/32 [00:05<00:05,  3.02it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.15it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.87it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.97it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.03it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.16it/s] 41%|████      | 13/32 [00:04<00:06,  2.90it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.97it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.04it/s]100%|██████████| 32/32 [00:10<00:00,  3.16it/s]100%|██████████| 32/32 [00:10<00:00,  3.00it/s]
 44%|████▍     | 14/32 [00:05<00:06,  2.91it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.95it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.03it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.92it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.97it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.90it/s] 50%|█████     | 16/32 [00:05<00:05,  2.92it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.96it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.90it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.94it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.88it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.93it/s] 72%|███████▏  | 23/32 [00:07<00:03,  2.98it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.92it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.94it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.98it/s]100%|██████████| 32/32 [00:11<00:00,  2.90it/s]100%|██████████| 32/32 [00:11<00:00,  2.87it/s]
 62%|██████▎   | 20/32 [00:07<00:04,  2.92it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.94it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.93it/s] 81%|████████▏ | 26/32 [00:08<00:02,  2.90it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.93it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.88it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.93it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.86it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.94it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.86it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.95it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.85it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.95it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.85it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.95it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.89it/s]
 88%|████████▊ | 28/32 [00:09<00:01,  2.93it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.93it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.94it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.95it/s]100%|██████████| 32/32 [00:11<00:00,  2.93it/s]100%|██████████| 32/32 [00:11<00:00,  2.86it/s]
I0318 06:39:16.564404 1104620 finetune.py:45] layer 8_q initial loss 0.013111068867146969
W0318 06:39:16.564682 1104620 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:39:17.566650 1104620 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:39:18.521181 1104801 finetune.py:45] layer 9_q initial loss 0.014320356771349907
W0318 06:39:18.521490 1104801 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

8_q proxy err 0.0019152446184307337 tr(WHW.T) 7228.1201171875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.69it/s]W0318 06:39:19.431094 1104801 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:00<00:13,  2.26it/s]  9%|▉         | 3/32 [00:01<00:11,  2.54it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.69it/s]9_q proxy err 0.0021194927394390106 tr(WHW.T) 6970.3359375
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.78it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.84it/s]  3%|▎         | 1/32 [00:00<00:18,  1.64it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.87it/s]  6%|▋         | 2/32 [00:00<00:13,  2.16it/s]I0318 06:39:21.564284 1105152 finetune.py:45] layer 10_q initial loss 0.024309955537319183
W0318 06:39:21.564501 1105152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:02<00:08,  2.91it/s]  9%|▉         | 3/32 [00:01<00:11,  2.46it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.97it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.62it/s] 31%|███▏      | 10/32 [00:03<00:07,  3.03it/s]I0318 06:39:22.384918 1105740 finetune.py:45] layer 11_q initial loss 0.018146157264709473
W0318 06:39:22.385120 1105740 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:39:22.404605 1105152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:01<00:09,  2.72it/s] 34%|███▍      | 11/32 [00:03<00:06,  3.04it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.77it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.06it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.82it/s]W0318 06:39:23.221930 1105740 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 13/32 [00:04<00:06,  3.07it/s]10_q proxy err 0.002216111868619919 tr(WHW.T) 6915.87109375
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.86it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.11it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.88it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.11it/s]  3%|▎         | 1/32 [00:00<00:17,  1.78it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.90it/s] 50%|█████     | 16/32 [00:05<00:05,  3.13it/s]11_q proxy err 0.002449048450216651 tr(WHW.T) 7027.10986328125
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:00<00:12,  2.36it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.14it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.92it/s]  9%|▉         | 3/32 [00:01<00:10,  2.64it/s]  3%|▎         | 1/32 [00:00<00:17,  1.74it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.14it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.93it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.78it/s]  6%|▋         | 2/32 [00:00<00:12,  2.31it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.14it/s] 41%|████      | 13/32 [00:04<00:06,  2.94it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.87it/s]  9%|▉         | 3/32 [00:01<00:11,  2.57it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.15it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.95it/s] 19%|█▉        | 6/32 [00:02<00:08,  2.92it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.16it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.71it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.95it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.96it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.15it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.77it/s] 50%|█████     | 16/32 [00:05<00:05,  2.95it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.98it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.16it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.84it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.96it/s] 28%|██▊       | 9/32 [00:03<00:07,  3.00it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.17it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.88it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.96it/s] 31%|███▏      | 10/32 [00:03<00:07,  3.00it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.14it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.90it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.95it/s] 34%|███▍      | 11/32 [00:03<00:06,  3.01it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.16it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.92it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.96it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.02it/s] 84%|████████▍ | 27/32 [00:08<00:01,  3.15it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.93it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.96it/s] 41%|████      | 13/32 [00:04<00:06,  3.02it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.15it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.94it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.95it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.01it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.16it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.94it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.96it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.02it/s] 94%|█████████▍| 30/32 [00:09<00:00,  3.16it/s] 41%|████      | 13/32 [00:04<00:06,  2.93it/s] 50%|█████     | 16/32 [00:05<00:05,  3.02it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.96it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.18it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.94it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.03it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.96it/s]100%|██████████| 32/32 [00:10<00:00,  3.18it/s]100%|██████████| 32/32 [00:10<00:00,  3.03it/s]
 47%|████▋     | 15/32 [00:05<00:05,  2.95it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.02it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.95it/s] 50%|█████     | 16/32 [00:05<00:05,  2.94it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.96it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.89it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.95it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.97it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.90it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.97it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.95it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.88it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.97it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.99it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.92it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.97it/s] 72%|███████▏  | 23/32 [00:07<00:03,  3.00it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.91it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.95it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.95it/s]100%|██████████| 32/32 [00:11<00:00,  2.86it/s]100%|██████████| 32/32 [00:11<00:00,  2.85it/s]
 69%|██████▉   | 22/32 [00:07<00:03,  2.94it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.92it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.94it/s] 81%|████████▏ | 26/32 [00:08<00:02,  2.90it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.94it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.89it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.95it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.87it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.95it/s] 91%|█████████ | 29/32 [00:09<00:01,  2.87it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.96it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.87it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.96it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.87it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.96it/s]100%|██████████| 32/32 [00:11<00:00,  2.86it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]
 94%|█████████▍| 30/32 [00:10<00:00,  2.96it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.98it/s]100%|██████████| 32/32 [00:11<00:00,  2.99it/s]100%|██████████| 32/32 [00:11<00:00,  2.90it/s]
I0318 06:39:35.877797 1104620 finetune.py:45] layer 8_k initial loss 0.013097282499074936
W0318 06:39:35.878333 1104620 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:39:36.858892 1104620 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_k proxy err 0.001315501518547535 tr(WHW.T) 10639.1015625
  0%|          | 0/32 [00:00<?, ?it/s]I0318 06:39:38.140556 1104801 finetune.py:45] layer 9_k initial loss 0.014319449663162231
W0318 06:39:38.140767 1104801 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:00<00:18,  1.68it/s]  6%|▋         | 2/32 [00:00<00:13,  2.24it/s]W0318 06:39:39.057803 1104801 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:01<00:11,  2.51it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.65it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.74it/s]9_k proxy err 0.0014130751369521022 tr(WHW.T) 10987.3515625
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.80it/s]  3%|▎         | 1/32 [00:00<00:18,  1.65it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.85it/s]I0318 06:39:40.956160 1105152 finetune.py:45] layer 10_k initial loss 0.024303235113620758
W0318 06:39:40.956375 1105152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:02<00:08,  2.91it/s]  6%|▋         | 2/32 [00:00<00:13,  2.20it/s]I0318 06:39:41.333667 1105740 finetune.py:45] layer 11_k initial loss 0.01813538745045662
W0318 06:39:41.333866 1105740 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:03<00:07,  2.93it/s]  9%|▉         | 3/32 [00:01<00:11,  2.45it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.99it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.63it/s]W0318 06:39:41.795009 1105152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:03<00:07,  3.00it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.69it/s]W0318 06:39:42.156608 1105740 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 38%|███▊      | 12/32 [00:04<00:06,  3.03it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.77it/s] 41%|████      | 13/32 [00:04<00:06,  3.07it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.83it/s]10_k proxy err 0.0014825074467808008 tr(WHW.T) 10996.244140625
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.09it/s]11_k proxy err 0.001627674326300621 tr(WHW.T) 10511.23046875
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.86it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.10it/s]  3%|▎         | 1/32 [00:00<00:17,  1.77it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.88it/s] 50%|█████     | 16/32 [00:05<00:05,  3.11it/s]  6%|▋         | 2/32 [00:00<00:12,  2.34it/s]  3%|▎         | 1/32 [00:00<00:18,  1.70it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.90it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.12it/s]  9%|▉         | 3/32 [00:01<00:11,  2.62it/s]  6%|▋         | 2/32 [00:00<00:13,  2.27it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.92it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.13it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.76it/s]  9%|▉         | 3/32 [00:01<00:11,  2.55it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.93it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.14it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.86it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.71it/s] 41%|████      | 13/32 [00:04<00:06,  2.94it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.14it/s] 19%|█▉        | 6/32 [00:02<00:08,  2.93it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.83it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.95it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.15it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.98it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.87it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.95it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.15it/s] 25%|██▌       | 8/32 [00:02<00:07,  3.01it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.91it/s] 50%|█████     | 16/32 [00:05<00:05,  2.95it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.15it/s] 28%|██▊       | 9/32 [00:03<00:07,  3.03it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.94it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.15it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.95it/s] 31%|███▏      | 10/32 [00:03<00:07,  3.03it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.94it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.15it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.95it/s] 34%|███▍      | 11/32 [00:03<00:06,  3.05it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.96it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.15it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.95it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.04it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.98it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.15it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.95it/s] 41%|████      | 13/32 [00:04<00:06,  3.03it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.00it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.15it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.95it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.04it/s] 41%|████      | 13/32 [00:04<00:06,  3.01it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.15it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.94it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.04it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.00it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.15it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.93it/s] 50%|█████     | 16/32 [00:05<00:05,  3.04it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.97it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.14it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.93it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.05it/s]100%|██████████| 32/32 [00:10<00:00,  3.15it/s]100%|██████████| 32/32 [00:10<00:00,  3.01it/s]
 50%|█████     | 16/32 [00:05<00:05,  2.97it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.93it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.04it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.99it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.89it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.98it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.98it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.87it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.95it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.99it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.88it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.95it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.00it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.97it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.88it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.00it/s] 72%|███████▏  | 23/32 [00:07<00:03,  2.97it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.87it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.98it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.93it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.84it/s] 72%|███████▏  | 23/32 [00:07<00:03,  2.97it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.90it/s]100%|██████████| 32/32 [00:11<00:00,  2.81it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]
 75%|███████▌  | 24/32 [00:08<00:02,  2.99it/s] 81%|████████▏ | 26/32 [00:08<00:02,  2.88it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.99it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.87it/s] 81%|████████▏ | 26/32 [00:08<00:02,  2.99it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.86it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.99it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.98it/s] 91%|█████████ | 29/32 [00:09<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:09<00:01,  3.00it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.86it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.99it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.86it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.98it/s]100%|██████████| 32/32 [00:11<00:00,  2.86it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]
100%|██████████| 32/32 [00:10<00:00,  2.98it/s]100%|██████████| 32/32 [00:10<00:00,  2.92it/s]
I0318 06:39:55.481863 1104620 finetune.py:45] layer 8_o initial loss 0.013068200089037418
W0318 06:39:55.482054 1104620 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:39:56.463495 1104620 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:39:57.875461 1104801 finetune.py:45] layer 9_o initial loss 0.013930031098425388
W0318 06:39:57.875689 1104801 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

8_o proxy err 0.009322059340775013 tr(WHW.T) 20.092191696166992
  0%|          | 0/32 [00:00<?, ?it/s]W0318 06:39:58.748634 1104801 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:39,  1.28s/it]9_o proxy err 0.010176516138017178 tr(WHW.T) 25.610172271728516
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:02<00:33,  1.13s/it]I0318 06:40:00.230874 1105740 finetune.py:45] layer 11_o initial loss 0.018483677878975868
W0318 06:40:00.231102 1105740 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0318 06:40:00.384954 1105152 finetune.py:45] layer 10_o initial loss 0.023799089714884758
W0318 06:40:00.385135 1105152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:40:01.055990 1105740 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:40:01.165951 1105152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:03<00:30,  1.06s/it]  3%|▎         | 1/32 [00:01<00:40,  1.32s/it] 12%|█▎        | 4/32 [00:04<00:28,  1.03s/it]11_o proxy err 0.011276268400251865 tr(WHW.T) 36.654052734375
  0%|          | 0/32 [00:00<?, ?it/s]10_o proxy err 0.010279691778123379 tr(WHW.T) 35.184165954589844
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:02<00:34,  1.14s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.00s/it]  9%|▉         | 3/32 [00:03<00:31,  1.08s/it]  3%|▎         | 1/32 [00:01<00:38,  1.24s/it]  3%|▎         | 1/32 [00:01<00:38,  1.25s/it] 19%|█▉        | 6/32 [00:06<00:25,  1.01it/s] 12%|█▎        | 4/32 [00:04<00:31,  1.12s/it]  6%|▋         | 2/32 [00:02<00:35,  1.20s/it]  6%|▋         | 2/32 [00:02<00:35,  1.20s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.04s/it] 16%|█▌        | 5/32 [00:05<00:29,  1.08s/it]  9%|▉         | 3/32 [00:03<00:31,  1.10s/it]  9%|▉         | 3/32 [00:03<00:31,  1.10s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.01s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.05s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.06s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.05s/it] 28%|██▊       | 9/32 [00:09<00:22,  1.00it/s] 16%|█▌        | 5/32 [00:05<00:28,  1.04s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.04s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.03s/it] 31%|███▏      | 10/32 [00:10<00:21,  1.02it/s] 19%|█▉        | 6/32 [00:06<00:26,  1.02s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.03s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.01s/it] 34%|███▍      | 11/32 [00:11<00:20,  1.03it/s] 22%|██▏       | 7/32 [00:07<00:25,  1.01s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.02s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.00s/it] 38%|███▊      | 12/32 [00:12<00:19,  1.03it/s] 25%|██▌       | 8/32 [00:08<00:24,  1.01s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.02s/it] 25%|██▌       | 8/32 [00:08<00:23,  1.00it/s] 41%|████      | 13/32 [00:13<00:18,  1.04it/s] 28%|██▊       | 9/32 [00:09<00:23,  1.00s/it] 28%|██▊       | 9/32 [00:09<00:22,  1.01it/s] 34%|███▍      | 11/32 [00:11<00:21,  1.01s/it] 44%|████▍     | 14/32 [00:13<00:17,  1.04it/s] 31%|███▏      | 10/32 [00:10<00:21,  1.00it/s] 31%|███▏      | 10/32 [00:10<00:21,  1.01it/s] 38%|███▊      | 12/32 [00:12<00:20,  1.01s/it] 47%|████▋     | 15/32 [00:14<00:16,  1.04it/s] 34%|███▍      | 11/32 [00:11<00:20,  1.01it/s] 34%|███▍      | 11/32 [00:11<00:20,  1.01it/s] 41%|████      | 13/32 [00:13<00:19,  1.01s/it] 50%|█████     | 16/32 [00:15<00:15,  1.04it/s] 38%|███▊      | 12/32 [00:12<00:19,  1.01it/s] 38%|███▊      | 12/32 [00:12<00:19,  1.02it/s] 44%|████▍     | 14/32 [00:14<00:18,  1.01s/it] 53%|█████▎    | 17/32 [00:16<00:14,  1.04it/s] 41%|████      | 13/32 [00:13<00:18,  1.01it/s] 41%|████      | 13/32 [00:13<00:18,  1.02it/s] 47%|████▋     | 15/32 [00:15<00:17,  1.01s/it] 56%|█████▋    | 18/32 [00:17<00:13,  1.04it/s] 44%|████▍     | 14/32 [00:14<00:17,  1.02it/s] 44%|████▍     | 14/32 [00:14<00:17,  1.00it/s] 50%|█████     | 16/32 [00:16<00:16,  1.00s/it] 59%|█████▉    | 19/32 [00:18<00:12,  1.05it/s] 47%|████▋     | 15/32 [00:15<00:16,  1.02it/s] 47%|████▋     | 15/32 [00:15<00:16,  1.00it/s] 53%|█████▎    | 17/32 [00:17<00:15,  1.00s/it] 62%|██████▎   | 20/32 [00:19<00:11,  1.05it/s] 50%|█████     | 16/32 [00:16<00:15,  1.02it/s] 50%|█████     | 16/32 [00:16<00:15,  1.01it/s] 66%|██████▌   | 21/32 [00:20<00:10,  1.05it/s] 56%|█████▋    | 18/32 [00:18<00:14,  1.00s/it] 53%|█████▎    | 17/32 [00:17<00:14,  1.02it/s] 53%|█████▎    | 17/32 [00:17<00:14,  1.00it/s] 69%|██████▉   | 22/32 [00:21<00:09,  1.05it/s] 59%|█████▉    | 19/32 [00:19<00:13,  1.00s/it] 56%|█████▋    | 18/32 [00:18<00:13,  1.02it/s] 56%|█████▋    | 18/32 [00:18<00:13,  1.01it/s] 72%|███████▏  | 23/32 [00:22<00:08,  1.05it/s] 62%|██████▎   | 20/32 [00:20<00:12,  1.00s/it] 59%|█████▉    | 19/32 [00:19<00:12,  1.02it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.05it/s] 59%|█████▉    | 19/32 [00:19<00:12,  1.01it/s] 66%|██████▌   | 21/32 [00:21<00:10,  1.00it/s] 62%|██████▎   | 20/32 [00:20<00:11,  1.02it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.05it/s] 62%|██████▎   | 20/32 [00:20<00:11,  1.01it/s] 69%|██████▉   | 22/32 [00:22<00:10,  1.00s/it] 81%|████████▏ | 26/32 [00:25<00:05,  1.05it/s] 66%|██████▌   | 21/32 [00:21<00:10,  1.02it/s] 66%|██████▌   | 21/32 [00:21<00:10,  1.00it/s] 72%|███████▏  | 23/32 [00:23<00:08,  1.00it/s] 84%|████████▍ | 27/32 [00:26<00:04,  1.05it/s] 69%|██████▉   | 22/32 [00:22<00:09,  1.02it/s] 69%|██████▉   | 22/32 [00:22<00:09,  1.01it/s] 75%|███████▌  | 24/32 [00:24<00:07,  1.00it/s] 88%|████████▊ | 28/32 [00:27<00:03,  1.05it/s] 72%|███████▏  | 23/32 [00:23<00:08,  1.02it/s] 72%|███████▏  | 23/32 [00:23<00:08,  1.01it/s] 78%|███████▊  | 25/32 [00:25<00:07,  1.00s/it] 91%|█████████ | 29/32 [00:28<00:02,  1.05it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.02it/s] 75%|███████▌  | 24/32 [00:24<00:07,  1.01it/s] 81%|████████▏ | 26/32 [00:26<00:06,  1.00s/it] 94%|█████████▍| 30/32 [00:29<00:01,  1.05it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.02it/s] 78%|███████▊  | 25/32 [00:25<00:06,  1.01it/s] 84%|████████▍ | 27/32 [00:27<00:04,  1.00it/s] 97%|█████████▋| 31/32 [00:30<00:00,  1.05it/s] 81%|████████▏ | 26/32 [00:25<00:05,  1.02it/s] 81%|████████▏ | 26/32 [00:26<00:05,  1.01it/s] 88%|████████▊ | 28/32 [00:28<00:03,  1.00it/s]100%|██████████| 32/32 [00:31<00:00,  1.05it/s]100%|██████████| 32/32 [00:31<00:00,  1.03it/s]
 84%|████████▍ | 27/32 [00:26<00:04,  1.02it/s] 84%|████████▍ | 27/32 [00:27<00:04,  1.01it/s] 91%|█████████ | 29/32 [00:29<00:03,  1.00s/it] 88%|████████▊ | 28/32 [00:27<00:03,  1.00it/s] 88%|████████▊ | 28/32 [00:28<00:03,  1.01it/s] 94%|█████████▍| 30/32 [00:30<00:02,  1.01s/it] 91%|█████████ | 29/32 [00:28<00:02,  1.00it/s] 91%|█████████ | 29/32 [00:29<00:02,  1.01it/s] 97%|█████████▋| 31/32 [00:31<00:01,  1.01s/it] 94%|█████████▍| 30/32 [00:29<00:01,  1.01it/s] 94%|█████████▍| 30/32 [00:30<00:01,  1.01it/s]100%|██████████| 32/32 [00:32<00:00,  1.01s/it]100%|██████████| 32/32 [00:32<00:00,  1.02s/it]
 97%|█████████▋| 31/32 [00:30<00:00,  1.00it/s] 97%|█████████▋| 31/32 [00:31<00:00,  1.01it/s]100%|██████████| 32/32 [00:31<00:00,  1.00s/it]100%|██████████| 32/32 [00:31<00:00,  1.00it/s]
100%|██████████| 32/32 [00:32<00:00,  1.01it/s]100%|██████████| 32/32 [00:32<00:00,  1.00s/it]
W0318 06:40:35.558000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.558000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.558000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.558000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.558000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.559000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.559000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.587000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.587000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.587000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.587000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.587000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.602000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.602000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.602000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.602000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.602000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.750000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.750000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.750000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.750000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.750000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.973000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.973000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.973000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.974000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.974000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.974000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.974000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.994000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.994000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.994000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.994000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:35.994000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:36.061000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:36.061000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:36.061000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:36.061000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:36.061000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:36.897000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:37.193000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:37.194000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:37.194000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:37.194000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:37.194000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:37.194000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:40:37.194000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:37.213000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:37.214000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:37.214000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:37.214000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:37.214000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:37.458000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:37.458000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:37.458000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:37.458000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:37.458000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:37.708000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:38.933000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:38.933000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:40:38.933000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:38.933000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:38.933000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:38.933000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:38.933000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:38.961000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:38.962000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:38.962000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:38.962000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:38.962000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:38.976000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:38.976000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:38.977000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:38.977000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:38.977000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.130000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.130000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.130000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.130000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.130000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.370000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.370000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.370000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.370000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.370000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.370000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.371000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.391000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.391000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.391000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.391000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.392000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.459000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.459000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.459000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.459000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:39.460000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.364000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.554000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.554000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.554000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.554000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.554000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.554000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.554000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.582000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.582000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.582000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.582000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.582000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.597000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.597000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.597000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.597000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.597000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.687000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.687000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.687000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.687000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.687000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.687000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.687000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.692000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.693000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.693000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.693000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.693000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.693000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.693000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.716000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.716000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.716000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.716000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.716000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.717000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.717000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.717000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.717000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.717000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.737000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.737000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.737000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.738000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.738000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.747000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.747000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.747000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.748000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.748000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.900000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.900000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.900000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.900000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.900000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.967000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.967000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.967000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.967000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.967000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.967000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.968000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.986000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.986000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.986000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.986000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.986000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.986000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.987000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.987000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.987000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:40.987000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.049000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.049000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.049000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.049000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.049000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.132000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.132000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.132000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.132000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.132000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.132000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.132000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.154000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.154000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.154000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.154000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.154000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.221000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.221000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.221000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.221000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.221000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.266000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:41.878000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.118000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.169000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.170000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.170000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.170000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.170000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.170000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.170000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.189000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.189000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.189000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.189000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.189000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.431000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.431000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.431000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.431000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.431000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.436000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.436000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.436000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.436000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.436000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.437000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.437000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.459000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.459000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.459000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.459000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.459000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.681000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.722000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.722000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.722000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.722000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.722000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:40:42.995000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0318 06:40:44.122335 1104620 finetune.py:45] layer 8_up initial loss 0.012818781659007072
W0318 06:40:44.122588 1104620 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:40:45.025629 1104620 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_up proxy err 0.009362825192511082 tr(WHW.T) 866.3126220703125
  0%|          | 0/32 [00:00<?, ?it/s]I0318 06:40:47.614487 1104801 finetune.py:45] layer 9_up initial loss 0.013702785596251488
W0318 06:40:47.614641 1104801 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:41,  1.34s/it]W0318 06:40:48.439208 1104801 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:40:48.621645 1105740 finetune.py:45] layer 11_up initial loss 0.018199235200881958
W0318 06:40:48.621929 1105740 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:02<00:34,  1.16s/it]I0318 06:40:49.371181 1105152 finetune.py:45] layer 10_up initial loss 0.0234612375497818
W0318 06:40:49.371400 1105152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:40:49.398081 1105740 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

9_up proxy err 0.009176707826554775 tr(WHW.T) 970.8983154296875
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:03<00:31,  1.08s/it]W0318 06:40:50.132006 1105152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

11_up proxy err 0.009338689967989922 tr(WHW.T) 1139.604248046875
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:04<00:29,  1.05s/it]  3%|▎         | 1/32 [00:01<00:40,  1.31s/it]10_up proxy err 0.008803085424005985 tr(WHW.T) 1080.1982421875
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:05<00:27,  1.02s/it]  3%|▎         | 1/32 [00:01<00:39,  1.29s/it]  6%|▋         | 2/32 [00:02<00:33,  1.13s/it]  3%|▎         | 1/32 [00:01<00:38,  1.24s/it] 19%|█▉        | 6/32 [00:06<00:25,  1.00it/s]  6%|▋         | 2/32 [00:02<00:33,  1.11s/it]  9%|▉         | 3/32 [00:03<00:30,  1.07s/it]  6%|▋         | 2/32 [00:02<00:32,  1.08s/it] 22%|██▏       | 7/32 [00:07<00:24,  1.01it/s]  9%|▉         | 3/32 [00:03<00:30,  1.05s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.04s/it]  9%|▉         | 3/32 [00:03<00:29,  1.03s/it] 25%|██▌       | 8/32 [00:08<00:23,  1.02it/s] 12%|█▎        | 4/32 [00:04<00:28,  1.03s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.02s/it] 12%|█▎        | 4/32 [00:04<00:28,  1.01s/it] 28%|██▊       | 9/32 [00:09<00:22,  1.03it/s] 16%|█▌        | 5/32 [00:05<00:27,  1.01s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.01s/it] 16%|█▌        | 5/32 [00:05<00:26,  1.00it/s] 31%|███▏      | 10/32 [00:10<00:21,  1.03it/s] 19%|█▉        | 6/32 [00:06<00:26,  1.00s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.00s/it] 19%|█▉        | 6/32 [00:06<00:25,  1.01it/s] 34%|███▍      | 11/32 [00:11<00:20,  1.04it/s] 22%|██▏       | 7/32 [00:07<00:24,  1.00it/s] 25%|██▌       | 8/32 [00:08<00:23,  1.00it/s] 22%|██▏       | 7/32 [00:07<00:24,  1.02it/s] 38%|███▊      | 12/32 [00:12<00:19,  1.04it/s] 25%|██▌       | 8/32 [00:08<00:23,  1.00it/s] 28%|██▊       | 9/32 [00:09<00:22,  1.00it/s] 25%|██▌       | 8/32 [00:08<00:23,  1.02it/s] 41%|████      | 13/32 [00:13<00:18,  1.04it/s] 28%|██▊       | 9/32 [00:09<00:22,  1.00it/s] 31%|███▏      | 10/32 [00:10<00:21,  1.00it/s] 28%|██▊       | 9/32 [00:09<00:22,  1.02it/s] 44%|████▍     | 14/32 [00:13<00:17,  1.04it/s] 31%|███▏      | 10/32 [00:10<00:21,  1.00it/s] 34%|███▍      | 11/32 [00:11<00:20,  1.00it/s] 31%|███▏      | 10/32 [00:10<00:21,  1.02it/s] 47%|████▋     | 15/32 [00:14<00:16,  1.04it/s] 34%|███▍      | 11/32 [00:11<00:20,  1.00it/s] 38%|███▊      | 12/32 [00:12<00:19,  1.00it/s] 34%|███▍      | 11/32 [00:10<00:20,  1.02it/s] 50%|█████     | 16/32 [00:15<00:15,  1.04it/s] 38%|███▊      | 12/32 [00:12<00:19,  1.01it/s] 41%|████      | 13/32 [00:13<00:18,  1.00it/s] 38%|███▊      | 12/32 [00:11<00:19,  1.03it/s] 53%|█████▎    | 17/32 [00:16<00:14,  1.04it/s] 41%|████      | 13/32 [00:13<00:18,  1.01it/s] 44%|████▍     | 14/32 [00:14<00:17,  1.00it/s] 41%|████      | 13/32 [00:12<00:18,  1.03it/s] 56%|█████▋    | 18/32 [00:17<00:13,  1.04it/s] 44%|████▍     | 14/32 [00:14<00:17,  1.01it/s] 47%|████▋     | 15/32 [00:15<00:16,  1.01it/s] 44%|████▍     | 14/32 [00:13<00:17,  1.03it/s] 59%|█████▉    | 19/32 [00:18<00:12,  1.04it/s] 47%|████▋     | 15/32 [00:15<00:16,  1.01it/s] 50%|█████     | 16/32 [00:16<00:15,  1.01it/s] 47%|████▋     | 15/32 [00:14<00:16,  1.03it/s] 62%|██████▎   | 20/32 [00:19<00:11,  1.04it/s] 50%|█████     | 16/32 [00:16<00:15,  1.01it/s] 53%|█████▎    | 17/32 [00:17<00:14,  1.01it/s] 66%|██████▌   | 21/32 [00:20<00:10,  1.04it/s] 50%|█████     | 16/32 [00:15<00:15,  1.03it/s] 53%|█████▎    | 17/32 [00:17<00:14,  1.01it/s] 56%|█████▋    | 18/32 [00:18<00:13,  1.01it/s] 69%|██████▉   | 22/32 [00:21<00:09,  1.04it/s] 53%|█████▎    | 17/32 [00:16<00:14,  1.03it/s] 56%|█████▋    | 18/32 [00:18<00:13,  1.01it/s] 59%|█████▉    | 19/32 [00:19<00:12,  1.01it/s] 72%|███████▏  | 23/32 [00:22<00:08,  1.04it/s] 56%|█████▋    | 18/32 [00:17<00:13,  1.03it/s] 59%|█████▉    | 19/32 [00:19<00:12,  1.00it/s] 62%|██████▎   | 20/32 [00:20<00:11,  1.01it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.04it/s] 59%|█████▉    | 19/32 [00:18<00:12,  1.03it/s] 62%|██████▎   | 20/32 [00:20<00:11,  1.01it/s] 66%|██████▌   | 21/32 [00:21<00:10,  1.01it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.04it/s] 62%|██████▎   | 20/32 [00:19<00:11,  1.03it/s] 66%|██████▌   | 21/32 [00:21<00:10,  1.01it/s] 69%|██████▉   | 22/32 [00:22<00:09,  1.01it/s] 81%|████████▏ | 26/32 [00:25<00:05,  1.04it/s] 66%|██████▌   | 21/32 [00:20<00:10,  1.03it/s] 69%|██████▉   | 22/32 [00:22<00:09,  1.01it/s] 72%|███████▏  | 23/32 [00:23<00:08,  1.01it/s] 84%|████████▍ | 27/32 [00:26<00:04,  1.05it/s] 69%|██████▉   | 22/32 [00:21<00:09,  1.03it/s] 72%|███████▏  | 23/32 [00:23<00:08,  1.01it/s] 75%|███████▌  | 24/32 [00:24<00:07,  1.01it/s] 88%|████████▊ | 28/32 [00:27<00:03,  1.04it/s] 72%|███████▏  | 23/32 [00:22<00:08,  1.03it/s] 75%|███████▌  | 24/32 [00:24<00:07,  1.01it/s] 91%|█████████ | 29/32 [00:28<00:02,  1.04it/s] 78%|███████▊  | 25/32 [00:25<00:06,  1.01it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.03it/s] 78%|███████▊  | 25/32 [00:25<00:06,  1.01it/s] 94%|█████████▍| 30/32 [00:29<00:01,  1.04it/s] 81%|████████▏ | 26/32 [00:26<00:05,  1.01it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.03it/s] 81%|████████▏ | 26/32 [00:26<00:05,  1.01it/s] 97%|█████████▋| 31/32 [00:30<00:00,  1.04it/s] 84%|████████▍ | 27/32 [00:27<00:04,  1.01it/s] 81%|████████▏ | 26/32 [00:25<00:05,  1.03it/s] 84%|████████▍ | 27/32 [00:27<00:04,  1.01it/s]100%|██████████| 32/32 [00:31<00:00,  1.04it/s]100%|██████████| 32/32 [00:31<00:00,  1.03it/s]
 88%|████████▊ | 28/32 [00:28<00:03,  1.01it/s] 84%|████████▍ | 27/32 [00:26<00:04,  1.03it/s] 88%|████████▊ | 28/32 [00:28<00:03,  1.01it/s] 88%|████████▊ | 28/32 [00:27<00:03,  1.01it/s] 91%|█████████ | 29/32 [00:29<00:03,  1.02s/it] 91%|█████████ | 29/32 [00:29<00:02,  1.01it/s] 91%|█████████ | 29/32 [00:28<00:02,  1.00it/s] 94%|█████████▍| 30/32 [00:30<00:02,  1.01s/it] 94%|█████████▍| 30/32 [00:30<00:01,  1.01it/s] 94%|█████████▍| 30/32 [00:29<00:02,  1.01s/it] 97%|█████████▋| 31/32 [00:31<00:01,  1.02s/it] 97%|█████████▋| 31/32 [00:30<00:00,  1.01it/s] 97%|█████████▋| 31/32 [00:30<00:01,  1.02s/it]100%|██████████| 32/32 [00:32<00:00,  1.04s/it]100%|██████████| 32/32 [00:32<00:00,  1.01s/it]
100%|██████████| 32/32 [00:31<00:00,  1.01it/s]100%|██████████| 32/32 [00:31<00:00,  1.00it/s]
100%|██████████| 32/32 [00:31<00:00,  1.02s/it]100%|██████████| 32/32 [00:31<00:00,  1.01it/s]
I0318 06:41:25.405235 1104620 finetune.py:45] layer 8_gate initial loss 0.012554621323943138
W0318 06:41:25.405526 1104620 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:41:26.231222 1104620 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:41:29.432369 1105740 finetune.py:45] layer 11_gate initial loss 0.017919518053531647
W0318 06:41:29.432590 1105740 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0318 06:41:29.455929 1104801 finetune.py:45] layer 9_gate initial loss 0.013422777876257896
W0318 06:41:29.456176 1104801 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

8_gate proxy err 0.005026338156312704 tr(WHW.T) 1970.85693359375
  0%|          | 0/86 [00:00<?, ?it/s]W0318 06:41:30.169503 1105740 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:41:30.175068 1104801 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:41:30.527760 1105152 finetune.py:45] layer 10_gate initial loss 0.02328510954976082
W0318 06:41:30.527955 1105152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  1%|          | 1/86 [00:00<00:50,  1.68it/s]  2%|▏         | 2/86 [00:00<00:38,  2.21it/s]W0318 06:41:31.217375 1105152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 3/86 [00:01<00:33,  2.47it/s]  5%|▍         | 4/86 [00:01<00:31,  2.63it/s]  6%|▌         | 5/86 [00:01<00:30,  2.70it/s]  7%|▋         | 6/86 [00:02<00:29,  2.75it/s]  8%|▊         | 7/86 [00:02<00:27,  2.83it/s]9_gate proxy err 0.004934744909405708 tr(WHW.T) 2132.693359375
  0%|          | 0/86 [00:00<?, ?it/s]11_gate proxy err 0.004990212619304657 tr(WHW.T) 2392.71630859375
  0%|          | 0/86 [00:00<?, ?it/s]  9%|▉         | 8/86 [00:02<00:26,  2.91it/s] 10%|█         | 9/86 [00:03<00:26,  2.93it/s]  1%|          | 1/86 [00:00<00:50,  1.67it/s]  1%|          | 1/86 [00:00<00:50,  1.67it/s] 12%|█▏        | 10/86 [00:03<00:25,  2.99it/s]  2%|▏         | 2/86 [00:00<00:37,  2.24it/s]  2%|▏         | 2/86 [00:00<00:37,  2.26it/s]10_gate proxy err 0.004835563711822033 tr(WHW.T) 2260.883056640625
  0%|          | 0/86 [00:00<?, ?it/s] 13%|█▎        | 11/86 [00:03<00:24,  3.03it/s]  3%|▎         | 3/86 [00:01<00:32,  2.53it/s]  3%|▎         | 3/86 [00:01<00:32,  2.54it/s] 14%|█▍        | 12/86 [00:04<00:24,  2.99it/s]  5%|▍         | 4/86 [00:01<00:30,  2.69it/s]  5%|▍         | 4/86 [00:01<00:30,  2.68it/s]  1%|          | 1/86 [00:00<00:51,  1.66it/s] 15%|█▌        | 13/86 [00:04<00:24,  3.02it/s]  6%|▌         | 5/86 [00:01<00:28,  2.80it/s]  6%|▌         | 5/86 [00:01<00:28,  2.80it/s]  2%|▏         | 2/86 [00:00<00:37,  2.27it/s] 16%|█▋        | 14/86 [00:04<00:24,  2.99it/s]  7%|▋         | 6/86 [00:02<00:27,  2.86it/s]  3%|▎         | 3/86 [00:01<00:32,  2.56it/s]  7%|▋         | 6/86 [00:02<00:28,  2.85it/s] 17%|█▋        | 15/86 [00:05<00:23,  3.02it/s]  8%|▊         | 7/86 [00:02<00:27,  2.91it/s]  5%|▍         | 4/86 [00:01<00:30,  2.73it/s]  8%|▊         | 7/86 [00:02<00:27,  2.88it/s] 19%|█▊        | 16/86 [00:05<00:23,  3.03it/s]  9%|▉         | 8/86 [00:02<00:26,  2.94it/s]  6%|▌         | 5/86 [00:01<00:28,  2.84it/s]  9%|▉         | 8/86 [00:02<00:26,  2.90it/s] 20%|█▉        | 17/86 [00:05<00:22,  3.05it/s] 10%|█         | 9/86 [00:03<00:26,  2.96it/s]  7%|▋         | 6/86 [00:02<00:27,  2.90it/s] 10%|█         | 9/86 [00:03<00:26,  2.90it/s] 21%|██        | 18/86 [00:06<00:22,  3.05it/s] 12%|█▏        | 10/86 [00:03<00:25,  2.97it/s]  8%|▊         | 7/86 [00:02<00:26,  2.95it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.91it/s] 22%|██▏       | 19/86 [00:06<00:21,  3.06it/s] 13%|█▎        | 11/86 [00:03<00:25,  2.98it/s]  9%|▉         | 8/86 [00:02<00:26,  2.97it/s] 13%|█▎        | 11/86 [00:03<00:25,  2.92it/s] 23%|██▎       | 20/86 [00:06<00:21,  3.07it/s] 14%|█▍        | 12/86 [00:04<00:24,  2.97it/s] 10%|█         | 9/86 [00:03<00:25,  2.99it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.93it/s] 24%|██▍       | 21/86 [00:07<00:21,  3.06it/s] 15%|█▌        | 13/86 [00:04<00:24,  2.97it/s] 12%|█▏        | 10/86 [00:03<00:25,  2.98it/s] 15%|█▌        | 13/86 [00:04<00:24,  2.94it/s] 26%|██▌       | 22/86 [00:07<00:20,  3.06it/s] 16%|█▋        | 14/86 [00:04<00:24,  2.98it/s] 13%|█▎        | 11/86 [00:03<00:25,  3.00it/s] 16%|█▋        | 14/86 [00:04<00:24,  2.94it/s] 27%|██▋       | 23/86 [00:07<00:20,  3.01it/s] 17%|█▋        | 15/86 [00:05<00:23,  2.99it/s] 14%|█▍        | 12/86 [00:04<00:24,  3.00it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.94it/s] 28%|██▊       | 24/86 [00:08<00:20,  3.04it/s] 19%|█▊        | 16/86 [00:05<00:23,  2.98it/s] 15%|█▌        | 13/86 [00:04<00:24,  3.00it/s] 19%|█▊        | 16/86 [00:05<00:23,  2.93it/s] 29%|██▉       | 25/86 [00:08<00:20,  3.04it/s] 20%|█▉        | 17/86 [00:05<00:23,  2.98it/s] 16%|█▋        | 14/86 [00:04<00:23,  3.02it/s] 30%|███       | 26/86 [00:08<00:19,  3.06it/s] 20%|█▉        | 17/86 [00:06<00:23,  2.95it/s] 21%|██        | 18/86 [00:06<00:22,  2.99it/s] 17%|█▋        | 15/86 [00:05<00:23,  3.03it/s] 31%|███▏      | 27/86 [00:09<00:19,  3.06it/s] 21%|██        | 18/86 [00:06<00:23,  2.94it/s] 22%|██▏       | 19/86 [00:06<00:22,  2.98it/s] 19%|█▊        | 16/86 [00:05<00:23,  3.02it/s] 33%|███▎      | 28/86 [00:09<00:18,  3.06it/s] 22%|██▏       | 19/86 [00:06<00:22,  2.94it/s] 23%|██▎       | 20/86 [00:06<00:22,  2.98it/s] 20%|█▉        | 17/86 [00:05<00:22,  3.02it/s] 34%|███▎      | 29/86 [00:09<00:18,  3.07it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.95it/s] 24%|██▍       | 21/86 [00:07<00:21,  2.98it/s] 21%|██        | 18/86 [00:06<00:22,  3.03it/s] 35%|███▍      | 30/86 [00:10<00:18,  3.07it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.95it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.99it/s] 22%|██▏       | 19/86 [00:06<00:22,  3.03it/s] 36%|███▌      | 31/86 [00:10<00:17,  3.07it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.94it/s] 27%|██▋       | 23/86 [00:07<00:21,  2.99it/s] 23%|██▎       | 20/86 [00:06<00:21,  3.02it/s] 37%|███▋      | 32/86 [00:10<00:17,  3.06it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.95it/s] 28%|██▊       | 24/86 [00:08<00:20,  2.99it/s] 24%|██▍       | 21/86 [00:07<00:21,  3.02it/s] 38%|███▊      | 33/86 [00:11<00:17,  3.00it/s] 28%|██▊       | 24/86 [00:08<00:20,  2.96it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.99it/s] 26%|██▌       | 22/86 [00:07<00:21,  3.02it/s] 40%|███▉      | 34/86 [00:11<00:17,  3.04it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.97it/s] 30%|███       | 26/86 [00:08<00:20,  2.99it/s] 27%|██▋       | 23/86 [00:07<00:20,  3.01it/s] 41%|████      | 35/86 [00:11<00:16,  3.04it/s] 30%|███       | 26/86 [00:09<00:20,  2.93it/s] 31%|███▏      | 27/86 [00:09<00:19,  2.98it/s] 28%|██▊       | 24/86 [00:08<00:20,  2.99it/s] 42%|████▏     | 36/86 [00:12<00:16,  3.01it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.91it/s] 33%|███▎      | 28/86 [00:09<00:19,  2.98it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.99it/s] 43%|████▎     | 37/86 [00:12<00:16,  3.04it/s] 33%|███▎      | 28/86 [00:09<00:19,  2.91it/s] 34%|███▎      | 29/86 [00:09<00:19,  2.98it/s] 30%|███       | 26/86 [00:08<00:20,  3.00it/s] 44%|████▍     | 38/86 [00:12<00:15,  3.05it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.92it/s] 35%|███▍      | 30/86 [00:10<00:18,  2.98it/s] 31%|███▏      | 27/86 [00:09<00:19,  3.00it/s] 45%|████▌     | 39/86 [00:13<00:15,  3.07it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.92it/s] 36%|███▌      | 31/86 [00:10<00:18,  2.99it/s] 33%|███▎      | 28/86 [00:09<00:19,  3.00it/s] 47%|████▋     | 40/86 [00:13<00:14,  3.09it/s] 36%|███▌      | 31/86 [00:10<00:18,  2.94it/s] 37%|███▋      | 32/86 [00:10<00:18,  2.99it/s] 34%|███▎      | 29/86 [00:09<00:18,  3.00it/s] 48%|████▊     | 41/86 [00:13<00:14,  3.10it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.95it/s] 38%|███▊      | 33/86 [00:11<00:17,  2.99it/s] 35%|███▍      | 30/86 [00:10<00:18,  3.01it/s] 49%|████▉     | 42/86 [00:14<00:14,  3.09it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.94it/s] 40%|███▉      | 34/86 [00:11<00:17,  2.99it/s] 36%|███▌      | 31/86 [00:10<00:18,  3.00it/s] 50%|█████     | 43/86 [00:14<00:14,  3.07it/s] 40%|███▉      | 34/86 [00:11<00:17,  2.94it/s] 41%|████      | 35/86 [00:11<00:17,  2.99it/s] 37%|███▋      | 32/86 [00:10<00:17,  3.00it/s] 51%|█████     | 44/86 [00:14<00:14,  2.99it/s] 41%|████      | 35/86 [00:12<00:17,  2.92it/s] 42%|████▏     | 36/86 [00:12<00:16,  2.99it/s] 38%|███▊      | 33/86 [00:11<00:17,  3.01it/s] 52%|█████▏    | 45/86 [00:15<00:13,  3.01it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.94it/s] 43%|████▎     | 37/86 [00:12<00:16,  3.00it/s] 40%|███▉      | 34/86 [00:11<00:17,  3.01it/s] 53%|█████▎    | 46/86 [00:15<00:13,  3.04it/s] 43%|████▎     | 37/86 [00:12<00:16,  2.95it/s] 44%|████▍     | 38/86 [00:12<00:16,  3.00it/s] 41%|████      | 35/86 [00:11<00:16,  3.03it/s] 55%|█████▍    | 47/86 [00:15<00:12,  3.04it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.95it/s] 45%|████▌     | 39/86 [00:13<00:15,  3.01it/s] 42%|████▏     | 36/86 [00:12<00:16,  3.03it/s] 56%|█████▌    | 48/86 [00:16<00:12,  3.06it/s] 45%|████▌     | 39/86 [00:13<00:15,  2.96it/s] 47%|████▋     | 40/86 [00:13<00:15,  3.01it/s] 43%|████▎     | 37/86 [00:12<00:16,  3.03it/s] 57%|█████▋    | 49/86 [00:16<00:12,  3.08it/s] 47%|████▋     | 40/86 [00:13<00:15,  2.97it/s] 48%|████▊     | 41/86 [00:13<00:14,  3.01it/s] 44%|████▍     | 38/86 [00:12<00:15,  3.03it/s] 58%|█████▊    | 50/86 [00:16<00:11,  3.08it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.97it/s] 59%|█████▉    | 51/86 [00:17<00:11,  3.09it/s] 49%|████▉     | 42/86 [00:14<00:14,  3.00it/s] 45%|████▌     | 39/86 [00:13<00:15,  3.02it/s] 49%|████▉     | 42/86 [00:14<00:14,  2.97it/s] 60%|██████    | 52/86 [00:17<00:10,  3.10it/s] 50%|█████     | 43/86 [00:14<00:14,  3.00it/s] 47%|████▋     | 40/86 [00:13<00:15,  3.02it/s] 62%|██████▏   | 53/86 [00:17<00:10,  3.09it/s] 50%|█████     | 43/86 [00:14<00:14,  2.96it/s] 48%|████▊     | 41/86 [00:13<00:14,  3.03it/s] 51%|█████     | 44/86 [00:14<00:14,  3.00it/s] 63%|██████▎   | 54/86 [00:18<00:10,  3.07it/s] 51%|█████     | 44/86 [00:15<00:14,  2.95it/s] 49%|████▉     | 42/86 [00:14<00:14,  3.03it/s] 52%|█████▏    | 45/86 [00:15<00:13,  2.99it/s] 64%|██████▍   | 55/86 [00:18<00:10,  3.08it/s] 50%|█████     | 43/86 [00:14<00:14,  3.03it/s] 52%|█████▏    | 45/86 [00:15<00:13,  2.95it/s] 53%|█████▎    | 46/86 [00:15<00:13,  2.99it/s] 65%|██████▌   | 56/86 [00:18<00:09,  3.08it/s] 51%|█████     | 44/86 [00:14<00:13,  3.03it/s] 53%|█████▎    | 46/86 [00:15<00:13,  2.97it/s] 55%|█████▍    | 47/86 [00:15<00:13,  3.00it/s] 66%|██████▋   | 57/86 [00:19<00:09,  3.07it/s] 52%|█████▏    | 45/86 [00:15<00:13,  3.02it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.97it/s] 56%|█████▌    | 48/86 [00:16<00:12,  3.00it/s] 67%|██████▋   | 58/86 [00:19<00:09,  3.02it/s] 53%|█████▎    | 46/86 [00:15<00:13,  3.03it/s] 57%|█████▋    | 49/86 [00:16<00:12,  3.00it/s] 56%|█████▌    | 48/86 [00:16<00:12,  2.97it/s] 69%|██████▊   | 59/86 [00:19<00:08,  3.04it/s] 55%|█████▍    | 47/86 [00:15<00:12,  3.03it/s] 58%|█████▊    | 50/86 [00:16<00:11,  3.01it/s] 57%|█████▋    | 49/86 [00:16<00:12,  2.97it/s] 70%|██████▉   | 60/86 [00:20<00:08,  3.06it/s] 56%|█████▌    | 48/86 [00:16<00:12,  3.04it/s] 59%|█████▉    | 51/86 [00:17<00:11,  3.01it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.96it/s] 71%|███████   | 61/86 [00:20<00:08,  3.06it/s] 57%|█████▋    | 49/86 [00:16<00:12,  3.04it/s] 60%|██████    | 52/86 [00:17<00:11,  3.01it/s] 59%|█████▉    | 51/86 [00:17<00:11,  2.97it/s] 72%|███████▏  | 62/86 [00:20<00:07,  3.04it/s] 58%|█████▊    | 50/86 [00:16<00:11,  3.03it/s] 62%|██████▏   | 53/86 [00:17<00:10,  3.01it/s] 60%|██████    | 52/86 [00:17<00:11,  2.97it/s] 73%|███████▎  | 63/86 [00:21<00:07,  3.05it/s] 59%|█████▉    | 51/86 [00:17<00:11,  3.03it/s] 63%|██████▎   | 54/86 [00:18<00:10,  3.00it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.96it/s] 74%|███████▍  | 64/86 [00:21<00:07,  3.07it/s] 60%|██████    | 52/86 [00:17<00:11,  3.02it/s] 64%|██████▍   | 55/86 [00:18<00:10,  3.00it/s] 63%|██████▎   | 54/86 [00:18<00:10,  2.98it/s] 76%|███████▌  | 65/86 [00:21<00:06,  3.08it/s] 62%|██████▏   | 53/86 [00:17<00:10,  3.02it/s] 65%|██████▌   | 56/86 [00:18<00:10,  3.00it/s] 64%|██████▍   | 55/86 [00:18<00:10,  2.98it/s] 77%|███████▋  | 66/86 [00:21<00:06,  3.08it/s] 63%|██████▎   | 54/86 [00:18<00:10,  3.01it/s] 66%|██████▋   | 57/86 [00:19<00:09,  2.99it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.96it/s] 78%|███████▊  | 67/86 [00:22<00:06,  3.08it/s] 64%|██████▍   | 55/86 [00:18<00:10,  3.01it/s] 67%|██████▋   | 58/86 [00:19<00:09,  2.99it/s] 66%|██████▋   | 57/86 [00:19<00:09,  2.97it/s] 79%|███████▉  | 68/86 [00:22<00:05,  3.10it/s] 65%|██████▌   | 56/86 [00:18<00:09,  3.02it/s] 69%|██████▊   | 59/86 [00:19<00:09,  2.98it/s] 67%|██████▋   | 58/86 [00:19<00:09,  2.98it/s] 80%|████████  | 69/86 [00:22<00:05,  3.11it/s] 66%|██████▋   | 57/86 [00:19<00:09,  3.01it/s] 70%|██████▉   | 60/86 [00:20<00:08,  2.99it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.98it/s] 81%|████████▏ | 70/86 [00:23<00:05,  3.09it/s] 67%|██████▋   | 58/86 [00:19<00:09,  3.01it/s] 71%|███████   | 61/86 [00:20<00:08,  2.99it/s] 70%|██████▉   | 60/86 [00:20<00:08,  2.97it/s] 83%|████████▎ | 71/86 [00:23<00:04,  3.06it/s] 69%|██████▊   | 59/86 [00:19<00:08,  3.01it/s] 72%|███████▏  | 62/86 [00:20<00:08,  2.99it/s] 71%|███████   | 61/86 [00:20<00:08,  2.97it/s] 84%|████████▎ | 72/86 [00:23<00:04,  3.07it/s] 70%|██████▉   | 60/86 [00:20<00:08,  3.01it/s] 73%|███████▎  | 63/86 [00:21<00:07,  2.98it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.96it/s] 85%|████████▍ | 73/86 [00:24<00:04,  3.01it/s] 71%|███████   | 61/86 [00:20<00:08,  3.01it/s] 74%|███████▍  | 64/86 [00:21<00:07,  2.99it/s] 73%|███████▎  | 63/86 [00:21<00:07,  2.97it/s] 86%|████████▌ | 74/86 [00:24<00:03,  3.03it/s] 72%|███████▏  | 62/86 [00:20<00:07,  3.01it/s] 76%|███████▌  | 65/86 [00:21<00:07,  2.99it/s] 74%|███████▍  | 64/86 [00:21<00:07,  2.97it/s] 87%|████████▋ | 75/86 [00:24<00:03,  3.06it/s] 73%|███████▎  | 63/86 [00:21<00:07,  3.00it/s] 77%|███████▋  | 66/86 [00:22<00:06,  2.96it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.97it/s] 88%|████████▊ | 76/86 [00:25<00:03,  3.06it/s] 74%|███████▍  | 64/86 [00:21<00:07,  3.01it/s] 78%|███████▊  | 67/86 [00:22<00:06,  2.97it/s] 77%|███████▋  | 66/86 [00:22<00:06,  2.98it/s] 90%|████████▉ | 77/86 [00:25<00:02,  3.06it/s] 76%|███████▌  | 65/86 [00:21<00:06,  3.01it/s] 79%|███████▉  | 68/86 [00:23<00:06,  2.98it/s] 78%|███████▊  | 67/86 [00:22<00:06,  2.98it/s] 91%|█████████ | 78/86 [00:25<00:02,  3.06it/s] 77%|███████▋  | 66/86 [00:22<00:06,  3.00it/s] 80%|████████  | 69/86 [00:23<00:05,  2.98it/s] 79%|███████▉  | 68/86 [00:23<00:06,  2.98it/s] 92%|█████████▏| 79/86 [00:26<00:02,  3.05it/s] 78%|███████▊  | 67/86 [00:22<00:06,  3.00it/s] 81%|████████▏ | 70/86 [00:23<00:05,  2.97it/s] 80%|████████  | 69/86 [00:23<00:05,  2.98it/s] 93%|█████████▎| 80/86 [00:26<00:01,  3.06it/s] 79%|███████▉  | 68/86 [00:22<00:05,  3.01it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.98it/s] 81%|████████▏ | 70/86 [00:23<00:05,  2.98it/s] 94%|█████████▍| 81/86 [00:26<00:01,  3.07it/s] 80%|████████  | 69/86 [00:23<00:05,  3.01it/s] 84%|████████▎ | 72/86 [00:24<00:04,  2.98it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.98it/s] 95%|█████████▌| 82/86 [00:27<00:01,  3.07it/s] 81%|████████▏ | 70/86 [00:23<00:05,  3.00it/s] 85%|████████▍ | 73/86 [00:24<00:04,  2.99it/s] 84%|████████▎ | 72/86 [00:24<00:04,  2.98it/s] 97%|█████████▋| 83/86 [00:27<00:00,  3.09it/s] 83%|████████▎ | 71/86 [00:23<00:04,  3.01it/s] 86%|████████▌ | 74/86 [00:25<00:04,  3.00it/s] 85%|████████▍ | 73/86 [00:24<00:04,  2.98it/s] 98%|█████████▊| 84/86 [00:27<00:00,  3.09it/s] 84%|████████▎ | 72/86 [00:24<00:04,  3.00it/s] 87%|████████▋ | 75/86 [00:25<00:03,  3.00it/s] 86%|████████▌ | 74/86 [00:25<00:04,  2.99it/s] 99%|█████████▉| 85/86 [00:28<00:00,  3.10it/s] 85%|████████▍ | 73/86 [00:24<00:04,  3.01it/s] 88%|████████▊ | 76/86 [00:25<00:03,  3.00it/s]100%|██████████| 86/86 [00:28<00:00,  3.10it/s]100%|██████████| 86/86 [00:28<00:00,  3.02it/s]
 87%|████████▋ | 75/86 [00:25<00:03,  2.98it/s] 86%|████████▌ | 74/86 [00:24<00:04,  2.99it/s] 90%|████████▉ | 77/86 [00:26<00:03,  2.99it/s] 88%|████████▊ | 76/86 [00:25<00:03,  2.97it/s] 87%|████████▋ | 75/86 [00:25<00:03,  3.00it/s] 91%|█████████ | 78/86 [00:26<00:02,  2.99it/s] 90%|████████▉ | 77/86 [00:26<00:03,  2.97it/s] 88%|████████▊ | 76/86 [00:25<00:03,  2.95it/s] 92%|█████████▏| 79/86 [00:26<00:02,  2.94it/s] 91%|█████████ | 78/86 [00:26<00:02,  2.95it/s] 90%|████████▉ | 77/86 [00:25<00:03,  2.90it/s] 93%|█████████▎| 80/86 [00:27<00:02,  2.90it/s] 92%|█████████▏| 79/86 [00:26<00:02,  2.96it/s] 91%|█████████ | 78/86 [00:26<00:02,  2.89it/s] 94%|█████████▍| 81/86 [00:27<00:01,  2.89it/s] 93%|█████████▎| 80/86 [00:27<00:02,  2.98it/s] 92%|█████████▏| 79/86 [00:26<00:02,  2.92it/s] 94%|█████████▍| 81/86 [00:27<00:01,  3.00it/s] 95%|█████████▌| 82/86 [00:27<00:01,  2.93it/s] 93%|█████████▎| 80/86 [00:26<00:02,  2.94it/s] 95%|█████████▌| 82/86 [00:27<00:01,  2.99it/s] 97%|█████████▋| 83/86 [00:28<00:01,  2.93it/s] 94%|█████████▍| 81/86 [00:27<00:01,  2.94it/s] 97%|█████████▋| 83/86 [00:28<00:01,  2.99it/s] 98%|█████████▊| 84/86 [00:28<00:00,  2.92it/s] 95%|█████████▌| 82/86 [00:27<00:01,  2.94it/s] 98%|█████████▊| 84/86 [00:28<00:00,  2.98it/s] 99%|█████████▉| 85/86 [00:28<00:00,  2.93it/s] 97%|█████████▋| 83/86 [00:27<00:01,  2.96it/s] 99%|█████████▉| 85/86 [00:28<00:00,  2.98it/s]100%|██████████| 86/86 [00:29<00:00,  2.94it/s]100%|██████████| 86/86 [00:29<00:00,  2.96it/s]
 98%|█████████▊| 84/86 [00:28<00:00,  2.96it/s]100%|██████████| 86/86 [00:29<00:00,  2.97it/s]100%|██████████| 86/86 [00:29<00:00,  2.93it/s]
 99%|█████████▉| 85/86 [00:28<00:00,  2.93it/s]100%|██████████| 86/86 [00:28<00:00,  2.91it/s]100%|██████████| 86/86 [00:28<00:00,  2.97it/s]
W0318 06:42:06.107000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.107000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.107000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.107000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.107000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.108000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.108000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.149000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.149000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.149000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.149000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.149000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.164000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.164000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.164000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.164000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.165000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.326000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.327000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.327000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.327000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.327000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.635000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.635000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.635000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.635000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.635000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.635000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.635000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.667000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.667000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.668000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.668000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.668000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.737000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.737000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.737000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.737000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:06.737000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:07.885000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:07.891000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:07.897000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:07.897000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.224000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.224000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.224000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.224000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.224000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.224000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.225000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.265000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.265000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.265000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.265000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.265000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.280000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.280000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.280000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.280000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.280000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.332000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.332000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.332000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.332000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.332000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.332000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.332000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.361000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.361000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.361000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.361000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.361000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.442000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.442000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.442000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.442000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.442000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.572000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.572000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.572000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.572000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.572000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.572000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.573000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.612000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.612000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.612000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.612000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.612000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.626000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.627000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.627000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.627000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.627000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.695000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.696000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.696000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.696000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.696000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.696000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.696000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.696000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.747000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.747000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.747000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.747000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.747000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.747000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.748000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.778000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.778000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.778000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.778000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.778000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.787000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.787000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.787000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.787000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.787000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.844000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.845000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.845000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.845000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.845000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.978000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.978000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.978000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.978000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:08.978000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.094000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.094000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.094000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.094000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.094000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.094000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.095000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.123000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.123000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.124000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.124000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.124000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.158000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.159000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.159000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.159000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.159000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.159000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.159000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.191000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.191000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.191000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.191000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.191000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.200000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.200000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.200000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.200000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.200000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.215000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.215000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.215000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.215000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.216000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.301000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.306000 140697854920512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.376000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.376000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.376000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.376000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.377000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.687000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.687000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.687000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.687000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.687000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.687000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.688000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.718000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.718000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.718000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.718000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.718000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.784000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.784000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.784000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.784000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.784000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.981000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.986000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.992000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:09.993000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.332000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.337000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.343000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.343000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.410000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.410000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.410000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.410000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.410000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.410000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.410000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.440000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.440000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.441000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.441000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.441000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.770000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.770000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.770000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.770000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.770000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.770000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.770000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.770000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.770000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.770000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.770000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.770000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.770000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.770000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.770000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.801000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.801000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.801000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.801000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.802000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.913000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.926000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.934000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:10.934000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.055000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.055000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.055000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.055000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.056000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.138000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.139000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.139000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.139000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.139000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.139000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.139000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.139000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.376000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.376000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.376000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.376000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.376000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.377000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.377000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.391000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.396000 140064471213888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.409000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.409000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.409000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.409000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.409000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.422000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.423000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.423000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.423000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.423000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.744000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.749000 140529091503936 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.761000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.761000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.762000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.762000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.762000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.762000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.762000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:11.762000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:12.063000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:12.064000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:12.064000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:12.064000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:12.064000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:12.403000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:12.409000 139916159047488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0318 06:42:16.676147 1104620 finetune.py:45] layer 8_down initial loss 0.012420154176652431
W0318 06:42:16.676362 1104620 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:42:17.220639 1104620 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

8_down proxy err 0.012521510943770409 tr(WHW.T) 37.1777229309082
I0318 06:42:18.167423 1104801 finetune.py:45] layer 9_down initial loss 0.013233844190835953
W0318 06:42:18.167667 1104801 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0318 06:42:18.170163 1105740 finetune.py:45] layer 11_down initial loss 0.01764819398522377
W0318 06:42:18.170346 1105740 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:42:18.623509 1105740 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0318 06:42:18.699991 1104801 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

11_down proxy err 0.012986380606889725 tr(WHW.T) 56.135292053222656
9_down proxy err 0.012843177653849125 tr(WHW.T) 42.994815826416016
I0318 06:42:19.297266 1105152 finetune.py:45] layer 10_down initial loss 0.02289414592087269
W0318 06:42:19.297502 1105152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:42:19.769900 1105152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

10_down proxy err 0.012256188318133354 tr(WHW.T) 52.33583450317383
I0318 06:42:22.332535 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 12 in 1.3197219371795654s
I0318 06:42:22.785296 1092738 quantize_finetune_llama.py:159] layer 13 gpu 1
I0318 06:42:24.550820 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 13 in 1.3614370822906494s
I0318 06:42:24.700422 1109641 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:42:24.700594 1109641 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:42:24.700663 1109641 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:42:24.896622 1109641 config.py:58] PyTorch version 2.4.0 available.
I0318 06:42:25.018680 1092738 quantize_finetune_llama.py:159] layer 14 gpu 2
I0318 06:42:26.742100 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 14 in 1.3122920989990234s
I0318 06:42:26.889938 1109803 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:42:26.890122 1109803 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:42:26.890191 1109803 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:42:27.153228 1109641 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0318 06:42:27.242758 1109803 config.py:58] PyTorch version 2.4.0 available.
I0318 06:42:27.254816 1092738 quantize_finetune_llama.py:159] layer 15 gpu 3
W0318 06:42:27.495177 1109641 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]I0318 06:42:29.098801 1110237 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:42:29.098957 1110237 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:42:29.099014 1110237 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:42:29.141447 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 15 in 1.4874136447906494s
I0318 06:42:29.448389 1110237 config.py:58] PyTorch version 2.4.0 available.
I0318 06:42:29.594076 1109803 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0318 06:42:29.640821 1092738 quantize_finetune_llama.py:159] layer 16 gpu 0
  3%|▎         | 1/32 [00:01<00:45,  1.47s/it]W0318 06:42:30.114231 1109803 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:01<00:24,  1.25it/s]  9%|▉         | 3/32 [00:02<00:17,  1.70it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.07it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s]  0%|          | 0/32 [00:00<?, ?it/s]I0318 06:42:31.490849 1110755 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:42:31.490963 1110755 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:42:31.491026 1110755 utils.py:162] NumExpr defaulting to 16 threads.
 19%|█▉        | 6/32 [00:03<00:10,  2.52it/s]I0318 06:42:31.688906 1110755 config.py:58] PyTorch version 2.4.0 available.
I0318 06:42:31.873999 1110237 data_utils.py:336] using 256 training seqs, 128 validation seqs
 22%|██▏       | 7/32 [00:03<00:09,  2.60it/s]W0318 06:42:32.280060 1110237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:03<00:08,  2.73it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.80it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.88it/s]  3%|▎         | 1/32 [00:01<00:52,  1.69s/it]  0%|          | 0/32 [00:00<?, ?it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.97it/s]  6%|▋         | 2/32 [00:02<00:26,  1.11it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.90it/s]  9%|▉         | 3/32 [00:02<00:18,  1.54it/s]I0318 06:42:33.782271 1110755 data_utils.py:336] using 256 training seqs, 128 validation seqs
 41%|████      | 13/32 [00:05<00:06,  2.93it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.88it/s]W0318 06:42:34.126688 1110755 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:05<00:06,  2.96it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.17it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.00it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.37it/s] 50%|█████     | 16/32 [00:06<00:05,  3.05it/s]  3%|▎         | 1/32 [00:01<00:53,  1.74s/it]  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.05it/s]  6%|▋         | 2/32 [00:02<00:27,  1.10it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.66it/s]  9%|▉         | 3/32 [00:02<00:18,  1.56it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.96it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.73it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.93it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.01it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.24it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.04it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.84it/s]  3%|▎         | 1/32 [00:01<00:47,  1.53s/it] 66%|██████▌   | 21/32 [00:08<00:03,  3.07it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.46it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.84it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.08it/s]  6%|▋         | 2/32 [00:01<00:24,  1.21it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.61it/s] 41%|████      | 13/32 [00:05<00:06,  2.89it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.10it/s]  9%|▉         | 3/32 [00:02<00:17,  1.67it/s] 25%|██▌       | 8/32 [00:04<00:08,  2.75it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.92it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.12it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.85it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.03it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.93it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.14it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.91it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 50%|█████     | 16/32 [00:06<00:05,  2.95it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.14it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.96it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.50it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.96it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.15it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.99it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.66it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.14it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.96it/s] 41%|████      | 13/32 [00:05<00:06,  3.00it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.77it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.15it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.98it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.02it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.85it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.16it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.97it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.03it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.90it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.16it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.96it/s] 50%|█████     | 16/32 [00:06<00:05,  3.04it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.94it/s]100%|██████████| 32/32 [00:11<00:00,  3.16it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  2.97it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.06it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.97it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.97it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.06it/s] 41%|████      | 13/32 [00:05<00:06,  2.95it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.96it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.04it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.91it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.98it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.03it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.94it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.05it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.97it/s] 50%|█████     | 16/32 [00:06<00:05,  2.93it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.05it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.97it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.95it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.07it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.98it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.99it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.05it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.95it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.01it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.05it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.94it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.02it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.05it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.95it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.03it/s]W0318 06:42:43.272000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.272000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.272000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.272000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.272000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.272000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.272000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.297000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.297000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.297000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.297000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.297000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.313000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.313000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.313000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.314000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.314000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  3.06it/s]100%|██████████| 32/32 [00:12<00:00,  2.97it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  3.03it/s]W0318 06:42:43.621000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.621000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.621000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.621000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:43.621000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  3.06it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.04it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.01it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.05it/s]W0318 06:42:44.473000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:44.473000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:44.473000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:44.473000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:44.473000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:44.473000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:44.473000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:44.490000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:44.490000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:44.490000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:44.490000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:44.490000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  3.01it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.06it/s]W0318 06:42:44.721000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:44.721000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:44.721000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:44.721000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:44.721000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  2.98it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.06it/s]100%|██████████| 32/32 [00:11<00:00,  2.98it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
 84%|████████▍ | 27/32 [00:10<00:01,  3.05it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.05it/s]W0318 06:42:45.842000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:45.842000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:45.842000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:45.842000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:45.842000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:45.842000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:45.842000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:45.859000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:45.859000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:45.859000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:45.859000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:45.859000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:10<00:00,  3.05it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.04it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.04it/s]W0318 06:42:46.728000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.728000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.729000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.729000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.729000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.844000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.844000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.844000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.844000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.844000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.844000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.844000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.870000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.870000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.870000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.871000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.871000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:11<00:00,  3.04it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
W0318 06:42:46.887000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.887000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.887000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.887000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:46.887000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:47.204000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:47.205000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:47.205000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:47.205000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:47.205000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0318 06:42:48.059000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.059000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.059000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.059000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.060000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.060000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.060000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.076000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.076000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.077000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.077000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.077000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.307000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.307000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.307000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.307000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.308000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.333000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.333000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.333000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.333000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.333000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.334000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.334000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.360000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.360000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.360000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.360000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.360000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.376000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.376000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.377000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.377000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.377000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.687000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.687000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.687000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.687000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:48.687000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.431000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.431000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.431000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.431000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.431000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.431000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.431000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.448000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.449000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.449000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.449000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.449000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.541000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.541000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.541000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.541000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.541000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.541000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.541000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.558000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.559000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.559000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.559000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.559000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.791000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.792000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.792000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.792000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:49.792000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.007000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.007000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.007000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.008000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.008000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.008000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.008000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.035000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.035000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.035000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.035000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.035000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.053000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.053000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.053000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.053000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.053000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.320000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.320000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.320000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.320000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.320000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.386000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.386000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.386000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.386000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.386000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0318 06:42:50.930000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.930000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.930000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.930000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.930000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.930000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.930000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.948000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.948000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.948000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.948000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:50.948000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.312000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.313000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.313000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.313000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.313000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.313000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.313000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.331000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.332000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.332000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.332000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.332000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.580000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.580000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.581000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.581000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.581000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.862000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.862000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.862000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.862000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:51.862000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0318 06:42:52.784000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:52.784000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:42:52.784000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:52.784000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:42:52.784000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:52.784000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:52.784000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:52.803000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:52.803000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:52.803000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:52.803000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:52.803000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
I0318 06:42:52.872313 1109641 finetune.py:45] layer 12_v initial loss 0.017272112891077995
W0318 06:42:52.872542 1109641 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:42:53.708000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:42:53.708000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:42:53.708000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:42:53.708000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:42:53.708000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:42:53.826018 1109641 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
12_v proxy err 0.009535515680909157 tr(WHW.T) 703.318603515625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.56it/s]  6%|▋         | 2/32 [00:00<00:14,  2.14it/s]I0318 06:42:55.933500 1109803 finetune.py:45] layer 13_v initial loss 0.02392338030040264
W0318 06:42:55.933843 1109803 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:01<00:11,  2.44it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.60it/s] 16%|█▌        | 5/32 [00:02<00:09,  2.70it/s]W0318 06:42:56.980062 1109803 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:02<00:09,  2.76it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.81it/s]I0318 06:42:57.724039 1110237 finetune.py:45] layer 14_v initial loss 0.025445125997066498
W0318 06:42:57.724439 1110237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:08,  2.83it/s]13_v proxy err 0.010592550039291382 tr(WHW.T) 714.5677490234375
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.85it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.86it/s]W0318 06:42:58.667590 1110237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:00<00:18,  1.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.87it/s]  6%|▋         | 2/32 [00:00<00:13,  2.19it/s]I0318 06:42:59.225555 1110755 finetune.py:45] layer 15_v initial loss 0.02620769292116165
W0318 06:42:59.225907 1110755 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:04<00:06,  2.89it/s]  9%|▉         | 3/32 [00:01<00:11,  2.48it/s] 41%|████      | 13/32 [00:04<00:06,  2.93it/s]14_v proxy err 0.01065311674028635 tr(WHW.T) 706.1612548828125
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.63it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.97it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.71it/s]W0318 06:43:00.082016 1110755 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:00<00:19,  1.62it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.98it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.81it/s]  6%|▋         | 2/32 [00:00<00:13,  2.25it/s] 50%|█████     | 16/32 [00:05<00:05,  3.00it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.88it/s]  9%|▉         | 3/32 [00:01<00:11,  2.54it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.03it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.91it/s]15_v proxy err 0.010379329323768616 tr(WHW.T) 762.7275390625
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.72it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.06it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.94it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.84it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.07it/s]  3%|▎         | 1/32 [00:00<00:18,  1.65it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.92it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.07it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.88it/s]  6%|▋         | 2/32 [00:00<00:13,  2.26it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.95it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.08it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.91it/s]  9%|▉         | 3/32 [00:01<00:11,  2.55it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.96it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.08it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.94it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.72it/s] 41%|████      | 13/32 [00:04<00:06,  2.99it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.08it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.96it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.83it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.98it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.08it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.98it/s] 19%|█▉        | 6/32 [00:02<00:08,  2.89it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.99it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.08it/s] 34%|███▍      | 11/32 [00:03<00:06,  3.02it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.93it/s] 50%|█████     | 16/32 [00:05<00:05,  3.00it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.08it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.01it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.95it/s] 53%|█████▎    | 17/32 [00:05<00:05,  3.00it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.08it/s] 41%|████      | 13/32 [00:04<00:06,  3.03it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.97it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.97it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.08it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.04it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.99it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.98it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.08it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.04it/s] 34%|███▍      | 11/32 [00:03<00:06,  3.01it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.99it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.08it/s] 50%|█████     | 16/32 [00:05<00:05,  3.01it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.02it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.99it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.07it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.03it/s] 41%|████      | 13/32 [00:04<00:06,  3.03it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.99it/s]100%|██████████| 32/32 [00:10<00:00,  3.08it/s]100%|██████████| 32/32 [00:10<00:00,  2.93it/s]
 56%|█████▋    | 18/32 [00:06<00:04,  3.02it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.03it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.96it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.01it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.98it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.94it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.03it/s] 50%|█████     | 16/32 [00:05<00:05,  2.94it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.95it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.04it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.94it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.96it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.04it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.92it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.97it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.06it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.96it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.98it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.05it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.94it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.97it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.04it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.92it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.98it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.02it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.90it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.98it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.01it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.88it/s]100%|██████████| 32/32 [00:11<00:00,  2.98it/s]100%|██████████| 32/32 [00:11<00:00,  2.90it/s]
 88%|████████▊ | 28/32 [00:09<00:01,  3.02it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.86it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.00it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.86it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.99it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.86it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.98it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.87it/s]100%|██████████| 32/32 [00:10<00:00,  2.99it/s]100%|██████████| 32/32 [00:10<00:00,  2.94it/s]
 88%|████████▊ | 28/32 [00:09<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.86it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.85it/s]100%|██████████| 32/32 [00:11<00:00,  2.85it/s]100%|██████████| 32/32 [00:11<00:00,  2.87it/s]
I0318 06:43:12.560529 1109641 finetune.py:45] layer 12_q initial loss 0.0172578115016222
W0318 06:43:12.560820 1109641 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:43:13.499503 1109641 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_q proxy err 0.0025946174282580614 tr(WHW.T) 7045.6435546875
  0%|          | 0/32 [00:00<?, ?it/s]I0318 06:43:15.191000 1109803 finetune.py:45] layer 13_q initial loss 0.023531902581453323
W0318 06:43:15.191303 1109803 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:00<00:18,  1.66it/s]  6%|▋         | 2/32 [00:00<00:13,  2.23it/s]  9%|▉         | 3/32 [00:01<00:11,  2.50it/s]W0318 06:43:16.146278 1109803 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:01<00:10,  2.64it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.73it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.79it/s]I0318 06:43:17.035645 1110237 finetune.py:45] layer 14_q initial loss 0.025443026795983315
W0318 06:43:17.035887 1110237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

13_q proxy err 0.002711925655603409 tr(WHW.T) 6956.0361328125
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.81it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.84it/s]  3%|▎         | 1/32 [00:00<00:18,  1.70it/s]W0318 06:43:17.907845 1110237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:03<00:08,  2.86it/s]  6%|▋         | 2/32 [00:00<00:13,  2.25it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.86it/s]I0318 06:43:18.452965 1110755 finetune.py:45] layer 15_q initial loss 0.026212669909000397
W0318 06:43:18.453204 1110755 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:01<00:11,  2.49it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.92it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.66it/s]14_q proxy err 0.0027357302606105804 tr(WHW.T) 7077.060546875
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.95it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.75it/s]W0318 06:43:19.270669 1110755 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 13/32 [00:04<00:06,  2.97it/s]  3%|▎         | 1/32 [00:00<00:17,  1.74it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.80it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.96it/s]  6%|▋         | 2/32 [00:00<00:12,  2.32it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.86it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.99it/s]  9%|▉         | 3/32 [00:01<00:11,  2.60it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.88it/s]15_q proxy err 0.002529903082177043 tr(WHW.T) 7252.0009765625
  0%|          | 0/32 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:05<00:05,  3.00it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.72it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.88it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.02it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.80it/s]  3%|▎         | 1/32 [00:00<00:17,  1.77it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.89it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.05it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.87it/s]  6%|▋         | 2/32 [00:00<00:12,  2.34it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.91it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.05it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.92it/s]  9%|▉         | 3/32 [00:01<00:11,  2.61it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.92it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.06it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.94it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.75it/s] 41%|████      | 13/32 [00:04<00:06,  2.93it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.05it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.96it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.83it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.93it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.06it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.97it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.89it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.93it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.05it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.98it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.93it/s] 50%|█████     | 16/32 [00:05<00:05,  2.94it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.05it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.99it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.95it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.94it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.06it/s] 41%|████      | 13/32 [00:04<00:06,  2.99it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.97it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.92it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.05it/s] 44%|████▍     | 14/32 [00:04<00:06,  3.00it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.98it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.93it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.05it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.02it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.99it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.06it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.93it/s] 50%|█████     | 16/32 [00:05<00:05,  3.02it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.00it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.05it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.93it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.01it/s] 41%|████      | 13/32 [00:04<00:06,  3.00it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.06it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.94it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.01it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.00it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.06it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.94it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.00it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.00it/s]100%|██████████| 32/32 [00:10<00:00,  3.06it/s]100%|██████████| 32/32 [00:10<00:00,  2.93it/s]
 75%|███████▌  | 24/32 [00:08<00:02,  2.94it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.00it/s] 50%|█████     | 16/32 [00:05<00:05,  3.00it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.94it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.00it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.95it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.94it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.97it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.93it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.93it/s] 72%|███████▏  | 23/32 [00:07<00:03,  2.97it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.94it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.92it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.98it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.94it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.94it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.00it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.96it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.95it/s] 81%|████████▏ | 26/32 [00:08<00:02,  3.00it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.93it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.95it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.00it/s] 72%|███████▏  | 23/32 [00:07<00:03,  2.90it/s]100%|██████████| 32/32 [00:11<00:00,  2.95it/s]100%|██████████| 32/32 [00:11<00:00,  2.87it/s]
 88%|████████▊ | 28/32 [00:09<00:01,  3.01it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.89it/s] 91%|█████████ | 29/32 [00:09<00:01,  2.96it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.88it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.94it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.86it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.92it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.85it/s]100%|██████████| 32/32 [00:10<00:00,  2.95it/s]100%|██████████| 32/32 [00:10<00:00,  2.92it/s]
 88%|████████▊ | 28/32 [00:09<00:01,  2.85it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.85it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.85it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  2.88it/s]100%|██████████| 32/32 [00:11<00:00,  2.88it/s]
I0318 06:43:32.372545 1109641 finetune.py:45] layer 12_k initial loss 0.017264550551772118
W0318 06:43:32.372793 1109641 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:43:33.214370 1109641 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_k proxy err 0.0017872168682515621 tr(WHW.T) 10893.65625
  0%|          | 0/32 [00:00<?, ?it/s]I0318 06:43:34.493686 1109803 finetune.py:45] layer 13_k initial loss 0.023194460198283195
W0318 06:43:34.493865 1109803 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:00<00:18,  1.67it/s]  6%|▋         | 2/32 [00:00<00:13,  2.23it/s]W0318 06:43:35.408854 1109803 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:01<00:11,  2.51it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.67it/s]I0318 06:43:36.335685 1110237 finetune.py:45] layer 14_k initial loss 0.025425495579838753
W0318 06:43:36.335944 1110237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:01<00:09,  2.75it/s]13_k proxy err 0.0018757878569886088 tr(WHW.T) 10426.6328125
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.80it/s]  3%|▎         | 1/32 [00:00<00:17,  1.73it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.83it/s]W0318 06:43:37.169776 1110237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:00<00:13,  2.27it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.86it/s]I0318 06:43:37.521205 1110755 finetune.py:45] layer 15_k initial loss 0.02620258741080761
W0318 06:43:37.521396 1110755 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:01<00:11,  2.53it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.91it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.96it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.69it/s]14_k proxy err 0.0017853701720014215 tr(WHW.T) 11295.16796875
  0%|          | 0/32 [00:00<?, ?it/s]W0318 06:43:38.308507 1110755 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:03<00:07,  3.00it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.77it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.00it/s]  3%|▎         | 1/32 [00:00<00:17,  1.79it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.84it/s] 41%|████      | 13/32 [00:04<00:06,  3.02it/s]  6%|▋         | 2/32 [00:00<00:12,  2.35it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.86it/s]15_k proxy err 0.0017777503235265613 tr(WHW.T) 11072.396484375
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.05it/s]  9%|▉         | 3/32 [00:01<00:11,  2.61it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.89it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.07it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.76it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.92it/s]  3%|▎         | 1/32 [00:00<00:17,  1.77it/s] 50%|█████     | 16/32 [00:05<00:05,  3.08it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.86it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.93it/s]  6%|▋         | 2/32 [00:00<00:12,  2.35it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.07it/s] 19%|█▉        | 6/32 [00:02<00:08,  2.92it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.95it/s]  9%|▉         | 3/32 [00:01<00:11,  2.63it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.07it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.96it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.96it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.77it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.09it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.96it/s] 41%|████      | 13/32 [00:04<00:06,  2.94it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.86it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.10it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.96it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.93it/s] 19%|█▉        | 6/32 [00:02<00:08,  2.91it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.10it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.99it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.95it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.95it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.09it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.99it/s] 50%|█████     | 16/32 [00:05<00:05,  2.95it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.96it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.10it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.01it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.96it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.98it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.10it/s] 41%|████      | 13/32 [00:04<00:06,  3.02it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.97it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.99it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.09it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.00it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.94it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.99it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.08it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.99it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.94it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.00it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.08it/s] 50%|█████     | 16/32 [00:05<00:05,  3.00it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.96it/s] 41%|████      | 13/32 [00:04<00:06,  3.01it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.09it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.01it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.94it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.03it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.10it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.00it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.96it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.04it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.10it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.01it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.99it/s] 50%|█████     | 16/32 [00:05<00:05,  3.04it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.11it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.03it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.00it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.04it/s]100%|██████████| 32/32 [00:10<00:00,  3.11it/s]100%|██████████| 32/32 [00:10<00:00,  2.97it/s]
 66%|██████▌   | 21/32 [00:07<00:03,  3.01it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.99it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.01it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.04it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.00it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.95it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.02it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.99it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.95it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.03it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.99it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.95it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.03it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.99it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.97it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.03it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.96it/s] 72%|███████▏  | 23/32 [00:07<00:03,  2.96it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.00it/s]100%|██████████| 32/32 [00:11<00:00,  2.96it/s]100%|██████████| 32/32 [00:11<00:00,  2.90it/s]
 75%|███████▌  | 24/32 [00:08<00:02,  2.93it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.00it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.90it/s] 91%|█████████ | 29/32 [00:09<00:01,  2.96it/s] 81%|████████▏ | 26/32 [00:08<00:02,  2.88it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.95it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.88it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.95it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.87it/s]100%|██████████| 32/32 [00:10<00:00,  2.98it/s]100%|██████████| 32/32 [00:10<00:00,  2.94it/s]
 91%|█████████ | 29/32 [00:09<00:01,  2.86it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.86it/s]100%|██████████| 32/32 [00:11<00:00,  2.86it/s]100%|██████████| 32/32 [00:11<00:00,  2.90it/s]
I0318 06:43:51.912066 1109641 finetune.py:45] layer 12_o initial loss 0.017072077840566635
W0318 06:43:51.912286 1109641 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:43:52.747811 1109641 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:43:53.665281 1109803 finetune.py:45] layer 13_o initial loss 0.022832926362752914
W0318 06:43:53.665677 1109803 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

12_o proxy err 0.011704766191542149 tr(WHW.T) 39.29071044921875
  0%|          | 0/32 [00:00<?, ?it/s]W0318 06:43:54.529609 1109803 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:39,  1.28s/it]I0318 06:43:55.615401 1110237 finetune.py:45] layer 14_o initial loss 0.02504858374595642
W0318 06:43:55.615659 1110237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

13_o proxy err 0.011148538440465927 tr(WHW.T) 45.8377571105957
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:02<00:33,  1.13s/it]W0318 06:43:56.448304 1110237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:43:56.618541 1110755 finetune.py:45] layer 15_o initial loss 0.02581525221467018
W0318 06:43:56.618802 1110755 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:40,  1.31s/it]  9%|▉         | 3/32 [00:03<00:30,  1.06s/it]W0318 06:43:57.542576 1110755 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_o proxy err 0.012097357772290707 tr(WHW.T) 50.921180725097656
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:02<00:33,  1.13s/it] 12%|█▎        | 4/32 [00:04<00:28,  1.03s/it]15_o proxy err 0.011023267172276974 tr(WHW.T) 59.61665344238281
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:38,  1.23s/it]  9%|▉         | 3/32 [00:03<00:31,  1.08s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.00s/it]  6%|▋         | 2/32 [00:02<00:32,  1.09s/it]  3%|▎         | 1/32 [00:01<00:38,  1.25s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.05s/it] 19%|█▉        | 6/32 [00:06<00:25,  1.01it/s]  9%|▉         | 3/32 [00:03<00:30,  1.04s/it]  6%|▋         | 2/32 [00:02<00:32,  1.09s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.04s/it] 22%|██▏       | 7/32 [00:07<00:24,  1.02it/s] 12%|█▎        | 4/32 [00:04<00:28,  1.01s/it]  9%|▉         | 3/32 [00:03<00:30,  1.05s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.03s/it] 25%|██▌       | 8/32 [00:08<00:23,  1.02it/s] 16%|█▌        | 5/32 [00:05<00:27,  1.00s/it] 12%|█▎        | 4/32 [00:04<00:28,  1.02s/it] 28%|██▊       | 9/32 [00:09<00:22,  1.03it/s] 22%|██▏       | 7/32 [00:07<00:25,  1.02s/it] 19%|█▉        | 6/32 [00:06<00:25,  1.01it/s] 16%|█▌        | 5/32 [00:05<00:27,  1.01s/it] 31%|███▏      | 10/32 [00:10<00:21,  1.03it/s] 25%|██▌       | 8/32 [00:08<00:24,  1.02s/it] 22%|██▏       | 7/32 [00:07<00:24,  1.02it/s] 19%|█▉        | 6/32 [00:06<00:25,  1.00it/s] 34%|███▍      | 11/32 [00:10<00:20,  1.04it/s] 28%|██▊       | 9/32 [00:09<00:23,  1.01s/it] 25%|██▌       | 8/32 [00:08<00:23,  1.02it/s] 22%|██▏       | 7/32 [00:07<00:24,  1.01it/s] 38%|███▊      | 12/32 [00:11<00:19,  1.04it/s] 31%|███▏      | 10/32 [00:10<00:22,  1.01s/it] 28%|██▊       | 9/32 [00:09<00:22,  1.03it/s] 25%|██▌       | 8/32 [00:08<00:23,  1.01it/s] 41%|████      | 13/32 [00:12<00:18,  1.04it/s] 34%|███▍      | 11/32 [00:11<00:21,  1.01s/it] 31%|███▏      | 10/32 [00:09<00:21,  1.03it/s] 44%|████▍     | 14/32 [00:13<00:17,  1.04it/s] 28%|██▊       | 9/32 [00:09<00:22,  1.01it/s] 38%|███▊      | 12/32 [00:12<00:20,  1.01s/it] 34%|███▍      | 11/32 [00:10<00:20,  1.03it/s] 47%|████▋     | 15/32 [00:14<00:16,  1.04it/s] 31%|███▏      | 10/32 [00:10<00:21,  1.01it/s] 41%|████      | 13/32 [00:13<00:19,  1.01s/it] 38%|███▊      | 12/32 [00:11<00:19,  1.03it/s] 50%|█████     | 16/32 [00:15<00:15,  1.04it/s] 34%|███▍      | 11/32 [00:11<00:20,  1.01it/s] 44%|████▍     | 14/32 [00:14<00:18,  1.00s/it] 41%|████      | 13/32 [00:12<00:18,  1.03it/s] 53%|█████▎    | 17/32 [00:16<00:14,  1.04it/s] 38%|███▊      | 12/32 [00:12<00:19,  1.01it/s] 47%|████▋     | 15/32 [00:15<00:17,  1.00s/it] 44%|████▍     | 14/32 [00:13<00:17,  1.03it/s] 56%|█████▋    | 18/32 [00:17<00:13,  1.04it/s] 41%|████      | 13/32 [00:13<00:18,  1.01it/s] 50%|█████     | 16/32 [00:16<00:16,  1.00s/it] 47%|████▋     | 15/32 [00:14<00:16,  1.03it/s] 59%|█████▉    | 19/32 [00:18<00:12,  1.04it/s] 44%|████▍     | 14/32 [00:14<00:17,  1.01it/s] 53%|█████▎    | 17/32 [00:17<00:15,  1.01s/it] 50%|█████     | 16/32 [00:15<00:15,  1.03it/s] 62%|██████▎   | 20/32 [00:19<00:11,  1.04it/s] 47%|████▋     | 15/32 [00:15<00:16,  1.01it/s] 56%|█████▋    | 18/32 [00:18<00:14,  1.01s/it] 53%|█████▎    | 17/32 [00:16<00:14,  1.03it/s] 66%|██████▌   | 21/32 [00:20<00:10,  1.04it/s] 50%|█████     | 16/32 [00:16<00:15,  1.01it/s] 59%|█████▉    | 19/32 [00:19<00:13,  1.01s/it] 56%|█████▋    | 18/32 [00:17<00:13,  1.03it/s] 69%|██████▉   | 22/32 [00:21<00:09,  1.04it/s] 53%|█████▎    | 17/32 [00:17<00:14,  1.01it/s] 62%|██████▎   | 20/32 [00:20<00:12,  1.01s/it] 59%|█████▉    | 19/32 [00:18<00:12,  1.03it/s] 72%|███████▏  | 23/32 [00:22<00:08,  1.04it/s] 56%|█████▋    | 18/32 [00:17<00:13,  1.01it/s] 66%|██████▌   | 21/32 [00:21<00:11,  1.01s/it] 62%|██████▎   | 20/32 [00:19<00:11,  1.03it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.04it/s] 59%|█████▉    | 19/32 [00:18<00:12,  1.02it/s] 69%|██████▉   | 22/32 [00:22<00:10,  1.01s/it] 66%|██████▌   | 21/32 [00:20<00:10,  1.03it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.04it/s] 62%|██████▎   | 20/32 [00:19<00:11,  1.02it/s] 69%|██████▉   | 22/32 [00:21<00:09,  1.03it/s] 72%|███████▏  | 23/32 [00:23<00:09,  1.01s/it] 81%|████████▏ | 26/32 [00:25<00:05,  1.04it/s] 66%|██████▌   | 21/32 [00:20<00:10,  1.02it/s] 72%|███████▏  | 23/32 [00:22<00:08,  1.03it/s] 75%|███████▌  | 24/32 [00:24<00:08,  1.01s/it] 84%|████████▍ | 27/32 [00:26<00:04,  1.04it/s] 69%|██████▉   | 22/32 [00:21<00:09,  1.02it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.03it/s] 78%|███████▊  | 25/32 [00:25<00:07,  1.00s/it] 88%|████████▊ | 28/32 [00:27<00:03,  1.04it/s] 72%|███████▏  | 23/32 [00:22<00:08,  1.02it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.03it/s] 81%|████████▏ | 26/32 [00:26<00:06,  1.00s/it] 91%|█████████ | 29/32 [00:28<00:02,  1.04it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.02it/s] 81%|████████▏ | 26/32 [00:25<00:05,  1.03it/s] 84%|████████▍ | 27/32 [00:27<00:05,  1.01s/it] 94%|█████████▍| 30/32 [00:29<00:01,  1.04it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.01it/s] 84%|████████▍ | 27/32 [00:26<00:04,  1.03it/s] 97%|█████████▋| 31/32 [00:30<00:00,  1.04it/s] 88%|████████▊ | 28/32 [00:28<00:04,  1.01s/it] 81%|████████▏ | 26/32 [00:25<00:05,  1.02it/s] 88%|████████▊ | 28/32 [00:27<00:03,  1.03it/s]100%|██████████| 32/32 [00:31<00:00,  1.04it/s]100%|██████████| 32/32 [00:31<00:00,  1.03it/s]
 91%|█████████ | 29/32 [00:29<00:03,  1.01s/it] 84%|████████▍ | 27/32 [00:26<00:04,  1.01it/s] 91%|█████████ | 29/32 [00:28<00:02,  1.03it/s] 94%|█████████▍| 30/32 [00:30<00:02,  1.01s/it] 88%|████████▊ | 28/32 [00:27<00:04,  1.00s/it] 94%|█████████▍| 30/32 [00:29<00:01,  1.02it/s] 97%|█████████▋| 31/32 [00:31<00:01,  1.01s/it] 91%|█████████ | 29/32 [00:28<00:03,  1.00s/it] 97%|█████████▋| 31/32 [00:30<00:00,  1.03it/s]100%|██████████| 32/32 [00:32<00:00,  1.01s/it]100%|██████████| 32/32 [00:32<00:00,  1.02s/it]
 94%|█████████▍| 30/32 [00:29<00:01,  1.00it/s]100%|██████████| 32/32 [00:31<00:00,  1.02it/s]100%|██████████| 32/32 [00:31<00:00,  1.02it/s]
 97%|█████████▋| 31/32 [00:30<00:00,  1.01it/s]100%|██████████| 32/32 [00:31<00:00,  1.01it/s]100%|██████████| 32/32 [00:31<00:00,  1.01it/s]
W0318 06:44:31.751000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.751000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.751000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.751000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.751000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.751000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.751000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.778000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.778000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.778000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.778000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.778000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.793000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.793000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.793000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.793000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.793000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.947000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.947000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.947000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.947000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:31.947000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:32.167000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:32.167000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:32.167000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:32.167000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:32.167000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:44:32.167000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:32.167000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:32.187000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:32.187000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:32.187000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:32.187000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:32.187000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:32.253000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:32.254000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:32.254000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:32.254000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:32.254000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.086000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.380000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.380000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.380000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.380000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.380000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.380000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.380000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.399000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.400000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.400000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.400000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.400000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.642000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.642000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.642000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.642000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.642000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:33.891000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.602000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.602000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.602000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.602000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.602000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.602000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.603000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.631000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.631000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.631000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.631000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.631000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.646000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.646000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.646000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.646000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.646000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.796000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.796000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.796000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.796000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:34.796000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.010000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.011000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.011000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.011000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.011000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.011000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.011000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.032000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.032000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.032000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.033000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.033000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.094000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.094000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.094000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.094000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.094000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.935000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.953000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.953000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.953000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.953000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.953000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.954000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.954000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.982000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.982000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.982000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.982000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.982000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.997000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.997000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.997000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.997000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:35.997000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.149000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.149000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.149000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.149000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.149000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.233000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.233000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.233000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.233000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.233000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.233000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.233000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.253000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.253000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.253000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.253000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.253000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.374000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.374000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.374000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.374000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.374000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.375000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.375000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.396000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.396000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.396000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.396000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.396000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.461000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.461000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.461000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.461000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.461000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.503000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.503000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.503000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.503000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.503000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.757000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.959000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.960000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.960000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.960000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.960000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.960000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.960000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.991000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.991000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.991000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.991000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:36.991000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.008000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.008000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.008000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.008000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.008000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.171000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.171000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.171000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.171000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.172000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.308000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.405000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.405000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.405000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.406000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.406000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.406000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.406000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.429000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.429000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.429000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.429000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.429000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.497000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.497000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.497000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.497000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.498000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.615000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.615000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.615000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.615000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.615000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.615000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.615000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.636000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.636000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.636000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.636000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.636000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.887000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.888000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.888000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.888000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:37.888000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:38.139000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:38.413000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:38.735000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:38.736000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:38.736000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:44:38.736000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:44:38.736000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:38.736000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:38.736000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:38.759000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:38.759000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:38.759000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:38.760000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:38.760000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:39.027000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:44:39.027000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:44:39.027000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:44:39.027000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:44:39.028000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:44:39.302000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0318 06:44:40.382651 1109641 finetune.py:45] layer 12_up initial loss 0.016879547387361526
W0318 06:44:40.382959 1109641 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:44:41.239970 1109641 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_up proxy err 0.009484817273914814 tr(WHW.T) 1228.29833984375
  0%|          | 0/32 [00:00<?, ?it/s]I0318 06:44:42.796555 1109803 finetune.py:45] layer 13_up initial loss 0.022415127605199814
W0318 06:44:42.796722 1109803 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:44:43.609680 1109803 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:41,  1.33s/it]I0318 06:44:44.522511 1110237 finetune.py:45] layer 14_up initial loss 0.024480514228343964
W0318 06:44:44.522904 1110237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

13_up proxy err 0.009187239222228527 tr(WHW.T) 1367.6219482421875
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:02<00:34,  1.16s/it]W0318 06:44:45.378298 1110237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:44:45.407575 1110755 finetune.py:45] layer 15_up initial loss 0.02542128786444664
W0318 06:44:45.407769 1110755 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:03<00:31,  1.08s/it]W0318 06:44:46.175028 1110755 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:41,  1.32s/it]14_up proxy err 0.009518600068986416 tr(WHW.T) 1464.7156982421875
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:04<00:29,  1.04s/it]  6%|▋         | 2/32 [00:02<00:34,  1.15s/it]15_up proxy err 0.009216343984007835 tr(WHW.T) 1641.0224609375
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:05<00:27,  1.01s/it]  3%|▎         | 1/32 [00:01<00:39,  1.27s/it]  9%|▉         | 3/32 [00:03<00:31,  1.09s/it]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it] 19%|█▉        | 6/32 [00:06<00:25,  1.00it/s]  6%|▋         | 2/32 [00:02<00:32,  1.10s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.06s/it]  6%|▋         | 2/32 [00:02<00:33,  1.11s/it] 22%|██▏       | 7/32 [00:07<00:24,  1.01it/s]  9%|▉         | 3/32 [00:03<00:30,  1.05s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.05s/it]  9%|▉         | 3/32 [00:03<00:30,  1.06s/it] 25%|██▌       | 8/32 [00:08<00:23,  1.02it/s] 12%|█▎        | 4/32 [00:04<00:28,  1.02s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.04s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.04s/it] 28%|██▊       | 9/32 [00:09<00:22,  1.03it/s] 16%|█▌        | 5/32 [00:05<00:27,  1.00s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.03s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.03s/it] 31%|███▏      | 10/32 [00:10<00:21,  1.03it/s] 19%|█▉        | 6/32 [00:06<00:25,  1.01it/s] 25%|██▌       | 8/32 [00:08<00:24,  1.03s/it] 34%|███▍      | 11/32 [00:11<00:20,  1.03it/s] 19%|█▉        | 6/32 [00:06<00:26,  1.02s/it] 22%|██▏       | 7/32 [00:07<00:24,  1.01it/s] 28%|██▊       | 9/32 [00:09<00:23,  1.03s/it] 38%|███▊      | 12/32 [00:12<00:19,  1.03it/s] 22%|██▏       | 7/32 [00:07<00:25,  1.01s/it] 25%|██▌       | 8/32 [00:08<00:23,  1.02it/s] 31%|███▏      | 10/32 [00:10<00:22,  1.02s/it] 41%|████      | 13/32 [00:13<00:18,  1.03it/s] 25%|██▌       | 8/32 [00:08<00:24,  1.01s/it] 28%|██▊       | 9/32 [00:09<00:22,  1.02it/s] 34%|███▍      | 11/32 [00:11<00:21,  1.02s/it] 44%|████▍     | 14/32 [00:13<00:17,  1.04it/s] 28%|██▊       | 9/32 [00:09<00:23,  1.00s/it] 31%|███▏      | 10/32 [00:10<00:21,  1.02it/s] 38%|███▊      | 12/32 [00:12<00:20,  1.02s/it] 47%|████▋     | 15/32 [00:14<00:16,  1.04it/s] 34%|███▍      | 11/32 [00:11<00:20,  1.03it/s] 31%|███▏      | 10/32 [00:10<00:22,  1.00s/it] 41%|████      | 13/32 [00:13<00:19,  1.02s/it] 50%|█████     | 16/32 [00:15<00:15,  1.04it/s] 38%|███▊      | 12/32 [00:12<00:19,  1.02it/s] 34%|███▍      | 11/32 [00:11<00:21,  1.00s/it] 53%|█████▎    | 17/32 [00:16<00:14,  1.04it/s] 44%|████▍     | 14/32 [00:14<00:18,  1.02s/it] 41%|████      | 13/32 [00:12<00:18,  1.03it/s] 38%|███▊      | 12/32 [00:12<00:20,  1.00s/it] 56%|█████▋    | 18/32 [00:17<00:13,  1.04it/s] 47%|████▋     | 15/32 [00:15<00:17,  1.02s/it] 44%|████▍     | 14/32 [00:13<00:17,  1.02it/s] 41%|████      | 13/32 [00:13<00:19,  1.00s/it] 59%|█████▉    | 19/32 [00:18<00:12,  1.04it/s] 50%|█████     | 16/32 [00:16<00:16,  1.02s/it] 47%|████▋     | 15/32 [00:14<00:16,  1.02it/s] 44%|████▍     | 14/32 [00:14<00:18,  1.00s/it] 62%|██████▎   | 20/32 [00:19<00:11,  1.04it/s] 50%|█████     | 16/32 [00:15<00:15,  1.02it/s] 53%|█████▎    | 17/32 [00:17<00:15,  1.02s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.00s/it] 66%|██████▌   | 21/32 [00:20<00:10,  1.04it/s] 53%|█████▎    | 17/32 [00:16<00:14,  1.03it/s] 56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it] 50%|█████     | 16/32 [00:16<00:16,  1.00s/it] 69%|██████▉   | 22/32 [00:21<00:09,  1.04it/s] 56%|█████▋    | 18/32 [00:17<00:13,  1.03it/s] 59%|█████▉    | 19/32 [00:19<00:13,  1.02s/it] 53%|█████▎    | 17/32 [00:17<00:14,  1.00it/s] 72%|███████▏  | 23/32 [00:22<00:08,  1.04it/s] 59%|█████▉    | 19/32 [00:18<00:12,  1.02it/s] 62%|██████▎   | 20/32 [00:20<00:12,  1.02s/it] 56%|█████▋    | 18/32 [00:18<00:13,  1.00it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.04it/s] 62%|██████▎   | 20/32 [00:19<00:11,  1.02it/s] 66%|██████▌   | 21/32 [00:21<00:11,  1.02s/it] 59%|█████▉    | 19/32 [00:19<00:12,  1.00it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.04it/s] 66%|██████▌   | 21/32 [00:20<00:10,  1.03it/s] 69%|██████▉   | 22/32 [00:22<00:10,  1.02s/it] 62%|██████▎   | 20/32 [00:20<00:11,  1.00it/s] 81%|████████▏ | 26/32 [00:25<00:05,  1.04it/s] 69%|██████▉   | 22/32 [00:21<00:09,  1.02it/s] 66%|██████▌   | 21/32 [00:21<00:10,  1.00it/s] 72%|███████▏  | 23/32 [00:23<00:09,  1.02s/it] 84%|████████▍ | 27/32 [00:26<00:04,  1.04it/s] 72%|███████▏  | 23/32 [00:22<00:08,  1.03it/s] 69%|██████▉   | 22/32 [00:22<00:09,  1.00it/s] 75%|███████▌  | 24/32 [00:24<00:08,  1.02s/it] 88%|████████▊ | 28/32 [00:27<00:03,  1.04it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.03it/s] 72%|███████▏  | 23/32 [00:23<00:08,  1.00it/s] 78%|███████▊  | 25/32 [00:25<00:07,  1.02s/it] 91%|█████████ | 29/32 [00:28<00:02,  1.04it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.03it/s] 75%|███████▌  | 24/32 [00:24<00:07,  1.00it/s] 81%|████████▏ | 26/32 [00:26<00:06,  1.02s/it] 94%|█████████▍| 30/32 [00:29<00:01,  1.04it/s] 81%|████████▏ | 26/32 [00:25<00:05,  1.03it/s] 78%|███████▊  | 25/32 [00:25<00:06,  1.00it/s] 84%|████████▍ | 27/32 [00:27<00:05,  1.02s/it] 97%|█████████▋| 31/32 [00:30<00:00,  1.04it/s] 84%|████████▍ | 27/32 [00:26<00:04,  1.03it/s] 81%|████████▏ | 26/32 [00:26<00:05,  1.00it/s] 88%|████████▊ | 28/32 [00:28<00:04,  1.02s/it]100%|██████████| 32/32 [00:31<00:00,  1.04it/s]100%|██████████| 32/32 [00:31<00:00,  1.02it/s]
 88%|████████▊ | 28/32 [00:27<00:03,  1.02it/s] 84%|████████▍ | 27/32 [00:27<00:05,  1.00s/it] 91%|█████████ | 29/32 [00:29<00:03,  1.02s/it] 91%|█████████ | 29/32 [00:28<00:02,  1.02it/s] 88%|████████▊ | 28/32 [00:28<00:04,  1.01s/it] 94%|█████████▍| 30/32 [00:30<00:02,  1.02s/it] 94%|█████████▍| 30/32 [00:29<00:01,  1.02it/s] 91%|█████████ | 29/32 [00:29<00:03,  1.02s/it] 97%|█████████▋| 31/32 [00:31<00:01,  1.02s/it] 97%|█████████▋| 31/32 [00:30<00:00,  1.02it/s] 94%|█████████▍| 30/32 [00:30<00:02,  1.03s/it]100%|██████████| 32/32 [00:32<00:00,  1.02s/it]100%|██████████| 32/32 [00:32<00:00,  1.03s/it]
100%|██████████| 32/32 [00:31<00:00,  1.03it/s]100%|██████████| 32/32 [00:31<00:00,  1.02it/s]
 97%|█████████▋| 31/32 [00:31<00:01,  1.03s/it]100%|██████████| 32/32 [00:32<00:00,  1.04s/it]100%|██████████| 32/32 [00:32<00:00,  1.01s/it]
I0318 06:45:21.793677 1109641 finetune.py:45] layer 12_gate initial loss 0.01655770093202591
W0318 06:45:21.793906 1109641 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:45:22.530490 1109641 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:45:24.803780 1109803 finetune.py:45] layer 13_gate initial loss 0.02183913253247738
W0318 06:45:24.803957 1109803 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0318 06:45:25.344398 1110237 finetune.py:45] layer 14_gate initial loss 0.024121036753058434
W0318 06:45:25.344717 1110237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:45:25.501667 1109803 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_gate proxy err 0.005319356918334961 tr(WHW.T) 2381.99462890625
  0%|          | 0/86 [00:00<?, ?it/s]W0318 06:45:26.150180 1110237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  1%|          | 1/86 [00:00<00:51,  1.64it/s]  2%|▏         | 2/86 [00:00<00:37,  2.22it/s]  3%|▎         | 3/86 [00:01<00:33,  2.48it/s]I0318 06:45:27.108232 1110755 finetune.py:45] layer 15_gate initial loss 0.025074975565075874
W0318 06:45:27.108422 1110755 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  5%|▍         | 4/86 [00:01<00:30,  2.67it/s]  6%|▌         | 5/86 [00:01<00:28,  2.80it/s]W0318 06:45:27.782652 1110755 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  7%|▋         | 6/86 [00:02<00:27,  2.89it/s]13_gate proxy err 0.005112397018820047 tr(WHW.T) 2601.50390625
  0%|          | 0/86 [00:00<?, ?it/s]  8%|▊         | 7/86 [00:02<00:27,  2.91it/s]  9%|▉         | 8/86 [00:02<00:26,  2.91it/s]  1%|          | 1/86 [00:00<00:50,  1.67it/s]14_gate proxy err 0.005466614384204149 tr(WHW.T) 2682.584228515625
  0%|          | 0/86 [00:00<?, ?it/s] 10%|█         | 9/86 [00:03<00:26,  2.94it/s]  2%|▏         | 2/86 [00:00<00:37,  2.23it/s] 12%|█▏        | 10/86 [00:03<00:25,  2.95it/s]  1%|          | 1/86 [00:00<00:48,  1.74it/s]  3%|▎         | 3/86 [00:01<00:33,  2.51it/s] 13%|█▎        | 11/86 [00:03<00:25,  2.95it/s]  2%|▏         | 2/86 [00:00<00:36,  2.33it/s]  5%|▍         | 4/86 [00:01<00:30,  2.67it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.96it/s]  3%|▎         | 3/86 [00:01<00:31,  2.60it/s]  6%|▌         | 5/86 [00:01<00:29,  2.76it/s] 15%|█▌        | 13/86 [00:04<00:24,  2.99it/s]  5%|▍         | 4/86 [00:01<00:29,  2.74it/s]15_gate proxy err 0.005473895464092493 tr(WHW.T) 2905.139892578125
  0%|          | 0/86 [00:00<?, ?it/s]  7%|▋         | 6/86 [00:02<00:28,  2.81it/s] 16%|█▋        | 14/86 [00:04<00:23,  3.02it/s]  6%|▌         | 5/86 [00:01<00:28,  2.84it/s]  8%|▊         | 7/86 [00:02<00:27,  2.86it/s] 17%|█▋        | 15/86 [00:05<00:23,  3.04it/s]  1%|          | 1/86 [00:00<00:48,  1.74it/s]  7%|▋         | 6/86 [00:02<00:27,  2.89it/s]  9%|▉         | 8/86 [00:02<00:27,  2.88it/s] 19%|█▊        | 16/86 [00:05<00:23,  3.04it/s]  2%|▏         | 2/86 [00:00<00:36,  2.32it/s]  8%|▊         | 7/86 [00:02<00:27,  2.92it/s] 10%|█         | 9/86 [00:03<00:26,  2.90it/s] 20%|█▉        | 17/86 [00:05<00:22,  3.05it/s]  3%|▎         | 3/86 [00:01<00:31,  2.60it/s]  9%|▉         | 8/86 [00:02<00:26,  2.93it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.90it/s] 21%|██        | 18/86 [00:06<00:22,  3.05it/s]  5%|▍         | 4/86 [00:01<00:29,  2.75it/s] 10%|█         | 9/86 [00:03<00:26,  2.95it/s] 13%|█▎        | 11/86 [00:04<00:25,  2.92it/s] 22%|██▏       | 19/86 [00:06<00:21,  3.05it/s]  6%|▌         | 5/86 [00:01<00:28,  2.82it/s] 12%|█▏        | 10/86 [00:03<00:25,  2.97it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.93it/s] 23%|██▎       | 20/86 [00:06<00:21,  3.05it/s]  7%|▋         | 6/86 [00:02<00:27,  2.90it/s] 13%|█▎        | 11/86 [00:03<00:25,  3.00it/s] 15%|█▌        | 13/86 [00:04<00:24,  2.93it/s] 24%|██▍       | 21/86 [00:07<00:21,  3.05it/s]  8%|▊         | 7/86 [00:02<00:26,  2.95it/s] 14%|█▍        | 12/86 [00:04<00:24,  3.00it/s] 16%|█▋        | 14/86 [00:05<00:24,  2.93it/s] 26%|██▌       | 22/86 [00:07<00:21,  3.04it/s]  9%|▉         | 8/86 [00:02<00:26,  2.97it/s] 15%|█▌        | 13/86 [00:04<00:24,  2.99it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.94it/s] 27%|██▋       | 23/86 [00:07<00:20,  3.05it/s] 10%|█         | 9/86 [00:03<00:25,  2.98it/s] 16%|█▋        | 14/86 [00:04<00:23,  3.00it/s] 19%|█▊        | 16/86 [00:05<00:23,  2.93it/s] 28%|██▊       | 24/86 [00:08<00:20,  3.05it/s] 12%|█▏        | 10/86 [00:03<00:25,  3.00it/s] 17%|█▋        | 15/86 [00:05<00:23,  3.00it/s] 20%|█▉        | 17/86 [00:06<00:23,  2.93it/s] 29%|██▉       | 25/86 [00:08<00:19,  3.06it/s] 13%|█▎        | 11/86 [00:03<00:24,  3.00it/s] 19%|█▊        | 16/86 [00:05<00:23,  3.00it/s] 21%|██        | 18/86 [00:06<00:23,  2.94it/s] 30%|███       | 26/86 [00:08<00:19,  3.07it/s] 14%|█▍        | 12/86 [00:04<00:24,  3.00it/s] 20%|█▉        | 17/86 [00:05<00:22,  3.00it/s] 31%|███▏      | 27/86 [00:09<00:19,  3.07it/s] 22%|██▏       | 19/86 [00:06<00:22,  2.94it/s] 15%|█▌        | 13/86 [00:04<00:24,  3.02it/s] 21%|██        | 18/86 [00:06<00:22,  3.02it/s] 33%|███▎      | 28/86 [00:09<00:18,  3.08it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.94it/s] 16%|█▋        | 14/86 [00:04<00:23,  3.03it/s] 22%|██▏       | 19/86 [00:06<00:22,  3.00it/s] 34%|███▎      | 29/86 [00:09<00:18,  3.07it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.94it/s] 17%|█▋        | 15/86 [00:05<00:23,  3.03it/s] 23%|██▎       | 20/86 [00:06<00:21,  3.01it/s] 35%|███▍      | 30/86 [00:10<00:18,  3.07it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.94it/s] 19%|█▊        | 16/86 [00:05<00:23,  3.02it/s] 24%|██▍       | 21/86 [00:07<00:21,  2.99it/s] 36%|███▌      | 31/86 [00:10<00:17,  3.06it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.93it/s] 20%|█▉        | 17/86 [00:05<00:22,  3.02it/s] 26%|██▌       | 22/86 [00:07<00:21,  3.00it/s] 37%|███▋      | 32/86 [00:10<00:17,  3.07it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.95it/s] 21%|██        | 18/86 [00:06<00:22,  3.02it/s] 27%|██▋       | 23/86 [00:07<00:20,  3.00it/s] 38%|███▊      | 33/86 [00:11<00:17,  3.08it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.94it/s] 22%|██▏       | 19/86 [00:06<00:22,  3.01it/s] 28%|██▊       | 24/86 [00:08<00:20,  3.01it/s] 40%|███▉      | 34/86 [00:11<00:16,  3.07it/s] 23%|██▎       | 20/86 [00:06<00:21,  3.01it/s] 30%|███       | 26/86 [00:09<00:20,  2.94it/s] 29%|██▉       | 25/86 [00:08<00:20,  3.00it/s] 41%|████      | 35/86 [00:11<00:16,  3.08it/s] 24%|██▍       | 21/86 [00:07<00:21,  3.02it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.93it/s] 30%|███       | 26/86 [00:08<00:20,  2.99it/s] 42%|████▏     | 36/86 [00:12<00:16,  3.08it/s] 26%|██▌       | 22/86 [00:07<00:21,  3.03it/s] 33%|███▎      | 28/86 [00:09<00:19,  2.93it/s] 31%|███▏      | 27/86 [00:09<00:19,  2.99it/s] 43%|████▎     | 37/86 [00:12<00:15,  3.08it/s] 27%|██▋       | 23/86 [00:07<00:20,  3.04it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.94it/s] 33%|███▎      | 28/86 [00:09<00:19,  2.99it/s] 44%|████▍     | 38/86 [00:12<00:15,  3.07it/s] 28%|██▊       | 24/86 [00:08<00:20,  3.04it/s] 35%|███▍      | 30/86 [00:10<00:18,  2.95it/s] 34%|███▎      | 29/86 [00:09<00:18,  3.01it/s] 45%|████▌     | 39/86 [00:13<00:15,  3.06it/s] 29%|██▉       | 25/86 [00:08<00:20,  3.04it/s] 36%|███▌      | 31/86 [00:10<00:18,  2.96it/s] 35%|███▍      | 30/86 [00:10<00:18,  3.02it/s] 47%|████▋     | 40/86 [00:13<00:14,  3.07it/s] 30%|███       | 26/86 [00:08<00:19,  3.03it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.96it/s] 36%|███▌      | 31/86 [00:10<00:18,  3.02it/s] 48%|████▊     | 41/86 [00:13<00:14,  3.08it/s] 31%|███▏      | 27/86 [00:09<00:19,  3.03it/s] 38%|███▊      | 33/86 [00:11<00:17,  2.95it/s] 37%|███▋      | 32/86 [00:10<00:17,  3.01it/s] 49%|████▉     | 42/86 [00:14<00:14,  3.07it/s] 33%|███▎      | 28/86 [00:09<00:19,  3.03it/s] 40%|███▉      | 34/86 [00:11<00:17,  2.95it/s] 38%|███▊      | 33/86 [00:11<00:17,  3.01it/s] 50%|█████     | 43/86 [00:14<00:13,  3.08it/s] 34%|███▎      | 29/86 [00:09<00:18,  3.03it/s] 41%|████      | 35/86 [00:12<00:17,  2.95it/s] 40%|███▉      | 34/86 [00:11<00:17,  3.00it/s] 51%|█████     | 44/86 [00:14<00:13,  3.08it/s] 35%|███▍      | 30/86 [00:10<00:18,  3.03it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.93it/s] 41%|████      | 35/86 [00:11<00:17,  2.99it/s] 52%|█████▏    | 45/86 [00:15<00:13,  3.07it/s] 36%|███▌      | 31/86 [00:10<00:18,  3.02it/s] 43%|████▎     | 37/86 [00:12<00:16,  2.94it/s] 42%|████▏     | 36/86 [00:12<00:16,  2.99it/s] 53%|█████▎    | 46/86 [00:15<00:13,  3.06it/s] 37%|███▋      | 32/86 [00:10<00:17,  3.03it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.94it/s] 43%|████▎     | 37/86 [00:12<00:16,  2.99it/s] 55%|█████▍    | 47/86 [00:15<00:12,  3.07it/s] 38%|███▊      | 33/86 [00:11<00:17,  3.03it/s] 45%|████▌     | 39/86 [00:13<00:16,  2.94it/s] 44%|████▍     | 38/86 [00:12<00:16,  2.99it/s] 56%|█████▌    | 48/86 [00:16<00:12,  3.07it/s] 40%|███▉      | 34/86 [00:11<00:17,  3.01it/s] 47%|████▋     | 40/86 [00:13<00:15,  2.94it/s] 45%|████▌     | 39/86 [00:13<00:15,  3.00it/s] 57%|█████▋    | 49/86 [00:16<00:12,  3.06it/s] 41%|████      | 35/86 [00:11<00:16,  3.02it/s] 47%|████▋     | 40/86 [00:13<00:15,  3.01it/s] 58%|█████▊    | 50/86 [00:16<00:11,  3.07it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.94it/s] 42%|████▏     | 36/86 [00:12<00:16,  3.02it/s] 59%|█████▉    | 51/86 [00:17<00:11,  3.07it/s] 48%|████▊     | 41/86 [00:13<00:14,  3.02it/s] 49%|████▉     | 42/86 [00:14<00:14,  2.95it/s] 43%|████▎     | 37/86 [00:12<00:16,  3.02it/s] 60%|██████    | 52/86 [00:17<00:11,  3.07it/s] 49%|████▉     | 42/86 [00:14<00:14,  3.02it/s] 50%|█████     | 43/86 [00:14<00:14,  2.95it/s] 44%|████▍     | 38/86 [00:12<00:15,  3.03it/s] 62%|██████▏   | 53/86 [00:17<00:10,  3.07it/s] 50%|█████     | 43/86 [00:14<00:14,  3.01it/s] 51%|█████     | 44/86 [00:15<00:14,  2.95it/s] 45%|████▌     | 39/86 [00:13<00:15,  3.02it/s] 63%|██████▎   | 54/86 [00:17<00:10,  3.07it/s] 51%|█████     | 44/86 [00:14<00:13,  3.00it/s] 52%|█████▏    | 45/86 [00:15<00:13,  2.93it/s] 47%|████▋     | 40/86 [00:13<00:15,  3.02it/s] 64%|██████▍   | 55/86 [00:18<00:10,  3.06it/s] 52%|█████▏    | 45/86 [00:15<00:13,  2.99it/s] 53%|█████▎    | 46/86 [00:15<00:13,  2.92it/s] 48%|████▊     | 41/86 [00:13<00:14,  3.02it/s] 65%|██████▌   | 56/86 [00:18<00:09,  3.06it/s] 53%|█████▎    | 46/86 [00:15<00:13,  3.00it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.92it/s] 49%|████▉     | 42/86 [00:14<00:14,  3.03it/s] 66%|██████▋   | 57/86 [00:18<00:09,  3.05it/s] 55%|█████▍    | 47/86 [00:15<00:12,  3.01it/s] 56%|█████▌    | 48/86 [00:16<00:12,  2.93it/s] 50%|█████     | 43/86 [00:14<00:14,  3.03it/s] 67%|██████▋   | 58/86 [00:19<00:09,  3.05it/s] 56%|█████▌    | 48/86 [00:16<00:12,  3.02it/s] 57%|█████▋    | 49/86 [00:16<00:12,  2.93it/s] 51%|█████     | 44/86 [00:14<00:13,  3.04it/s] 69%|██████▊   | 59/86 [00:19<00:08,  3.06it/s] 57%|█████▋    | 49/86 [00:16<00:12,  3.02it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.94it/s] 52%|█████▏    | 45/86 [00:15<00:13,  3.04it/s] 70%|██████▉   | 60/86 [00:19<00:08,  3.06it/s] 58%|█████▊    | 50/86 [00:16<00:11,  3.03it/s] 59%|█████▉    | 51/86 [00:17<00:11,  2.93it/s] 53%|█████▎    | 46/86 [00:15<00:13,  3.03it/s] 71%|███████   | 61/86 [00:20<00:08,  3.07it/s] 59%|█████▉    | 51/86 [00:17<00:11,  3.01it/s] 60%|██████    | 52/86 [00:17<00:11,  2.92it/s] 55%|█████▍    | 47/86 [00:15<00:12,  3.02it/s] 72%|███████▏  | 62/86 [00:20<00:07,  3.07it/s] 60%|██████    | 52/86 [00:17<00:11,  2.99it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.92it/s] 56%|█████▌    | 48/86 [00:16<00:12,  3.03it/s] 73%|███████▎  | 63/86 [00:20<00:07,  3.08it/s] 62%|██████▏   | 53/86 [00:17<00:11,  2.99it/s] 63%|██████▎   | 54/86 [00:18<00:10,  2.91it/s] 57%|█████▋    | 49/86 [00:16<00:12,  3.01it/s] 74%|███████▍  | 64/86 [00:21<00:07,  3.06it/s] 63%|██████▎   | 54/86 [00:18<00:10,  2.99it/s] 64%|██████▍   | 55/86 [00:18<00:10,  2.91it/s] 58%|█████▊    | 50/86 [00:16<00:11,  3.01it/s] 76%|███████▌  | 65/86 [00:21<00:06,  3.07it/s] 64%|██████▍   | 55/86 [00:18<00:10,  2.99it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.93it/s] 59%|█████▉    | 51/86 [00:17<00:11,  3.02it/s] 77%|███████▋  | 66/86 [00:21<00:06,  3.07it/s] 65%|██████▌   | 56/86 [00:18<00:10,  3.00it/s] 66%|██████▋   | 57/86 [00:19<00:09,  2.93it/s] 60%|██████    | 52/86 [00:17<00:11,  3.03it/s] 78%|███████▊  | 67/86 [00:22<00:06,  3.06it/s] 66%|██████▋   | 57/86 [00:19<00:09,  2.99it/s] 62%|██████▏   | 53/86 [00:17<00:10,  3.03it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.90it/s] 79%|███████▉  | 68/86 [00:22<00:05,  3.05it/s] 67%|██████▋   | 58/86 [00:19<00:09,  2.99it/s] 63%|██████▎   | 54/86 [00:18<00:10,  3.04it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.91it/s] 80%|████████  | 69/86 [00:22<00:05,  3.06it/s] 69%|██████▊   | 59/86 [00:19<00:08,  3.01it/s] 64%|██████▍   | 55/86 [00:18<00:10,  3.03it/s] 70%|██████▉   | 60/86 [00:20<00:08,  2.93it/s] 81%|████████▏ | 70/86 [00:23<00:05,  3.06it/s] 70%|██████▉   | 60/86 [00:20<00:08,  3.02it/s] 65%|██████▌   | 56/86 [00:18<00:09,  3.03it/s] 71%|███████   | 61/86 [00:21<00:08,  2.94it/s] 83%|████████▎ | 71/86 [00:23<00:04,  3.06it/s] 71%|███████   | 61/86 [00:20<00:08,  3.02it/s] 66%|██████▋   | 57/86 [00:19<00:09,  3.03it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.94it/s] 84%|████████▎ | 72/86 [00:23<00:04,  3.06it/s] 72%|███████▏  | 62/86 [00:20<00:07,  3.01it/s] 67%|██████▋   | 58/86 [00:19<00:09,  3.03it/s] 85%|████████▍ | 73/86 [00:24<00:04,  3.06it/s] 73%|███████▎  | 63/86 [00:21<00:07,  2.91it/s] 73%|███████▎  | 63/86 [00:21<00:07,  2.98it/s] 69%|██████▊   | 59/86 [00:19<00:08,  3.03it/s] 86%|████████▌ | 74/86 [00:24<00:03,  3.07it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.91it/s] 74%|███████▍  | 64/86 [00:21<00:07,  2.98it/s] 70%|██████▉   | 60/86 [00:20<00:08,  3.03it/s] 87%|████████▋ | 75/86 [00:24<00:03,  3.07it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.93it/s] 76%|███████▌  | 65/86 [00:21<00:07,  2.98it/s] 71%|███████   | 61/86 [00:20<00:08,  3.03it/s] 88%|████████▊ | 76/86 [00:25<00:03,  3.08it/s] 77%|███████▋  | 66/86 [00:22<00:06,  2.93it/s] 77%|███████▋  | 66/86 [00:22<00:06,  2.99it/s] 72%|███████▏  | 62/86 [00:20<00:07,  3.02it/s] 90%|████████▉ | 77/86 [00:25<00:02,  3.08it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.93it/s] 78%|███████▊  | 67/86 [00:22<00:06,  2.99it/s] 73%|███████▎  | 63/86 [00:21<00:07,  3.02it/s] 91%|█████████ | 78/86 [00:25<00:02,  3.07it/s] 79%|███████▉  | 68/86 [00:23<00:06,  2.94it/s] 79%|███████▉  | 68/86 [00:22<00:06,  3.00it/s] 74%|███████▍  | 64/86 [00:21<00:07,  3.02it/s] 92%|█████████▏| 79/86 [00:26<00:02,  3.07it/s] 80%|████████  | 69/86 [00:23<00:05,  2.93it/s] 80%|████████  | 69/86 [00:23<00:05,  2.97it/s] 76%|███████▌  | 65/86 [00:21<00:06,  3.03it/s] 93%|█████████▎| 80/86 [00:26<00:01,  3.08it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.92it/s] 81%|████████▏ | 70/86 [00:23<00:05,  2.98it/s] 77%|███████▋  | 66/86 [00:22<00:06,  3.03it/s] 94%|█████████▍| 81/86 [00:26<00:01,  3.08it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.93it/s] 83%|████████▎ | 71/86 [00:23<00:05,  2.98it/s] 78%|███████▊  | 67/86 [00:22<00:06,  3.04it/s] 95%|█████████▌| 82/86 [00:27<00:01,  3.08it/s] 84%|████████▎ | 72/86 [00:24<00:04,  2.93it/s] 84%|████████▎ | 72/86 [00:24<00:04,  2.99it/s] 79%|███████▉  | 68/86 [00:22<00:05,  3.03it/s] 97%|█████████▋| 83/86 [00:27<00:00,  3.07it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.92it/s] 85%|████████▍ | 73/86 [00:24<00:04,  2.98it/s] 98%|█████████▊| 84/86 [00:27<00:00,  3.07it/s] 80%|████████  | 69/86 [00:23<00:05,  3.01it/s] 86%|████████▌ | 74/86 [00:25<00:04,  2.94it/s] 86%|████████▌ | 74/86 [00:24<00:04,  2.99it/s] 99%|█████████▉| 85/86 [00:28<00:00,  3.06it/s] 81%|████████▏ | 70/86 [00:23<00:05,  3.02it/s] 87%|████████▋ | 75/86 [00:25<00:03,  2.95it/s] 87%|████████▋ | 75/86 [00:25<00:03,  3.00it/s]100%|██████████| 86/86 [00:28<00:00,  3.06it/s]100%|██████████| 86/86 [00:28<00:00,  3.03it/s]
 83%|████████▎ | 71/86 [00:23<00:04,  3.03it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.94it/s] 88%|████████▊ | 76/86 [00:25<00:03,  2.98it/s] 84%|████████▎ | 72/86 [00:24<00:04,  3.02it/s] 90%|████████▉ | 77/86 [00:26<00:03,  2.95it/s] 90%|████████▉ | 77/86 [00:25<00:03,  2.99it/s] 85%|████████▍ | 73/86 [00:24<00:04,  2.96it/s] 91%|█████████ | 78/86 [00:26<00:02,  2.96it/s] 91%|█████████ | 78/86 [00:26<00:02,  3.00it/s] 86%|████████▌ | 74/86 [00:24<00:04,  2.93it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.95it/s] 92%|█████████▏| 79/86 [00:26<00:02,  3.00it/s] 87%|████████▋ | 75/86 [00:25<00:03,  2.93it/s] 93%|█████████▎| 80/86 [00:27<00:02,  2.95it/s] 93%|█████████▎| 80/86 [00:26<00:01,  3.01it/s] 88%|████████▊ | 76/86 [00:25<00:03,  2.94it/s] 94%|█████████▍| 81/86 [00:27<00:01,  2.93it/s] 94%|█████████▍| 81/86 [00:27<00:01,  3.00it/s] 90%|████████▉ | 77/86 [00:25<00:03,  2.95it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.92it/s] 95%|█████████▌| 82/86 [00:27<00:01,  2.98it/s] 91%|█████████ | 78/86 [00:26<00:02,  2.98it/s] 97%|█████████▋| 83/86 [00:28<00:01,  2.93it/s] 97%|█████████▋| 83/86 [00:27<00:01,  2.99it/s] 92%|█████████▏| 79/86 [00:26<00:02,  3.00it/s] 98%|█████████▊| 84/86 [00:28<00:00,  2.95it/s] 98%|█████████▊| 84/86 [00:28<00:00,  3.00it/s] 93%|█████████▎| 80/86 [00:26<00:01,  3.01it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.95it/s] 99%|█████████▉| 85/86 [00:28<00:00,  2.99it/s] 94%|█████████▍| 81/86 [00:27<00:01,  3.01it/s]100%|██████████| 86/86 [00:29<00:00,  2.96it/s]100%|██████████| 86/86 [00:29<00:00,  2.91it/s]
100%|██████████| 86/86 [00:28<00:00,  3.00it/s]100%|██████████| 86/86 [00:28<00:00,  2.98it/s]
 95%|█████████▌| 82/86 [00:27<00:01,  3.02it/s] 97%|█████████▋| 83/86 [00:27<00:00,  3.01it/s] 98%|█████████▊| 84/86 [00:28<00:00,  3.02it/s] 99%|█████████▉| 85/86 [00:28<00:00,  3.01it/s]100%|██████████| 86/86 [00:28<00:00,  3.02it/s]100%|██████████| 86/86 [00:28<00:00,  2.99it/s]
W0318 06:46:00.798000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:00.798000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:00.798000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:00.798000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:00.798000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:00.798000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:00.799000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:00.839000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:00.839000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:00.839000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:00.839000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:00.840000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:00.854000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:00.855000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:00.855000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:00.855000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:00.855000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.016000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.016000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.016000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.016000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.017000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.328000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.328000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.328000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.328000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.328000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.328000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.328000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.359000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.359000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.359000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.359000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.360000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.428000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.428000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.428000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.428000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:01.428000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:02.568000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:02.580000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:02.587000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:02.587000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.014000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.014000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.014000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.014000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.014000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.014000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.014000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.042000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.043000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.043000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.043000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.043000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.371000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.371000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.371000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.371000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.371000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.371000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.371000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.371000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.652000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.652000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.653000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.653000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.653000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.971000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:03.976000 139641019983680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.170000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.170000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.170000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.170000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.171000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.171000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.171000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.179000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.179000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.179000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.179000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.179000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.179000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.179000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.212000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.212000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.212000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.212000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.212000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.219000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.219000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.219000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.219000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.219000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.227000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.227000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.227000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.227000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.227000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.234000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.234000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.234000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.234000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.234000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.392000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.392000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.392000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.392000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.392000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.395000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.396000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.396000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.396000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.396000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.704000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.704000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.704000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.704000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.704000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.705000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.705000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.708000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.708000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.709000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.709000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.709000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.709000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.709000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.736000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.736000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.736000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.736000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.736000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.741000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.741000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.741000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.741000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.741000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.803000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.804000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.804000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.804000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.804000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.808000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.808000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.809000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.809000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:04.809000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.507000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.507000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.507000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.507000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.507000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.507000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.508000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.549000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.549000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.549000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.549000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.549000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.564000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.564000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.564000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.564000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.564000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.736000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.736000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.737000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.737000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.737000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.944000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.946000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.956000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.958000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.964000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.964000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.966000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:05.966000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.069000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.069000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.069000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.069000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.070000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.070000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.070000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.104000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.104000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.104000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.104000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.104000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.176000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.176000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.176000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.176000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.177000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.393000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.393000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.393000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.393000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.393000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.393000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.393000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.400000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.400000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.400000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.400000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.400000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.400000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.400000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.422000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.422000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.422000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.423000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.423000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.433000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.433000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.433000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.433000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.433000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.770000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.770000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.770000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.770000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.770000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.771000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.771000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.771000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.771000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.772000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.772000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.772000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.772000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.772000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.772000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:06.772000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.052000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.052000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.052000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.052000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.052000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.055000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.055000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.055000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.056000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.056000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.371000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.376000 140715002574656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.378000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.383000 140588472162112 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.395000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.406000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.414000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.414000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.876000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.877000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.877000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.877000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.877000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.877000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.877000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.911000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.911000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.911000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.911000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:07.911000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:08.262000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:08.262000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:08.262000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:08.262000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:08.262000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:08.262000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:08.263000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:08.263000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:08.562000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:08.562000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:08.562000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:08.562000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:08.562000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:08.908000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:08.913000 139790355597120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0318 06:46:10.979216 1109641 finetune.py:45] layer 12_down initial loss 0.016314929351210594
W0318 06:46:10.979416 1109641 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:46:11.462183 1109641 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

12_down proxy err 0.013161132112145424 tr(WHW.T) 64.17742919921875
I0318 06:46:13.918689 1109803 finetune.py:45] layer 13_down initial loss 0.021630024537444115
W0318 06:46:13.918885 1109803 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0318 06:46:13.945881 1110237 finetune.py:45] layer 14_down initial loss 0.023794488981366158
W0318 06:46:13.946067 1110237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:46:14.376560 1109803 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0318 06:46:14.403927 1110237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

13_down proxy err 0.01302743423730135 tr(WHW.T) 79.35888671875
14_down proxy err 0.013406332582235336 tr(WHW.T) 90.28681945800781
I0318 06:46:15.675722 1110755 finetune.py:45] layer 15_down initial loss 0.0247474554926157
W0318 06:46:15.675956 1110755 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:46:16.129380 1110755 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

15_down proxy err 0.013139022514224052 tr(WHW.T) 114.08998107910156
I0318 06:46:18.247475 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 16 in 1.5162804126739502s
I0318 06:46:18.717464 1092738 quantize_finetune_llama.py:159] layer 17 gpu 1
I0318 06:46:20.677538 1114631 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:46:20.677675 1114631 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:46:20.677738 1114631 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:46:20.866543 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 17 in 1.4887468814849854s
I0318 06:46:20.920990 1114631 config.py:58] PyTorch version 2.4.0 available.
I0318 06:46:21.344355 1092738 quantize_finetune_llama.py:159] layer 18 gpu 2
I0318 06:46:23.180976 1114631 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0318 06:46:23.269056 1114793 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:46:23.269182 1114793 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:46:23.269243 1114793 utils.py:162] NumExpr defaulting to 16 threads.
W0318 06:46:23.538006 1114631 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:46:23.541590 1114793 config.py:58] PyTorch version 2.4.0 available.
I0318 06:46:23.676989 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 18 in 1.5786075592041016s
I0318 06:46:24.119873 1092738 quantize_finetune_llama.py:159] layer 19 gpu 3
  0%|          | 0/32 [00:00<?, ?it/s]I0318 06:46:25.730714 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 19 in 1.152334213256836s
I0318 06:46:25.937250 1114793 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0318 06:46:26.068215 1115251 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:46:26.068348 1115251 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:46:26.068412 1115251 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:46:26.184086 1092738 quantize_finetune_llama.py:159] layer 20 gpu 0
  3%|▎         | 1/32 [00:01<00:45,  1.46s/it]W0318 06:46:26.405146 1114793 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:46:26.416269 1115251 config.py:58] PyTorch version 2.4.0 available.
  6%|▋         | 2/32 [00:01<00:23,  1.25it/s]  9%|▉         | 3/32 [00:02<00:16,  1.71it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.04it/s]  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.47it/s]I0318 06:46:28.164069 1115750 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:46:28.164172 1115750 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:46:28.164228 1115750 utils.py:162] NumExpr defaulting to 16 threads.
 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s]I0318 06:46:28.357137 1115750 config.py:58] PyTorch version 2.4.0 available.
 25%|██▌       | 8/32 [00:03<00:08,  2.72it/s]I0318 06:46:28.752739 1115251 data_utils.py:336] using 256 training seqs, 128 validation seqs
 28%|██▊       | 9/32 [00:04<00:08,  2.79it/s]W0318 06:46:29.147813 1115251 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:48,  1.58s/it] 31%|███▏      | 10/32 [00:04<00:07,  2.78it/s]  6%|▋         | 2/32 [00:01<00:25,  1.18it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.84it/s]  9%|▉         | 3/32 [00:02<00:17,  1.64it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.89it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.02it/s]  0%|          | 0/32 [00:00<?, ?it/s] 41%|████      | 13/32 [00:05<00:06,  2.87it/s]I0318 06:46:30.434781 1115750 data_utils.py:336] using 256 training seqs, 128 validation seqs
 16%|█▌        | 5/32 [00:02<00:11,  2.27it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.88it/s]W0318 06:46:30.775306 1115750 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:03<00:10,  2.47it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.90it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.62it/s] 50%|█████     | 16/32 [00:06<00:05,  2.96it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.97it/s]  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.82it/s]  3%|▎         | 1/32 [00:01<00:51,  1.66s/it] 56%|█████▋    | 18/32 [00:07<00:04,  2.98it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.89it/s]  6%|▋         | 2/32 [00:02<00:26,  1.13it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.99it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.93it/s]  9%|▉         | 3/32 [00:02<00:18,  1.56it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.98it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.97it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.92it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.99it/s] 41%|████      | 13/32 [00:05<00:06,  3.00it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.20it/s]  3%|▎         | 1/32 [00:01<00:47,  1.53s/it] 69%|██████▉   | 22/32 [00:08<00:03,  3.00it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.03it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.41it/s]  6%|▋         | 2/32 [00:01<00:24,  1.21it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.00it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.04it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s]  9%|▉         | 3/32 [00:02<00:17,  1.66it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.01it/s] 50%|█████     | 16/32 [00:06<00:05,  3.06it/s] 25%|██▌       | 8/32 [00:04<00:08,  2.69it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.02it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.00it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.08it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.78it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.98it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.08it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.83it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.49it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.99it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.07it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.86it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.99it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.07it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.88it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s] 91%|█████████ | 29/32 [00:10<00:01,  3.00it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.06it/s] 41%|████      | 13/32 [00:05<00:06,  2.89it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.82it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.00it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.08it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.92it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.87it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.01it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.08it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.93it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  3.01it/s]100%|██████████| 32/32 [00:11<00:00,  2.69it/s]
 75%|███████▌  | 24/32 [00:09<00:02,  3.07it/s] 50%|█████     | 16/32 [00:06<00:05,  2.95it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.93it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.07it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.93it/s] 41%|████      | 13/32 [00:05<00:06,  2.96it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.03it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.98it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.89it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.00it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.00it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.88it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.99it/s] 50%|█████     | 16/32 [00:06<00:05,  3.01it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.86it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.98it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.01it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.89it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.01it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.00it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.05it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.92it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.00it/s]100%|██████████| 32/32 [00:11<00:00,  3.05it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
 72%|███████▏  | 23/32 [00:09<00:03,  2.92it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.00it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.93it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.00it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.90it/s]W0318 06:46:40.074000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.074000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.074000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.074000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.074000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.074000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.074000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.099000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.099000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.100000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.100000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.100000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.115000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.116000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.116000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.116000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.116000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  3.00it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.92it/s]W0318 06:46:40.427000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.427000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.427000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.427000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:40.427000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:08<00:02,  3.00it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.89it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.00it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.88it/s]W0318 06:46:41.291000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:41.292000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:41.292000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:41.292000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:41.292000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:41.292000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:41.292000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:09<00:02,  3.00it/s]W0318 06:46:41.309000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:41.309000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:41.309000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:41.309000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:41.309000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.89it/s]W0318 06:46:41.551000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:41.551000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:41.551000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:41.551000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:41.551000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:09<00:01,  3.00it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.91it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.00it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.92it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.00it/s]100%|██████████| 32/32 [00:12<00:00,  2.94it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
 91%|█████████ | 29/32 [00:10<00:00,  3.00it/s]W0318 06:46:42.637000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.638000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.638000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.638000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.638000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.638000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.638000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.663000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.663000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.664000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.664000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.664000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.680000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.681000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.681000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.681000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.681000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.684000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.684000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.685000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.685000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.685000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.685000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.685000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.702000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.702000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.702000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.702000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:42.702000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  3.00it/s]W0318 06:46:43.011000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.011000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.011000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.012000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.012000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  3.00it/s]W0318 06:46:43.578000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.578000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.578000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.578000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.578000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:11<00:00,  3.01it/s]100%|██████████| 32/32 [00:11<00:00,  2.70it/s]
W0318 06:46:43.900000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.900000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.900000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.900000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.900000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.900000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.900000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.918000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.919000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.919000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.919000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:43.919000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0318 06:46:44.160000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:44.160000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:44.160000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:44.160000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:44.161000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.324000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.324000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.324000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.324000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.324000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.324000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.324000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.343000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.343000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.343000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.343000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.343000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.822000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.822000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.822000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.822000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.823000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.823000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.823000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.850000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.850000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.850000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.850000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.850000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.867000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.867000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.867000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.867000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:45.868000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.198000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.198000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.198000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.199000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.199000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.272000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.272000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.272000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.272000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.272000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.759000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.759000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.759000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.759000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.759000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.759000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.759000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.784000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.784000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.785000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.785000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.785000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.801000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.801000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.801000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.801000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:46.801000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0318 06:46:47.115000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.115000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.115000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.116000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.116000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.116000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.116000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.116000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.116000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.116000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.116000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.116000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.134000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.134000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.134000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.134000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.135000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.381000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.381000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.381000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.381000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.381000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.974000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.975000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.975000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.975000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.975000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.975000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.975000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.993000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.993000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.993000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.993000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:47.993000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:48.226000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:48.226000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:48.226000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:48.227000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:48.227000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:48.602000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:48.602000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:48.602000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:48.603000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:48.603000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:48.603000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:48.603000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:48.621000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:48.621000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:48.621000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:48.621000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:48.621000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:49.374000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:49.374000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:49.374000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:49.374000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:46:49.374000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:46:49.375000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:49.375000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:49.392000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:49.392000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:49.393000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:49.393000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:49.393000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:49.562000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:49.562000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:49.562000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:46:49.562000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:49.562000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0318 06:46:50.290000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:46:50.290000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:46:50.290000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:46:50.291000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:46:50.291000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
I0318 06:46:50.446919 1114631 finetune.py:45] layer 16_v initial loss 0.036618635058403015
W0318 06:46:50.447446 1114631 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0318 06:46:51.728639 1114631 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:46:52.001930 1114793 finetune.py:45] layer 17_v initial loss 0.043309662491083145
W0318 06:46:52.002169 1114793 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

16_v proxy err 0.011613759212195873 tr(WHW.T) 780.7407836914062
  0%|          | 0/32 [00:00<?, ?it/s]W0318 06:46:52.984938 1114793 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:00<00:18,  1.63it/s]  6%|▋         | 2/32 [00:00<00:13,  2.24it/s]  9%|▉         | 3/32 [00:01<00:11,  2.54it/s]17_v proxy err 0.011424158699810505 tr(WHW.T) 845.765380859375
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.68it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.80it/s]  3%|▎         | 1/32 [00:00<00:19,  1.58it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.87it/s]  6%|▋         | 2/32 [00:00<00:13,  2.15it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.90it/s]  9%|▉         | 3/32 [00:01<00:11,  2.43it/s]I0318 06:46:55.798998 1115251 finetune.py:45] layer 18_v initial loss 0.04368308559060097
W0318 06:46:55.799183 1115251 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:02<00:08,  2.91it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.59it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.93it/s] 16%|█▌        | 5/32 [00:02<00:09,  2.71it/s]I0318 06:46:56.428630 1115750 finetune.py:45] layer 19_v initial loss 0.04433860629796982
W0318 06:46:56.429036 1115750 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:03<00:07,  2.93it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.80it/s]W0318 06:46:56.717885 1115251 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:03<00:07,  2.99it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.86it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.99it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.91it/s]W0318 06:46:57.302738 1115750 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 13/32 [00:04<00:06,  2.99it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.94it/s]18_v proxy err 0.012091480195522308 tr(WHW.T) 1003.7705688476562
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.99it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.97it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.02it/s] 34%|███▍      | 11/32 [00:03<00:06,  3.00it/s]  3%|▎         | 1/32 [00:00<00:18,  1.66it/s]19_v proxy err 0.012043030001223087 tr(WHW.T) 1019.1412353515625
  0%|          | 0/32 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:05<00:05,  3.03it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.02it/s]  6%|▋         | 2/32 [00:00<00:13,  2.25it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.03it/s] 41%|████      | 13/32 [00:04<00:06,  3.03it/s]  3%|▎         | 1/32 [00:00<00:18,  1.66it/s]  9%|▉         | 3/32 [00:01<00:11,  2.53it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.03it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.04it/s]  6%|▋         | 2/32 [00:00<00:13,  2.25it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.67it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.04it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.01it/s]  9%|▉         | 3/32 [00:01<00:11,  2.54it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.74it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.03it/s] 50%|█████     | 16/32 [00:05<00:05,  3.01it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.69it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.81it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.04it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.02it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.78it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.87it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.04it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.04it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.84it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.88it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.05it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.03it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.88it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.88it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.05it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.01it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.91it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.92it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.06it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.03it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.93it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.91it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.05it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.01it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.93it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.89it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.03it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.03it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.94it/s] 41%|████      | 13/32 [00:04<00:06,  2.91it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.03it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.03it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.95it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.04it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.91it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.02it/s] 41%|████      | 13/32 [00:04<00:06,  2.95it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.05it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.92it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.04it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.96it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.05it/s] 50%|█████     | 16/32 [00:05<00:05,  2.93it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.06it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.95it/s]100%|██████████| 32/32 [00:10<00:00,  3.05it/s]100%|██████████| 32/32 [00:10<00:00,  2.95it/s]
 53%|█████▎    | 17/32 [00:06<00:05,  2.93it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.06it/s] 50%|█████     | 16/32 [00:05<00:05,  2.96it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.93it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.02it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.96it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.92it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.02it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.95it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.90it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.01it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.96it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.91it/s]100%|██████████| 32/32 [00:10<00:00,  3.03it/s]100%|██████████| 32/32 [00:10<00:00,  2.93it/s]
 62%|██████▎   | 20/32 [00:07<00:04,  2.96it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.90it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.95it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.86it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.96it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.87it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.96it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.86it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.97it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.85it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.94it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.84it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.94it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.81it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.95it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.80it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.94it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.80it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.96it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.80it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.98it/s]100%|██████████| 32/32 [00:11<00:00,  2.78it/s]100%|██████████| 32/32 [00:11<00:00,  2.82it/s]
 97%|█████████▋| 31/32 [00:10<00:00,  2.94it/s]100%|██████████| 32/32 [00:11<00:00,  2.95it/s]100%|██████████| 32/32 [00:11<00:00,  2.89it/s]
I0318 06:47:11.378668 1114793 finetune.py:45] layer 17_q initial loss 0.042867302894592285
W0318 06:47:11.378839 1114793 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0318 06:47:11.389092 1114631 finetune.py:45] layer 16_q initial loss 0.036444876343011856
W0318 06:47:11.389348 1114631 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:47:12.277909 1114793 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:47:12.334558 1114631 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

17_q proxy err 0.00281361467204988 tr(WHW.T) 7163.2734375
  0%|          | 0/32 [00:00<?, ?it/s]16_q proxy err 0.0026550444308668375 tr(WHW.T) 7193.3974609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.68it/s]  3%|▎         | 1/32 [00:00<00:18,  1.71it/s]  6%|▋         | 2/32 [00:00<00:13,  2.24it/s]  6%|▋         | 2/32 [00:00<00:13,  2.27it/s]  9%|▉         | 3/32 [00:01<00:11,  2.50it/s]  9%|▉         | 3/32 [00:01<00:11,  2.53it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.65it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.67it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.74it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.77it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.79it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.83it/s]I0318 06:47:15.721379 1115251 finetune.py:45] layer 18_q initial loss 0.043678779155015945
W0318 06:47:15.721565 1115251 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0318 06:47:15.912659 1115750 finetune.py:45] layer 19_q initial loss 0.04428132250905037
W0318 06:47:15.912864 1115750 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:02<00:08,  2.86it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.88it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.92it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.92it/s]W0318 06:47:16.548593 1115251 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:03<00:07,  2.97it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.97it/s]W0318 06:47:16.741510 1115750 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:03<00:07,  2.97it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.97it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.99it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.99it/s]18_q proxy err 0.0029349138494580984 tr(WHW.T) 7510.48046875
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.01it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.02it/s]19_q proxy err 0.003078847425058484 tr(WHW.T) 6944.4130859375
  0%|          | 0/32 [00:00<?, ?it/s] 41%|████      | 13/32 [00:04<00:06,  3.01it/s] 41%|████      | 13/32 [00:04<00:06,  3.04it/s]  3%|▎         | 1/32 [00:00<00:17,  1.74it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.03it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.04it/s]  3%|▎         | 1/32 [00:00<00:17,  1.74it/s]  6%|▋         | 2/32 [00:00<00:12,  2.31it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.06it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.05it/s]  6%|▋         | 2/32 [00:00<00:12,  2.31it/s]  9%|▉         | 3/32 [00:01<00:11,  2.59it/s] 50%|█████     | 16/32 [00:05<00:05,  3.08it/s] 50%|█████     | 16/32 [00:05<00:05,  3.06it/s]  9%|▉         | 3/32 [00:01<00:11,  2.58it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.73it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.06it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.06it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.71it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.81it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.07it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.06it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.81it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.88it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.08it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.06it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.87it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.92it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.09it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.07it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.92it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.94it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.08it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.07it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.94it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.94it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.07it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.07it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.95it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.96it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.08it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.06it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.96it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.96it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.08it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.06it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.96it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.96it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.07it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.06it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.96it/s] 41%|████      | 13/32 [00:04<00:06,  2.96it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.07it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.06it/s] 41%|████      | 13/32 [00:04<00:06,  2.96it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.96it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.09it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.06it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.97it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.97it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.09it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.06it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.97it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.07it/s] 50%|█████     | 16/32 [00:05<00:05,  2.97it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.05it/s] 50%|█████     | 16/32 [00:05<00:05,  2.97it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.07it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.96it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.05it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.97it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.06it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.94it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.06it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.97it/s]100%|██████████| 32/32 [00:10<00:00,  3.07it/s]100%|██████████| 32/32 [00:10<00:00,  2.97it/s]
 59%|█████▉    | 19/32 [00:06<00:04,  2.95it/s]100%|██████████| 32/32 [00:10<00:00,  3.07it/s]100%|██████████| 32/32 [00:10<00:00,  2.97it/s]
 59%|█████▉    | 19/32 [00:06<00:04,  2.97it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.93it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.98it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.90it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.99it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.90it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.99it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.88it/s] 72%|███████▏  | 23/32 [00:07<00:03,  2.98it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.90it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.99it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.98it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.83it/s] 81%|████████▏ | 26/32 [00:08<00:02,  2.98it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.82it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.96it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.81it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.96it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.79it/s] 91%|█████████ | 29/32 [00:09<00:01,  2.98it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.79it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.98it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.99it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.79it/s]100%|██████████| 32/32 [00:10<00:00,  2.99it/s]100%|██████████| 32/32 [00:10<00:00,  2.92it/s]
100%|██████████| 32/32 [00:11<00:00,  2.79it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]
I0318 06:47:30.376020 1114793 finetune.py:45] layer 17_k initial loss 0.04248636215925217
W0318 06:47:30.376195 1114793 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:47:31.247315 1114793 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:47:31.505335 1114631 finetune.py:45] layer 16_k initial loss 0.036527737975120544
W0318 06:47:31.505686 1114631 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

17_k proxy err 0.001970777055248618 tr(WHW.T) 10697.431640625
  0%|          | 0/32 [00:00<?, ?it/s]W0318 06:47:32.399758 1114631 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:00<00:18,  1.71it/s]  6%|▋         | 2/32 [00:00<00:13,  2.27it/s]16_k proxy err 0.0017348320689052343 tr(WHW.T) 11630.361328125
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:01<00:11,  2.52it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.66it/s]  3%|▎         | 1/32 [00:00<00:18,  1.70it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.74it/s]  6%|▋         | 2/32 [00:00<00:13,  2.22it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.79it/s]  9%|▉         | 3/32 [00:01<00:11,  2.49it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.83it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.64it/s]I0318 06:47:35.154199 1115750 finetune.py:45] layer 19_k initial loss 0.04427928104996681
W0318 06:47:35.154571 1115750 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:02<00:08,  2.86it/s]I0318 06:47:35.406654 1115251 finetune.py:45] layer 18_k initial loss 0.043653491884469986
W0318 06:47:35.406884 1115251 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:01<00:09,  2.75it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.91it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.84it/s]W0318 06:47:35.978001 1115750 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:03<00:07,  2.92it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.88it/s]W0318 06:47:36.288671 1115251 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:03<00:07,  2.97it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.92it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.97it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.97it/s] 41%|████      | 13/32 [00:04<00:06,  2.99it/s]19_k proxy err 0.002108244691044092 tr(WHW.T) 10548.48828125
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.98it/s]18_k proxy err 0.002202906645834446 tr(WHW.T) 10462.6650390625
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.01it/s] 34%|███▍      | 11/32 [00:03<00:07,  3.00it/s]  3%|▎         | 1/32 [00:00<00:17,  1.74it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.04it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.00it/s]  3%|▎         | 1/32 [00:00<00:17,  1.76it/s]  6%|▋         | 2/32 [00:00<00:12,  2.31it/s] 50%|█████     | 16/32 [00:05<00:05,  3.04it/s] 41%|████      | 13/32 [00:04<00:06,  3.01it/s]  6%|▋         | 2/32 [00:00<00:13,  2.29it/s]  9%|▉         | 3/32 [00:01<00:11,  2.58it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.05it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.01it/s]  9%|▉         | 3/32 [00:01<00:11,  2.58it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.71it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.07it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.03it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.72it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.07it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.80it/s] 50%|█████     | 16/32 [00:05<00:05,  3.03it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.79it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.07it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.86it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.02it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.86it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.09it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.88it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.02it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.90it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.09it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.91it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.03it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.89it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.06it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.92it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.04it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.08it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.92it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.94it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.05it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.08it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.94it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.96it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.05it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.06it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.94it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.96it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.04it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.06it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.94it/s] 41%|████      | 13/32 [00:04<00:06,  2.97it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.04it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.08it/s] 41%|████      | 13/32 [00:04<00:06,  2.95it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.97it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.03it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.06it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.93it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.96it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.03it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.05it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.94it/s] 50%|█████     | 16/32 [00:05<00:05,  2.96it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.03it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.07it/s] 50%|█████     | 16/32 [00:05<00:05,  2.96it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.96it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.02it/s]100%|██████████| 32/32 [00:10<00:00,  3.08it/s]100%|██████████| 32/32 [00:10<00:00,  2.96it/s]
 53%|█████▎    | 17/32 [00:05<00:05,  2.97it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.97it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.02it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.94it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.97it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.02it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.92it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.97it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.03it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.91it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.98it/s]100%|██████████| 32/32 [00:10<00:00,  3.04it/s]100%|██████████| 32/32 [00:10<00:00,  2.94it/s]
 66%|██████▌   | 21/32 [00:07<00:03,  2.92it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.98it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.92it/s] 72%|███████▏  | 23/32 [00:07<00:03,  2.96it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.96it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.88it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.96it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.86it/s] 81%|████████▏ | 26/32 [00:08<00:02,  2.96it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.84it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.99it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.83it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.00it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.82it/s] 91%|█████████ | 29/32 [00:09<00:01,  2.98it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.81it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.97it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.81it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.97it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.81it/s]100%|██████████| 32/32 [00:11<00:00,  2.98it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]
 97%|█████████▋| 31/32 [00:10<00:00,  2.81it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]
I0318 06:47:49.454578 1114793 finetune.py:45] layer 17_o initial loss 0.04195347800850868
W0318 06:47:49.454821 1114793 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:47:50.320828 1114793 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

17_o proxy err 0.011842211708426476 tr(WHW.T) 58.14826965332031
  0%|          | 0/32 [00:00<?, ?it/s]I0318 06:47:51.939056 1114631 finetune.py:45] layer 16_o initial loss 0.03655027225613594
W0318 06:47:51.939427 1114631 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:47:52.837788 1114631 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:40,  1.30s/it]  6%|▋         | 2/32 [00:02<00:34,  1.14s/it]16_o proxy err 0.01025472953915596 tr(WHW.T) 88.22785186767578
  0%|          | 0/32 [00:00<?, ?it/s]I0318 06:47:54.721364 1115750 finetune.py:45] layer 19_o initial loss 0.04519946128129959
W0318 06:47:54.722804 1115750 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:03<00:31,  1.09s/it]I0318 06:47:55.182795 1115251 finetune.py:45] layer 18_o initial loss 0.04312148317694664
W0318 06:47:55.183024 1115251 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:38,  1.25s/it]W0318 06:47:55.756767 1115750 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:04<00:29,  1.04s/it]W0318 06:47:55.996333 1115251 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:02<00:33,  1.11s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.03s/it]19_o proxy err 0.011892350390553474 tr(WHW.T) 62.29168701171875
  0%|          | 0/32 [00:00<?, ?it/s]18_o proxy err 0.011071247979998589 tr(WHW.T) 69.96559143066406
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:03<00:30,  1.05s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.00s/it]  3%|▎         | 1/32 [00:01<00:38,  1.25s/it] 12%|█▎        | 4/32 [00:04<00:28,  1.02s/it]  3%|▎         | 1/32 [00:01<00:39,  1.26s/it] 22%|██▏       | 7/32 [00:07<00:24,  1.01it/s]  6%|▋         | 2/32 [00:02<00:32,  1.10s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.00s/it]  6%|▋         | 2/32 [00:02<00:33,  1.11s/it] 25%|██▌       | 8/32 [00:08<00:23,  1.02it/s]  9%|▉         | 3/32 [00:03<00:30,  1.04s/it] 19%|█▉        | 6/32 [00:06<00:25,  1.00it/s]  9%|▉         | 3/32 [00:03<00:30,  1.06s/it] 28%|██▊       | 9/32 [00:09<00:22,  1.03it/s] 12%|█▎        | 4/32 [00:04<00:28,  1.02s/it] 22%|██▏       | 7/32 [00:07<00:24,  1.01it/s] 12%|█▎        | 4/32 [00:04<00:29,  1.04s/it] 31%|███▏      | 10/32 [00:10<00:21,  1.03it/s] 16%|█▌        | 5/32 [00:05<00:27,  1.01s/it] 25%|██▌       | 8/32 [00:08<00:23,  1.01it/s] 16%|█▌        | 5/32 [00:05<00:27,  1.03s/it] 34%|███▍      | 11/32 [00:11<00:20,  1.03it/s] 19%|█▉        | 6/32 [00:06<00:26,  1.00s/it] 28%|██▊       | 9/32 [00:09<00:22,  1.02it/s] 19%|█▉        | 6/32 [00:06<00:26,  1.02s/it] 38%|███▊      | 12/32 [00:12<00:19,  1.03it/s] 22%|██▏       | 7/32 [00:07<00:24,  1.00it/s] 31%|███▏      | 10/32 [00:10<00:21,  1.02it/s] 22%|██▏       | 7/32 [00:07<00:25,  1.01s/it] 41%|████      | 13/32 [00:13<00:18,  1.04it/s] 25%|██▌       | 8/32 [00:08<00:23,  1.01it/s] 34%|███▍      | 11/32 [00:11<00:20,  1.02it/s] 25%|██▌       | 8/32 [00:08<00:24,  1.00s/it] 44%|████▍     | 14/32 [00:13<00:17,  1.04it/s] 28%|██▊       | 9/32 [00:09<00:22,  1.01it/s] 38%|███▊      | 12/32 [00:12<00:19,  1.02it/s] 28%|██▊       | 9/32 [00:09<00:23,  1.00s/it] 47%|████▋     | 15/32 [00:14<00:16,  1.04it/s] 31%|███▏      | 10/32 [00:10<00:21,  1.02it/s] 41%|████      | 13/32 [00:12<00:18,  1.02it/s] 31%|███▏      | 10/32 [00:10<00:22,  1.00s/it] 50%|█████     | 16/32 [00:15<00:15,  1.04it/s] 34%|███▍      | 11/32 [00:11<00:20,  1.02it/s] 44%|████▍     | 14/32 [00:13<00:17,  1.03it/s] 53%|█████▎    | 17/32 [00:16<00:14,  1.04it/s] 34%|███▍      | 11/32 [00:11<00:20,  1.00it/s] 38%|███▊      | 12/32 [00:12<00:19,  1.02it/s] 47%|████▋     | 15/32 [00:14<00:16,  1.03it/s] 56%|█████▋    | 18/32 [00:17<00:13,  1.04it/s] 38%|███▊      | 12/32 [00:12<00:19,  1.00it/s] 50%|█████     | 16/32 [00:15<00:15,  1.02it/s] 41%|████      | 13/32 [00:13<00:18,  1.02it/s] 59%|█████▉    | 19/32 [00:18<00:12,  1.04it/s] 41%|████      | 13/32 [00:13<00:18,  1.00it/s] 53%|█████▎    | 17/32 [00:16<00:14,  1.02it/s] 44%|████▍     | 14/32 [00:14<00:17,  1.02it/s] 62%|██████▎   | 20/32 [00:19<00:11,  1.04it/s] 44%|████▍     | 14/32 [00:14<00:17,  1.00it/s] 56%|█████▋    | 18/32 [00:17<00:13,  1.02it/s] 47%|████▋     | 15/32 [00:15<00:16,  1.02it/s] 66%|██████▌   | 21/32 [00:20<00:10,  1.04it/s] 47%|████▋     | 15/32 [00:15<00:16,  1.01it/s] 59%|█████▉    | 19/32 [00:18<00:12,  1.02it/s] 50%|█████     | 16/32 [00:15<00:15,  1.02it/s] 69%|██████▉   | 22/32 [00:21<00:09,  1.04it/s] 50%|█████     | 16/32 [00:16<00:15,  1.00it/s] 62%|██████▎   | 20/32 [00:19<00:11,  1.02it/s] 53%|█████▎    | 17/32 [00:16<00:14,  1.02it/s] 72%|███████▏  | 23/32 [00:22<00:08,  1.04it/s] 53%|█████▎    | 17/32 [00:17<00:14,  1.01it/s] 66%|██████▌   | 21/32 [00:20<00:10,  1.02it/s] 56%|█████▋    | 18/32 [00:17<00:13,  1.02it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.04it/s] 56%|█████▋    | 18/32 [00:18<00:13,  1.01it/s] 59%|█████▉    | 19/32 [00:18<00:12,  1.02it/s] 69%|██████▉   | 22/32 [00:21<00:09,  1.02it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.04it/s] 59%|█████▉    | 19/32 [00:19<00:12,  1.00it/s] 62%|██████▎   | 20/32 [00:19<00:11,  1.03it/s] 72%|███████▏  | 23/32 [00:22<00:08,  1.02it/s] 81%|████████▏ | 26/32 [00:25<00:05,  1.04it/s] 62%|██████▎   | 20/32 [00:20<00:12,  1.00s/it] 66%|██████▌   | 21/32 [00:20<00:10,  1.03it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.02it/s] 84%|████████▍ | 27/32 [00:26<00:04,  1.04it/s] 66%|██████▌   | 21/32 [00:21<00:10,  1.00it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.02it/s] 69%|██████▉   | 22/32 [00:21<00:09,  1.02it/s] 88%|████████▊ | 28/32 [00:27<00:03,  1.04it/s] 69%|██████▉   | 22/32 [00:22<00:10,  1.00s/it] 81%|████████▏ | 26/32 [00:25<00:05,  1.02it/s] 72%|███████▏  | 23/32 [00:22<00:08,  1.02it/s] 91%|█████████ | 29/32 [00:28<00:02,  1.04it/s] 72%|███████▏  | 23/32 [00:23<00:08,  1.00it/s] 84%|████████▍ | 27/32 [00:26<00:04,  1.02it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.02it/s] 94%|█████████▍| 30/32 [00:29<00:01,  1.04it/s] 75%|███████▌  | 24/32 [00:24<00:07,  1.00it/s] 88%|████████▊ | 28/32 [00:27<00:03,  1.03it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.03it/s] 97%|█████████▋| 31/32 [00:30<00:00,  1.04it/s] 78%|███████▊  | 25/32 [00:25<00:07,  1.00s/it] 91%|█████████ | 29/32 [00:28<00:02,  1.03it/s] 81%|████████▏ | 26/32 [00:25<00:05,  1.03it/s]100%|██████████| 32/32 [00:31<00:00,  1.04it/s]100%|██████████| 32/32 [00:31<00:00,  1.02it/s]
 81%|████████▏ | 26/32 [00:26<00:06,  1.00s/it] 94%|█████████▍| 30/32 [00:29<00:01,  1.03it/s] 84%|████████▍ | 27/32 [00:26<00:04,  1.03it/s] 84%|████████▍ | 27/32 [00:27<00:05,  1.01s/it] 97%|█████████▋| 31/32 [00:30<00:00,  1.03it/s] 88%|████████▊ | 28/32 [00:27<00:03,  1.02it/s] 88%|████████▊ | 28/32 [00:28<00:04,  1.01s/it]100%|██████████| 32/32 [00:31<00:00,  1.03it/s]100%|██████████| 32/32 [00:31<00:00,  1.01it/s]
 91%|█████████ | 29/32 [00:28<00:02,  1.02it/s] 91%|█████████ | 29/32 [00:29<00:03,  1.01s/it] 94%|█████████▍| 30/32 [00:29<00:01,  1.00it/s] 94%|█████████▍| 30/32 [00:30<00:02,  1.01s/it] 97%|█████████▋| 31/32 [00:30<00:00,  1.00it/s] 97%|█████████▋| 31/32 [00:31<00:01,  1.00s/it]100%|██████████| 32/32 [00:31<00:00,  1.00it/s]100%|██████████| 32/32 [00:31<00:00,  1.01it/s]
W0318 06:48:29.115000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.115000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.116000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.116000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.116000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.116000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.116000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.144000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.144000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.144000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.144000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.144000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.159000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.159000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.159000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.159000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.159000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.310000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.310000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.310000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.310000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.310000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:32<00:00,  1.00s/it]100%|██████████| 32/32 [00:32<00:00,  1.01s/it]
W0318 06:48:29.527000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.527000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.527000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.527000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.527000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.527000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.528000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.548000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.548000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.548000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.548000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.548000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.611000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.611000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.611000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.612000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:29.612000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:30.481000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:30.792000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:30.792000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:30.793000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:30.793000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:30.793000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:48:30.793000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:30.793000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:30.813000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:30.813000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:30.813000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:30.813000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:30.814000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:31.061000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:31.061000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:31.061000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:31.061000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:31.061000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:31.318000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.359000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.359000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.359000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.359000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.359000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.359000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.359000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.387000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.387000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.387000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.387000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.387000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.402000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.402000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.402000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.402000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.402000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.552000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.552000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.552000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.552000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.552000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.766000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.766000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.766000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.766000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.766000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.767000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.767000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.786000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.786000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.786000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.786000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.786000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.848000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.848000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.848000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.848000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:32.848000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:33.675000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:33.968000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:48:33.969000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:33.969000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:33.969000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:33.969000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:33.969000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:33.969000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:33.989000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:33.989000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:33.989000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:33.989000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:33.989000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:34.235000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:34.235000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:34.235000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:34.236000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:34.236000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:34.491000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.391000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.391000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.391000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.391000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.391000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.392000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.392000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.419000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.419000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.419000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.419000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.419000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.434000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.434000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.434000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.434000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.434000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.583000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.583000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.584000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.584000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.584000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.799000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.799000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.799000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.799000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.799000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.799000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.800000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.820000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.820000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.820000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.820000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.820000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.882000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.882000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.882000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.882000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:35.882000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.301000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.302000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.302000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.302000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.302000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.302000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.302000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.333000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.333000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.333000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.333000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.333000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.349000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.349000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.349000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.349000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.349000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.509000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.509000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.509000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.509000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.509000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.739000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.739000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.740000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.740000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.740000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.740000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.740000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.760000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.760000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.760000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.760000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.760000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.762000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.827000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.827000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.827000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.827000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:36.827000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:37.086000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:37.086000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:48:37.086000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:37.086000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:37.086000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:37.086000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:37.087000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:37.107000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:37.108000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:37.108000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:37.108000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:37.108000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
I0318 06:48:37.307406 1114793 finetune.py:45] layer 17_up initial loss 0.04151449352502823
W0318 06:48:37.307597 1114793 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:48:37.372000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:37.372000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:37.372000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:37.372000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:37.372000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:37.644000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:37.702000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:38.001000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:48:38.001000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:38.001000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:38.001000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:48:38.001000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:38.001000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:38.001000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:38.023000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:38.023000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:38.023000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:38.023000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:38.023000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:38.057818 1114793 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:48:38.283000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:48:38.283000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:48:38.283000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:48:38.283000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:48:38.283000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:48:38.550000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
17_up proxy err 0.010147484950721264 tr(WHW.T) 1921.078369140625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:40,  1.32s/it]I0318 06:48:41.043904 1114631 finetune.py:45] layer 16_up initial loss 0.03607118874788284
W0318 06:48:41.044178 1114631 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:02<00:34,  1.15s/it]W0318 06:48:41.901837 1114631 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:03<00:31,  1.10s/it]16_up proxy err 0.009005133993923664 tr(WHW.T) 1890.938232421875
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:04<00:30,  1.08s/it]I0318 06:48:44.029958 1115750 finetune.py:45] layer 19_up initial loss 0.04471440985798836
W0318 06:48:44.030181 1115750 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0318 06:48:44.727164 1115251 finetune.py:45] layer 18_up initial loss 0.04243718460202217
W0318 06:48:44.727397 1115251 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:05<00:28,  1.06s/it]  3%|▎         | 1/32 [00:01<00:40,  1.31s/it]W0318 06:48:44.819272 1115750 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:48:45.540175 1115251 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:06<00:27,  1.04s/it]  6%|▋         | 2/32 [00:02<00:34,  1.14s/it]19_up proxy err 0.010966935195028782 tr(WHW.T) 2149.330078125
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:07<00:25,  1.03s/it]  9%|▉         | 3/32 [00:03<00:30,  1.07s/it]18_up proxy err 0.01079986710101366 tr(WHW.T) 2023.1834716796875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.27s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.01s/it] 12%|█▎        | 4/32 [00:04<00:28,  1.03s/it]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it]  6%|▋         | 2/32 [00:02<00:33,  1.11s/it] 28%|██▊       | 9/32 [00:09<00:22,  1.01it/s] 16%|█▌        | 5/32 [00:05<00:27,  1.02s/it]  6%|▋         | 2/32 [00:02<00:33,  1.12s/it]  9%|▉         | 3/32 [00:03<00:30,  1.06s/it] 31%|███▏      | 10/32 [00:10<00:21,  1.02it/s] 19%|█▉        | 6/32 [00:06<00:26,  1.01s/it]  9%|▉         | 3/32 [00:03<00:30,  1.07s/it] 12%|█▎        | 4/32 [00:04<00:28,  1.03s/it] 34%|███▍      | 11/32 [00:11<00:20,  1.02it/s] 22%|██▏       | 7/32 [00:07<00:25,  1.00s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.04s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.02s/it] 38%|███▊      | 12/32 [00:12<00:19,  1.03it/s] 25%|██▌       | 8/32 [00:08<00:23,  1.01it/s] 16%|█▌        | 5/32 [00:05<00:27,  1.03s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.01s/it] 41%|████      | 13/32 [00:13<00:18,  1.03it/s] 28%|██▊       | 9/32 [00:09<00:22,  1.01it/s] 19%|█▉        | 6/32 [00:06<00:26,  1.02s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.00s/it] 44%|████▍     | 14/32 [00:14<00:17,  1.03it/s] 31%|███▏      | 10/32 [00:10<00:21,  1.02it/s] 22%|██▏       | 7/32 [00:07<00:25,  1.02s/it] 25%|██▌       | 8/32 [00:08<00:23,  1.00it/s] 47%|████▋     | 15/32 [00:15<00:16,  1.04it/s] 34%|███▍      | 11/32 [00:11<00:20,  1.02it/s] 25%|██▌       | 8/32 [00:08<00:24,  1.01s/it] 28%|██▊       | 9/32 [00:09<00:22,  1.01it/s] 50%|█████     | 16/32 [00:16<00:15,  1.04it/s] 38%|███▊      | 12/32 [00:12<00:19,  1.02it/s] 28%|██▊       | 9/32 [00:09<00:23,  1.01s/it] 31%|███▏      | 10/32 [00:10<00:21,  1.01it/s] 53%|█████▎    | 17/32 [00:17<00:14,  1.04it/s] 41%|████      | 13/32 [00:13<00:18,  1.02it/s] 31%|███▏      | 10/32 [00:10<00:22,  1.01s/it] 34%|███▍      | 11/32 [00:11<00:20,  1.01it/s] 56%|█████▋    | 18/32 [00:18<00:13,  1.04it/s] 44%|████▍     | 14/32 [00:14<00:17,  1.02it/s] 34%|███▍      | 11/32 [00:11<00:21,  1.01s/it] 38%|███▊      | 12/32 [00:12<00:19,  1.01it/s] 59%|█████▉    | 19/32 [00:19<00:12,  1.04it/s] 47%|████▋     | 15/32 [00:15<00:16,  1.02it/s] 38%|███▊      | 12/32 [00:12<00:20,  1.01s/it] 41%|████      | 13/32 [00:13<00:18,  1.01it/s] 62%|██████▎   | 20/32 [00:20<00:11,  1.03it/s] 50%|█████     | 16/32 [00:16<00:15,  1.02it/s] 41%|████      | 13/32 [00:13<00:19,  1.01s/it] 66%|██████▌   | 21/32 [00:20<00:10,  1.03it/s] 44%|████▍     | 14/32 [00:14<00:17,  1.01it/s] 53%|█████▎    | 17/32 [00:17<00:14,  1.02it/s] 44%|████▍     | 14/32 [00:14<00:18,  1.02s/it] 69%|██████▉   | 22/32 [00:21<00:09,  1.03it/s] 47%|████▋     | 15/32 [00:15<00:16,  1.01it/s] 56%|█████▋    | 18/32 [00:18<00:13,  1.02it/s] 50%|█████     | 16/32 [00:16<00:15,  1.01it/s] 47%|████▋     | 15/32 [00:15<00:17,  1.03s/it] 72%|███████▏  | 23/32 [00:23<00:08,  1.01it/s] 59%|█████▉    | 19/32 [00:19<00:12,  1.02it/s] 53%|█████▎    | 17/32 [00:17<00:14,  1.01it/s] 75%|███████▌  | 24/32 [00:24<00:07,  1.00it/s] 50%|█████     | 16/32 [00:16<00:16,  1.04s/it] 62%|██████▎   | 20/32 [00:19<00:11,  1.02it/s] 56%|█████▋    | 18/32 [00:18<00:13,  1.01it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.01it/s] 53%|█████▎    | 17/32 [00:17<00:15,  1.02s/it] 66%|██████▌   | 21/32 [00:20<00:10,  1.02it/s] 81%|████████▏ | 26/32 [00:25<00:05,  1.02it/s] 59%|█████▉    | 19/32 [00:19<00:12,  1.01it/s] 56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it] 69%|██████▉   | 22/32 [00:21<00:09,  1.02it/s] 84%|████████▍ | 27/32 [00:26<00:04,  1.02it/s] 62%|██████▎   | 20/32 [00:20<00:11,  1.01it/s] 59%|█████▉    | 19/32 [00:19<00:13,  1.02s/it] 72%|███████▏  | 23/32 [00:22<00:08,  1.02it/s] 88%|████████▊ | 28/32 [00:27<00:03,  1.03it/s] 66%|██████▌   | 21/32 [00:21<00:10,  1.01it/s] 62%|██████▎   | 20/32 [00:20<00:12,  1.01s/it] 75%|███████▌  | 24/32 [00:23<00:07,  1.02it/s] 91%|█████████ | 29/32 [00:28<00:02,  1.03it/s] 69%|██████▉   | 22/32 [00:22<00:09,  1.02it/s] 66%|██████▌   | 21/32 [00:21<00:11,  1.01s/it] 78%|███████▊  | 25/32 [00:24<00:06,  1.02it/s] 94%|█████████▍| 30/32 [00:29<00:01,  1.03it/s] 72%|███████▏  | 23/32 [00:23<00:08,  1.01it/s] 69%|██████▉   | 22/32 [00:22<00:10,  1.01s/it] 81%|████████▏ | 26/32 [00:25<00:05,  1.02it/s] 97%|█████████▋| 31/32 [00:30<00:00,  1.03it/s] 75%|███████▌  | 24/32 [00:24<00:07,  1.01it/s] 72%|███████▏  | 23/32 [00:23<00:09,  1.01s/it] 84%|████████▍ | 27/32 [00:26<00:04,  1.02it/s]100%|██████████| 32/32 [00:31<00:00,  1.03it/s]100%|██████████| 32/32 [00:31<00:00,  1.01it/s]
 78%|███████▊  | 25/32 [00:25<00:06,  1.01it/s] 88%|████████▊ | 28/32 [00:27<00:03,  1.02it/s] 75%|███████▌  | 24/32 [00:24<00:08,  1.01s/it] 81%|████████▏ | 26/32 [00:26<00:05,  1.01it/s] 91%|█████████ | 29/32 [00:28<00:02,  1.01it/s] 78%|███████▊  | 25/32 [00:25<00:07,  1.02s/it] 84%|████████▍ | 27/32 [00:27<00:04,  1.01it/s] 94%|█████████▍| 30/32 [00:29<00:01,  1.02it/s] 81%|████████▏ | 26/32 [00:26<00:06,  1.02s/it] 88%|████████▊ | 28/32 [00:27<00:03,  1.01it/s] 97%|█████████▋| 31/32 [00:30<00:00,  1.02it/s] 84%|████████▍ | 27/32 [00:27<00:05,  1.03s/it] 91%|█████████ | 29/32 [00:28<00:02,  1.02it/s]100%|██████████| 32/32 [00:31<00:00,  1.03it/s]100%|██████████| 32/32 [00:31<00:00,  1.01it/s]
 88%|████████▊ | 28/32 [00:28<00:04,  1.05s/it] 94%|█████████▍| 30/32 [00:29<00:01,  1.01it/s] 91%|█████████ | 29/32 [00:29<00:03,  1.06s/it] 97%|█████████▋| 31/32 [00:30<00:00,  1.00it/s] 94%|█████████▍| 30/32 [00:30<00:02,  1.06s/it]I0318 06:49:17.963673 1114793 finetune.py:45] layer 17_gate initial loss 0.04109327122569084
W0318 06:49:17.963979 1114793 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:32<00:00,  1.00s/it]100%|██████████| 32/32 [00:32<00:00,  1.00s/it]
W0318 06:49:18.646458 1114793 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:31<00:01,  1.05s/it]100%|██████████| 32/32 [00:32<00:00,  1.05s/it]100%|██████████| 32/32 [00:32<00:00,  1.03s/it]
17_gate proxy err 0.005983305163681507 tr(WHW.T) 3571.315673828125
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:49,  1.71it/s]  2%|▏         | 2/86 [00:00<00:36,  2.28it/s]  3%|▎         | 3/86 [00:01<00:32,  2.52it/s]I0318 06:49:23.181201 1114631 finetune.py:45] layer 16_gate initial loss 0.03550735488533974
W0318 06:49:23.181458 1114631 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  5%|▍         | 4/86 [00:01<00:30,  2.66it/s]  6%|▌         | 5/86 [00:01<00:29,  2.74it/s]W0318 06:49:23.927089 1114631 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  7%|▋         | 6/86 [00:02<00:28,  2.79it/s]  8%|▊         | 7/86 [00:02<00:27,  2.83it/s]  9%|▉         | 8/86 [00:02<00:27,  2.86it/s] 10%|█         | 9/86 [00:03<00:26,  2.87it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.89it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.88it/s]I0318 06:49:25.843564 1115750 finetune.py:45] layer 19_gate initial loss 0.044085633009672165
W0318 06:49:25.843761 1115750 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 14%|█▍        | 12/86 [00:04<00:25,  2.88it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.90it/s]W0318 06:49:26.504931 1115750 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▋        | 14/86 [00:05<00:24,  2.90it/s]16_gate proxy err 0.00539505435153842 tr(WHW.T) 3369.8583984375
  0%|          | 0/86 [00:00<?, ?it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.90it/s]I0318 06:49:27.263201 1115251 finetune.py:45] layer 18_gate initial loss 0.04215088486671448
W0318 06:49:27.263411 1115251 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▊        | 16/86 [00:05<00:24,  2.91it/s]  1%|          | 1/86 [00:00<00:51,  1.65it/s] 20%|█▉        | 17/86 [00:06<00:23,  2.95it/s]  2%|▏         | 2/86 [00:00<00:37,  2.23it/s]W0318 06:49:27.936519 1115251 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 21%|██        | 18/86 [00:06<00:22,  2.97it/s]  3%|▎         | 3/86 [00:01<00:33,  2.49it/s] 22%|██▏       | 19/86 [00:06<00:22,  2.98it/s]  5%|▍         | 4/86 [00:01<00:31,  2.64it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.96it/s]  6%|▌         | 5/86 [00:01<00:29,  2.73it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.93it/s]  7%|▋         | 6/86 [00:02<00:28,  2.79it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.96it/s]19_gate proxy err 0.007221597712486982 tr(WHW.T) 3687.51220703125
  0%|          | 0/86 [00:00<?, ?it/s]  8%|▊         | 7/86 [00:02<00:27,  2.88it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.97it/s]  9%|▉         | 8/86 [00:02<00:26,  2.92it/s]  1%|          | 1/86 [00:00<00:49,  1.72it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.95it/s] 10%|█         | 9/86 [00:03<00:26,  2.95it/s]  2%|▏         | 2/86 [00:00<00:36,  2.30it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.95it/s] 12%|█▏        | 10/86 [00:03<00:25,  2.97it/s]  3%|▎         | 3/86 [00:01<00:32,  2.57it/s] 30%|███       | 26/86 [00:09<00:20,  2.95it/s] 13%|█▎        | 11/86 [00:03<00:25,  2.97it/s]18_gate proxy err 0.006479901727288961 tr(WHW.T) 3783.07568359375
  0%|          | 0/86 [00:00<?, ?it/s]  5%|▍         | 4/86 [00:01<00:30,  2.71it/s] 31%|███▏      | 27/86 [00:09<00:19,  2.98it/s] 14%|█▍        | 12/86 [00:04<00:24,  2.98it/s]  6%|▌         | 5/86 [00:01<00:29,  2.79it/s] 33%|███▎      | 28/86 [00:09<00:19,  3.01it/s] 15%|█▌        | 13/86 [00:04<00:24,  2.99it/s]  1%|          | 1/86 [00:00<00:49,  1.71it/s]  7%|▋         | 6/86 [00:02<00:27,  2.86it/s] 34%|███▎      | 29/86 [00:10<00:18,  3.03it/s] 16%|█▋        | 14/86 [00:04<00:24,  2.99it/s]  2%|▏         | 2/86 [00:00<00:37,  2.27it/s]  8%|▊         | 7/86 [00:02<00:27,  2.88it/s] 35%|███▍      | 30/86 [00:10<00:18,  3.04it/s] 17%|█▋        | 15/86 [00:05<00:23,  2.98it/s]  3%|▎         | 3/86 [00:01<00:32,  2.53it/s]  9%|▉         | 8/86 [00:02<00:26,  2.90it/s] 36%|███▌      | 31/86 [00:10<00:18,  3.05it/s] 19%|█▊        | 16/86 [00:05<00:23,  3.00it/s]  5%|▍         | 4/86 [00:01<00:30,  2.69it/s] 10%|█         | 9/86 [00:03<00:26,  2.92it/s] 37%|███▋      | 32/86 [00:11<00:17,  3.07it/s] 20%|█▉        | 17/86 [00:05<00:22,  3.00it/s]  6%|▌         | 5/86 [00:01<00:29,  2.76it/s] 38%|███▊      | 33/86 [00:11<00:17,  3.07it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.92it/s] 21%|██        | 18/86 [00:06<00:22,  2.99it/s]  7%|▋         | 6/86 [00:02<00:28,  2.80it/s] 40%|███▉      | 34/86 [00:11<00:16,  3.06it/s] 13%|█▎        | 11/86 [00:03<00:25,  2.94it/s] 22%|██▏       | 19/86 [00:06<00:22,  3.00it/s]  8%|▊         | 7/86 [00:02<00:27,  2.85it/s] 41%|████      | 35/86 [00:12<00:16,  3.08it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.94it/s] 23%|██▎       | 20/86 [00:06<00:21,  3.01it/s]  9%|▉         | 8/86 [00:02<00:27,  2.88it/s] 42%|████▏     | 36/86 [00:12<00:16,  3.08it/s] 15%|█▌        | 13/86 [00:04<00:24,  2.94it/s] 24%|██▍       | 21/86 [00:07<00:21,  3.00it/s] 10%|█         | 9/86 [00:03<00:26,  2.89it/s] 43%|████▎     | 37/86 [00:12<00:15,  3.08it/s] 16%|█▋        | 14/86 [00:04<00:24,  2.96it/s] 26%|██▌       | 22/86 [00:07<00:21,  3.00it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.91it/s] 44%|████▍     | 38/86 [00:13<00:15,  3.09it/s] 17%|█▋        | 15/86 [00:05<00:23,  2.96it/s] 27%|██▋       | 23/86 [00:07<00:21,  3.00it/s] 45%|████▌     | 39/86 [00:13<00:15,  3.10it/s] 13%|█▎        | 11/86 [00:03<00:25,  2.93it/s] 19%|█▊        | 16/86 [00:05<00:23,  2.97it/s] 28%|██▊       | 24/86 [00:08<00:20,  2.99it/s] 47%|████▋     | 40/86 [00:13<00:14,  3.10it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.92it/s] 20%|█▉        | 17/86 [00:05<00:23,  2.97it/s] 29%|██▉       | 25/86 [00:08<00:20,  3.00it/s] 48%|████▊     | 41/86 [00:13<00:14,  3.10it/s] 15%|█▌        | 13/86 [00:04<00:24,  2.93it/s] 21%|██        | 18/86 [00:06<00:22,  2.98it/s] 30%|███       | 26/86 [00:08<00:20,  3.00it/s] 49%|████▉     | 42/86 [00:14<00:14,  3.09it/s] 16%|█▋        | 14/86 [00:05<00:24,  2.93it/s] 22%|██▏       | 19/86 [00:06<00:22,  2.98it/s] 31%|███▏      | 27/86 [00:09<00:19,  3.00it/s] 50%|█████     | 43/86 [00:14<00:13,  3.08it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.91it/s] 23%|██▎       | 20/86 [00:06<00:22,  2.98it/s] 33%|███▎      | 28/86 [00:09<00:19,  3.01it/s] 51%|█████     | 44/86 [00:14<00:13,  3.07it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.91it/s] 24%|██▍       | 21/86 [00:07<00:21,  2.99it/s] 34%|███▎      | 29/86 [00:09<00:18,  3.02it/s] 52%|█████▏    | 45/86 [00:15<00:13,  3.07it/s] 20%|█▉        | 17/86 [00:06<00:23,  2.92it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.98it/s] 35%|███▍      | 30/86 [00:10<00:18,  3.02it/s] 53%|█████▎    | 46/86 [00:15<00:13,  3.07it/s] 21%|██        | 18/86 [00:06<00:23,  2.93it/s] 27%|██▋       | 23/86 [00:07<00:21,  2.97it/s] 36%|███▌      | 31/86 [00:10<00:18,  3.02it/s] 55%|█████▍    | 47/86 [00:15<00:12,  3.08it/s] 22%|██▏       | 19/86 [00:06<00:22,  2.94it/s] 28%|██▊       | 24/86 [00:08<00:20,  2.98it/s] 37%|███▋      | 32/86 [00:10<00:17,  3.01it/s] 56%|█████▌    | 48/86 [00:16<00:12,  3.08it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.93it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.97it/s] 38%|███▊      | 33/86 [00:11<00:17,  3.01it/s] 57%|█████▋    | 49/86 [00:16<00:11,  3.09it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.93it/s] 30%|███       | 26/86 [00:08<00:20,  2.98it/s] 40%|███▉      | 34/86 [00:11<00:17,  3.01it/s] 58%|█████▊    | 50/86 [00:16<00:11,  3.07it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.94it/s] 31%|███▏      | 27/86 [00:09<00:19,  2.98it/s] 41%|████      | 35/86 [00:11<00:16,  3.00it/s] 59%|█████▉    | 51/86 [00:17<00:11,  3.07it/s] 33%|███▎      | 28/86 [00:09<00:19,  2.98it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.93it/s] 42%|████▏     | 36/86 [00:12<00:16,  3.02it/s] 60%|██████    | 52/86 [00:17<00:11,  3.08it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.98it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.93it/s] 43%|████▎     | 37/86 [00:12<00:16,  3.02it/s] 62%|██████▏   | 53/86 [00:17<00:10,  3.07it/s] 35%|███▍      | 30/86 [00:10<00:18,  2.98it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.93it/s] 44%|████▍     | 38/86 [00:12<00:15,  3.02it/s] 63%|██████▎   | 54/86 [00:18<00:10,  3.07it/s] 36%|███▌      | 31/86 [00:10<00:18,  2.99it/s] 30%|███       | 26/86 [00:09<00:20,  2.93it/s] 45%|████▌     | 39/86 [00:13<00:15,  3.02it/s] 64%|██████▍   | 55/86 [00:18<00:10,  3.08it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.99it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.92it/s] 47%|████▋     | 40/86 [00:13<00:15,  3.02it/s] 65%|██████▌   | 56/86 [00:18<00:09,  3.06it/s] 38%|███▊      | 33/86 [00:11<00:17,  3.00it/s] 48%|████▊     | 41/86 [00:13<00:14,  3.01it/s] 33%|███▎      | 28/86 [00:09<00:19,  2.92it/s] 66%|██████▋   | 57/86 [00:19<00:09,  3.06it/s] 40%|███▉      | 34/86 [00:11<00:17,  3.01it/s] 49%|████▉     | 42/86 [00:14<00:14,  3.01it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.93it/s] 67%|██████▋   | 58/86 [00:19<00:09,  3.07it/s] 41%|████      | 35/86 [00:11<00:16,  3.01it/s] 50%|█████     | 43/86 [00:14<00:14,  3.01it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.93it/s] 69%|██████▊   | 59/86 [00:19<00:08,  3.07it/s] 42%|████▏     | 36/86 [00:12<00:16,  3.01it/s] 51%|█████     | 44/86 [00:14<00:13,  3.02it/s] 70%|██████▉   | 60/86 [00:20<00:08,  3.07it/s] 36%|███▌      | 31/86 [00:10<00:18,  2.93it/s] 43%|████▎     | 37/86 [00:12<00:16,  3.00it/s] 52%|█████▏    | 45/86 [00:15<00:13,  3.02it/s] 71%|███████   | 61/86 [00:20<00:08,  3.07it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.94it/s] 44%|████▍     | 38/86 [00:13<00:16,  3.00it/s] 53%|█████▎    | 46/86 [00:15<00:13,  3.02it/s] 72%|███████▏  | 62/86 [00:20<00:07,  3.04it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.93it/s] 45%|████▌     | 39/86 [00:13<00:15,  2.99it/s] 55%|█████▍    | 47/86 [00:15<00:12,  3.02it/s] 73%|███████▎  | 63/86 [00:21<00:07,  3.03it/s] 40%|███▉      | 34/86 [00:11<00:17,  2.92it/s] 47%|████▋     | 40/86 [00:13<00:15,  2.98it/s] 56%|█████▌    | 48/86 [00:16<00:12,  3.02it/s] 74%|███████▍  | 64/86 [00:21<00:07,  3.05it/s] 41%|████      | 35/86 [00:12<00:17,  2.93it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.98it/s] 57%|█████▋    | 49/86 [00:16<00:12,  3.03it/s] 76%|███████▌  | 65/86 [00:21<00:06,  3.06it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.94it/s] 49%|████▉     | 42/86 [00:14<00:14,  2.99it/s] 58%|█████▊    | 50/86 [00:16<00:11,  3.02it/s] 77%|███████▋  | 66/86 [00:22<00:06,  3.07it/s] 43%|████▎     | 37/86 [00:12<00:16,  2.94it/s] 50%|█████     | 43/86 [00:14<00:14,  3.01it/s] 78%|███████▊  | 67/86 [00:22<00:06,  3.08it/s] 59%|█████▉    | 51/86 [00:17<00:11,  3.01it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.95it/s] 79%|███████▉  | 68/86 [00:22<00:05,  3.09it/s] 51%|█████     | 44/86 [00:14<00:13,  3.02it/s] 60%|██████    | 52/86 [00:17<00:11,  3.01it/s] 45%|████▌     | 39/86 [00:13<00:15,  2.96it/s] 80%|████████  | 69/86 [00:23<00:05,  3.09it/s] 52%|█████▏    | 45/86 [00:15<00:13,  3.01it/s] 62%|██████▏   | 53/86 [00:17<00:10,  3.01it/s] 47%|████▋     | 40/86 [00:13<00:15,  2.95it/s] 81%|████████▏ | 70/86 [00:23<00:05,  3.08it/s] 53%|█████▎    | 46/86 [00:15<00:13,  3.01it/s] 63%|██████▎   | 54/86 [00:18<00:10,  3.01it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.95it/s] 83%|████████▎ | 71/86 [00:23<00:04,  3.09it/s] 55%|█████▍    | 47/86 [00:15<00:12,  3.01it/s] 64%|██████▍   | 55/86 [00:18<00:10,  3.00it/s] 49%|████▉     | 42/86 [00:14<00:14,  2.96it/s] 84%|████████▎ | 72/86 [00:24<00:04,  3.07it/s] 56%|█████▌    | 48/86 [00:16<00:12,  3.01it/s] 65%|██████▌   | 56/86 [00:18<00:10,  3.00it/s] 50%|█████     | 43/86 [00:14<00:14,  2.92it/s] 85%|████████▍ | 73/86 [00:24<00:04,  3.06it/s] 57%|█████▋    | 49/86 [00:16<00:12,  3.02it/s] 66%|██████▋   | 57/86 [00:19<00:09,  3.00it/s] 51%|█████     | 44/86 [00:15<00:14,  2.93it/s] 86%|████████▌ | 74/86 [00:24<00:03,  3.08it/s] 58%|█████▊    | 50/86 [00:16<00:11,  3.02it/s] 67%|██████▋   | 58/86 [00:19<00:09,  2.99it/s] 52%|█████▏    | 45/86 [00:15<00:13,  2.95it/s] 87%|████████▋ | 75/86 [00:25<00:03,  3.07it/s] 59%|█████▉    | 51/86 [00:17<00:11,  3.01it/s] 69%|██████▊   | 59/86 [00:19<00:09,  2.99it/s] 53%|█████▎    | 46/86 [00:15<00:13,  2.94it/s] 88%|████████▊ | 76/86 [00:25<00:03,  3.07it/s] 60%|██████    | 52/86 [00:17<00:11,  3.00it/s] 70%|██████▉   | 60/86 [00:20<00:08,  2.99it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.96it/s] 90%|████████▉ | 77/86 [00:25<00:02,  3.09it/s] 62%|██████▏   | 53/86 [00:17<00:10,  3.01it/s] 71%|███████   | 61/86 [00:20<00:08,  3.00it/s] 56%|█████▌    | 48/86 [00:16<00:12,  2.97it/s] 91%|█████████ | 78/86 [00:26<00:02,  3.09it/s] 63%|██████▎   | 54/86 [00:18<00:10,  3.01it/s] 72%|███████▏  | 62/86 [00:20<00:08,  3.00it/s] 57%|█████▋    | 49/86 [00:16<00:12,  2.94it/s] 92%|█████████▏| 79/86 [00:26<00:02,  3.07it/s] 64%|██████▍   | 55/86 [00:18<00:10,  2.99it/s] 73%|███████▎  | 63/86 [00:21<00:07,  3.00it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.95it/s] 93%|█████████▎| 80/86 [00:26<00:01,  3.08it/s] 65%|██████▌   | 56/86 [00:18<00:10,  2.99it/s] 74%|███████▍  | 64/86 [00:21<00:07,  3.01it/s] 59%|█████▉    | 51/86 [00:17<00:11,  2.95it/s] 94%|█████████▍| 81/86 [00:26<00:01,  3.09it/s] 66%|██████▋   | 57/86 [00:19<00:09,  3.00it/s] 76%|███████▌  | 65/86 [00:21<00:07,  2.99it/s] 60%|██████    | 52/86 [00:17<00:11,  2.93it/s] 95%|█████████▌| 82/86 [00:27<00:01,  3.05it/s] 67%|██████▋   | 58/86 [00:19<00:09,  3.01it/s] 77%|███████▋  | 66/86 [00:22<00:06,  2.99it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.93it/s] 97%|█████████▋| 83/86 [00:27<00:00,  3.07it/s] 69%|██████▊   | 59/86 [00:19<00:08,  3.00it/s] 78%|███████▊  | 67/86 [00:22<00:06,  3.00it/s] 98%|█████████▊| 84/86 [00:27<00:00,  3.09it/s] 63%|██████▎   | 54/86 [00:18<00:10,  2.95it/s] 70%|██████▉   | 60/86 [00:20<00:08,  3.01it/s] 79%|███████▉  | 68/86 [00:22<00:05,  3.00it/s] 99%|█████████▉| 85/86 [00:28<00:00,  3.08it/s] 64%|██████▍   | 55/86 [00:18<00:10,  2.94it/s] 71%|███████   | 61/86 [00:20<00:08,  3.00it/s] 80%|████████  | 69/86 [00:23<00:05,  3.02it/s]100%|██████████| 86/86 [00:28<00:00,  3.08it/s]100%|██████████| 86/86 [00:28<00:00,  3.00it/s]
 65%|██████▌   | 56/86 [00:19<00:10,  2.95it/s] 72%|███████▏  | 62/86 [00:20<00:08,  3.00it/s] 81%|████████▏ | 70/86 [00:23<00:05,  3.01it/s] 66%|██████▋   | 57/86 [00:19<00:09,  2.94it/s] 73%|███████▎  | 63/86 [00:21<00:07,  2.99it/s] 83%|████████▎ | 71/86 [00:23<00:04,  3.02it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.89it/s] 74%|███████▍  | 64/86 [00:21<00:07,  2.99it/s] 84%|████████▎ | 72/86 [00:24<00:04,  3.01it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.87it/s] 76%|███████▌  | 65/86 [00:21<00:07,  2.99it/s] 85%|████████▍ | 73/86 [00:24<00:04,  3.01it/s] 70%|██████▉   | 60/86 [00:20<00:08,  2.90it/s] 77%|███████▋  | 66/86 [00:22<00:06,  2.97it/s] 86%|████████▌ | 74/86 [00:24<00:03,  3.01it/s] 71%|███████   | 61/86 [00:21<00:08,  2.87it/s] 78%|███████▊  | 67/86 [00:22<00:06,  2.98it/s] 87%|████████▋ | 75/86 [00:25<00:03,  3.02it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.88it/s] 88%|████████▊ | 76/86 [00:25<00:03,  3.02it/s] 79%|███████▉  | 68/86 [00:23<00:06,  2.99it/s] 73%|███████▎  | 63/86 [00:21<00:07,  2.90it/s] 90%|████████▉ | 77/86 [00:25<00:02,  3.03it/s] 80%|████████  | 69/86 [00:23<00:05,  2.99it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.91it/s] 91%|█████████ | 78/86 [00:26<00:02,  3.02it/s] 81%|████████▏ | 70/86 [00:23<00:05,  2.99it/s] 92%|█████████▏| 79/86 [00:26<00:02,  3.02it/s] 83%|████████▎ | 71/86 [00:24<00:04,  3.00it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.93it/s] 93%|█████████▎| 80/86 [00:26<00:01,  3.02it/s] 77%|███████▋  | 66/86 [00:22<00:06,  2.95it/s] 84%|████████▎ | 72/86 [00:24<00:04,  3.00it/s] 94%|█████████▍| 81/86 [00:27<00:01,  3.03it/s] 85%|████████▍ | 73/86 [00:24<00:04,  2.99it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.94it/s] 95%|█████████▌| 82/86 [00:27<00:01,  3.03it/s] 86%|████████▌ | 74/86 [00:25<00:04,  2.98it/s] 79%|███████▉  | 68/86 [00:23<00:06,  2.94it/s] 97%|█████████▋| 83/86 [00:27<00:00,  3.03it/s] 87%|████████▋ | 75/86 [00:25<00:03,  2.98it/s] 80%|████████  | 69/86 [00:23<00:05,  2.95it/s] 98%|█████████▊| 84/86 [00:28<00:00,  3.02it/s] 88%|████████▊ | 76/86 [00:25<00:03,  2.98it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.93it/s] 99%|█████████▉| 85/86 [00:28<00:00,  3.02it/s] 90%|████████▉ | 77/86 [00:26<00:03,  2.98it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.94it/s]100%|██████████| 86/86 [00:28<00:00,  3.03it/s]100%|██████████| 86/86 [00:28<00:00,  2.98it/s]
 91%|█████████ | 78/86 [00:26<00:02,  2.99it/s] 84%|████████▎ | 72/86 [00:24<00:04,  2.96it/s] 92%|█████████▏| 79/86 [00:26<00:02,  2.98it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.95it/s] 93%|█████████▎| 80/86 [00:27<00:02,  2.89it/s] 86%|████████▌ | 74/86 [00:25<00:04,  2.93it/s]W0318 06:49:56.655000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.655000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.655000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.655000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.655000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.655000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.655000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.698000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.698000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.698000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.699000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.699000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.714000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.714000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.714000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.714000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.714000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 87%|████████▋ | 75/86 [00:25<00:03,  2.94it/s]W0318 06:49:56.878000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.879000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.879000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.879000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:49:56.879000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 81/86 [00:27<00:01,  2.87it/s]W0318 06:49:57.192000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:49:57.193000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:49:57.193000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:49:57.193000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:49:57.193000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:49:57.193000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:49:57.193000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 76/86 [00:26<00:03,  2.93it/s] 95%|█████████▌| 82/86 [00:27<00:01,  2.88it/s]W0318 06:49:57.226000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:49:57.226000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:49:57.226000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:49:57.226000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:49:57.226000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:49:57.293000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:49:57.293000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:49:57.294000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:49:57.294000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:49:57.294000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 90%|████████▉ | 77/86 [00:26<00:03,  2.94it/s] 97%|█████████▋| 83/86 [00:28<00:01,  2.89it/s] 91%|█████████ | 78/86 [00:26<00:02,  2.94it/s] 98%|█████████▊| 84/86 [00:28<00:00,  2.91it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.93it/s] 99%|█████████▉| 85/86 [00:28<00:00,  2.93it/s]W0318 06:49:58.444000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:49:58.458000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:49:58.465000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:49:58.466000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 93%|█████████▎| 80/86 [00:27<00:02,  2.94it/s]100%|██████████| 86/86 [00:29<00:00,  2.95it/s]100%|██████████| 86/86 [00:29<00:00,  2.95it/s]
W0318 06:49:58.885000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:49:58.885000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:49:58.885000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:49:58.885000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:49:58.885000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:49:58.885000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:49:58.885000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 81/86 [00:27<00:01,  2.95it/s]W0318 06:49:58.914000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:49:58.914000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:49:58.914000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:49:58.914000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:49:58.915000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:49:59.249000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 95%|█████████▌| 82/86 [00:28<00:01,  2.94it/s]W0318 06:49:59.250000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:49:59.250000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:49:59.250000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:49:59.250000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:49:59.250000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:49:59.250000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:49:59.250000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:49:59.536000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:49:59.536000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:49:59.536000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:49:59.536000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:49:59.536000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 83/86 [00:28<00:01,  2.94it/s]W0318 06:49:59.856000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:49:59.861000 140250040059712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 98%|█████████▊| 84/86 [00:28<00:00,  2.95it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.92it/s]100%|██████████| 86/86 [00:29<00:00,  2.95it/s]100%|██████████| 86/86 [00:29<00:00,  2.91it/s]
W0318 06:50:02.371000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.372000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.372000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.372000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.372000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.372000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.372000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.414000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.414000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.414000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.414000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.414000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.430000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.430000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.430000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.430000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.430000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.594000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.594000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.595000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.595000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.595000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.902000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.902000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.903000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.903000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.903000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.903000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.903000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.934000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.934000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.934000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.934000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:02.934000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:03.003000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:03.004000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:03.004000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:03.004000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:03.004000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.144000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.150000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.155000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.156000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.583000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.583000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.584000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.584000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.584000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.584000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.584000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.616000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.616000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.616000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.616000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.616000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.944000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.944000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.945000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.945000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.945000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.945000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.945000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:04.945000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.041000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.042000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.042000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.042000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.042000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.042000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.042000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.082000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.082000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.082000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.082000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.082000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.097000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.097000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.097000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.097000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.097000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.228000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.228000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.229000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.229000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.229000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.256000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.256000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.256000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.256000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.256000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.550000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.563000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.563000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.563000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.563000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.563000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.563000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.564000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.565000 140658846365504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.596000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.597000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.597000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.597000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.597000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.664000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.664000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.664000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.664000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:05.664000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
I0318 06:50:06.394709 1114793 finetune.py:45] layer 17_down initial loss 0.040466032922267914
W0318 06:50:06.394935 1114793 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:50:06.794000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:06.808000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:06.815000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:06.815000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:06.851865 1114793 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

17_down proxy err 0.013916069641709328 tr(WHW.T) 165.43487548828125
W0318 06:50:07.244000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.244000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.244000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.244000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.244000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.244000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.244000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.273000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.273000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.273000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.273000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.273000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.625000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.626000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.626000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.626000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.626000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.626000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.626000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.626000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.726000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.726000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.726000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.726000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.726000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.726000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.726000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.768000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.768000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.768000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.768000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.768000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.784000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.784000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.784000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.784000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.784000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.927000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.927000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.927000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.927000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.927000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.948000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.949000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.949000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.949000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:07.949000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.261000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.261000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.261000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.261000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.261000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.261000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.262000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.264000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.269000 140645193676608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.293000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.293000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.293000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.293000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.293000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.363000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.363000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.363000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.363000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:08.364000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:09.534000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:09.547000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:09.555000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:09.555000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:09.983000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:09.983000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:09.983000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:09.983000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:09.983000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:09.983000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:09.984000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.012000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.012000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.012000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.012000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.012000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.349000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.349000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.349000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.349000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.349000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.349000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.349000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.349000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.642000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.642000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.642000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.642000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.642000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.969000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:10.974000 139922655590208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0318 06:50:12.547502 1114631 finetune.py:45] layer 16_down initial loss 0.03502081334590912
W0318 06:50:12.547755 1114631 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:50:13.036135 1114631 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

16_down proxy err 0.01335593406111002 tr(WHW.T) 152.02943420410156
I0318 06:50:15.047155 1115750 finetune.py:45] layer 19_down initial loss 0.04356565698981285
W0318 06:50:15.047381 1115750 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:50:15.566563 1115750 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

19_down proxy err 0.013564672321081161 tr(WHW.T) 222.93173217773438
I0318 06:50:16.869105 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 20 in 1.250173807144165s
I0318 06:50:17.299532 1092738 quantize_finetune_llama.py:159] layer 21 gpu 1
I0318 06:50:17.558859 1115251 finetune.py:45] layer 18_down initial loss 0.04173135757446289
W0318 06:50:17.559091 1115251 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:50:18.033699 1115251 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

18_down proxy err 0.013696550391614437 tr(WHW.T) 198.52691650390625
I0318 06:50:19.233676 1119643 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:50:19.233780 1119643 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:50:19.233838 1119643 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:50:19.420637 1119643 config.py:58] PyTorch version 2.4.0 available.
I0318 06:50:21.497413 1119643 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0318 06:50:21.687572 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 21 in 1.4108860492706299s
W0318 06:50:22.010092 1119643 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:50:22.174534 1092738 quantize_finetune_llama.py:159] layer 22 gpu 2
  0%|          | 0/32 [00:00<?, ?it/s]I0318 06:50:23.740112 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 22 in 1.1229839324951172s
I0318 06:50:24.078914 1120111 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:50:24.079041 1120111 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:50:24.079107 1120111 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:50:24.234483 1092738 quantize_finetune_llama.py:159] layer 23 gpu 3
I0318 06:50:24.288708 1120111 config.py:58] PyTorch version 2.4.0 available.
  3%|▎         | 1/32 [00:01<00:44,  1.43s/it]  6%|▋         | 2/32 [00:01<00:23,  1.28it/s]  9%|▉         | 3/32 [00:02<00:16,  1.73it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.12it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s]I0318 06:50:26.123340 1120356 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:50:26.123453 1120356 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:50:26.123515 1120356 utils.py:162] NumExpr defaulting to 16 threads.
 19%|█▉        | 6/32 [00:03<00:09,  2.61it/s]I0318 06:50:26.289158 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 23 in 1.5673744678497314s
I0318 06:50:26.311115 1120356 config.py:58] PyTorch version 2.4.0 available.
I0318 06:50:26.459593 1120111 data_utils.py:336] using 256 training seqs, 128 validation seqs
 22%|██▏       | 7/32 [00:03<00:09,  2.62it/s]I0318 06:50:26.784100 1092738 quantize_finetune_llama.py:159] layer 24 gpu 0
W0318 06:50:26.888098 1120111 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.75it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.81it/s]  0%|          | 0/32 [00:00<?, ?it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.86it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.88it/s] 41%|████      | 13/32 [00:05<00:06,  2.78it/s]I0318 06:50:28.707777 1120792 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:50:28.707896 1120792 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:50:28.707952 1120792 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:50:28.890367 1120356 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0318 06:50:28.890641 1120792 config.py:58] PyTorch version 2.4.0 available.
 44%|████▍     | 14/32 [00:05<00:06,  2.88it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.93it/s]W0318 06:50:29.393029 1120356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:48,  1.57s/it] 50%|█████     | 16/32 [00:06<00:05,  2.95it/s]  6%|▋         | 2/32 [00:01<00:25,  1.18it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.97it/s]  9%|▉         | 3/32 [00:02<00:17,  1.64it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.01it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.00it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.03it/s]  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.22it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.01it/s]I0318 06:50:30.978210 1120792 data_utils.py:336] using 256 training seqs, 128 validation seqs
 19%|█▉        | 6/32 [00:03<00:10,  2.41it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.94it/s]W0318 06:50:31.332247 1120792 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.95it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.99it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.78it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.01it/s]  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:49,  1.59s/it] 31%|███▏      | 10/32 [00:04<00:07,  2.87it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.96it/s]  6%|▋         | 2/32 [00:01<00:25,  1.16it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.91it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.96it/s]  9%|▉         | 3/32 [00:02<00:18,  1.60it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.96it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.01it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.94it/s] 41%|████      | 13/32 [00:05<00:06,  2.98it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.03it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.20it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.01it/s]  3%|▎         | 1/32 [00:01<00:46,  1.51s/it] 91%|█████████ | 29/32 [00:10<00:00,  3.06it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.41it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.04it/s]  6%|▋         | 2/32 [00:01<00:24,  1.23it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.08it/s] 50%|█████     | 16/32 [00:06<00:05,  3.06it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s]  9%|▉         | 3/32 [00:02<00:17,  1.69it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.10it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.08it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.05it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.67it/s]100%|██████████| 32/32 [00:11<00:00,  3.12it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
 56%|█████▋    | 18/32 [00:07<00:04,  3.03it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.97it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.47it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.75it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.98it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.61it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.79it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.96it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.82it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.99it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.78it/s] 41%|████      | 13/32 [00:05<00:06,  2.84it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.02it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.86it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.85it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.03it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.90it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.03it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.94it/s] 50%|█████     | 16/32 [00:06<00:05,  2.84it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.05it/s] 41%|████      | 13/32 [00:05<00:06,  2.97it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.85it/s]W0318 06:50:37.967000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:37.967000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:37.967000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:37.967000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:37.967000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:37.967000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:37.967000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:37.993000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:37.993000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:37.993000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:37.993000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:37.993000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:38.010000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:38.010000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:38.010000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:38.010000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:38.010000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  3.06it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.98it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.84it/s]W0318 06:50:38.318000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:38.318000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:38.318000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:38.318000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:38.318000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  3.06it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.00it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.07it/s] 50%|█████     | 16/32 [00:06<00:05,  3.01it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.86it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.08it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.01it/s]W0318 06:50:39.169000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:39.169000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:39.169000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:39.169000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:39.169000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:39.169000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:39.169000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:39.186000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:39.186000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:39.186000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:39.187000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:39.187000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:03,  2.87it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.08it/s]W0318 06:50:39.421000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:39.421000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:39.421000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:39.421000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:39.421000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:07<00:04,  3.02it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  3.07it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
 59%|█████▉    | 19/32 [00:07<00:04,  3.02it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.85it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.96it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.79it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.94it/s]W0318 06:50:40.595000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:40.595000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:40.596000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:40.596000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:40.596000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:40.596000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:40.596000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:40.615000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:40.615000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:40.615000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:40.615000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:40.615000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.93it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.80it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.95it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.97it/s]W0318 06:50:41.513000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:41.513000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:41.513000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:41.513000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:41.513000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:11<00:01,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.98it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 91%|█████████ | 29/32 [00:11<00:01,  2.86it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.99it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.02it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.90it/s]W0318 06:50:42.724000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:42.725000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:42.725000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:42.725000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:42.725000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:42.725000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:42.725000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:42.752000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:42.752000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:42.752000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:42.752000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:42.752000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:42.769000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:42.769000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:42.769000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:42.770000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:42.770000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  2.99it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.84it/s]W0318 06:50:43.101000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:43.101000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:43.101000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:43.101000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:43.101000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:10<00:01,  2.95it/s]100%|██████████| 32/32 [00:12<00:00,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
 94%|█████████▍| 30/32 [00:11<00:00,  2.92it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.90it/s]W0318 06:50:44.030000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:44.030000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:44.030000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:44.030000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:44.030000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:44.030000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:44.030000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:44.049000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:44.049000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:44.049000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:44.049000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:44.049000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:11<00:00,  2.88it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
W0318 06:50:44.305000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:44.305000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:44.305000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:44.305000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:44.305000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:45.518000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:45.518000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:45.519000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:45.519000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:45.519000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:45.519000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:45.519000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:45.537000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:45.537000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:45.537000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:45.537000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:45.537000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.478000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.478000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.478000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.478000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.478000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.495000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.495000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.495000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.495000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.495000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.495000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.496000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.523000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.523000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.523000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.523000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.523000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.540000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.540000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.540000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.540000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.540000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.875000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.875000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.875000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.876000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:46.876000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0318 06:50:47.387997 1119643 finetune.py:45] layer 20_v initial loss 0.057311050593853
W0318 06:50:47.388353 1119643 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:50:47.449000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.449000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.449000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.449000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.449000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.450000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.450000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.474000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.475000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.475000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.475000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.475000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.490000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.490000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.491000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.491000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.491000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.795000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.795000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.795000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.795000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.795000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.795000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.795000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.814000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.814000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.814000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.814000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.814000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.821000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.821000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.821000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.821000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:47.821000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:48.070000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:48.070000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:48.071000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:48.071000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:48.071000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:48.448184 1119643 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:50:48.747000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:48.747000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:48.748000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:48.748000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:48.748000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:48.748000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:48.748000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:48.766000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:48.766000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:48.766000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:48.766000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:48.767000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:49.018000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:49.018000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:49.018000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:49.018000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:49.018000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:49.288000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:49.288000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:49.288000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:49.289000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:49.289000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:49.289000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:49.289000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:49.307000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:49.307000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:49.307000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:49.308000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:49.308000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
20_v proxy err 0.013028477318584919 tr(WHW.T) 990.5983276367188
  0%|          | 0/32 [00:00<?, ?it/s]W0318 06:50:50.232000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:50.232000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:50.232000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:50.232000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:50.232000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:50.233000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:50:50.233000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:50:50.251000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:50.251000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:50.251000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:50.251000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:50.251000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:50.253000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:50.253000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:50.253000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:50.253000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:50:50.253000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:19,  1.60it/s]  6%|▋         | 2/32 [00:00<00:13,  2.19it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
  9%|▉         | 3/32 [00:01<00:11,  2.48it/s]W0318 06:50:51.190000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:50:51.190000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:50:51.190000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:50:51.190000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:50:51.190000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:01<00:10,  2.64it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.73it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 19%|█▉        | 6/32 [00:02<00:09,  2.81it/s]I0318 06:50:52.210561 1120111 finetune.py:45] layer 21_v initial loss 0.06878221035003662
W0318 06:50:52.210986 1120111 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:02<00:08,  2.86it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.88it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.90it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.91it/s]W0318 06:50:53.489654 1120111 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:04<00:07,  2.93it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.93it/s] 41%|████      | 13/32 [00:04<00:06,  2.93it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.93it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.93it/s]21_v proxy err 0.01346211601048708 tr(WHW.T) 1144.565673828125
  0%|          | 0/32 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:05<00:05,  2.93it/s]  3%|▎         | 1/32 [00:00<00:19,  1.62it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.93it/s]  6%|▋         | 2/32 [00:00<00:13,  2.22it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.94it/s]I0318 06:50:56.115492 1120356 finetune.py:45] layer 22_v initial loss 0.0656743198633194
W0318 06:50:56.115789 1120356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:01<00:11,  2.52it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.94it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.68it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.94it/s]I0318 06:50:56.874610 1120792 finetune.py:45] layer 23_v initial loss 0.07572709769010544
W0318 06:50:56.874907 1120792 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:01<00:09,  2.80it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.97it/s]W0318 06:50:57.100761 1120356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:02<00:09,  2.89it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.98it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.93it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.04it/s]W0318 06:50:57.712112 1120792 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:02<00:08,  2.95it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.03it/s]22_v proxy err 0.013001613318920135 tr(WHW.T) 1243.2529296875
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.07it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.95it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.10it/s]  3%|▎         | 1/32 [00:00<00:18,  1.65it/s]23_v proxy err 0.013487638905644417 tr(WHW.T) 1486.037353515625
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.91it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.12it/s]  6%|▋         | 2/32 [00:00<00:13,  2.21it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.95it/s]  3%|▎         | 1/32 [00:00<00:18,  1.64it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.11it/s]  9%|▉         | 3/32 [00:01<00:11,  2.47it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.95it/s]  6%|▋         | 2/32 [00:00<00:13,  2.22it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.12it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.63it/s] 41%|████      | 13/32 [00:04<00:06,  2.97it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.13it/s]  9%|▉         | 3/32 [00:01<00:11,  2.50it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.98it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.72it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.11it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.66it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.99it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.77it/s]100%|██████████| 32/32 [00:10<00:00,  3.12it/s]100%|██████████| 32/32 [00:10<00:00,  2.92it/s]
 16%|█▌        | 5/32 [00:01<00:09,  2.76it/s] 50%|█████     | 16/32 [00:05<00:05,  2.97it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.81it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.78it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.98it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.76it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.77it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.94it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.76it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.81it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.94it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.78it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.79it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.96it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.80it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.86it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.00it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.00it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.84it/s] 41%|████      | 13/32 [00:04<00:06,  2.82it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.01it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.82it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.80it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.02it/s] 41%|████      | 13/32 [00:04<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.78it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.00it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.78it/s] 50%|█████     | 16/32 [00:05<00:05,  2.77it/s] 81%|████████▏ | 26/32 [00:08<00:02,  2.99it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.78it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.96it/s] 50%|█████     | 16/32 [00:05<00:05,  2.77it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.75it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.99it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.77it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.01it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.78it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.02it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.77it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.03it/s] 66%|██████▌   | 21/32 [00:07<00:04,  2.71it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.77it/s]100%|██████████| 32/32 [00:10<00:00,  3.04it/s]100%|██████████| 32/32 [00:10<00:00,  2.92it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  2.72it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.77it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.72it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.76it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.72it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.76it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.72it/s]I0318 06:51:07.321608 1119643 finetune.py:45] layer 20_q initial loss 0.057247843593358994
W0318 06:51:07.321918 1119643 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 75%|███████▌  | 24/32 [00:08<00:02,  2.78it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.78it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.84it/s]W0318 06:51:08.179313 1119643 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 88%|████████▊ | 28/32 [00:10<00:01,  2.81it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.85it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.80it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.85it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.78it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.84it/s]20_q proxy err 0.0030935597606003284 tr(WHW.T) 7150.947265625
  0%|          | 0/32 [00:00<?, ?it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.77it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.82it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]100%|██████████| 32/32 [00:11<00:00,  2.73it/s]
  3%|▎         | 1/32 [00:00<00:18,  1.69it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.81it/s]  6%|▋         | 2/32 [00:00<00:13,  2.26it/s]100%|██████████| 32/32 [00:11<00:00,  2.81it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]
  9%|▉         | 3/32 [00:01<00:11,  2.54it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.68it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.75it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.82it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.86it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.88it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.89it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.89it/s]I0318 06:51:13.144113 1120111 finetune.py:45] layer 21_q initial loss 0.06863559782505035
W0318 06:51:13.144548 1120111 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:04<00:07,  2.91it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.92it/s] 41%|████      | 13/32 [00:04<00:06,  2.93it/s]W0318 06:51:14.099516 1120111 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:05<00:06,  2.94it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.96it/s] 50%|█████     | 16/32 [00:05<00:05,  2.96it/s]21_q proxy err 0.0033633713610470295 tr(WHW.T) 7064.314453125
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.95it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.95it/s]  3%|▎         | 1/32 [00:00<00:17,  1.78it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.95it/s]  6%|▋         | 2/32 [00:00<00:12,  2.34it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.95it/s]  9%|▉         | 3/32 [00:01<00:10,  2.64it/s]I0318 06:51:16.549740 1120356 finetune.py:45] layer 22_q initial loss 0.06554410606622696
W0318 06:51:16.549969 1120356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 66%|██████▌   | 21/32 [00:07<00:03,  2.96it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.79it/s]I0318 06:51:16.782264 1120792 finetune.py:45] layer 23_q initial loss 0.07559472322463989
W0318 06:51:16.782487 1120792 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:01<00:09,  2.88it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.98it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.01it/s] 19%|█▉        | 6/32 [00:02<00:08,  2.94it/s]W0318 06:51:17.496984 1120356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:51:17.613368 1120792 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:08<00:02,  3.03it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.95it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.02it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.97it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.08it/s] 28%|██▊       | 9/32 [00:03<00:07,  3.00it/s]22_q proxy err 0.0033222190104424953 tr(WHW.T) 7746.84765625
  0%|          | 0/32 [00:00<?, ?it/s]23_q proxy err 0.003907435107976198 tr(WHW.T) 7346.6103515625
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.12it/s] 31%|███▏      | 10/32 [00:03<00:07,  3.00it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.12it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.98it/s]  3%|▎         | 1/32 [00:00<00:17,  1.75it/s]  3%|▎         | 1/32 [00:00<00:18,  1.71it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.14it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.99it/s]  6%|▋         | 2/32 [00:00<00:12,  2.31it/s]  6%|▋         | 2/32 [00:00<00:13,  2.30it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.15it/s] 41%|████      | 13/32 [00:04<00:06,  2.98it/s]  9%|▉         | 3/32 [00:01<00:11,  2.57it/s]  9%|▉         | 3/32 [00:01<00:11,  2.58it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.16it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.98it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.71it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.73it/s]100%|██████████| 32/32 [00:10<00:00,  3.17it/s]100%|██████████| 32/32 [00:10<00:00,  2.94it/s]
 47%|████▋     | 15/32 [00:05<00:05,  2.99it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.80it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.81it/s] 50%|█████     | 16/32 [00:05<00:05,  2.98it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.80it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.80it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.99it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.80it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.84it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.99it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.83it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.85it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.00it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.85it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.90it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.00it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.88it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.91it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.02it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.85it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.89it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.02it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.82it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.87it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.01it/s] 41%|████      | 13/32 [00:04<00:06,  2.80it/s] 41%|████      | 13/32 [00:04<00:06,  2.86it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.03it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.78it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.04it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.84it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.77it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.05it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.84it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.06it/s] 50%|█████     | 16/32 [00:05<00:05,  2.76it/s] 50%|█████     | 16/32 [00:05<00:05,  2.84it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.05it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.04it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.75it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.84it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.03it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.75it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.83it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.04it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.75it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.83it/s]100%|██████████| 32/32 [00:10<00:00,  3.03it/s]100%|██████████| 32/32 [00:10<00:00,  2.96it/s]
 66%|██████▌   | 21/32 [00:07<00:03,  2.83it/s] 66%|██████▌   | 21/32 [00:07<00:04,  2.75it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.84it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.84it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.75it/s]I0318 06:51:26.922214 1119643 finetune.py:45] layer 20_k initial loss 0.05715665593743324
W0318 06:51:26.922449 1119643 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 75%|███████▌  | 24/32 [00:08<00:02,  2.88it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.80it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.92it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s]W0318 06:51:27.699556 1119643 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 81%|████████▏ | 26/32 [00:09<00:02,  2.94it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.87it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.96it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.89it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.99it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.92it/s]20_k proxy err 0.0022101812064647675 tr(WHW.T) 10386.24609375
  0%|          | 0/32 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.01it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.93it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.02it/s]  3%|▎         | 1/32 [00:00<00:16,  1.86it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.95it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.04it/s]  6%|▋         | 2/32 [00:00<00:12,  2.47it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.97it/s]100%|██████████| 32/32 [00:11<00:00,  3.05it/s]100%|██████████| 32/32 [00:11<00:00,  2.86it/s]
  9%|▉         | 3/32 [00:01<00:10,  2.76it/s]100%|██████████| 32/32 [00:11<00:00,  2.97it/s]100%|██████████| 32/32 [00:11<00:00,  2.80it/s]
 12%|█▎        | 4/32 [00:01<00:09,  2.89it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.93it/s] 19%|█▉        | 6/32 [00:02<00:08,  2.96it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.96it/s] 25%|██▌       | 8/32 [00:02<00:07,  3.02it/s] 28%|██▊       | 9/32 [00:03<00:07,  3.01it/s] 31%|███▏      | 10/32 [00:03<00:07,  3.00it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.99it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.98it/s]I0318 06:51:33.163867 1120111 finetune.py:45] layer 21_k initial loss 0.0685047134757042
W0318 06:51:33.164187 1120111 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:04<00:06,  2.97it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.95it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.95it/s]W0318 06:51:34.054440 1120111 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 50%|█████     | 16/32 [00:05<00:05,  2.94it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.94it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.94it/s]21_k proxy err 0.0024343584664165974 tr(WHW.T) 9976.4658203125
  0%|          | 0/32 [00:00<?, ?it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.94it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.95it/s]  3%|▎         | 1/32 [00:00<00:17,  1.74it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.97it/s]I0318 06:51:35.978961 1120792 finetune.py:45] layer 23_k initial loss 0.07545135915279388
  6%|▋         | 2/32 [00:00<00:12,  2.32it/s]W0318 06:51:35.979350 1120792 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 69%|██████▉   | 22/32 [00:07<00:03,  2.98it/s]  9%|▉         | 3/32 [00:01<00:11,  2.60it/s] 72%|███████▏  | 23/32 [00:07<00:03,  2.98it/s]I0318 06:51:36.579875 1120356 finetune.py:45] layer 22_k initial loss 0.06545871496200562
W0318 06:51:36.580228 1120356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 4/32 [00:01<00:10,  2.76it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.02it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.84it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.06it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.86it/s]W0318 06:51:37.361924 1120792 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:51:37.399269 1120356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 81%|████████▏ | 26/32 [00:08<00:01,  3.08it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.90it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.07it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.91it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.11it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.95it/s]22_k proxy err 0.0024905959144234657 tr(WHW.T) 10603.205078125
  0%|          | 0/32 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.14it/s]23_k proxy err 0.0029216455295681953 tr(WHW.T) 9982.23828125
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.96it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.15it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.98it/s]  3%|▎         | 1/32 [00:00<00:17,  1.75it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.15it/s]  3%|▎         | 1/32 [00:00<00:18,  1.67it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.97it/s]  6%|▋         | 2/32 [00:00<00:13,  2.28it/s]100%|██████████| 32/32 [00:10<00:00,  3.16it/s]100%|██████████| 32/32 [00:10<00:00,  2.98it/s]
  6%|▋         | 2/32 [00:00<00:13,  2.22it/s] 41%|████      | 13/32 [00:04<00:06,  2.96it/s]  9%|▉         | 3/32 [00:01<00:11,  2.52it/s]  9%|▉         | 3/32 [00:01<00:11,  2.48it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.97it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.60it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.65it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.98it/s] 16%|█▌        | 5/32 [00:01<00:10,  2.69it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.75it/s] 50%|█████     | 16/32 [00:05<00:05,  2.98it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.72it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.76it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.96it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.79it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.74it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.99it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.79it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.00it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.77it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.00it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.75it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.03it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.72it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.73it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.03it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.73it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.73it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.02it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.73it/s] 41%|████      | 13/32 [00:04<00:06,  2.72it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.03it/s] 41%|████      | 13/32 [00:04<00:06,  2.74it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.04it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.72it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.02it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.72it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.74it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.03it/s] 50%|█████     | 16/32 [00:05<00:05,  2.72it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.03it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.72it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.03it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.72it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.73it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.04it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.04it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s]100%|██████████| 32/32 [00:10<00:00,  3.00it/s]100%|██████████| 32/32 [00:10<00:00,  2.94it/s]
 62%|██████▎   | 20/32 [00:07<00:04,  2.75it/s]I0318 06:51:46.236649 1119643 finetune.py:45] layer 20_o initial loss 0.05708237737417221
W0318 06:51:46.237008 1119643 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 66%|██████▌   | 21/32 [00:07<00:04,  2.73it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.78it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.82it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.81it/s]W0318 06:51:47.052314 1119643 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:08<00:03,  2.86it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.80it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.84it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.86it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.82it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.90it/s]20_o proxy err 0.008744646795094013 tr(WHW.T) 100.31707000732422
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.85it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.95it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.97it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.90it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.99it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.90it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.00it/s]  3%|▎         | 1/32 [00:01<00:36,  1.18s/it] 97%|█████████▋| 31/32 [00:11<00:00,  2.92it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.99it/s]100%|██████████| 32/32 [00:11<00:00,  2.92it/s]100%|██████████| 32/32 [00:11<00:00,  2.75it/s]
100%|██████████| 32/32 [00:11<00:00,  3.02it/s]100%|██████████| 32/32 [00:11<00:00,  2.78it/s]
  6%|▋         | 2/32 [00:02<00:30,  1.03s/it]  9%|▉         | 3/32 [00:03<00:29,  1.01s/it] 12%|█▎        | 4/32 [00:04<00:27,  1.01it/s]I0318 06:51:53.195786 1120111 finetune.py:45] layer 21_o initial loss 0.06759805977344513
W0318 06:51:53.196063 1120111 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:05<00:26,  1.01it/s]W0318 06:51:54.066184 1120111 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:06<00:25,  1.00it/s]21_o proxy err 0.011134957894682884 tr(WHW.T) 75.50972747802734
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:07<00:25,  1.00s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.00s/it]  3%|▎         | 1/32 [00:01<00:39,  1.27s/it]I0318 06:51:56.597629 1120356 finetune.py:45] layer 22_o initial loss 0.06444490700960159
W0318 06:51:56.597859 1120356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0318 06:51:57.129339 1120792 finetune.py:45] layer 23_o initial loss 0.07522325217723846
W0318 06:51:57.129681 1120792 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:09<00:22,  1.00it/s]W0318 06:51:57.426602 1120356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:02<00:33,  1.10s/it]W0318 06:51:57.938897 1120792 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:10<00:21,  1.01it/s]  9%|▉         | 3/32 [00:03<00:30,  1.05s/it]22_o proxy err 0.008861611597239971 tr(WHW.T) 114.30064392089844
  0%|          | 0/32 [00:00<?, ?it/s]23_o proxy err 0.012387135066092014 tr(WHW.T) 85.13458251953125
  0%|          | 0/32 [00:00<?, ?it/s] 34%|███▍      | 11/32 [00:10<00:20,  1.03it/s] 12%|█▎        | 4/32 [00:04<00:28,  1.03s/it]  3%|▎         | 1/32 [00:01<00:38,  1.24s/it] 38%|███▊      | 12/32 [00:11<00:19,  1.04it/s]  3%|▎         | 1/32 [00:01<00:38,  1.25s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.02s/it]  6%|▋         | 2/32 [00:02<00:33,  1.10s/it] 41%|████      | 13/32 [00:12<00:18,  1.05it/s]  6%|▋         | 2/32 [00:02<00:33,  1.11s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.01s/it]  9%|▉         | 3/32 [00:03<00:30,  1.05s/it] 44%|████▍     | 14/32 [00:13<00:17,  1.06it/s]  9%|▉         | 3/32 [00:03<00:30,  1.06s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.01s/it] 12%|█▎        | 4/32 [00:04<00:28,  1.03s/it] 47%|████▋     | 15/32 [00:14<00:16,  1.06it/s] 12%|█▎        | 4/32 [00:04<00:29,  1.04s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.00s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.03s/it] 50%|█████     | 16/32 [00:15<00:15,  1.06it/s] 16%|█▌        | 5/32 [00:05<00:27,  1.02s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.01s/it] 53%|█████▎    | 17/32 [00:16<00:14,  1.07it/s] 19%|█▉        | 6/32 [00:06<00:26,  1.02s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.01s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.00s/it] 56%|█████▋    | 18/32 [00:17<00:13,  1.07it/s] 22%|██▏       | 7/32 [00:07<00:25,  1.01s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.01s/it] 34%|███▍      | 11/32 [00:11<00:20,  1.00it/s] 59%|█████▉    | 19/32 [00:18<00:12,  1.07it/s] 25%|██▌       | 8/32 [00:08<00:24,  1.01s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.00s/it] 38%|███▊      | 12/32 [00:12<00:19,  1.00it/s] 62%|██████▎   | 20/32 [00:19<00:11,  1.07it/s] 28%|██▊       | 9/32 [00:09<00:23,  1.01s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.00s/it] 41%|████      | 13/32 [00:13<00:19,  1.00s/it] 66%|██████▌   | 21/32 [00:20<00:10,  1.07it/s] 31%|███▏      | 10/32 [00:10<00:22,  1.01s/it] 31%|███▏      | 10/32 [00:10<00:21,  1.00it/s] 69%|██████▉   | 22/32 [00:21<00:09,  1.07it/s] 44%|████▍     | 14/32 [00:14<00:18,  1.00s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.00s/it] 34%|███▍      | 11/32 [00:11<00:20,  1.00it/s] 72%|███████▏  | 23/32 [00:22<00:08,  1.07it/s] 47%|████▋     | 15/32 [00:15<00:17,  1.00s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.01s/it] 38%|███▊      | 12/32 [00:12<00:19,  1.01it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.08it/s] 50%|█████     | 16/32 [00:16<00:16,  1.01s/it] 41%|████      | 13/32 [00:13<00:19,  1.00s/it] 78%|███████▊  | 25/32 [00:24<00:06,  1.07it/s] 41%|████      | 13/32 [00:13<00:18,  1.00it/s] 53%|█████▎    | 17/32 [00:17<00:15,  1.00s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.00s/it] 81%|████████▏ | 26/32 [00:24<00:05,  1.08it/s] 44%|████▍     | 14/32 [00:14<00:17,  1.00it/s] 56%|█████▋    | 18/32 [00:18<00:14,  1.00s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.00s/it] 84%|████████▍ | 27/32 [00:25<00:04,  1.08it/s] 47%|████▋     | 15/32 [00:15<00:16,  1.00it/s] 59%|█████▉    | 19/32 [00:19<00:12,  1.00it/s] 50%|█████     | 16/32 [00:16<00:16,  1.00s/it] 88%|████████▊ | 28/32 [00:26<00:03,  1.08it/s] 50%|█████     | 16/32 [00:16<00:15,  1.01it/s] 62%|██████▎   | 20/32 [00:20<00:12,  1.00s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.00s/it] 91%|█████████ | 29/32 [00:27<00:02,  1.08it/s] 53%|█████▎    | 17/32 [00:17<00:14,  1.01it/s] 66%|██████▌   | 21/32 [00:21<00:11,  1.00s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.00s/it] 94%|█████████▍| 30/32 [00:28<00:01,  1.08it/s] 56%|█████▋    | 18/32 [00:18<00:13,  1.01it/s] 69%|██████▉   | 22/32 [00:22<00:10,  1.00s/it] 97%|█████████▋| 31/32 [00:29<00:00,  1.08it/s] 59%|█████▉    | 19/32 [00:19<00:13,  1.00s/it] 59%|█████▉    | 19/32 [00:19<00:12,  1.01it/s] 72%|███████▏  | 23/32 [00:23<00:09,  1.00s/it]100%|██████████| 32/32 [00:30<00:00,  1.08it/s]100%|██████████| 32/32 [00:30<00:00,  1.05it/s]
 62%|██████▎   | 20/32 [00:20<00:12,  1.00s/it] 62%|██████▎   | 20/32 [00:20<00:11,  1.00it/s] 75%|███████▌  | 24/32 [00:24<00:07,  1.00it/s] 66%|██████▌   | 21/32 [00:21<00:11,  1.02s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.01s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.00s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.02s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.01s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.00s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.02s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.01s/it] 84%|████████▍ | 27/32 [00:27<00:05,  1.00s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.01s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.00s/it] 88%|████████▊ | 28/32 [00:28<00:04,  1.00s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.01s/it] 78%|███████▊  | 25/32 [00:25<00:06,  1.00it/s] 91%|█████████ | 29/32 [00:29<00:03,  1.00s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.01s/it]W0318 06:52:25.174000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.174000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.175000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.175000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.175000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.175000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.175000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.202000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.203000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.203000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.203000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.203000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.217000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.218000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.218000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.218000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.218000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:26<00:05,  1.01it/s]W0318 06:52:25.366000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.366000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.367000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.367000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.367000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.581000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.581000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.581000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.581000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.581000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.581000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.581000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.602000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.602000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.602000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.602000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.602000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:30<00:02,  1.01s/it]W0318 06:52:25.667000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.668000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.668000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.668000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:25.668000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:27<00:05,  1.01s/it] 84%|████████▍ | 27/32 [00:27<00:04,  1.01it/s]W0318 06:52:26.494000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:31<00:01,  1.01s/it]W0318 06:52:26.787000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:26.787000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:26.787000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:26.787000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:26.788000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:52:26.788000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:26.788000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:26.808000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:26.808000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:26.808000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:26.808000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:26.808000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:27.057000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:27.057000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:27.057000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:27.057000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:27.057000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:28<00:04,  1.01s/it] 88%|████████▊ | 28/32 [00:28<00:03,  1.01it/s]W0318 06:52:27.305000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:32<00:00,  1.01s/it]100%|██████████| 32/32 [00:32<00:00,  1.01s/it]
 91%|█████████ | 29/32 [00:29<00:03,  1.01s/it] 91%|█████████ | 29/32 [00:29<00:02,  1.01it/s] 94%|█████████▍| 30/32 [00:30<00:02,  1.01s/it] 94%|█████████▍| 30/32 [00:30<00:02,  1.00s/it] 97%|█████████▋| 31/32 [00:31<00:01,  1.03s/it] 97%|█████████▋| 31/32 [00:31<00:01,  1.02s/it]100%|██████████| 32/32 [00:32<00:00,  1.05s/it]100%|██████████| 32/32 [00:32<00:00,  1.02s/it]
100%|██████████| 32/32 [00:32<00:00,  1.03s/it]100%|██████████| 32/32 [00:32<00:00,  1.01s/it]
I0318 06:52:33.898068 1119643 finetune.py:45] layer 20_up initial loss 0.056044045835733414
W0318 06:52:33.898264 1119643 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:52:34.176000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.176000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.176000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.177000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.177000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.177000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.177000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.205000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.206000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.206000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.206000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.206000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.221000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.221000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.221000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.221000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.221000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.375000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.375000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.375000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.376000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.376000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.600000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.600000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.600000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.600000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.600000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.600000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.600000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.621000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.621000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.621000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.621000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.621000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.654805 1119643 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:52:34.684000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.684000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.684000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.684000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:34.684000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:35.518000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:35.815000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:52:35.815000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:35.815000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:35.815000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:35.815000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:35.815000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:35.815000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:35.835000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:35.835000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:35.836000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:35.836000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:35.836000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
20_up proxy err 0.010672645643353462 tr(WHW.T) 2340.89404296875
  0%|          | 0/32 [00:00<?, ?it/s]W0318 06:52:36.085000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:36.085000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:36.085000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:36.085000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:36.085000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:36.336000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:37,  1.22s/it]W0318 06:52:37.846000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.846000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.846000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.846000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.846000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.846000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.846000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.874000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.874000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.874000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.874000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.874000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.889000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.889000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.889000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.889000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.889000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.927000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.928000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.928000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.928000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.928000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.928000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.928000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.958000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.958000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.958000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.958000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.958000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.974000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.975000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.975000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.975000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:37.975000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.040000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.040000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.040000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.040000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.040000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:02<00:31,  1.05s/it]W0318 06:52:38.139000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.139000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.139000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.139000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.139000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.254000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.254000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.254000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.254000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.254000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.254000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.254000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.275000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.275000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.275000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.275000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.275000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.336000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.337000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.337000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.337000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.337000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.375000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.375000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.376000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.376000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.376000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.376000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.376000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.400000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.400000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.400000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.400000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.400000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.468000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.468000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.468000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.468000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:38.468000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:03<00:28,  1.01it/s]W0318 06:52:39.160000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.376000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.455000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.455000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.455000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.455000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.455000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.455000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.456000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.475000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.475000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.475000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.475000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.476000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.699000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.699000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.700000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.700000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.700000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.700000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.700000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.718000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.718000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.719000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.719000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.719000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.722000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.722000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.722000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.722000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.722000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:03<00:26,  1.04it/s]W0318 06:52:39.967000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.984000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.984000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.984000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.984000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:52:39.984000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:52:40.264000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:04<00:25,  1.05it/s] 19%|█▉        | 6/32 [00:05<00:24,  1.05it/s] 22%|██▏       | 7/32 [00:06<00:24,  1.04it/s]I0318 06:52:43.396640 1120111 finetune.py:45] layer 21_up initial loss 0.06682601571083069
W0318 06:52:43.396895 1120111 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:07<00:23,  1.02it/s]W0318 06:52:44.232507 1120111 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:08<00:22,  1.02it/s]21_up proxy err 0.01140011940151453 tr(WHW.T) 2361.64990234375
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:09<00:21,  1.01it/s]I0318 06:52:46.069962 1120356 finetune.py:45] layer 22_up initial loss 0.06380833685398102
W0318 06:52:46.070135 1120356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0318 06:52:46.662283 1120792 finetune.py:45] layer 23_up initial loss 0.07484917342662811
W0318 06:52:46.662672 1120792 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:10<00:20,  1.02it/s]  3%|▎         | 1/32 [00:01<00:40,  1.30s/it]W0318 06:52:46.853117 1120356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:52:47.545452 1120792 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 38%|███▊      | 12/32 [00:11<00:19,  1.02it/s]  6%|▋         | 2/32 [00:02<00:33,  1.13s/it]22_up proxy err 0.011551644653081894 tr(WHW.T) 2474.510009765625
  0%|          | 0/32 [00:00<?, ?it/s] 41%|████      | 13/32 [00:12<00:18,  1.03it/s]23_up proxy err 0.012259538285434246 tr(WHW.T) 2533.509765625
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:03<00:31,  1.07s/it]  3%|▎         | 1/32 [00:01<00:39,  1.29s/it] 44%|████▍     | 14/32 [00:13<00:17,  1.04it/s] 12%|█▎        | 4/32 [00:04<00:29,  1.05s/it]  3%|▎         | 1/32 [00:01<00:41,  1.33s/it]  6%|▋         | 2/32 [00:02<00:33,  1.13s/it] 47%|████▋     | 15/32 [00:14<00:16,  1.05it/s] 16%|█▌        | 5/32 [00:05<00:28,  1.04s/it]  6%|▋         | 2/32 [00:02<00:35,  1.17s/it]  9%|▉         | 3/32 [00:03<00:31,  1.07s/it] 50%|█████     | 16/32 [00:15<00:15,  1.06it/s] 19%|█▉        | 6/32 [00:06<00:26,  1.03s/it]  9%|▉         | 3/32 [00:03<00:32,  1.11s/it] 53%|█████▎    | 17/32 [00:16<00:14,  1.06it/s] 12%|█▎        | 4/32 [00:04<00:29,  1.05s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.03s/it] 12%|█▎        | 4/32 [00:04<00:30,  1.08s/it] 56%|█████▋    | 18/32 [00:17<00:13,  1.07it/s] 16%|█▌        | 5/32 [00:05<00:28,  1.04s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.02s/it] 59%|█████▉    | 19/32 [00:18<00:12,  1.07it/s] 16%|█▌        | 5/32 [00:05<00:28,  1.07s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.03s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.02s/it] 62%|██████▎   | 20/32 [00:19<00:11,  1.07it/s] 19%|█▉        | 6/32 [00:06<00:27,  1.05s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.03s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.02s/it] 66%|██████▌   | 21/32 [00:20<00:10,  1.07it/s] 22%|██▏       | 7/32 [00:07<00:26,  1.05s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.02s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.02s/it] 69%|██████▉   | 22/32 [00:21<00:09,  1.07it/s] 25%|██▌       | 8/32 [00:08<00:24,  1.04s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.02s/it] 72%|███████▏  | 23/32 [00:22<00:08,  1.07it/s] 38%|███▊      | 12/32 [00:12<00:20,  1.01s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.04s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.02s/it] 75%|███████▌  | 24/32 [00:22<00:07,  1.07it/s] 41%|████      | 13/32 [00:13<00:19,  1.01s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.03s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.02s/it] 78%|███████▊  | 25/32 [00:23<00:06,  1.07it/s] 44%|████▍     | 14/32 [00:14<00:18,  1.01s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.03s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.02s/it] 81%|████████▏ | 26/32 [00:24<00:05,  1.07it/s] 47%|████▋     | 15/32 [00:15<00:17,  1.01s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.03s/it] 41%|████      | 13/32 [00:13<00:19,  1.02s/it] 84%|████████▍ | 27/32 [00:25<00:04,  1.07it/s] 50%|█████     | 16/32 [00:16<00:16,  1.01s/it] 41%|████      | 13/32 [00:13<00:19,  1.03s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.02s/it] 88%|████████▊ | 28/32 [00:26<00:03,  1.07it/s] 53%|█████▎    | 17/32 [00:17<00:15,  1.01s/it] 91%|█████████ | 29/32 [00:27<00:02,  1.07it/s] 44%|████▍     | 14/32 [00:14<00:18,  1.03s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.02s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.01s/it] 94%|█████████▍| 30/32 [00:28<00:01,  1.07it/s] 47%|████▋     | 15/32 [00:15<00:17,  1.03s/it] 50%|█████     | 16/32 [00:16<00:16,  1.02s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.01s/it] 97%|█████████▋| 31/32 [00:29<00:00,  1.07it/s] 53%|█████▎    | 17/32 [00:17<00:15,  1.02s/it] 50%|█████     | 16/32 [00:16<00:16,  1.03s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.02s/it]100%|██████████| 32/32 [00:30<00:00,  1.07it/s]100%|██████████| 32/32 [00:30<00:00,  1.05it/s]
 56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.03s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.01s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.03s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.03s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.01s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.03s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.04s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.00s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.02s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.05s/it] 75%|███████▌  | 24/32 [00:24<00:07,  1.00it/s] 66%|██████▌   | 21/32 [00:21<00:11,  1.02s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.06s/it] 78%|███████▊  | 25/32 [00:25<00:06,  1.01it/s] 69%|██████▉   | 22/32 [00:22<00:10,  1.02s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.06s/it] 81%|████████▏ | 26/32 [00:26<00:05,  1.01it/s] 72%|███████▏  | 23/32 [00:23<00:09,  1.02s/it] 84%|████████▍ | 27/32 [00:27<00:04,  1.01it/s] 75%|███████▌  | 24/32 [00:24<00:08,  1.07s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.02s/it] 88%|████████▊ | 28/32 [00:28<00:03,  1.02it/s]I0318 06:53:14.105832 1119643 finetune.py:45] layer 20_gate initial loss 0.055193863809108734
W0318 06:53:14.106034 1119643 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 78%|███████▊  | 25/32 [00:26<00:07,  1.07s/it]W0318 06:53:14.773430 1119643 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 78%|███████▊  | 25/32 [00:26<00:07,  1.02s/it] 91%|█████████ | 29/32 [00:29<00:02,  1.02it/s] 81%|████████▏ | 26/32 [00:27<00:06,  1.05s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.03s/it] 94%|█████████▍| 30/32 [00:30<00:01,  1.01it/s] 84%|████████▍ | 27/32 [00:28<00:05,  1.06s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.03s/it] 97%|█████████▋| 31/32 [00:31<00:00,  1.02it/s] 88%|████████▊ | 28/32 [00:29<00:04,  1.06s/it]20_gate proxy err 0.007100169081240892 tr(WHW.T) 4024.626953125
  0%|          | 0/86 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:29<00:04,  1.03s/it]100%|██████████| 32/32 [00:32<00:00,  1.01it/s]100%|██████████| 32/32 [00:32<00:00,  1.01s/it]
 91%|█████████ | 29/32 [00:30<00:03,  1.05s/it]  1%|          | 1/86 [00:00<00:47,  1.78it/s]  2%|▏         | 2/86 [00:00<00:35,  2.39it/s]  3%|▎         | 3/86 [00:01<00:30,  2.69it/s] 91%|█████████ | 29/32 [00:30<00:03,  1.04s/it]  5%|▍         | 4/86 [00:01<00:28,  2.86it/s] 94%|█████████▍| 30/32 [00:31<00:02,  1.04s/it]  6%|▌         | 5/86 [00:01<00:27,  2.96it/s]  7%|▋         | 6/86 [00:02<00:26,  3.04it/s] 94%|█████████▍| 30/32 [00:31<00:02,  1.05s/it]  8%|▊         | 7/86 [00:02<00:25,  3.08it/s] 97%|█████████▋| 31/32 [00:32<00:01,  1.03s/it]  9%|▉         | 8/86 [00:02<00:25,  3.11it/s] 10%|█         | 9/86 [00:03<00:24,  3.14it/s] 97%|█████████▋| 31/32 [00:32<00:01,  1.05s/it] 12%|█▏        | 10/86 [00:03<00:24,  3.15it/s]100%|██████████| 32/32 [00:33<00:00,  1.02s/it]100%|██████████| 32/32 [00:33<00:00,  1.04s/it]
 13%|█▎        | 11/86 [00:03<00:23,  3.17it/s] 14%|█▍        | 12/86 [00:04<00:23,  3.13it/s] 15%|█▌        | 13/86 [00:04<00:23,  3.08it/s]100%|██████████| 32/32 [00:33<00:00,  1.07s/it]100%|██████████| 32/32 [00:33<00:00,  1.04s/it]
 16%|█▋        | 14/86 [00:04<00:23,  3.07it/s] 17%|█▋        | 15/86 [00:05<00:22,  3.10it/s] 19%|█▊        | 16/86 [00:05<00:22,  3.07it/s] 20%|█▉        | 17/86 [00:05<00:22,  3.09it/s] 21%|██        | 18/86 [00:05<00:22,  3.08it/s] 22%|██▏       | 19/86 [00:06<00:21,  3.05it/s] 23%|██▎       | 20/86 [00:06<00:21,  3.02it/s] 24%|██▍       | 21/86 [00:07<00:21,  2.99it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.98it/s] 27%|██▋       | 23/86 [00:07<00:21,  2.97it/s]I0318 06:53:25.817912 1120111 finetune.py:45] layer 21_gate initial loss 0.0659063383936882
W0318 06:53:25.818147 1120111 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 24/86 [00:08<00:20,  2.95it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.95it/s] 30%|███       | 26/86 [00:08<00:20,  2.95it/s]W0318 06:53:26.574447 1120111 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 27/86 [00:09<00:20,  2.95it/s] 33%|███▎      | 28/86 [00:09<00:19,  2.93it/s] 34%|███▎      | 29/86 [00:09<00:19,  2.94it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.94it/s] 36%|███▌      | 31/86 [00:10<00:18,  2.93it/s]I0318 06:53:28.553092 1120356 finetune.py:45] layer 22_gate initial loss 0.0628369152545929
W0318 06:53:28.553298 1120356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 37%|███▋      | 32/86 [00:10<00:18,  2.95it/s] 38%|███▊      | 33/86 [00:11<00:17,  3.00it/s] 40%|███▉      | 34/86 [00:11<00:17,  3.05it/s]W0318 06:53:29.289458 1120356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 35/86 [00:11<00:16,  3.05it/s]21_gate proxy err 0.007822742685675621 tr(WHW.T) 4004.375732421875
  0%|          | 0/86 [00:00<?, ?it/s] 42%|████▏     | 36/86 [00:12<00:16,  3.05it/s]I0318 06:53:30.173009 1120792 finetune.py:45] layer 23_gate initial loss 0.07375264167785645
W0318 06:53:30.173210 1120792 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 43%|████▎     | 37/86 [00:12<00:16,  3.03it/s]  1%|          | 1/86 [00:00<00:51,  1.65it/s] 44%|████▍     | 38/86 [00:12<00:15,  3.04it/s]  2%|▏         | 2/86 [00:00<00:37,  2.22it/s]W0318 06:53:30.864569 1120792 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 45%|████▌     | 39/86 [00:13<00:15,  3.02it/s]  3%|▎         | 3/86 [00:01<00:33,  2.47it/s] 47%|████▋     | 40/86 [00:13<00:15,  2.99it/s]  5%|▍         | 4/86 [00:01<00:31,  2.61it/s] 48%|████▊     | 41/86 [00:13<00:14,  3.00it/s]  6%|▌         | 5/86 [00:02<00:30,  2.68it/s] 49%|████▉     | 42/86 [00:14<00:14,  3.02it/s]22_gate proxy err 0.008103465661406517 tr(WHW.T) 4156.6396484375
  0%|          | 0/86 [00:00<?, ?it/s]  7%|▋         | 6/86 [00:02<00:28,  2.77it/s] 50%|█████     | 43/86 [00:14<00:14,  3.06it/s]  8%|▊         | 7/86 [00:02<00:28,  2.81it/s] 51%|█████     | 44/86 [00:14<00:13,  3.09it/s]  1%|          | 1/86 [00:00<00:49,  1.72it/s]  9%|▉         | 8/86 [00:03<00:27,  2.82it/s] 52%|█████▏    | 45/86 [00:14<00:13,  3.10it/s]  2%|▏         | 2/86 [00:00<00:37,  2.27it/s] 10%|█         | 9/86 [00:03<00:26,  2.85it/s] 53%|█████▎    | 46/86 [00:15<00:12,  3.12it/s]  3%|▎         | 3/86 [00:01<00:32,  2.52it/s] 55%|█████▍    | 47/86 [00:15<00:12,  3.12it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.85it/s]  5%|▍         | 4/86 [00:01<00:30,  2.66it/s] 56%|█████▌    | 48/86 [00:15<00:12,  3.14it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.86it/s]23_gate proxy err 0.008837341330945492 tr(WHW.T) 4097.5185546875
  0%|          | 0/86 [00:00<?, ?it/s]  6%|▌         | 5/86 [00:01<00:29,  2.75it/s] 57%|█████▋    | 49/86 [00:16<00:11,  3.15it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.87it/s]  7%|▋         | 6/86 [00:02<00:28,  2.79it/s] 58%|█████▊    | 50/86 [00:16<00:11,  3.13it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.86it/s]  1%|          | 1/86 [00:00<00:51,  1.64it/s]  8%|▊         | 7/86 [00:02<00:28,  2.82it/s] 59%|█████▉    | 51/86 [00:16<00:11,  3.13it/s] 16%|█▋        | 14/86 [00:05<00:25,  2.83it/s]  2%|▏         | 2/86 [00:00<00:38,  2.17it/s]  9%|▉         | 8/86 [00:02<00:27,  2.85it/s] 60%|██████    | 52/86 [00:17<00:10,  3.14it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.86it/s]  3%|▎         | 3/86 [00:01<00:33,  2.49it/s] 10%|█         | 9/86 [00:03<00:26,  2.86it/s] 62%|██████▏   | 53/86 [00:17<00:10,  3.14it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.88it/s]  5%|▍         | 4/86 [00:01<00:30,  2.66it/s] 63%|██████▎   | 54/86 [00:17<00:10,  3.14it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.87it/s]  6%|▌         | 5/86 [00:01<00:29,  2.77it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.87it/s] 64%|██████▍   | 55/86 [00:18<00:09,  3.14it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.88it/s]  7%|▋         | 6/86 [00:02<00:28,  2.84it/s] 21%|██        | 18/86 [00:06<00:23,  2.90it/s] 65%|██████▌   | 56/86 [00:18<00:09,  3.16it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.90it/s]  8%|▊         | 7/86 [00:02<00:27,  2.87it/s] 22%|██▏       | 19/86 [00:06<00:22,  2.91it/s] 66%|██████▋   | 57/86 [00:18<00:09,  3.16it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.90it/s]  9%|▉         | 8/86 [00:02<00:27,  2.88it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.91it/s] 67%|██████▋   | 58/86 [00:19<00:08,  3.15it/s] 16%|█▋        | 14/86 [00:05<00:24,  2.89it/s] 10%|█         | 9/86 [00:03<00:26,  2.92it/s] 69%|██████▊   | 59/86 [00:19<00:08,  3.15it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.91it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.90it/s] 70%|██████▉   | 60/86 [00:19<00:08,  3.16it/s] 12%|█▏        | 10/86 [00:03<00:25,  2.94it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.91it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.91it/s] 71%|███████   | 61/86 [00:20<00:07,  3.16it/s] 13%|█▎        | 11/86 [00:03<00:25,  2.94it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.88it/s] 20%|█▉        | 17/86 [00:06<00:23,  2.90it/s] 72%|███████▏  | 62/86 [00:20<00:07,  3.15it/s] 14%|█▍        | 12/86 [00:04<00:24,  2.96it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.90it/s] 21%|██        | 18/86 [00:06<00:23,  2.90it/s] 73%|███████▎  | 63/86 [00:20<00:07,  3.16it/s] 15%|█▌        | 13/86 [00:04<00:24,  2.98it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.93it/s] 22%|██▏       | 19/86 [00:06<00:23,  2.91it/s] 74%|███████▍  | 64/86 [00:21<00:06,  3.16it/s] 16%|█▋        | 14/86 [00:04<00:24,  3.00it/s] 30%|███       | 26/86 [00:09<00:20,  2.94it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.90it/s] 76%|███████▌  | 65/86 [00:21<00:06,  3.16it/s] 17%|█▋        | 15/86 [00:05<00:23,  2.98it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.95it/s] 77%|███████▋  | 66/86 [00:21<00:06,  3.15it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.90it/s] 19%|█▊        | 16/86 [00:05<00:23,  2.97it/s] 33%|███▎      | 28/86 [00:09<00:19,  2.94it/s] 78%|███████▊  | 67/86 [00:21<00:06,  3.15it/s] 26%|██▌       | 22/86 [00:07<00:22,  2.90it/s] 20%|█▉        | 17/86 [00:06<00:23,  2.96it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.93it/s] 79%|███████▉  | 68/86 [00:22<00:05,  3.15it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.89it/s] 21%|██        | 18/86 [00:06<00:22,  2.96it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.93it/s] 80%|████████  | 69/86 [00:22<00:05,  3.15it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.90it/s] 22%|██▏       | 19/86 [00:06<00:22,  2.98it/s] 36%|███▌      | 31/86 [00:10<00:18,  2.93it/s] 81%|████████▏ | 70/86 [00:22<00:05,  3.16it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.91it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.99it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.90it/s] 83%|████████▎ | 71/86 [00:23<00:04,  3.16it/s] 30%|███       | 26/86 [00:09<00:20,  2.91it/s] 24%|██▍       | 21/86 [00:07<00:21,  3.00it/s] 84%|████████▎ | 72/86 [00:23<00:04,  3.16it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.91it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.89it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.94it/s] 85%|████████▍ | 73/86 [00:23<00:04,  3.16it/s] 40%|███▉      | 34/86 [00:11<00:17,  2.91it/s] 33%|███▎      | 28/86 [00:09<00:19,  2.90it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.92it/s] 86%|████████▌ | 74/86 [00:24<00:03,  3.15it/s] 41%|████      | 35/86 [00:12<00:17,  2.91it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.90it/s] 87%|████████▋ | 75/86 [00:24<00:03,  3.15it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.89it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.92it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.90it/s] 88%|████████▊ | 76/86 [00:24<00:03,  3.16it/s] 29%|██▉       | 25/86 [00:08<00:21,  2.87it/s] 43%|████▎     | 37/86 [00:13<00:16,  2.94it/s] 36%|███▌      | 31/86 [00:10<00:18,  2.90it/s] 90%|████████▉ | 77/86 [00:25<00:02,  3.16it/s] 30%|███       | 26/86 [00:09<00:21,  2.85it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.94it/s] 91%|█████████ | 78/86 [00:25<00:02,  3.16it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.90it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.85it/s] 45%|████▌     | 39/86 [00:13<00:15,  2.95it/s] 92%|█████████▏| 79/86 [00:25<00:02,  3.16it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.91it/s] 33%|███▎      | 28/86 [00:09<00:20,  2.85it/s] 47%|████▋     | 40/86 [00:14<00:15,  2.96it/s] 93%|█████████▎| 80/86 [00:26<00:01,  3.16it/s] 40%|███▉      | 34/86 [00:11<00:17,  2.90it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.95it/s] 34%|███▎      | 29/86 [00:10<00:20,  2.82it/s] 94%|█████████▍| 81/86 [00:26<00:01,  3.16it/s] 41%|████      | 35/86 [00:12<00:17,  2.90it/s] 49%|████▉     | 42/86 [00:14<00:14,  2.93it/s] 35%|███▍      | 30/86 [00:10<00:20,  2.77it/s] 95%|█████████▌| 82/86 [00:26<00:01,  3.16it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.91it/s] 50%|█████     | 43/86 [00:15<00:14,  2.95it/s] 36%|███▌      | 31/86 [00:10<00:19,  2.79it/s] 97%|█████████▋| 83/86 [00:27<00:00,  3.16it/s] 43%|████▎     | 37/86 [00:12<00:16,  2.90it/s] 51%|█████     | 44/86 [00:15<00:14,  2.95it/s] 98%|█████████▊| 84/86 [00:27<00:00,  3.16it/s] 37%|███▋      | 32/86 [00:11<00:19,  2.80it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.90it/s] 52%|█████▏    | 45/86 [00:15<00:13,  2.96it/s] 99%|█████████▉| 85/86 [00:27<00:00,  3.17it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.80it/s] 45%|████▌     | 39/86 [00:13<00:16,  2.91it/s] 53%|█████▎    | 46/86 [00:16<00:13,  2.97it/s]100%|██████████| 86/86 [00:27<00:00,  3.13it/s]100%|██████████| 86/86 [00:27<00:00,  3.07it/s]
 40%|███▉      | 34/86 [00:11<00:18,  2.80it/s] 47%|████▋     | 40/86 [00:14<00:15,  2.90it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.97it/s] 41%|████      | 35/86 [00:12<00:18,  2.81it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.85it/s] 56%|█████▌    | 48/86 [00:16<00:12,  2.97it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.81it/s] 49%|████▉     | 42/86 [00:14<00:15,  2.81it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.94it/s] 43%|████▎     | 37/86 [00:13<00:17,  2.81it/s] 50%|█████     | 43/86 [00:15<00:15,  2.80it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.95it/s] 44%|████▍     | 38/86 [00:13<00:17,  2.82it/s] 51%|█████     | 44/86 [00:15<00:14,  2.81it/s] 59%|█████▉    | 51/86 [00:17<00:11,  2.94it/s] 45%|████▌     | 39/86 [00:13<00:16,  2.80it/s] 60%|██████    | 52/86 [00:18<00:11,  2.94it/s] 52%|█████▏    | 45/86 [00:15<00:14,  2.81it/s] 47%|████▋     | 40/86 [00:14<00:16,  2.81it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.95it/s] 53%|█████▎    | 46/86 [00:16<00:14,  2.84it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.82it/s] 63%|██████▎   | 54/86 [00:18<00:10,  2.96it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.85it/s] 49%|████▉     | 42/86 [00:14<00:15,  2.82it/s] 64%|██████▍   | 55/86 [00:19<00:10,  2.97it/s] 56%|█████▌    | 48/86 [00:16<00:13,  2.87it/s] 50%|█████     | 43/86 [00:15<00:15,  2.82it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.97it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.89it/s] 51%|█████     | 44/86 [00:15<00:14,  2.83it/s] 66%|██████▋   | 57/86 [00:19<00:09,  2.97it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.89it/s] 52%|█████▏    | 45/86 [00:15<00:14,  2.81it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.96it/s] 59%|█████▉    | 51/86 [00:17<00:12,  2.89it/s] 53%|█████▎    | 46/86 [00:16<00:14,  2.83it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.98it/s] 60%|██████    | 52/86 [00:18<00:11,  2.90it/s] 70%|██████▉   | 60/86 [00:20<00:08,  2.97it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.81it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.90it/s] 71%|███████   | 61/86 [00:21<00:08,  2.96it/s] 56%|█████▌    | 48/86 [00:16<00:13,  2.80it/s] 63%|██████▎   | 54/86 [00:18<00:11,  2.90it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.97it/s] 57%|█████▋    | 49/86 [00:17<00:13,  2.80it/s] 64%|██████▍   | 55/86 [00:19<00:10,  2.91it/s] 73%|███████▎  | 63/86 [00:21<00:07,  2.99it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.81it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.91it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.94it/s] 66%|██████▋   | 57/86 [00:19<00:10,  2.90it/s] 59%|█████▉    | 51/86 [00:18<00:12,  2.77it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.91it/s]W0318 06:53:52.291000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.291000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.292000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.292000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.292000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.292000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.292000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 67%|██████▋   | 58/86 [00:20<00:09,  2.90it/s]W0318 06:53:52.334000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.334000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.334000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.334000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.334000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.350000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.350000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.350000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.350000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.350000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 60%|██████    | 52/86 [00:18<00:12,  2.70it/s]W0318 06:53:52.517000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.517000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.517000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.517000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.517000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 77%|███████▋  | 66/86 [00:22<00:06,  2.90it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.89it/s] 62%|██████▏   | 53/86 [00:18<00:12,  2.71it/s]W0318 06:53:52.829000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.829000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.830000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.830000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.830000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.830000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.830000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.864000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.864000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.864000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.864000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.864000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 67/86 [00:23<00:06,  2.91it/s]W0318 06:53:52.936000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.936000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.936000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.936000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:53:52.937000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 70%|██████▉   | 60/86 [00:21<00:09,  2.86it/s] 63%|██████▎   | 54/86 [00:19<00:11,  2.73it/s] 79%|███████▉  | 68/86 [00:23<00:06,  2.92it/s] 71%|███████   | 61/86 [00:21<00:08,  2.88it/s] 64%|██████▍   | 55/86 [00:19<00:11,  2.75it/s] 80%|████████  | 69/86 [00:23<00:05,  2.93it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.90it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.74it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.92it/s] 73%|███████▎  | 63/86 [00:22<00:07,  2.91it/s]W0318 06:53:54.062000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.074000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.081000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.081000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 66%|██████▋   | 57/86 [00:20<00:10,  2.75it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.88it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.92it/s]W0318 06:53:54.497000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.497000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.497000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.498000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.498000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.498000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.498000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.528000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.528000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.528000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.528000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.528000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 67%|██████▋   | 58/86 [00:20<00:10,  2.74it/s] 84%|████████▎ | 72/86 [00:24<00:04,  2.86it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.91it/s]W0318 06:53:54.853000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.853000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.854000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.854000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.854000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.854000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.854000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:53:54.854000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 69%|██████▊   | 59/86 [00:20<00:09,  2.75it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.87it/s] 77%|███████▋  | 66/86 [00:23<00:06,  2.92it/s]W0318 06:53:55.137000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:53:55.137000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:53:55.137000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:53:55.137000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:53:55.138000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 70%|██████▉   | 60/86 [00:21<00:09,  2.76it/s] 86%|████████▌ | 74/86 [00:25<00:04,  2.88it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.92it/s]W0318 06:53:55.455000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:53:55.460000 140353755842368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 71%|███████   | 61/86 [00:21<00:09,  2.78it/s] 87%|████████▋ | 75/86 [00:25<00:03,  2.86it/s] 79%|███████▉  | 68/86 [00:23<00:06,  2.90it/s] 72%|███████▏  | 62/86 [00:22<00:08,  2.76it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.84it/s] 80%|████████  | 69/86 [00:24<00:05,  2.90it/s] 73%|███████▎  | 63/86 [00:22<00:08,  2.77it/s] 90%|████████▉ | 77/86 [00:26<00:03,  2.85it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.91it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.77it/s] 91%|█████████ | 78/86 [00:27<00:02,  2.85it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.92it/s] 76%|███████▌  | 65/86 [00:23<00:07,  2.76it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.86it/s] 84%|████████▎ | 72/86 [00:25<00:04,  2.90it/s] 77%|███████▋  | 66/86 [00:23<00:07,  2.78it/s] 93%|█████████▎| 80/86 [00:27<00:02,  2.90it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.85it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.79it/s] 94%|█████████▍| 81/86 [00:28<00:01,  2.90it/s] 86%|████████▌ | 74/86 [00:25<00:04,  2.81it/s] 79%|███████▉  | 68/86 [00:24<00:06,  2.81it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.89it/s] 87%|████████▋ | 75/86 [00:26<00:03,  2.79it/s] 97%|█████████▋| 83/86 [00:28<00:01,  2.91it/s] 80%|████████  | 69/86 [00:24<00:06,  2.83it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.77it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.91it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.81it/s] 90%|████████▉ | 77/86 [00:26<00:03,  2.76it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.90it/s] 83%|████████▎ | 71/86 [00:25<00:05,  2.80it/s] 91%|█████████ | 78/86 [00:27<00:02,  2.75it/s]100%|██████████| 86/86 [00:29<00:00,  2.90it/s]100%|██████████| 86/86 [00:29<00:00,  2.89it/s]
 84%|████████▎ | 72/86 [00:25<00:05,  2.79it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.74it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.77it/s] 93%|█████████▎| 80/86 [00:28<00:02,  2.74it/s] 86%|████████▌ | 74/86 [00:26<00:04,  2.75it/s] 94%|█████████▍| 81/86 [00:28<00:01,  2.74it/s] 87%|████████▋ | 75/86 [00:26<00:04,  2.74it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.73it/s] 88%|████████▊ | 76/86 [00:27<00:03,  2.73it/s] 97%|█████████▋| 83/86 [00:29<00:01,  2.72it/s] 90%|████████▉ | 77/86 [00:27<00:03,  2.73it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.72it/s] 91%|█████████ | 78/86 [00:27<00:02,  2.75it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.72it/s] 92%|█████████▏| 79/86 [00:28<00:02,  2.76it/s]100%|██████████| 86/86 [00:30<00:00,  2.72it/s]100%|██████████| 86/86 [00:30<00:00,  2.84it/s]
 93%|█████████▎| 80/86 [00:28<00:02,  2.77it/s]I0318 06:54:02.493807 1119643 finetune.py:45] layer 20_down initial loss 0.05460710451006889
W0318 06:54:02.494142 1119643 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 94%|█████████▍| 81/86 [00:28<00:01,  2.78it/s]W0318 06:54:02.975570 1119643 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 95%|█████████▌| 82/86 [00:29<00:01,  2.77it/s]20_down proxy err 0.013211242854595184 tr(WHW.T) 274.88140869140625
 97%|█████████▋| 83/86 [00:29<00:01,  2.78it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.79it/s] 99%|█████████▉| 85/86 [00:30<00:00,  2.78it/s]100%|██████████| 86/86 [00:30<00:00,  2.79it/s]100%|██████████| 86/86 [00:30<00:00,  2.80it/s]
W0318 06:54:06.350000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.350000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.351000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.351000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.351000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.351000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.351000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.397000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.397000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.397000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.398000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.398000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.414000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.414000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.414000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.414000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.414000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.588000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.588000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.588000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.588000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.588000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.926000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.926000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.927000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.927000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.927000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.927000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.927000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.961000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.961000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.961000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.961000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:06.961000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:07.033000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:07.033000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:07.033000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:07.033000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:07.033000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.232000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.244000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.252000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.252000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.686000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.686000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.686000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.686000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.686000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.687000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.687000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.725000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.725000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.725000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.725000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.725000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.802000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.802000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.802000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.802000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.803000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.803000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.803000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.844000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.844000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.844000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.844000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.844000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.859000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.859000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.859000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.859000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:08.859000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.019000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.019000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.019000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.019000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.020000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.061000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.061000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.061000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.061000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.061000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.061000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.062000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.062000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.326000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.326000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.326000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.326000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.326000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.326000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.326000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.353000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.353000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.353000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.354000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.354000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.359000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.359000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.359000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.359000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.359000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.426000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.426000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.426000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.426000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.426000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.686000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:09.691000 140191148816192 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:10.560000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:10.571000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:10.578000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:10.578000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.000000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.000000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.000000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.000000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.000000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.000000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.000000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.036000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.036000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.036000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.036000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.036000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.363000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.363000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.363000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.364000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.364000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.364000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.364000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.364000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.544000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.544000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.544000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.544000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.544000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.545000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.545000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.589000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.590000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.590000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.590000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.590000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.606000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.606000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.607000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.607000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.607000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.649000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.650000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.650000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.650000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.650000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.784000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.784000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.784000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.784000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.784000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.975000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:11.980000 140510241277760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:12.120000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:12.120000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:12.120000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:12.120000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:12.120000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:12.120000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:12.121000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:12.156000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:12.156000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:12.156000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:12.157000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:12.157000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:12.230000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:12.230000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:12.230000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:12.230000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:12.230000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:13.467000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:13.481000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:13.490000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:13.490000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:13.951000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:13.952000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:13.952000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:13.952000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:13.952000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:13.952000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:13.952000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:13.985000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:13.986000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:13.986000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:13.986000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:13.986000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:14.344000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:14.344000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:14.344000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:14.344000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:14.344000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:14.344000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:14.344000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:14.344000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:14.649000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:14.650000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:14.650000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:14.650000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:14.650000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:14.999000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:15.004000 139975442421568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0318 06:54:16.555384 1120111 finetune.py:45] layer 21_down initial loss 0.0650709941983223
W0318 06:54:16.555787 1120111 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:54:17.152121 1120111 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

21_down proxy err 0.013828694820404053 tr(WHW.T) 276.585693359375
I0318 06:54:18.662324 1120356 finetune.py:45] layer 22_down initial loss 0.062122493982315063
W0318 06:54:18.662586 1120356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:54:19.119248 1120356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

22_down proxy err 0.013951030559837818 tr(WHW.T) 311.8799133300781
I0318 06:54:20.732881 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 24 in 1.1744277477264404s
I0318 06:54:21.077755 1092738 quantize_finetune_llama.py:159] layer 25 gpu 1
I0318 06:54:21.881804 1120792 finetune.py:45] layer 23_down initial loss 0.0727861076593399
W0318 06:54:21.882114 1120792 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:54:22.359162 1120792 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

I0318 06:54:22.638681 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 25 in 1.1681287288665771s
23_down proxy err 0.014435875229537487 tr(WHW.T) 321.3388671875
I0318 06:54:23.015120 1092738 quantize_finetune_llama.py:159] layer 26 gpu 2
I0318 06:54:23.030135 1124705 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:54:23.030241 1124705 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:54:23.030309 1124705 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:54:23.242189 1124705 config.py:58] PyTorch version 2.4.0 available.
I0318 06:54:25.058237 1124865 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:54:25.058346 1124865 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:54:25.058408 1124865 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:54:25.249904 1124865 config.py:58] PyTorch version 2.4.0 available.
I0318 06:54:25.538837 1124705 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0318 06:54:25.868403 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 26 in 1.2734251022338867s
W0318 06:54:26.117731 1124705 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:54:26.310983 1092738 quantize_finetune_llama.py:159] layer 27 gpu 3
  0%|          | 0/32 [00:00<?, ?it/s]I0318 06:54:27.559586 1124865 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0318 06:54:27.846985 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 27 in 1.1614322662353516s
W0318 06:54:27.967964 1124865 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:54:28.210628 1092738 quantize_finetune_llama.py:159] layer 28 gpu 0
I0318 06:54:28.289934 1125391 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:54:28.290051 1125391 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:54:28.290138 1125391 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:54:28.486285 1125391 config.py:58] PyTorch version 2.4.0 available.
  3%|▎         | 1/32 [00:01<00:44,  1.42s/it]  6%|▋         | 2/32 [00:01<00:23,  1.26it/s]  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:02<00:18,  1.59it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.92it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.17it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.35it/s]I0318 06:54:30.659912 1125833 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:54:30.660030 1125833 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:54:30.660092 1125833 utils.py:162] NumExpr defaulting to 16 threads.
 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s]  3%|▎         | 1/32 [00:01<00:51,  1.66s/it]I0318 06:54:30.845871 1125833 config.py:58] PyTorch version 2.4.0 available.
I0318 06:54:30.987489 1125391 data_utils.py:336] using 256 training seqs, 128 validation seqs
 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s]  6%|▋         | 2/32 [00:02<00:26,  1.12it/s]W0318 06:54:31.348559 1125391 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:04<00:08,  2.67it/s]  9%|▉         | 3/32 [00:02<00:18,  1.55it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.89it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.78it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.16it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.85it/s]  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.38it/s] 41%|████      | 13/32 [00:05<00:06,  2.83it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s]I0318 06:54:33.019711 1125833 data_utils.py:336] using 256 training seqs, 128 validation seqs
 44%|████▍     | 14/32 [00:06<00:06,  2.82it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.59it/s]W0318 06:54:33.399024 1125833 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 15/32 [00:06<00:06,  2.83it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.64it/s] 50%|█████     | 16/32 [00:06<00:05,  2.85it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s]  3%|▎         | 1/32 [00:01<00:50,  1.64s/it] 53%|█████▎    | 17/32 [00:07<00:05,  2.83it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.71it/s]  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:02<00:26,  1.13it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.84it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.75it/s]  9%|▉         | 3/32 [00:02<00:18,  1.55it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 41%|████      | 13/32 [00:05<00:06,  2.78it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.90it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.89it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.83it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.18it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.94it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.87it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.39it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.95it/s]  3%|▎         | 1/32 [00:01<00:47,  1.53s/it] 50%|█████     | 16/32 [00:06<00:05,  2.87it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.91it/s]  6%|▋         | 2/32 [00:01<00:25,  1.19it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.88it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.63it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.93it/s]  9%|▉         | 3/32 [00:02<00:17,  1.64it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.92it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.73it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.95it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.99it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.92it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.75it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.91it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.20it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.86it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.74it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.88it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.35it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.82it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.76it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.80it/s] 41%|████      | 13/32 [00:05<00:06,  2.77it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.87it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.54it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.79it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.76it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.79it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.78it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.87it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.82it/s] 50%|█████     | 16/32 [00:06<00:05,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.92it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
 34%|███▍      | 11/32 [00:05<00:07,  2.78it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.88it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.85it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.80it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.87it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.82it/s] 41%|████      | 13/32 [00:05<00:06,  2.78it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.85it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.82it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.80it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.85it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.83it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.87it/s] 50%|█████     | 16/32 [00:06<00:05,  2.89it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.91it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.91it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.93it/s]100%|██████████| 32/32 [00:12<00:00,  2.93it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
 72%|███████▏  | 23/32 [00:09<00:03,  2.93it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.95it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.89it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.91it/s]W0318 06:54:42.527000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.527000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.527000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.527000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.528000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.528000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.528000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:10<00:02,  2.89it/s]W0318 06:54:42.553000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.553000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.553000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.553000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.554000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.570000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.570000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.570000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.571000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.571000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:08<00:04,  2.93it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.86it/s]W0318 06:54:42.904000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.905000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.905000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.905000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:42.905000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:03,  2.87it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.82it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.81it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.81it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.79it/s]W0318 06:54:43.831000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:43.831000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:43.831000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:43.832000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:43.832000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:43.832000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:43.832000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:43.850000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:43.850000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:43.850000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:43.850000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:43.850000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.81it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.78it/s]W0318 06:54:44.100000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.100000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.100000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.100000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.101000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.79it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.80it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.79it/s]W0318 06:54:44.874000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.874000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.875000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.875000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.875000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.875000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.875000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.902000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.902000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.902000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.902000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.902000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.919000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.919000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.920000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.920000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:44.920000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
 84%|████████▍ | 27/32 [00:10<00:01,  2.80it/s]W0318 06:54:45.259000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:45.260000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:45.260000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:45.260000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:45.260000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:45.323000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:45.323000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:45.324000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:45.324000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:45.324000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:45.324000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:45.324000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:45.342000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:45.342000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:45.342000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:45.342000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:45.342000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:11<00:01,  2.80it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.79it/s]W0318 06:54:46.190000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.190000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.190000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.191000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.191000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.191000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.191000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.209000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.209000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.209000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.209000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.210000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s]W0318 06:54:46.282000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.282000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.282000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.282000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.282000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.460000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.460000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.461000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.461000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:46.461000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:12<00:00,  2.81it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
100%|██████████| 32/32 [00:12<00:00,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
W0318 06:54:47.693000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:47.693000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:47.694000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:47.694000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:47.694000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:47.694000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:47.694000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:47.712000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:47.713000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:47.713000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:47.713000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:47.713000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.522000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.523000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.523000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.523000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.523000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.523000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.523000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.550000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.550000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.550000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.550000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.550000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.567000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.567000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.567000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.567000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.567000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.669000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.669000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.669000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.669000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.669000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.899000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.900000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.900000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.900000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:48.900000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0318 06:54:49.814000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:49.814000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:49.814000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:49.814000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:49.814000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:49.815000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:49.815000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:49.833000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:49.833000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:49.833000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:49.833000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:49.833000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.084000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.084000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.084000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.084000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.084000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.227000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.228000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.228000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.228000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.228000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.228000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.228000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.256000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.256000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.256000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.256000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.256000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.273000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.273000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.273000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.273000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.273000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.613000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.613000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.613000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.613000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:50.613000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.294000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.295000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.295000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.295000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.295000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.295000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.295000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.313000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.314000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.314000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.314000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.314000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.541000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.541000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.541000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.541000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.541000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.542000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.542000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.560000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.560000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.560000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.560000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.560000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.810000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.811000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.811000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.811000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:51.811000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:52.254000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:52.254000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:52.254000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:52.254000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:52.255000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
I0318 06:54:52.589694 1124705 finetune.py:45] layer 24_v initial loss 0.08617693930864334
W0318 06:54:52.590263 1124705 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0318 06:54:53.040000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:53.040000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:53.041000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:53.041000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:54:53.041000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:54:53.041000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:53.041000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:53.059000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:53.059000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:53.059000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:53.059000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:53.060000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:53.998000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:54:53.998000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:54:53.998000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:54:53.998000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:54:53.998000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:54:54.506010 1124705 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0318 06:54:54.946014 1124865 finetune.py:45] layer 25_v initial loss 0.10284207761287689
W0318 06:54:54.946439 1124865 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:54:55.968138 1124865 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_v proxy err 0.01378687284886837 tr(WHW.T) 1394.900634765625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.55it/s]  6%|▋         | 2/32 [00:00<00:14,  2.12it/s]25_v proxy err 0.01419574674218893 tr(WHW.T) 1707.664794921875
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:01<00:12,  2.38it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.52it/s]  3%|▎         | 1/32 [00:00<00:20,  1.52it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.58it/s]  6%|▋         | 2/32 [00:01<00:14,  2.09it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.63it/s]  9%|▉         | 3/32 [00:01<00:12,  2.37it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.69it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.51it/s]I0318 06:54:59.239298 1125391 finetune.py:45] layer 26_v initial loss 0.12043077498674393
W0318 06:54:59.240646 1125391 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.60it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.78it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.66it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.81it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.71it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.82it/s]W0318 06:55:00.679843 1125391 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:03<00:08,  2.79it/s]I0318 06:55:00.780362 1125833 finetune.py:45] layer 27_v initial loss 0.33041974902153015
W0318 06:55:00.780877 1125833 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:04<00:06,  2.83it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.82it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.81it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s]26_v proxy err 0.014354145154356956 tr(WHW.T) 1668.884521484375
  0%|          | 0/32 [00:00<?, ?it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.83it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.83it/s] 50%|█████     | 16/32 [00:05<00:05,  2.83it/s] 41%|████      | 13/32 [00:04<00:06,  2.83it/s]  3%|▎         | 1/32 [00:00<00:21,  1.47it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.82it/s]W0318 06:55:02.741668 1125833 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:01<00:14,  2.03it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.83it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.81it/s]  9%|▉         | 3/32 [00:01<00:12,  2.30it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.83it/s] 50%|█████     | 16/32 [00:05<00:05,  2.79it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.83it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.44it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.81it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.82it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.52it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.81it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.82it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.60it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.80it/s]27_v proxy err 0.013844257220625877 tr(WHW.T) 1799.3350830078125
  0%|          | 0/32 [00:00<?, ?it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.82it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.65it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.80it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.82it/s]  3%|▎         | 1/32 [00:00<00:19,  1.56it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.80it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s]  6%|▋         | 2/32 [00:01<00:14,  2.08it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.70it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.80it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.78it/s]  9%|▉         | 3/32 [00:01<00:12,  2.33it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.69it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.80it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.79it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.45it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.80it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.54it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.68it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.79it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.61it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.80it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.79it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.65it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.80it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.70it/s]100%|██████████| 32/32 [00:11<00:00,  2.80it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
 91%|█████████ | 29/32 [00:10<00:01,  2.79it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.71it/s] 50%|█████     | 16/32 [00:06<00:05,  2.70it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.79it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.73it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.71it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.80it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.74it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.71it/s]100%|██████████| 32/32 [00:11<00:00,  2.81it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
 38%|███▊      | 12/32 [00:04<00:07,  2.74it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s] 41%|████      | 13/32 [00:05<00:06,  2.76it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.70it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.76it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 50%|█████     | 16/32 [00:06<00:05,  2.77it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.74it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.76it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.77it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.77it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.78it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.75it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.74it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.71it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.79it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.73it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
 81%|████████▏ | 26/32 [00:09<00:02,  2.80it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.79it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.79it/s]I0318 06:55:15.701220 1124705 finetune.py:45] layer 24_q initial loss 0.08602727949619293
W0318 06:55:15.701661 1124705 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]100%|██████████| 32/32 [00:11<00:00,  2.70it/s]
I0318 06:55:16.421007 1124865 finetune.py:45] layer 25_q initial loss 0.10271276533603668
W0318 06:55:16.421482 1124865 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:55:17.041197 1124705 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:55:17.443198 1124865 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_q proxy err 0.003778090700507164 tr(WHW.T) 7020.447265625
  0%|          | 0/32 [00:00<?, ?it/s]25_q proxy err 0.004408922977745533 tr(WHW.T) 7162.16357421875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.65it/s]  6%|▋         | 2/32 [00:00<00:13,  2.19it/s]  3%|▎         | 1/32 [00:00<00:18,  1.64it/s]  9%|▉         | 3/32 [00:01<00:11,  2.45it/s]  6%|▋         | 2/32 [00:00<00:13,  2.18it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.59it/s]  9%|▉         | 3/32 [00:01<00:11,  2.44it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.67it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.59it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.71it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.67it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.75it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.72it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.76it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.76it/s]I0318 06:55:21.396950 1125391 finetune.py:45] layer 26_q initial loss 0.12023277580738068
W0318 06:55:21.397618 1125391 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:03<00:08,  2.80it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.79it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.80it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.81it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.81it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.83it/s]W0318 06:55:22.627977 1125391 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 38%|███▊      | 12/32 [00:04<00:07,  2.83it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.80it/s] 41%|████      | 13/32 [00:04<00:06,  2.83it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.82it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.84it/s] 41%|████      | 13/32 [00:04<00:06,  2.82it/s]26_q proxy err 0.00392124243080616 tr(WHW.T) 7469.9833984375
  0%|          | 0/32 [00:00<?, ?it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.84it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.82it/s]I0318 06:55:24.043286 1125833 finetune.py:45] layer 27_q initial loss 0.32518061995506287
W0318 06:55:24.043718 1125833 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 50%|█████     | 16/32 [00:05<00:05,  2.84it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.82it/s]  3%|▎         | 1/32 [00:00<00:19,  1.59it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.85it/s] 50%|█████     | 16/32 [00:05<00:05,  2.82it/s]  6%|▋         | 2/32 [00:00<00:14,  2.11it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.85it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.81it/s]  9%|▉         | 3/32 [00:01<00:12,  2.34it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.83it/s]W0318 06:55:25.182430 1125833 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 56%|█████▋    | 18/32 [00:06<00:04,  2.80it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.47it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.83it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.80it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.56it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.81it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.80it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.60it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.81it/s]27_q proxy err 0.004240805748850107 tr(WHW.T) 7691.708984375
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.80it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.61it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.81it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.80it/s]  3%|▎         | 1/32 [00:00<00:19,  1.62it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.63it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.81it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s]  6%|▋         | 2/32 [00:00<00:14,  2.14it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.66it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.80it/s]  9%|▉         | 3/32 [00:01<00:12,  2.38it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.68it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.80it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.79it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.51it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.70it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.80it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.78it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.59it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.80it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.70it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.79it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.63it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.79it/s] 41%|████      | 13/32 [00:05<00:07,  2.71it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.67it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.79it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.72it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.79it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.80it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.72it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.76it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.70it/s]100%|██████████| 32/32 [00:11<00:00,  2.80it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]
 50%|█████     | 16/32 [00:06<00:05,  2.72it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.77it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.72it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.72it/s]100%|██████████| 32/32 [00:11<00:00,  2.78it/s]100%|██████████| 32/32 [00:11<00:00,  2.75it/s]
 34%|███▍      | 11/32 [00:04<00:07,  2.73it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.72it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.74it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s] 41%|████      | 13/32 [00:04<00:06,  2.73it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.71it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.73it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.77it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.73it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.75it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.79it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.78it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.79it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.78it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.79it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.78it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.80it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.79it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.77it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.79it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.77it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
 81%|████████▏ | 26/32 [00:09<00:02,  2.80it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.80it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.80it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.80it/s]I0318 06:55:37.118748 1124865 finetune.py:45] layer 25_k initial loss 0.10239871591329575
W0318 06:55:37.119062 1124865 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.81it/s]I0318 06:55:37.838873 1124705 finetune.py:45] layer 24_k initial loss 0.08578914403915405
W0318 06:55:37.839414 1124705 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:55:38.020009 1124865 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:11<00:00,  2.80it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
W0318 06:55:39.530374 1124705 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

25_k proxy err 0.0033079860731959343 tr(WHW.T) 9611.58984375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.64it/s]  6%|▋         | 2/32 [00:00<00:13,  2.18it/s]  9%|▉         | 3/32 [00:01<00:11,  2.42it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.55it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.65it/s]24_k proxy err 0.0026003820821642876 tr(WHW.T) 10323.43359375
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.72it/s]  3%|▎         | 1/32 [00:00<00:19,  1.62it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.77it/s]  6%|▋         | 2/32 [00:00<00:14,  2.14it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.81it/s]I0318 06:55:43.210887 1125391 finetune.py:45] layer 26_k initial loss 0.1200091615319252
W0318 06:55:43.211363 1125391 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:01<00:12,  2.38it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.82it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.53it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.83it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.84it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.69it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.86it/s]W0318 06:55:44.488928 1125391 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:02<00:09,  2.74it/s] 41%|████      | 13/32 [00:04<00:06,  2.85it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.76it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.86it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.75it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.85it/s]I0318 06:55:45.466962 1125833 finetune.py:45] layer 27_k initial loss 0.3154791593551636
W0318 06:55:45.467298 1125833 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

26_k proxy err 0.0028389019425958395 tr(WHW.T) 10487.8740234375
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.76it/s] 50%|█████     | 16/32 [00:05<00:05,  2.82it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.79it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s]  3%|▎         | 1/32 [00:00<00:19,  1.62it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.79it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.83it/s]  6%|▋         | 2/32 [00:00<00:14,  2.10it/s] 41%|████      | 13/32 [00:04<00:06,  2.80it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.83it/s]W0318 06:55:46.826147 1125833 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:01<00:12,  2.33it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.80it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.48it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.84it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.79it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.57it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.84it/s] 50%|█████     | 16/32 [00:05<00:05,  2.76it/s]27_k proxy err 0.0031325947493314743 tr(WHW.T) 10618.70703125
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.63it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.84it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.65it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.83it/s]  3%|▎         | 1/32 [00:00<00:19,  1.62it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.77it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.79it/s]  6%|▋         | 2/32 [00:00<00:13,  2.15it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.69it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.79it/s]  9%|▉         | 3/32 [00:01<00:12,  2.37it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.79it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.70it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.79it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.51it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.77it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.71it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.59it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.72it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.80it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.64it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.75it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.74it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.67it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.77it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.73it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.74it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.79it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.74it/s]100%|██████████| 32/32 [00:11<00:00,  2.77it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]
 81%|████████▏ | 26/32 [00:09<00:02,  2.79it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.71it/s] 50%|█████     | 16/32 [00:06<00:05,  2.74it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.71it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.73it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.74it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.72it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.74it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.73it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.76it/s] 41%|████      | 13/32 [00:04<00:06,  2.74it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.75it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.76it/s]100%|██████████| 32/32 [00:11<00:00,  2.77it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
 47%|████▋     | 15/32 [00:05<00:06,  2.75it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.77it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.77it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.78it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.77it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.79it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.78it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.77it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.77it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.78it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.79it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.68it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.79it/s]100%|██████████| 32/32 [00:11<00:00,  2.70it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
 81%|████████▏ | 26/32 [00:09<00:02,  2.79it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.80it/s]I0318 06:55:58.409857 1124865 finetune.py:45] layer 25_o initial loss 0.10219880938529968
W0318 06:55:58.410216 1124865 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:10<00:01,  2.81it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s]W0318 06:55:59.337536 1124865 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s]100%|██████████| 32/32 [00:11<00:00,  2.82it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
25_o proxy err 0.012105915695428848 tr(WHW.T) 83.535888671875
  0%|          | 0/32 [00:00<?, ?it/s]I0318 06:56:01.594438 1124705 finetune.py:45] layer 24_o initial loss 0.08444183319807053
W0318 06:56:01.594913 1124705 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:40,  1.32s/it]W0318 06:56:02.675174 1124705 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:02<00:34,  1.15s/it]  9%|▉         | 3/32 [00:03<00:31,  1.09s/it]24_o proxy err 0.00885667372494936 tr(WHW.T) 133.9879913330078
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:04<00:29,  1.07s/it]I0318 06:56:05.691635 1125391 finetune.py:45] layer 26_o initial loss 0.12150765210390091
W0318 06:56:05.693060 1125391 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:41,  1.35s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.06s/it]  6%|▋         | 2/32 [00:02<00:34,  1.17s/it]I0318 06:56:06.969216 1125833 finetune.py:45] layer 27_o initial loss 0.31666767597198486
W0318 06:56:06.969548 1125833 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:06<00:27,  1.05s/it]W0318 06:56:07.523729 1125391 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:56:07.868141 1125833 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:03<00:32,  1.13s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.05s/it] 12%|█▎        | 4/32 [00:04<00:30,  1.10s/it]27_o proxy err 0.010524359531700611 tr(WHW.T) 126.13690185546875
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:08<00:25,  1.04s/it]26_o proxy err 0.007251922506839037 tr(WHW.T) 202.8817138671875
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:05<00:29,  1.09s/it] 28%|██▊       | 9/32 [00:09<00:24,  1.05s/it]  3%|▎         | 1/32 [00:01<00:43,  1.39s/it]  3%|▎         | 1/32 [00:01<00:42,  1.36s/it] 19%|█▉        | 6/32 [00:06<00:28,  1.08s/it] 31%|███▏      | 10/32 [00:10<00:23,  1.05s/it]  6%|▋         | 2/32 [00:02<00:36,  1.22s/it]  6%|▋         | 2/32 [00:02<00:35,  1.19s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.08s/it] 34%|███▍      | 11/32 [00:11<00:22,  1.06s/it]  9%|▉         | 3/32 [00:03<00:33,  1.16s/it]  9%|▉         | 3/32 [00:03<00:33,  1.15s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.08s/it] 38%|███▊      | 12/32 [00:12<00:21,  1.06s/it] 12%|█▎        | 4/32 [00:04<00:31,  1.13s/it] 12%|█▎        | 4/32 [00:04<00:31,  1.12s/it] 28%|██▊       | 9/32 [00:09<00:24,  1.07s/it] 41%|████      | 13/32 [00:13<00:20,  1.06s/it] 16%|█▌        | 5/32 [00:05<00:30,  1.12s/it] 16%|█▌        | 5/32 [00:05<00:29,  1.10s/it] 31%|███▏      | 10/32 [00:10<00:23,  1.07s/it] 44%|████▍     | 14/32 [00:14<00:19,  1.06s/it] 19%|█▉        | 6/32 [00:06<00:28,  1.11s/it] 19%|█▉        | 6/32 [00:06<00:28,  1.08s/it] 34%|███▍      | 11/32 [00:12<00:22,  1.07s/it] 47%|████▋     | 15/32 [00:15<00:18,  1.06s/it] 22%|██▏       | 7/32 [00:07<00:27,  1.11s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.07s/it] 38%|███▊      | 12/32 [00:13<00:21,  1.08s/it] 50%|█████     | 16/32 [00:17<00:16,  1.06s/it] 25%|██▌       | 8/32 [00:09<00:26,  1.10s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.07s/it] 41%|████      | 13/32 [00:14<00:20,  1.08s/it] 53%|█████▎    | 17/32 [00:18<00:15,  1.06s/it] 28%|██▊       | 9/32 [00:10<00:25,  1.10s/it] 28%|██▊       | 9/32 [00:09<00:24,  1.08s/it] 44%|████▍     | 14/32 [00:15<00:19,  1.07s/it] 56%|█████▋    | 18/32 [00:19<00:14,  1.06s/it] 31%|███▏      | 10/32 [00:11<00:24,  1.10s/it] 31%|███▏      | 10/32 [00:10<00:23,  1.07s/it] 47%|████▋     | 15/32 [00:16<00:18,  1.08s/it] 59%|█████▉    | 19/32 [00:20<00:13,  1.06s/it] 34%|███▍      | 11/32 [00:12<00:23,  1.10s/it] 34%|███▍      | 11/32 [00:12<00:22,  1.07s/it] 50%|█████     | 16/32 [00:17<00:17,  1.07s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.06s/it] 38%|███▊      | 12/32 [00:13<00:21,  1.10s/it] 38%|███▊      | 12/32 [00:13<00:21,  1.07s/it] 53%|█████▎    | 17/32 [00:18<00:16,  1.08s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.05s/it] 41%|████      | 13/32 [00:14<00:20,  1.07s/it] 41%|████      | 13/32 [00:14<00:20,  1.10s/it] 56%|█████▋    | 18/32 [00:19<00:15,  1.07s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.05s/it] 44%|████▍     | 14/32 [00:15<00:19,  1.07s/it] 44%|████▍     | 14/32 [00:15<00:19,  1.10s/it] 59%|█████▉    | 19/32 [00:20<00:13,  1.07s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.05s/it] 47%|████▋     | 15/32 [00:16<00:18,  1.06s/it] 47%|████▋     | 15/32 [00:16<00:18,  1.09s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.05s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.08s/it] 50%|█████     | 16/32 [00:17<00:17,  1.07s/it] 50%|█████     | 16/32 [00:17<00:17,  1.09s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.05s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.08s/it] 53%|█████▎    | 17/32 [00:18<00:16,  1.07s/it] 53%|█████▎    | 17/32 [00:18<00:16,  1.09s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.05s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.08s/it] 56%|█████▋    | 18/32 [00:19<00:15,  1.08s/it] 56%|█████▋    | 18/32 [00:19<00:15,  1.08s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.05s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.08s/it] 59%|█████▉    | 19/32 [00:20<00:14,  1.09s/it] 59%|█████▉    | 19/32 [00:21<00:14,  1.09s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.06s/it] 75%|███████▌  | 24/32 [00:26<00:08,  1.07s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.07s/it] 62%|██████▎   | 20/32 [00:22<00:12,  1.08s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.04s/it] 78%|███████▊  | 25/32 [00:27<00:07,  1.05s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.04s/it] 66%|██████▌   | 21/32 [00:23<00:11,  1.05s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.02s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.02s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.03s/it] 69%|██████▉   | 22/32 [00:24<00:10,  1.04s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.01s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.01s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.03s/it] 72%|███████▏  | 23/32 [00:25<00:09,  1.04s/it]100%|██████████| 32/32 [00:33<00:00,  1.01s/it]100%|██████████| 32/32 [00:33<00:00,  1.05s/it]
 88%|████████▊ | 28/32 [00:29<00:04,  1.01s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.03s/it] 75%|███████▌  | 24/32 [00:26<00:08,  1.02s/it] 91%|█████████ | 29/32 [00:30<00:02,  1.00it/s] 78%|███████▊  | 25/32 [00:27<00:07,  1.01s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.03s/it] 94%|█████████▍| 30/32 [00:31<00:01,  1.01it/s] 81%|████████▏ | 26/32 [00:28<00:06,  1.03s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.04s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.01s/it] 84%|████████▍ | 27/32 [00:29<00:05,  1.04s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.06s/it]100%|██████████| 32/32 [00:34<00:00,  1.03s/it]100%|██████████| 32/32 [00:34<00:00,  1.06s/it]
 88%|████████▊ | 28/32 [00:30<00:04,  1.05s/it] 88%|████████▊ | 28/32 [00:30<00:04,  1.07s/it] 91%|█████████ | 29/32 [00:31<00:03,  1.06s/it] 91%|█████████ | 29/32 [00:31<00:03,  1.07s/it]W0318 06:56:41.046000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.046000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.046000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.046000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.047000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.047000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.047000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.077000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.077000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.077000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.077000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.077000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.093000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.093000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.093000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.094000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.094000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.254000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.254000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.254000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.254000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.254000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.488000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.488000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.488000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.488000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.488000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.488000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.489000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.511000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.511000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.511000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.511000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.511000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.577000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.578000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.578000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.578000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:41.578000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:32<00:02,  1.06s/it] 94%|█████████▍| 30/32 [00:32<00:02,  1.07s/it]W0318 06:56:42.476000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:33<00:01,  1.07s/it] 97%|█████████▋| 31/32 [00:33<00:01,  1.07s/it]W0318 06:56:42.796000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:42.796000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:42.796000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:56:42.796000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:42.796000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:42.796000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:42.797000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:42.819000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:42.819000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:42.819000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:42.819000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:42.819000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:43.083000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:43.083000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:43.083000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:43.083000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:43.083000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:43.353000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:34<00:00,  1.08s/it]100%|██████████| 32/32 [00:34<00:00,  1.08s/it]
100%|██████████| 32/32 [00:34<00:00,  1.07s/it]100%|██████████| 32/32 [00:34<00:00,  1.07s/it]
W0318 06:56:45.769000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.770000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.770000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.770000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.770000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.770000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.770000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.801000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.801000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.802000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.802000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.802000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.818000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.818000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.818000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.818000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.818000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.983000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.983000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.983000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.983000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:45.983000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:46.227000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:46.228000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:46.228000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:56:46.228000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:46.228000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:46.228000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:46.228000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:46.250000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:46.250000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:46.250000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:46.250000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:46.250000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:46.317000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:46.317000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:46.317000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:46.317000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:46.317000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:47.215000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:47.533000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:47.533000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:47.533000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:47.533000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:47.534000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:56:47.534000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:47.534000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:47.555000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:47.556000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:47.556000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:47.556000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:47.556000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:47.822000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:47.822000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:47.822000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:47.822000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:47.822000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:48.093000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0318 06:56:50.111010 1124865 finetune.py:45] layer 25_up initial loss 0.10103847086429596
W0318 06:56:50.111262 1124865 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:56:50.815000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:56:50.815000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:50.815000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:50.815000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:50.815000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:50.815000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:50.815000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:50.845000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:50.845000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:50.845000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:50.845000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:50.845000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:50.861000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:50.861000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:50.861000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:50.861000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:50.861000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:50.930695 1124865 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:56:51.019000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.019000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.019000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.019000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.020000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.028000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.028000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.028000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.028000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.029000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.029000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.029000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.060000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.060000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.060000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.060000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.060000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.077000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.077000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.077000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.077000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.077000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.239000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.240000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.240000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.240000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.240000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.251000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.251000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.251000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.251000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.252000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.252000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.252000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.274000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.274000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.274000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.274000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.274000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.340000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.340000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.340000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.340000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.340000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.477000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.477000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.477000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.477000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.477000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.478000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.478000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.500000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.500000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.500000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.500000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.500000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.567000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.567000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.567000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.567000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:51.567000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
25_up proxy err 0.01272448431700468 tr(WHW.T) 2805.72802734375
  0%|          | 0/32 [00:00<?, ?it/s]W0318 06:56:52.227000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.483000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.558000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.558000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.559000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.559000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.559000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.559000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.559000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.580000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.580000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.580000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.580000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.580000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.819000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.819000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.819000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.819000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.820000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.820000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.820000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.841000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.841000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.842000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.842000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.842000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.844000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.844000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.844000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.844000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:52.844000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:53.106000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:56:53.106000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:56:53.106000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:56:53.106000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:56:53.106000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:53.114000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:56:53.397000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:41,  1.35s/it]  6%|▋         | 2/32 [00:02<00:35,  1.17s/it]I0318 06:56:55.017910 1124705 finetune.py:45] layer 24_up initial loss 0.08334720134735107
W0318 06:56:55.018377 1124705 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:03<00:32,  1.11s/it]W0318 06:56:56.030493 1124705 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:04<00:30,  1.08s/it]24_up proxy err 0.012626591138541698 tr(WHW.T) 2621.764892578125
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:05<00:28,  1.07s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.06s/it]  3%|▎         | 1/32 [00:01<00:42,  1.36s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.05s/it]  6%|▋         | 2/32 [00:02<00:35,  1.18s/it]I0318 06:57:00.453632 1125833 finetune.py:45] layer 27_up initial loss 0.3125755190849304
W0318 06:57:00.454130 1125833 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0318 06:57:00.805630 1125391 finetune.py:45] layer 26_up initial loss 0.11987262964248657
W0318 06:57:00.806087 1125391 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:08<00:25,  1.05s/it]  9%|▉         | 3/32 [00:03<00:32,  1.11s/it]W0318 06:57:01.534881 1125833 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:57:01.846643 1125391 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:09<00:24,  1.04s/it] 12%|█▎        | 4/32 [00:04<00:30,  1.09s/it]27_up proxy err 0.010955478996038437 tr(WHW.T) 3691.557373046875
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:10<00:22,  1.05s/it] 16%|█▌        | 5/32 [00:05<00:29,  1.08s/it]26_up proxy err 0.01201220229268074 tr(WHW.T) 3154.7509765625
  0%|          | 0/32 [00:00<?, ?it/s] 34%|███▍      | 11/32 [00:11<00:21,  1.05s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.08s/it]  3%|▎         | 1/32 [00:01<00:43,  1.39s/it]  3%|▎         | 1/32 [00:01<00:42,  1.36s/it] 38%|███▊      | 12/32 [00:12<00:21,  1.05s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.07s/it]  6%|▋         | 2/32 [00:02<00:36,  1.22s/it]  6%|▋         | 2/32 [00:02<00:35,  1.19s/it] 41%|████      | 13/32 [00:13<00:20,  1.05s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.07s/it]  9%|▉         | 3/32 [00:03<00:33,  1.17s/it]  9%|▉         | 3/32 [00:03<00:33,  1.14s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.05s/it] 28%|██▊       | 9/32 [00:09<00:24,  1.06s/it] 12%|█▎        | 4/32 [00:04<00:32,  1.14s/it] 12%|█▎        | 4/32 [00:04<00:31,  1.11s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.05s/it] 31%|███▏      | 10/32 [00:10<00:23,  1.06s/it] 16%|█▌        | 5/32 [00:05<00:30,  1.13s/it] 16%|█▌        | 5/32 [00:05<00:29,  1.09s/it] 50%|█████     | 16/32 [00:17<00:16,  1.05s/it] 34%|███▍      | 11/32 [00:11<00:22,  1.06s/it] 19%|█▉        | 6/32 [00:06<00:29,  1.12s/it] 19%|█▉        | 6/32 [00:06<00:28,  1.08s/it] 53%|█████▎    | 17/32 [00:18<00:15,  1.05s/it] 38%|███▊      | 12/32 [00:12<00:21,  1.06s/it] 22%|██▏       | 7/32 [00:08<00:27,  1.11s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.07s/it] 56%|█████▋    | 18/32 [00:19<00:14,  1.05s/it] 41%|████      | 13/32 [00:14<00:20,  1.06s/it] 25%|██▌       | 8/32 [00:09<00:26,  1.11s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.07s/it] 59%|█████▉    | 19/32 [00:20<00:13,  1.05s/it] 44%|████▍     | 14/32 [00:15<00:19,  1.06s/it] 28%|██▊       | 9/32 [00:10<00:25,  1.10s/it] 28%|██▊       | 9/32 [00:09<00:24,  1.07s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.05s/it] 47%|████▋     | 15/32 [00:16<00:17,  1.05s/it] 31%|███▏      | 10/32 [00:11<00:24,  1.10s/it] 31%|███▏      | 10/32 [00:10<00:23,  1.08s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.05s/it] 50%|█████     | 16/32 [00:17<00:16,  1.06s/it] 34%|███▍      | 11/32 [00:12<00:22,  1.10s/it] 34%|███▍      | 11/32 [00:12<00:22,  1.09s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.05s/it] 53%|█████▎    | 17/32 [00:18<00:15,  1.06s/it] 38%|███▊      | 12/32 [00:13<00:21,  1.09s/it] 38%|███▊      | 12/32 [00:13<00:21,  1.08s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.05s/it] 56%|█████▋    | 18/32 [00:19<00:14,  1.06s/it] 41%|████      | 13/32 [00:14<00:20,  1.09s/it] 41%|████      | 13/32 [00:14<00:20,  1.08s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.05s/it] 59%|█████▉    | 19/32 [00:20<00:13,  1.06s/it] 44%|████▍     | 14/32 [00:15<00:19,  1.09s/it] 44%|████▍     | 14/32 [00:15<00:19,  1.08s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.05s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.06s/it] 47%|████▋     | 15/32 [00:16<00:18,  1.09s/it] 47%|████▋     | 15/32 [00:16<00:18,  1.08s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.05s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.06s/it] 50%|█████     | 16/32 [00:17<00:17,  1.09s/it] 50%|█████     | 16/32 [00:17<00:17,  1.09s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.05s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.05s/it] 53%|█████▎    | 17/32 [00:18<00:16,  1.09s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.05s/it] 53%|█████▎    | 17/32 [00:18<00:16,  1.09s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.05s/it] 56%|█████▋    | 18/32 [00:19<00:15,  1.09s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.05s/it] 56%|█████▋    | 18/32 [00:19<00:15,  1.08s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.05s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.05s/it] 59%|█████▉    | 19/32 [00:21<00:14,  1.09s/it] 59%|█████▉    | 19/32 [00:20<00:13,  1.08s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.06s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.05s/it] 62%|██████▎   | 20/32 [00:22<00:13,  1.09s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.07s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.06s/it]100%|██████████| 32/32 [00:33<00:00,  1.05s/it]100%|██████████| 32/32 [00:33<00:00,  1.06s/it]
 66%|██████▌   | 21/32 [00:23<00:11,  1.09s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.07s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.06s/it] 69%|██████▉   | 22/32 [00:24<00:10,  1.09s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.08s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.06s/it] 72%|███████▏  | 23/32 [00:25<00:09,  1.08s/it] 72%|███████▏  | 23/32 [00:25<00:09,  1.09s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.05s/it] 75%|███████▌  | 24/32 [00:26<00:08,  1.09s/it] 75%|███████▌  | 24/32 [00:26<00:08,  1.09s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.05s/it] 78%|███████▊  | 25/32 [00:27<00:07,  1.08s/it] 78%|███████▊  | 25/32 [00:27<00:07,  1.08s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.04s/it] 81%|████████▏ | 26/32 [00:28<00:06,  1.07s/it] 81%|████████▏ | 26/32 [00:28<00:06,  1.09s/it]100%|██████████| 32/32 [00:34<00:00,  1.04s/it]100%|██████████| 32/32 [00:34<00:00,  1.06s/it]
 84%|████████▍ | 27/32 [00:29<00:05,  1.07s/it] 84%|████████▍ | 27/32 [00:29<00:05,  1.09s/it]I0318 06:57:33.534842 1124865 finetune.py:45] layer 25_gate initial loss 0.09979057312011719
W0318 06:57:33.535202 1124865 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 28/32 [00:30<00:04,  1.07s/it] 88%|████████▊ | 28/32 [00:30<00:04,  1.10s/it]W0318 06:57:34.324469 1124865 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:31<00:03,  1.07s/it] 91%|█████████ | 29/32 [00:31<00:03,  1.10s/it] 94%|█████████▍| 30/32 [00:32<00:02,  1.08s/it] 94%|█████████▍| 30/32 [00:32<00:02,  1.10s/it] 97%|█████████▋| 31/32 [00:33<00:01,  1.08s/it] 97%|█████████▋| 31/32 [00:33<00:01,  1.09s/it]25_gate proxy err 0.008861267007887363 tr(WHW.T) 4666.439453125
  0%|          | 0/86 [00:00<?, ?it/s]100%|██████████| 32/32 [00:35<00:00,  1.07s/it]100%|██████████| 32/32 [00:35<00:00,  1.10s/it]
  1%|          | 1/86 [00:00<00:52,  1.63it/s]100%|██████████| 32/32 [00:34<00:00,  1.09s/it]100%|██████████| 32/32 [00:34<00:00,  1.09s/it]
  2%|▏         | 2/86 [00:00<00:38,  2.18it/s]  3%|▎         | 3/86 [00:01<00:34,  2.44it/s]  5%|▍         | 4/86 [00:01<00:31,  2.58it/s]  6%|▌         | 5/86 [00:02<00:30,  2.66it/s]  7%|▋         | 6/86 [00:02<00:29,  2.71it/s]I0318 06:57:39.870229 1124705 finetune.py:45] layer 24_gate initial loss 0.08232969045639038
W0318 06:57:39.870840 1124705 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  8%|▊         | 7/86 [00:02<00:28,  2.75it/s]  9%|▉         | 8/86 [00:03<00:28,  2.77it/s]W0318 06:57:40.780590 1124705 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 10%|█         | 9/86 [00:03<00:27,  2.76it/s] 12%|█▏        | 10/86 [00:03<00:27,  2.77it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.78it/s] 14%|█▍        | 12/86 [00:04<00:26,  2.79it/s] 15%|█▌        | 13/86 [00:04<00:26,  2.81it/s] 16%|█▋        | 14/86 [00:05<00:25,  2.81it/s] 17%|█▋        | 15/86 [00:05<00:25,  2.82it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.83it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.83it/s] 21%|██        | 18/86 [00:06<00:23,  2.85it/s] 22%|██▏       | 19/86 [00:06<00:23,  2.84it/s]24_gate proxy err 0.00903957150876522 tr(WHW.T) 4262.748046875
  0%|          | 0/86 [00:00<?, ?it/s] 23%|██▎       | 20/86 [00:07<00:23,  2.80it/s]  1%|          | 1/86 [00:00<00:53,  1.59it/s] 24%|██▍       | 21/86 [00:07<00:23,  2.80it/s]  2%|▏         | 2/86 [00:00<00:39,  2.15it/s] 26%|██▌       | 22/86 [00:08<00:22,  2.81it/s]  3%|▎         | 3/86 [00:01<00:34,  2.42it/s] 27%|██▋       | 23/86 [00:08<00:22,  2.82it/s]I0318 06:57:45.839627 1125833 finetune.py:45] layer 27_gate initial loss 0.30791613459587097
W0318 06:57:45.840154 1125833 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  5%|▍         | 4/86 [00:01<00:32,  2.55it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.82it/s]I0318 06:57:46.264312 1125391 finetune.py:45] layer 26_gate initial loss 0.11853065341711044
W0318 06:57:46.264696 1125391 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▌         | 5/86 [00:02<00:30,  2.64it/s] 29%|██▉       | 25/86 [00:09<00:21,  2.82it/s]W0318 06:57:46.690264 1125833 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  7%|▋         | 6/86 [00:02<00:29,  2.68it/s] 30%|███       | 26/86 [00:09<00:21,  2.82it/s]W0318 06:57:47.136240 1125391 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  8%|▊         | 7/86 [00:02<00:28,  2.73it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.81it/s]  9%|▉         | 8/86 [00:03<00:28,  2.77it/s] 33%|███▎      | 28/86 [00:10<00:20,  2.81it/s] 10%|█         | 9/86 [00:03<00:27,  2.80it/s] 34%|███▎      | 29/86 [00:10<00:20,  2.82it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.82it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.82it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.81it/s] 36%|███▌      | 31/86 [00:11<00:19,  2.81it/s] 14%|█▍        | 12/86 [00:04<00:26,  2.78it/s] 37%|███▋      | 32/86 [00:11<00:19,  2.82it/s] 15%|█▌        | 13/86 [00:04<00:26,  2.79it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.82it/s] 16%|█▋        | 14/86 [00:05<00:25,  2.78it/s] 40%|███▉      | 34/86 [00:12<00:18,  2.82it/s]27_gate proxy err 0.007750862743705511 tr(WHW.T) 5990.8251953125
  0%|          | 0/86 [00:00<?, ?it/s] 17%|█▋        | 15/86 [00:05<00:25,  2.76it/s] 41%|████      | 35/86 [00:12<00:18,  2.82it/s]26_gate proxy err 0.008271492086350918 tr(WHW.T) 5302.16357421875
  0%|          | 0/86 [00:00<?, ?it/s] 19%|█▊        | 16/86 [00:05<00:25,  2.78it/s] 42%|████▏     | 36/86 [00:13<00:17,  2.82it/s]  1%|          | 1/86 [00:00<00:52,  1.61it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.78it/s] 43%|████▎     | 37/86 [00:13<00:17,  2.81it/s]  2%|▏         | 2/86 [00:00<00:39,  2.12it/s]  1%|          | 1/86 [00:00<00:53,  1.58it/s] 21%|██        | 18/86 [00:06<00:24,  2.78it/s] 44%|████▍     | 38/86 [00:13<00:17,  2.81it/s]  3%|▎         | 3/86 [00:01<00:35,  2.36it/s]  2%|▏         | 2/86 [00:00<00:39,  2.10it/s] 22%|██▏       | 19/86 [00:07<00:24,  2.79it/s] 45%|████▌     | 39/86 [00:14<00:16,  2.80it/s]  5%|▍         | 4/86 [00:01<00:32,  2.49it/s]  3%|▎         | 3/86 [00:01<00:35,  2.34it/s] 23%|██▎       | 20/86 [00:07<00:23,  2.79it/s] 47%|████▋     | 40/86 [00:14<00:16,  2.80it/s]  6%|▌         | 5/86 [00:02<00:31,  2.57it/s]  5%|▍         | 4/86 [00:01<00:33,  2.48it/s] 24%|██▍       | 21/86 [00:07<00:23,  2.78it/s] 48%|████▊     | 41/86 [00:14<00:16,  2.80it/s]  7%|▋         | 6/86 [00:02<00:30,  2.61it/s]  6%|▌         | 5/86 [00:02<00:31,  2.56it/s] 26%|██▌       | 22/86 [00:08<00:22,  2.78it/s] 49%|████▉     | 42/86 [00:15<00:15,  2.80it/s]  8%|▊         | 7/86 [00:02<00:29,  2.64it/s]  7%|▋         | 6/86 [00:02<00:30,  2.61it/s] 27%|██▋       | 23/86 [00:08<00:22,  2.76it/s] 50%|█████     | 43/86 [00:15<00:15,  2.79it/s]  9%|▉         | 8/86 [00:03<00:29,  2.66it/s]  8%|▊         | 7/86 [00:02<00:29,  2.64it/s] 28%|██▊       | 24/86 [00:08<00:22,  2.77it/s] 51%|█████     | 44/86 [00:15<00:15,  2.79it/s] 10%|█         | 9/86 [00:03<00:28,  2.67it/s]  9%|▉         | 8/86 [00:03<00:29,  2.67it/s] 29%|██▉       | 25/86 [00:09<00:22,  2.77it/s] 52%|█████▏    | 45/86 [00:16<00:14,  2.79it/s] 12%|█▏        | 10/86 [00:03<00:28,  2.68it/s] 10%|█         | 9/86 [00:03<00:28,  2.70it/s] 30%|███       | 26/86 [00:09<00:21,  2.78it/s] 53%|█████▎    | 46/86 [00:16<00:14,  2.79it/s] 13%|█▎        | 11/86 [00:04<00:27,  2.69it/s] 12%|█▏        | 10/86 [00:03<00:28,  2.71it/s] 31%|███▏      | 27/86 [00:09<00:21,  2.79it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.79it/s] 14%|█▍        | 12/86 [00:04<00:27,  2.70it/s] 13%|█▎        | 11/86 [00:04<00:27,  2.72it/s] 33%|███▎      | 28/86 [00:10<00:20,  2.80it/s] 56%|█████▌    | 48/86 [00:17<00:13,  2.79it/s] 14%|█▍        | 12/86 [00:04<00:27,  2.73it/s] 15%|█▌        | 13/86 [00:05<00:27,  2.68it/s] 34%|███▎      | 29/86 [00:10<00:20,  2.80it/s] 57%|█████▋    | 49/86 [00:17<00:13,  2.79it/s] 15%|█▌        | 13/86 [00:05<00:26,  2.74it/s] 16%|█▋        | 14/86 [00:05<00:26,  2.68it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.80it/s] 58%|█████▊    | 50/86 [00:18<00:12,  2.79it/s] 16%|█▋        | 14/86 [00:05<00:26,  2.74it/s] 36%|███▌      | 31/86 [00:11<00:19,  2.81it/s] 17%|█▋        | 15/86 [00:05<00:26,  2.69it/s] 59%|█████▉    | 51/86 [00:18<00:12,  2.79it/s] 17%|█▋        | 15/86 [00:05<00:25,  2.74it/s] 37%|███▋      | 32/86 [00:11<00:19,  2.80it/s] 19%|█▊        | 16/86 [00:06<00:26,  2.69it/s] 60%|██████    | 52/86 [00:18<00:12,  2.79it/s] 38%|███▊      | 33/86 [00:12<00:18,  2.80it/s] 19%|█▊        | 16/86 [00:06<00:25,  2.72it/s] 20%|█▉        | 17/86 [00:06<00:25,  2.69it/s] 62%|██████▏   | 53/86 [00:19<00:11,  2.78it/s] 40%|███▉      | 34/86 [00:12<00:18,  2.79it/s] 20%|█▉        | 17/86 [00:06<00:25,  2.73it/s] 21%|██        | 18/86 [00:06<00:25,  2.70it/s] 63%|██████▎   | 54/86 [00:19<00:11,  2.79it/s] 41%|████      | 35/86 [00:12<00:18,  2.74it/s] 21%|██        | 18/86 [00:06<00:24,  2.72it/s] 64%|██████▍   | 55/86 [00:19<00:11,  2.79it/s] 22%|██▏       | 19/86 [00:07<00:24,  2.71it/s] 42%|████▏     | 36/86 [00:13<00:18,  2.76it/s] 22%|██▏       | 19/86 [00:07<00:24,  2.73it/s] 65%|██████▌   | 56/86 [00:20<00:10,  2.78it/s] 23%|██▎       | 20/86 [00:07<00:24,  2.71it/s] 43%|████▎     | 37/86 [00:13<00:17,  2.75it/s] 23%|██▎       | 20/86 [00:07<00:24,  2.73it/s] 66%|██████▋   | 57/86 [00:20<00:10,  2.78it/s] 24%|██▍       | 21/86 [00:08<00:23,  2.71it/s] 44%|████▍     | 38/86 [00:13<00:17,  2.76it/s] 67%|██████▋   | 58/86 [00:20<00:10,  2.79it/s] 24%|██▍       | 21/86 [00:07<00:23,  2.73it/s] 26%|██▌       | 22/86 [00:08<00:23,  2.72it/s] 45%|████▌     | 39/86 [00:14<00:17,  2.76it/s] 69%|██████▊   | 59/86 [00:21<00:09,  2.76it/s] 26%|██▌       | 22/86 [00:08<00:23,  2.67it/s] 27%|██▋       | 23/86 [00:08<00:23,  2.72it/s] 47%|████▋     | 40/86 [00:14<00:16,  2.75it/s] 70%|██████▉   | 60/86 [00:21<00:09,  2.77it/s] 27%|██▋       | 23/86 [00:08<00:23,  2.69it/s] 28%|██▊       | 24/86 [00:09<00:22,  2.72it/s] 71%|███████   | 61/86 [00:22<00:09,  2.77it/s] 48%|████▊     | 41/86 [00:14<00:16,  2.72it/s] 29%|██▉       | 25/86 [00:09<00:22,  2.72it/s] 28%|██▊       | 24/86 [00:09<00:22,  2.70it/s] 72%|███████▏  | 62/86 [00:22<00:08,  2.78it/s] 49%|████▉     | 42/86 [00:15<00:16,  2.74it/s] 30%|███       | 26/86 [00:09<00:22,  2.73it/s] 29%|██▉       | 25/86 [00:09<00:22,  2.69it/s] 73%|███████▎  | 63/86 [00:22<00:08,  2.77it/s] 50%|█████     | 43/86 [00:15<00:15,  2.74it/s] 31%|███▏      | 27/86 [00:10<00:21,  2.73it/s] 30%|███       | 26/86 [00:09<00:22,  2.66it/s] 74%|███████▍  | 64/86 [00:23<00:07,  2.77it/s] 51%|█████     | 44/86 [00:16<00:15,  2.74it/s] 33%|███▎      | 28/86 [00:10<00:21,  2.73it/s] 31%|███▏      | 27/86 [00:10<00:22,  2.67it/s] 76%|███████▌  | 65/86 [00:23<00:07,  2.78it/s] 52%|█████▏    | 45/86 [00:16<00:14,  2.76it/s] 34%|███▎      | 29/86 [00:10<00:20,  2.74it/s] 33%|███▎      | 28/86 [00:10<00:21,  2.67it/s] 77%|███████▋  | 66/86 [00:23<00:07,  2.78it/s] 53%|█████▎    | 46/86 [00:16<00:14,  2.76it/s] 35%|███▍      | 30/86 [00:11<00:20,  2.73it/s] 34%|███▎      | 29/86 [00:10<00:21,  2.67it/s] 78%|███████▊  | 67/86 [00:24<00:06,  2.78it/s] 55%|█████▍    | 47/86 [00:17<00:14,  2.75it/s] 36%|███▌      | 31/86 [00:11<00:20,  2.73it/s] 35%|███▍      | 30/86 [00:11<00:20,  2.69it/s] 79%|███████▉  | 68/86 [00:24<00:06,  2.78it/s] 56%|█████▌    | 48/86 [00:17<00:13,  2.72it/s] 37%|███▋      | 32/86 [00:12<00:19,  2.73it/s] 36%|███▌      | 31/86 [00:11<00:20,  2.70it/s] 80%|████████  | 69/86 [00:24<00:06,  2.79it/s] 57%|█████▋    | 49/86 [00:17<00:13,  2.74it/s] 38%|███▊      | 33/86 [00:12<00:19,  2.72it/s] 37%|███▋      | 32/86 [00:12<00:19,  2.71it/s] 81%|████████▏ | 70/86 [00:25<00:05,  2.79it/s] 58%|█████▊    | 50/86 [00:18<00:13,  2.74it/s] 40%|███▉      | 34/86 [00:12<00:19,  2.72it/s] 38%|███▊      | 33/86 [00:12<00:19,  2.72it/s] 83%|████████▎ | 71/86 [00:25<00:05,  2.79it/s] 59%|█████▉    | 51/86 [00:18<00:12,  2.75it/s] 41%|████      | 35/86 [00:13<00:18,  2.72it/s] 40%|███▉      | 34/86 [00:12<00:19,  2.73it/s] 84%|████████▎ | 72/86 [00:25<00:05,  2.79it/s] 60%|██████    | 52/86 [00:18<00:12,  2.76it/s] 42%|████▏     | 36/86 [00:13<00:18,  2.72it/s] 41%|████      | 35/86 [00:13<00:18,  2.73it/s] 85%|████████▍ | 73/86 [00:26<00:04,  2.79it/s] 62%|██████▏   | 53/86 [00:19<00:11,  2.77it/s] 43%|████▎     | 37/86 [00:13<00:18,  2.72it/s] 42%|████▏     | 36/86 [00:13<00:18,  2.73it/s] 86%|████████▌ | 74/86 [00:26<00:04,  2.79it/s] 63%|██████▎   | 54/86 [00:19<00:11,  2.77it/s] 44%|████▍     | 38/86 [00:14<00:17,  2.72it/s] 43%|████▎     | 37/86 [00:13<00:18,  2.70it/s] 87%|████████▋ | 75/86 [00:27<00:03,  2.79it/s] 64%|██████▍   | 55/86 [00:20<00:11,  2.75it/s] 45%|████▌     | 39/86 [00:14<00:17,  2.73it/s] 44%|████▍     | 38/86 [00:14<00:17,  2.71it/s] 88%|████████▊ | 76/86 [00:27<00:03,  2.79it/s] 65%|██████▌   | 56/86 [00:20<00:10,  2.74it/s] 47%|████▋     | 40/86 [00:14<00:16,  2.73it/s] 45%|████▌     | 39/86 [00:14<00:17,  2.71it/s] 90%|████████▉ | 77/86 [00:27<00:03,  2.79it/s] 66%|██████▋   | 57/86 [00:20<00:10,  2.76it/s] 48%|████▊     | 41/86 [00:15<00:16,  2.73it/s] 47%|████▋     | 40/86 [00:15<00:16,  2.72it/s] 91%|█████████ | 78/86 [00:28<00:02,  2.79it/s] 67%|██████▋   | 58/86 [00:21<00:10,  2.76it/s] 49%|████▉     | 42/86 [00:15<00:16,  2.72it/s] 48%|████▊     | 41/86 [00:15<00:16,  2.72it/s] 92%|█████████▏| 79/86 [00:28<00:02,  2.79it/s] 69%|██████▊   | 59/86 [00:21<00:09,  2.78it/s] 50%|█████     | 43/86 [00:16<00:15,  2.71it/s] 49%|████▉     | 42/86 [00:15<00:16,  2.72it/s] 93%|█████████▎| 80/86 [00:28<00:02,  2.79it/s] 70%|██████▉   | 60/86 [00:21<00:09,  2.79it/s] 51%|█████     | 44/86 [00:16<00:15,  2.71it/s] 50%|█████     | 43/86 [00:16<00:15,  2.72it/s] 94%|█████████▍| 81/86 [00:29<00:01,  2.79it/s] 71%|███████   | 61/86 [00:22<00:08,  2.79it/s] 52%|█████▏    | 45/86 [00:16<00:15,  2.71it/s] 51%|█████     | 44/86 [00:16<00:15,  2.72it/s] 95%|█████████▌| 82/86 [00:29<00:01,  2.79it/s] 72%|███████▏  | 62/86 [00:22<00:08,  2.80it/s] 53%|█████▎    | 46/86 [00:17<00:14,  2.72it/s] 52%|█████▏    | 45/86 [00:16<00:15,  2.72it/s] 97%|█████████▋| 83/86 [00:29<00:01,  2.79it/s] 73%|███████▎  | 63/86 [00:22<00:08,  2.79it/s] 55%|█████▍    | 47/86 [00:17<00:14,  2.71it/s] 53%|█████▎    | 46/86 [00:17<00:14,  2.72it/s] 98%|█████████▊| 84/86 [00:30<00:00,  2.79it/s] 74%|███████▍  | 64/86 [00:23<00:07,  2.79it/s] 56%|█████▌    | 48/86 [00:17<00:14,  2.71it/s] 55%|█████▍    | 47/86 [00:17<00:14,  2.72it/s] 99%|█████████▉| 85/86 [00:30<00:00,  2.79it/s] 76%|███████▌  | 65/86 [00:23<00:07,  2.77it/s] 57%|█████▋    | 49/86 [00:18<00:13,  2.72it/s] 56%|█████▌    | 48/86 [00:17<00:13,  2.73it/s]100%|██████████| 86/86 [00:30<00:00,  2.79it/s]100%|██████████| 86/86 [00:30<00:00,  2.78it/s]
 77%|███████▋  | 66/86 [00:24<00:07,  2.78it/s] 58%|█████▊    | 50/86 [00:18<00:13,  2.72it/s] 57%|█████▋    | 49/86 [00:18<00:13,  2.74it/s] 78%|███████▊  | 67/86 [00:24<00:06,  2.78it/s] 59%|█████▉    | 51/86 [00:19<00:12,  2.72it/s] 58%|█████▊    | 50/86 [00:18<00:13,  2.71it/s] 79%|███████▉  | 68/86 [00:24<00:06,  2.80it/s] 60%|██████    | 52/86 [00:19<00:12,  2.73it/s] 59%|█████▉    | 51/86 [00:19<00:13,  2.68it/s] 80%|████████  | 69/86 [00:25<00:06,  2.81it/s] 62%|██████▏   | 53/86 [00:19<00:12,  2.74it/s] 60%|██████    | 52/86 [00:19<00:12,  2.67it/s] 81%|████████▏ | 70/86 [00:25<00:05,  2.81it/s] 63%|██████▎   | 54/86 [00:20<00:11,  2.73it/s] 62%|██████▏   | 53/86 [00:19<00:12,  2.66it/s] 83%|████████▎ | 71/86 [00:25<00:05,  2.82it/s] 64%|██████▍   | 55/86 [00:20<00:11,  2.73it/s] 63%|██████▎   | 54/86 [00:20<00:12,  2.66it/s] 84%|████████▎ | 72/86 [00:26<00:04,  2.82it/s] 65%|██████▌   | 56/86 [00:20<00:11,  2.73it/s] 64%|██████▍   | 55/86 [00:20<00:11,  2.68it/s] 85%|████████▍ | 73/86 [00:26<00:04,  2.81it/s] 66%|██████▋   | 57/86 [00:21<00:10,  2.71it/s] 65%|██████▌   | 56/86 [00:20<00:11,  2.69it/s] 86%|████████▌ | 74/86 [00:26<00:04,  2.80it/s] 67%|██████▋   | 58/86 [00:21<00:10,  2.71it/s] 66%|██████▋   | 57/86 [00:21<00:10,  2.69it/s] 87%|████████▋ | 75/86 [00:27<00:03,  2.77it/s] 69%|██████▊   | 59/86 [00:21<00:09,  2.72it/s] 88%|████████▊ | 76/86 [00:27<00:03,  2.75it/s] 67%|██████▋   | 58/86 [00:21<00:10,  2.67it/s] 70%|██████▉   | 60/86 [00:22<00:09,  2.72it/s] 90%|████████▉ | 77/86 [00:27<00:03,  2.76it/s] 69%|██████▊   | 59/86 [00:22<00:10,  2.69it/s] 71%|███████   | 61/86 [00:22<00:09,  2.71it/s] 91%|█████████ | 78/86 [00:28<00:02,  2.78it/s] 70%|██████▉   | 60/86 [00:22<00:09,  2.69it/s] 72%|███████▏  | 62/86 [00:23<00:08,  2.71it/s] 92%|█████████▏| 79/86 [00:28<00:02,  2.79it/s] 71%|███████   | 61/86 [00:22<00:09,  2.69it/s] 73%|███████▎  | 63/86 [00:23<00:08,  2.71it/s] 93%|█████████▎| 80/86 [00:29<00:02,  2.80it/s] 72%|███████▏  | 62/86 [00:23<00:08,  2.70it/s] 74%|███████▍  | 64/86 [00:23<00:08,  2.69it/s] 94%|█████████▍| 81/86 [00:29<00:01,  2.80it/s] 73%|███████▎  | 63/86 [00:23<00:08,  2.71it/s] 76%|███████▌  | 65/86 [00:24<00:07,  2.70it/s] 95%|█████████▌| 82/86 [00:29<00:01,  2.80it/s] 74%|███████▍  | 64/86 [00:23<00:08,  2.72it/s] 77%|███████▋  | 66/86 [00:24<00:07,  2.71it/s] 97%|█████████▋| 83/86 [00:30<00:01,  2.81it/s] 76%|███████▌  | 65/86 [00:24<00:07,  2.72it/s] 98%|█████████▊| 84/86 [00:30<00:00,  2.80it/s] 78%|███████▊  | 67/86 [00:24<00:06,  2.72it/s] 77%|███████▋  | 66/86 [00:24<00:07,  2.72it/s] 99%|█████████▉| 85/86 [00:30<00:00,  2.79it/s] 79%|███████▉  | 68/86 [00:25<00:06,  2.70it/s] 78%|███████▊  | 67/86 [00:24<00:06,  2.72it/s]W0318 06:58:15.355000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.355000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.355000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.355000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.356000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.356000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.356000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.398000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.398000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.398000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.398000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.398000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.414000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.415000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.415000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.415000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.415000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
100%|██████████| 86/86 [00:31<00:00,  2.77it/s]100%|██████████| 86/86 [00:31<00:00,  2.76it/s]
W0318 06:58:15.588000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.588000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.589000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.589000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.589000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 80%|████████  | 69/86 [00:25<00:06,  2.70it/s] 79%|███████▉  | 68/86 [00:25<00:06,  2.72it/s]W0318 06:58:15.921000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.922000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.922000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.922000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.922000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.922000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.922000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.953000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.953000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.953000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.954000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:15.954000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 70/86 [00:26<00:05,  2.71it/s]W0318 06:58:16.027000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:16.027000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:16.027000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:16.027000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:16.027000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 80%|████████  | 69/86 [00:25<00:06,  2.70it/s] 83%|████████▎ | 71/86 [00:26<00:05,  2.70it/s] 81%|████████▏ | 70/86 [00:26<00:05,  2.69it/s] 84%|████████▎ | 72/86 [00:26<00:05,  2.71it/s] 83%|████████▎ | 71/86 [00:26<00:05,  2.70it/s] 85%|████████▍ | 73/86 [00:27<00:04,  2.71it/s] 84%|████████▎ | 72/86 [00:26<00:05,  2.71it/s]W0318 06:58:17.254000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:17.268000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:17.276000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:17.276000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 86%|████████▌ | 74/86 [00:27<00:04,  2.72it/s] 85%|████████▍ | 73/86 [00:27<00:04,  2.71it/s]W0318 06:58:17.737000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:17.737000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:17.737000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:58:17.737000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:17.738000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:17.738000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:17.738000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:17.770000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:17.770000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:17.770000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:17.770000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:17.770000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 87%|████████▋ | 75/86 [00:27<00:04,  2.72it/s] 86%|████████▌ | 74/86 [00:27<00:04,  2.72it/s]W0318 06:58:18.125000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:18.125000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:18.125000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:58:18.125000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:18.126000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:18.126000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:18.126000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:18.126000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 76/86 [00:28<00:03,  2.72it/s] 87%|████████▋ | 75/86 [00:27<00:04,  2.72it/s]W0318 06:58:18.433000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:18.434000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:18.434000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:18.434000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:18.434000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 90%|████████▉ | 77/86 [00:28<00:03,  2.72it/s] 88%|████████▊ | 76/86 [00:28<00:03,  2.73it/s]W0318 06:58:18.784000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:18.789000 140312743900992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 78/86 [00:28<00:02,  2.72it/s] 90%|████████▉ | 77/86 [00:28<00:03,  2.73it/s] 92%|█████████▏| 79/86 [00:29<00:02,  2.71it/s] 91%|█████████ | 78/86 [00:29<00:02,  2.74it/s] 93%|█████████▎| 80/86 [00:29<00:02,  2.71it/s] 92%|█████████▏| 79/86 [00:29<00:02,  2.74it/s] 94%|█████████▍| 81/86 [00:30<00:01,  2.72it/s] 93%|█████████▎| 80/86 [00:29<00:02,  2.74it/s] 95%|█████████▌| 82/86 [00:30<00:01,  2.73it/s] 94%|█████████▍| 81/86 [00:30<00:01,  2.75it/s] 97%|█████████▋| 83/86 [00:30<00:01,  2.74it/s] 95%|█████████▌| 82/86 [00:30<00:01,  2.74it/s] 98%|█████████▊| 84/86 [00:31<00:00,  2.75it/s] 97%|█████████▋| 83/86 [00:30<00:01,  2.74it/s] 99%|█████████▉| 85/86 [00:31<00:00,  2.75it/s] 98%|█████████▊| 84/86 [00:31<00:00,  2.74it/s]100%|██████████| 86/86 [00:31<00:00,  2.77it/s]100%|██████████| 86/86 [00:31<00:00,  2.70it/s]
 99%|█████████▉| 85/86 [00:31<00:00,  2.76it/s]100%|██████████| 86/86 [00:31<00:00,  2.75it/s]100%|██████████| 86/86 [00:31<00:00,  2.69it/s]
W0318 06:58:22.575000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.575000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.575000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.576000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.576000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.576000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.576000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.620000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.620000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.620000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.620000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.620000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.637000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.637000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.637000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.637000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.637000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.816000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.817000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.817000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.817000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:22.817000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:23.150000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:23.150000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:23.150000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:58:23.151000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:23.151000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:23.151000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:23.151000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:23.186000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:23.186000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:23.186000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:23.186000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:23.186000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:23.258000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:23.258000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:23.258000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:23.259000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:23.259000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:24.463000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:24.476000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:24.485000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:24.485000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:24.934000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:24.934000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:24.934000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:24.934000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:24.934000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:58:24.935000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:24.935000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:24.967000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:24.967000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:24.967000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:24.967000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:24.967000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:25.302000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:25.302000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:25.303000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:25.303000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:25.303000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:58:25.303000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:25.303000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:25.303000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:25.596000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:25.596000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:25.596000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:25.596000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:25.596000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
I0318 06:58:25.657904 1124865 finetune.py:45] layer 25_down initial loss 0.09886215627193451
W0318 06:58:25.658353 1124865 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:58:25.921000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:25.926000 139743180678976 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:26.171619 1124865 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

25_down proxy err 0.01417442224919796 tr(WHW.T) 373.4606018066406
W0318 06:58:28.800000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:58:28.800000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:28.800000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:28.800000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:28.801000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:28.801000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:28.801000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:28.847000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:28.847000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:28.847000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:28.847000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:28.847000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:28.864000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:28.864000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:28.864000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:28.864000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:28.865000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.046000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.046000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.046000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.046000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.046000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.385000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.385000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.386000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.386000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.386000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.386000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.386000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.391000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.391000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.392000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.392000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.392000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.392000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.392000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.420000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.420000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.421000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.421000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.421000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.438000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.438000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.438000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.438000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.438000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.454000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.454000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.454000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.454000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.455000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.494000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.494000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.495000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.495000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.495000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.632000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.632000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.632000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.632000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.632000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.982000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.982000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.982000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.983000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.983000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.983000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:29.983000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:58:30.019000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:30.019000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:30.019000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:30.019000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:30.019000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:30.093000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:30.093000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:30.094000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:30.094000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:30.094000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:30.752000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:30.766000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:30.775000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:30.775000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.238000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.238000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.238000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.238000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.238000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.238000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.239000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.269000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.269000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.270000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.270000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.270000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.338000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.352000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.361000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.361000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.627000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.628000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.628000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.628000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.628000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.628000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.628000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.628000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.814000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.814000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.815000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.815000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.815000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.815000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.815000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.843000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.843000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.843000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.843000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.843000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.930000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.930000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.930000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.930000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:31.930000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:32.168000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:58:32.168000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:32.168000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:32.169000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:32.169000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:32.169000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:32.169000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:32.169000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:32.272000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:32.277000 139663718446912 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:32.446000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:32.446000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:32.446000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:32.446000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:32.446000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:32.760000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:32.765000 140714887419712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0318 06:58:33.167252 1124705 finetune.py:45] layer 24_down initial loss 0.08138170838356018
W0318 06:58:33.167698 1124705 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:58:33.669970 1124705 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

24_down proxy err 0.014621603302657604 tr(WHW.T) 340.2240295410156
I0318 06:58:37.983756 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 28 in 1.539128065109253s
I0318 06:58:38.354669 1092738 quantize_finetune_llama.py:159] layer 29 gpu 1
I0318 06:58:39.195073 1125833 finetune.py:45] layer 27_down initial loss 0.30405715107917786
W0318 06:58:39.195416 1125833 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:58:39.659989 1125833 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

I0318 06:58:39.689253 1125391 finetune.py:45] layer 26_down initial loss 0.11676908284425735
W0318 06:58:39.689661 1125391 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

27_down proxy err 0.013847492635250092 tr(WHW.T) 466.93170166015625
W0318 06:58:40.184811 1125391 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

I0318 06:58:40.388377 1129876 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:58:40.388514 1129876 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:58:40.388577 1129876 utils.py:162] NumExpr defaulting to 16 threads.
26_down proxy err 0.014434343203902245 tr(WHW.T) 401.19378662109375
I0318 06:58:40.603449 1129876 config.py:58] PyTorch version 2.4.0 available.
I0318 06:58:43.046934 1129876 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0318 06:58:43.415656 1129876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]I0318 06:58:44.502985 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 29 in 1.728945255279541s
I0318 06:58:44.999386 1092738 quantize_finetune_llama.py:159] layer 30 gpu 2
  3%|▎         | 1/32 [00:01<00:47,  1.52s/it]  6%|▋         | 2/32 [00:01<00:24,  1.22it/s]  9%|▉         | 3/32 [00:02<00:17,  1.68it/s]I0318 06:58:46.978034 1130431 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:58:46.978197 1130431 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:58:46.978251 1130431 utils.py:162] NumExpr defaulting to 16 threads.
 12%|█▎        | 4/32 [00:02<00:13,  2.05it/s]I0318 06:58:47.191321 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 30 in 1.3887546062469482s
I0318 06:58:47.257022 1130431 config.py:58] PyTorch version 2.4.0 available.
 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s]I0318 06:58:47.576625 1092738 quantize_finetune_llama.py:159] layer 31 gpu 3
 19%|█▉        | 6/32 [00:03<00:10,  2.53it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.67it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.76it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.87it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.92it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.95it/s]I0318 06:58:49.529869 1130615 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:58:49.530122 1130615 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:58:49.530251 1130615 utils.py:162] NumExpr defaulting to 16 threads.
I0318 06:58:49.583152 1130431 data_utils.py:336] using 256 training seqs, 128 validation seqs
 38%|███▊      | 12/32 [00:05<00:06,  2.90it/s]I0318 06:58:49.726189 1130615 config.py:58] PyTorch version 2.4.0 available.
I0318 06:58:49.799974 1092738 quantize_finetune_llama.py:190] computed original embedding for layer 31 in 1.619236707687378s
W0318 06:58:49.945055 1130431 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 13/32 [00:05<00:06,  2.90it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.89it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.91it/s] 50%|█████     | 16/32 [00:06<00:05,  2.93it/s]  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.96it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.02it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.03it/s]I0318 06:58:52.024800 1130615 data_utils.py:336] using 256 training seqs, 128 validation seqs
 62%|██████▎   | 20/32 [00:07<00:03,  3.02it/s]I0318 06:58:52.325736 1131058 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0318 06:58:52.325914 1131058 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0318 06:58:52.325981 1131058 utils.py:162] NumExpr defaulting to 16 threads.
W0318 06:58:52.406552 1130615 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0318 06:58:52.613577 1131058 config.py:58] PyTorch version 2.4.0 available.
 66%|██████▌   | 21/32 [00:08<00:03,  3.03it/s]  3%|▎         | 1/32 [00:01<00:47,  1.55s/it] 69%|██████▉   | 22/32 [00:08<00:03,  3.06it/s]  6%|▋         | 2/32 [00:01<00:24,  1.21it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.07it/s]  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:02<00:17,  1.67it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.05it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.02it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.09it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.11it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.52it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.11it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.67it/s]  3%|▎         | 1/32 [00:01<00:46,  1.51s/it] 88%|████████▊ | 28/32 [00:10<00:01,  3.08it/s]I0318 06:58:54.911308 1131058 data_utils.py:336] using 256 training seqs, 128 validation seqs
 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.06it/s]  6%|▋         | 2/32 [00:01<00:24,  1.22it/s]W0318 06:58:55.256060 1131058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:04<00:08,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.03it/s]  9%|▉         | 3/32 [00:02<00:17,  1.66it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.85it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.00it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.99it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  2.98it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
 16%|█▌        | 5/32 [00:02<00:12,  2.22it/s]  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.89it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.40it/s] 41%|████      | 13/32 [00:05<00:06,  2.89it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.90it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.90it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.70it/s] 50%|█████     | 16/32 [00:06<00:05,  2.89it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s]  3%|▎         | 1/32 [00:01<00:53,  1.72s/it] 53%|█████▎    | 17/32 [00:06<00:05,  2.89it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.77it/s]  6%|▋         | 2/32 [00:02<00:27,  1.10it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.90it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.79it/s]  9%|▉         | 3/32 [00:02<00:18,  1.54it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.91it/s] 41%|████      | 13/32 [00:05<00:06,  2.79it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.90it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.91it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.80it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.17it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.92it/s]W0318 06:58:59.630000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:59.630000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:58:59.630000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:58:59.630000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:59.631000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:59.631000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:59.631000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:59.658000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:59.658000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:59.658000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:59.658000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:59.658000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:58:59.675000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:58:59.675000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:58:59.675000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:58:59.675000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:58:59.676000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 15/32 [00:06<00:06,  2.81it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.38it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.91it/s]W0318 06:59:00.007000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:00.008000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:00.008000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:00.008000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:00.008000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:05,  2.81it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.91it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.65it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.83it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.90it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.83it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.91it/s]W0318 06:59:00.929000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:59:00.929000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:00.929000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:00.929000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:00.929000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:00.930000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:00.930000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:59:00.948000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:00.948000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:00.948000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:00.948000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:00.948000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:01.198000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:01.198000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:07,  2.78it/s]W0318 06:59:01.198000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:01.198000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:01.199000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.90it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.81it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.83it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.91it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.83it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.83it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.92it/s] 41%|████      | 13/32 [00:05<00:06,  2.86it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.92it/s]W0318 06:59:02.413000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:02.414000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:02.414000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:02.414000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:59:02.414000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:59:02.414000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:02.414000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:02.432000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:02.432000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:02.432000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:59:02.432000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:02.432000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:06<00:06,  2.88it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.92it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.83it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.90it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.91it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.83it/s] 50%|█████     | 16/32 [00:06<00:05,  2.92it/s]100%|██████████| 32/32 [00:12<00:00,  2.91it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
 78%|███████▊  | 25/32 [00:09<00:02,  2.83it/s]W0318 06:59:03.369000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:03.369000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:03.369000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:59:03.369000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:03.369000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:07<00:05,  2.93it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.83it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.93it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 84%|████████▍ | 27/32 [00:10<00:01,  2.83it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.94it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.83it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.95it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.83it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.94it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.83it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.95it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.82it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.94it/s]100%|██████████| 32/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
 75%|███████▌  | 24/32 [00:09<00:02,  2.93it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.94it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.93it/s]W0318 06:59:06.658000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:06.658000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:06.658000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:06.658000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:06.659000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:59:06.659000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:59:06.659000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:06.685000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:06.686000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:06.686000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:06.686000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:06.686000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:06.703000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:06.703000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:06.703000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:06.703000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:06.703000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  2.94it/s]W0318 06:59:07.047000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:07.047000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:07.047000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:07.047000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:07.047000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  2.94it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.94it/s]W0318 06:59:07.973000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:07.973000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:07.973000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:59:07.973000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:07.974000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:07.974000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:59:07.974000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:07.993000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:07.993000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:07.993000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:07.993000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:07.994000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.93it/s]W0318 06:59:08.247000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:08.247000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:08.248000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:08.248000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:08.248000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  2.92it/s]100%|██████████| 32/32 [00:12<00:00,  2.92it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
I0318 06:59:09.111957 1129876 finetune.py:45] layer 28_v initial loss 0.5065358877182007
W0318 06:59:09.112443 1129876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 06:59:09.179000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.180000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.180000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.180000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.180000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.180000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.180000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.207000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.208000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.208000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.208000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.208000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.225000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.225000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.225000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.225000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.225000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.468000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.468000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.469000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.469000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.469000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.469000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.469000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.487000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.487000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.487000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.487000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.488000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.560000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.560000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.561000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.561000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:09.561000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.016335 1129876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 06:59:10.430000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.431000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.431000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.431000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.431000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.481000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.482000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.482000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.482000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.482000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.482000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.482000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.500000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.500000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.500000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.501000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.501000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.751000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.751000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.751000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.752000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:10.752000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
28_v proxy err 0.013496443629264832 tr(WHW.T) 2018.944091796875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.61it/s]W0318 06:59:11.961000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:11.961000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:11.961000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:11.961000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:11.961000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:59:11.961000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:59:11.961000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:11.979000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:11.979000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:11.979000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:11.979000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:11.979000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:00<00:13,  2.21it/s]W0318 06:59:12.076000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.076000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.077000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.077000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.077000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.077000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.077000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.104000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.104000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.104000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.104000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.105000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.121000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.122000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.122000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.122000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.122000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:01<00:11,  2.50it/s]W0318 06:59:12.457000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.457000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.457000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.457000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.457000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:01<00:10,  2.69it/s]W0318 06:59:12.919000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.919000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.920000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.920000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:12.920000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:01<00:09,  2.80it/s]W0318 06:59:13.732000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0318 06:59:13.732000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:13.732000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:13.732000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:13.732000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:13.732000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:13.732000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:02<00:12,  2.13it/s]W0318 06:59:13.750000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:13.751000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:13.751000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:13.751000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:13.751000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0318 06:59:14.001000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:14.001000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:14.001000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:14.001000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:14.001000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:02<00:10,  2.35it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.66it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.75it/s]W0318 06:59:15.216000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:15.216000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:15.216000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:15.216000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:59:15.216000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0318 06:59:15.216000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0318 06:59:15.217000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0318 06:59:15.234000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:15.235000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:15.235000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:15.235000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:59:15.235000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.87it/s] 41%|████      | 13/32 [00:04<00:06,  2.90it/s]W0318 06:59:16.180000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0318 06:59:16.180000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0318 06:59:16.180000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0318 06:59:16.180000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0318 06:59:16.180000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.92it/s]I0318 06:59:16.555939 1130431 finetune.py:45] layer 29_v initial loss 0.7060117125511169
W0318 06:59:16.556170 1130431 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 47%|████▋     | 15/32 [00:05<00:05,  2.94it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 50%|█████     | 16/32 [00:05<00:05,  2.95it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.97it/s]W0318 06:59:17.510786 1130431 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 56%|█████▋    | 18/32 [00:06<00:04,  2.98it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.98it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.98it/s]29_v proxy err 0.014090724289417267 tr(WHW.T) 1801.77294921875
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.97it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.98it/s]  3%|▎         | 1/32 [00:00<00:20,  1.54it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.98it/s]  6%|▋         | 2/32 [00:00<00:14,  2.14it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.98it/s]  9%|▉         | 3/32 [00:01<00:11,  2.43it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.98it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.60it/s]I0318 06:59:20.407830 1130615 finetune.py:45] layer 30_v initial loss 278.3388671875
W0318 06:59:20.408250 1130615 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:09<00:02,  2.98it/s] 16%|█▌        | 5/32 [00:02<00:09,  2.71it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.98it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.78it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.99it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.83it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.99it/s]W0318 06:59:21.611871 1130615 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:03<00:08,  2.84it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.99it/s]I0318 06:59:21.808942 1131058 finetune.py:45] layer 31_v initial loss 31.639184951782227
W0318 06:59:21.809170 1131058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:03<00:08,  2.85it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.99it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.85it/s]100%|██████████| 32/32 [00:11<00:00,  2.98it/s]100%|██████████| 32/32 [00:11<00:00,  2.82it/s]
30_v proxy err 0.012351076118648052 tr(WHW.T) 2261.48974609375
  0%|          | 0/32 [00:00<?, ?it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.87it/s]W0318 06:59:22.692440 1131058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 38%|███▊      | 12/32 [00:04<00:06,  2.89it/s]  3%|▎         | 1/32 [00:00<00:20,  1.53it/s] 41%|████      | 13/32 [00:04<00:06,  2.91it/s]  6%|▋         | 2/32 [00:01<00:14,  2.10it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.92it/s]31_v proxy err 0.011863728053867817 tr(WHW.T) 1268.2034912109375
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:01<00:12,  2.39it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.92it/s]  3%|▎         | 1/32 [00:00<00:19,  1.58it/s] 50%|█████     | 16/32 [00:05<00:05,  2.91it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.54it/s]  6%|▋         | 2/32 [00:00<00:13,  2.16it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.91it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.64it/s]  9%|▉         | 3/32 [00:01<00:11,  2.45it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.92it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.69it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.61it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.91it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.73it/s] 16%|█▌        | 5/32 [00:02<00:09,  2.70it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.90it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.77it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.76it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.91it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.81it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.93it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.80it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.83it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.94it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.82it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.93it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.84it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.80it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.92it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.85it/s] 41%|████      | 13/32 [00:04<00:06,  2.81it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.92it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.85it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.83it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.91it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.86it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.84it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.90it/s] 41%|████      | 13/32 [00:04<00:06,  2.87it/s] 50%|█████     | 16/32 [00:05<00:05,  2.84it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.89it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.87it/s]I0318 06:59:28.919160 1129876 finetune.py:45] layer 28_q initial loss 0.49429962038993835
W0318 06:59:28.919439 1129876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.90it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.88it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.82it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.90it/s] 50%|█████     | 16/32 [00:05<00:05,  2.88it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.82it/s]W0318 06:59:29.800168 1129876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:11<00:00,  2.85it/s]100%|██████████| 32/32 [00:11<00:00,  2.83it/s]
 53%|█████▎    | 17/32 [00:06<00:05,  2.88it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.79it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.87it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.80it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.88it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.80it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.89it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.82it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.90it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.83it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.90it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.90it/s]28_q proxy err 0.004140699747949839 tr(WHW.T) 7651.126953125
  0%|          | 0/32 [00:00<?, ?it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.85it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.89it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.85it/s]  3%|▎         | 1/32 [00:00<00:19,  1.62it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.89it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.83it/s]  6%|▋         | 2/32 [00:00<00:13,  2.19it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.88it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.83it/s]  9%|▉         | 3/32 [00:01<00:11,  2.47it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.88it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.81it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.61it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.90it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.82it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.71it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]
 19%|█▉        | 6/32 [00:02<00:09,  2.78it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.90it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.83it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.91it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  2.92it/s]100%|██████████| 32/32 [00:11<00:00,  2.82it/s]
 28%|██▊       | 9/32 [00:03<00:07,  2.89it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.90it/s]I0318 06:59:36.031952 1130431 finetune.py:45] layer 29_q initial loss 0.7136731147766113
W0318 06:59:36.032171 1130431 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:04<00:07,  2.89it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.90it/s] 41%|████      | 13/32 [00:04<00:06,  2.92it/s]W0318 06:59:36.921437 1130431 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:05<00:06,  2.93it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.94it/s] 50%|█████     | 16/32 [00:05<00:05,  2.96it/s]29_q proxy err 0.003847025567665696 tr(WHW.T) 7227.0009765625
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.96it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.95it/s]  3%|▎         | 1/32 [00:00<00:18,  1.65it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.95it/s]  6%|▋         | 2/32 [00:00<00:13,  2.22it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.93it/s]  9%|▉         | 3/32 [00:01<00:11,  2.50it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.94it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.62it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.94it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.73it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.95it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.78it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.95it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.84it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.96it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.87it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.96it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.86it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.97it/s]I0318 06:59:41.510242 1130615 finetune.py:45] layer 30_q initial loss 278.42333984375
W0318 06:59:41.510721 1130615 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0318 06:59:41.629011 1131058 finetune.py:45] layer 31_q initial loss 31.617877960205078
W0318 06:59:41.629255 1131058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:03<00:07,  2.88it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.97it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.89it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.96it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.90it/s]W0318 06:59:42.469567 1131058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:10<00:00,  2.94it/s] 41%|████      | 13/32 [00:04<00:06,  2.89it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.96it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.90it/s]100%|██████████| 32/32 [00:11<00:00,  2.97it/s]100%|██████████| 32/32 [00:11<00:00,  2.87it/s]
31_q proxy err 0.0028695319779217243 tr(WHW.T) 6858.09130859375
  0%|          | 0/32 [00:00<?, ?it/s]W0318 06:59:43.475810 1130615 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 15/32 [00:05<00:05,  2.91it/s] 50%|█████     | 16/32 [00:05<00:05,  2.91it/s]  3%|▎         | 1/32 [00:00<00:18,  1.66it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.90it/s]  6%|▋         | 2/32 [00:00<00:13,  2.24it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.91it/s]  9%|▉         | 3/32 [00:01<00:11,  2.51it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.91it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.66it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.90it/s]30_q proxy err 0.0037847240455448627 tr(WHW.T) 7815.9453125
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.75it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.91it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.81it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.91it/s]  3%|▎         | 1/32 [00:00<00:19,  1.58it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.85it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.91it/s]  6%|▋         | 2/32 [00:00<00:14,  2.14it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.87it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.91it/s]  9%|▉         | 3/32 [00:01<00:12,  2.41it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.86it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.91it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.53it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.87it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.91it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.59it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.88it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.91it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.66it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.89it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.91it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.71it/s] 41%|████      | 13/32 [00:04<00:06,  2.90it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.91it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.91it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.91it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.81it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.92it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.92it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.88it/s] 50%|█████     | 16/32 [00:05<00:05,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.92it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]
 34%|███▍      | 11/32 [00:04<00:07,  2.86it/s]I0318 06:59:49.554333 1129876 finetune.py:45] layer 28_k initial loss 0.4809039533138275
W0318 06:59:49.554731 1129876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 53%|█████▎    | 17/32 [00:06<00:05,  2.91it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.91it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.91it/s] 41%|████      | 13/32 [00:04<00:06,  2.95it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.94it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.97it/s]W0318 06:59:50.549064 1129876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 62%|██████▎   | 20/32 [00:07<00:04,  2.95it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.99it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.95it/s] 50%|█████     | 16/32 [00:05<00:05,  3.01it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.00it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.03it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.98it/s]28_k proxy err 0.003078424371778965 tr(WHW.T) 10544.826171875
  0%|          | 0/32 [00:00<?, ?it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.02it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.97it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.02it/s]  3%|▎         | 1/32 [00:00<00:18,  1.69it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.95it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.03it/s]  6%|▋         | 2/32 [00:00<00:13,  2.28it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.93it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.04it/s]  9%|▉         | 3/32 [00:01<00:11,  2.54it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.93it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.05it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.68it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.92it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.06it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.74it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.92it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.05it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.81it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.92it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.99it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.85it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.92it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.01it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.86it/s]
 84%|████████▍ | 27/32 [00:09<00:01,  3.03it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.92it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.05it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.95it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.05it/s]I0318 06:59:55.522996 1130431 finetune.py:45] layer 29_k initial loss 0.7032678127288818
W0318 06:59:55.523328 1130431 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:03<00:07,  2.97it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.03it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.97it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.04it/s] 41%|████      | 13/32 [00:04<00:06,  3.03it/s]100%|██████████| 32/32 [00:11<00:00,  3.04it/s]100%|██████████| 32/32 [00:11<00:00,  2.89it/s]
W0318 06:59:56.396499 1130431 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:04<00:05,  3.04it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.03it/s] 50%|█████     | 16/32 [00:05<00:05,  3.01it/s]29_k proxy err 0.002682557562366128 tr(WHW.T) 10558.609375
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.00it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.99it/s]  3%|▎         | 1/32 [00:00<00:18,  1.70it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.96it/s]  6%|▋         | 2/32 [00:00<00:13,  2.25it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.96it/s]  9%|▉         | 3/32 [00:01<00:11,  2.52it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.97it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.70it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.97it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.76it/s] 72%|███████▏  | 23/32 [00:07<00:03,  2.97it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.80it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.98it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.86it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.99it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.89it/s] 81%|████████▏ | 26/32 [00:08<00:02,  3.00it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.92it/s]I0318 07:00:00.922186 1131058 finetune.py:45] layer 31_k initial loss 30.711841583251953
W0318 07:00:00.922360 1131058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 84%|████████▍ | 27/32 [00:09<00:01,  2.99it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.96it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.04it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.96it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.07it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.99it/s]W0318 07:00:01.791693 1131058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:10<00:00,  3.07it/s] 41%|████      | 13/32 [00:04<00:06,  2.98it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.05it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.01it/s]100%|██████████| 32/32 [00:10<00:00,  3.06it/s]100%|██████████| 32/32 [00:10<00:00,  2.93it/s]
 47%|████▋     | 15/32 [00:05<00:05,  3.03it/s]31_k proxy err 0.0020678124856203794 tr(WHW.T) 10233.677734375
  0%|          | 0/32 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:05<00:05,  3.04it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.04it/s]  3%|▎         | 1/32 [00:00<00:18,  1.67it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.04it/s]  6%|▋         | 2/32 [00:00<00:13,  2.26it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.02it/s]  9%|▉         | 3/32 [00:01<00:11,  2.53it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.02it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.67it/s]I0318 07:00:04.600079 1130615 finetune.py:45] layer 30_k initial loss 267.67120361328125
W0318 07:00:04.601497 1130615 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 66%|██████▌   | 21/32 [00:07<00:03,  3.02it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.76it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.99it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.81it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.00it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.86it/s]W0318 07:00:05.681007 1130615 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:08<00:02,  2.99it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.88it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.97it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.90it/s] 81%|████████▏ | 26/32 [00:08<00:02,  2.98it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.92it/s]30_k proxy err 0.0028919659089297056 tr(WHW.T) 10521.625
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.00it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.93it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.00it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.93it/s]  3%|▎         | 1/32 [00:00<00:17,  1.77it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.01it/s] 41%|████      | 13/32 [00:04<00:06,  2.92it/s]  6%|▋         | 2/32 [00:00<00:12,  2.37it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.00it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.90it/s]  9%|▉         | 3/32 [00:01<00:11,  2.63it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.99it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.91it/s] 12%|█▎        | 4/32 [00:01<00:09,  2.80it/s]100%|██████████| 32/32 [00:10<00:00,  3.01it/s]100%|██████████| 32/32 [00:10<00:00,  2.92it/s]
 50%|█████     | 16/32 [00:05<00:05,  2.89it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.91it/s] 19%|█▉        | 6/32 [00:02<00:08,  2.97it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.88it/s]I0318 07:00:09.141120 1129876 finetune.py:45] layer 28_o initial loss 0.48936688899993896
W0318 07:00:09.141467 1129876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:02<00:08,  3.01it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.88it/s] 25%|██▌       | 8/32 [00:02<00:07,  3.06it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.92it/s] 28%|██▊       | 9/32 [00:03<00:07,  3.08it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.96it/s]W0318 07:00:09.977330 1129876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:03<00:07,  3.09it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.95it/s] 34%|███▍      | 11/32 [00:03<00:06,  3.10it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.94it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.11it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.95it/s] 41%|████      | 13/32 [00:04<00:06,  3.12it/s]28_o proxy err 0.008381053805351257 tr(WHW.T) 194.82411193847656
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.99it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.13it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.02it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.13it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.97it/s] 50%|█████     | 16/32 [00:05<00:05,  3.11it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.95it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.12it/s]  3%|▎         | 1/32 [00:01<00:38,  1.26s/it] 88%|████████▊ | 28/32 [00:09<00:01,  2.93it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.13it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.91it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.12it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.92it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.13it/s]  6%|▋         | 2/32 [00:02<00:33,  1.11s/it] 97%|█████████▋| 31/32 [00:10<00:00,  2.92it/s] 66%|██████▌   | 21/32 [00:06<00:03,  3.14it/s]100%|██████████| 32/32 [00:11<00:00,  2.90it/s]100%|██████████| 32/32 [00:11<00:00,  2.87it/s]
 69%|██████▉   | 22/32 [00:07<00:03,  3.13it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.13it/s]  9%|▉         | 3/32 [00:03<00:30,  1.06s/it] 75%|███████▌  | 24/32 [00:07<00:02,  3.13it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.13it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.13it/s] 12%|█▎        | 4/32 [00:04<00:29,  1.05s/it] 84%|████████▍ | 27/32 [00:08<00:01,  3.10it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.12it/s]I0318 07:00:16.207960 1130431 finetune.py:45] layer 29_o initial loss 0.6638734340667725
W0318 07:00:16.208356 1130431 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:09<00:00,  3.12it/s] 16%|█▌        | 5/32 [00:05<00:28,  1.04s/it] 94%|█████████▍| 30/32 [00:09<00:00,  3.12it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.12it/s]100%|██████████| 32/32 [00:10<00:00,  3.11it/s]100%|██████████| 32/32 [00:10<00:00,  3.05it/s]
 19%|█▉        | 6/32 [00:06<00:26,  1.04s/it]W0318 07:00:17.552374 1130431 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:07<00:25,  1.03s/it]29_o proxy err 0.008351128548383713 tr(WHW.T) 207.90545654296875
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:08<00:24,  1.02s/it]  3%|▎         | 1/32 [00:01<00:40,  1.30s/it]I0318 07:00:20.483722 1131058 finetune.py:45] layer 31_o initial loss 30.562397003173828
W0318 07:00:20.484019 1131058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:09<00:23,  1.01s/it]  6%|▋         | 2/32 [00:02<00:34,  1.16s/it]W0318 07:00:21.307883 1131058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:10<00:21,  1.00it/s]  9%|▉         | 3/32 [00:03<00:32,  1.12s/it]31_o proxy err 0.00390979228541255 tr(WHW.T) 457.7950744628906
  0%|          | 0/32 [00:00<?, ?it/s] 34%|███▍      | 11/32 [00:11<00:20,  1.01it/s] 12%|█▎        | 4/32 [00:04<00:30,  1.09s/it] 38%|███▊      | 12/32 [00:12<00:19,  1.03it/s]I0318 07:00:23.642622 1130615 finetune.py:45] layer 30_o initial loss 253.8004150390625
W0318 07:00:23.642893 1130615 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:37,  1.20s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.07s/it] 41%|████      | 13/32 [00:13<00:18,  1.04it/s]W0318 07:00:24.419500 1130615 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:02<00:31,  1.05s/it] 44%|████▍     | 14/32 [00:14<00:17,  1.04it/s] 19%|█▉        | 6/32 [00:06<00:27,  1.06s/it]  9%|▉         | 3/32 [00:03<00:29,  1.02s/it]30_o proxy err 0.007548580411821604 tr(WHW.T) 251.9690704345703
  0%|          | 0/32 [00:00<?, ?it/s] 47%|████▋     | 15/32 [00:15<00:16,  1.04it/s] 22%|██▏       | 7/32 [00:07<00:26,  1.04s/it] 12%|█▎        | 4/32 [00:04<00:27,  1.00it/s]  3%|▎         | 1/32 [00:01<00:37,  1.22s/it] 50%|█████     | 16/32 [00:16<00:15,  1.05it/s] 25%|██▌       | 8/32 [00:08<00:24,  1.04s/it] 16%|█▌        | 5/32 [00:05<00:26,  1.02it/s]  6%|▋         | 2/32 [00:02<00:32,  1.07s/it] 53%|█████▎    | 17/32 [00:16<00:14,  1.05it/s] 28%|██▊       | 9/32 [00:09<00:23,  1.03s/it] 19%|█▉        | 6/32 [00:06<00:25,  1.03it/s]  9%|▉         | 3/32 [00:03<00:29,  1.02s/it] 56%|█████▋    | 18/32 [00:17<00:13,  1.05it/s] 31%|███▏      | 10/32 [00:10<00:22,  1.02s/it] 22%|██▏       | 7/32 [00:06<00:24,  1.03it/s] 12%|█▎        | 4/32 [00:04<00:27,  1.00it/s] 59%|█████▉    | 19/32 [00:18<00:12,  1.05it/s] 25%|██▌       | 8/32 [00:07<00:23,  1.04it/s] 34%|███▍      | 11/32 [00:11<00:21,  1.02s/it] 16%|█▌        | 5/32 [00:05<00:26,  1.02it/s] 62%|██████▎   | 20/32 [00:19<00:11,  1.06it/s] 28%|██▊       | 9/32 [00:08<00:22,  1.04it/s] 38%|███▊      | 12/32 [00:12<00:20,  1.02s/it] 19%|█▉        | 6/32 [00:06<00:25,  1.03it/s] 66%|██████▌   | 21/32 [00:20<00:10,  1.05it/s] 31%|███▏      | 10/32 [00:09<00:21,  1.04it/s] 41%|████      | 13/32 [00:13<00:19,  1.02s/it] 22%|██▏       | 7/32 [00:06<00:24,  1.03it/s] 69%|██████▉   | 22/32 [00:21<00:09,  1.06it/s] 34%|███▍      | 11/32 [00:10<00:20,  1.04it/s] 44%|████▍     | 14/32 [00:14<00:18,  1.02s/it] 25%|██▌       | 8/32 [00:07<00:23,  1.03it/s] 72%|███████▏  | 23/32 [00:22<00:08,  1.06it/s] 38%|███▊      | 12/32 [00:11<00:19,  1.04it/s] 47%|████▋     | 15/32 [00:15<00:17,  1.02s/it] 28%|██▊       | 9/32 [00:08<00:22,  1.03it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.06it/s] 41%|████      | 13/32 [00:12<00:18,  1.04it/s] 50%|█████     | 16/32 [00:16<00:16,  1.02s/it] 31%|███▏      | 10/32 [00:09<00:21,  1.03it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.05it/s] 44%|████▍     | 14/32 [00:13<00:17,  1.03it/s] 34%|███▍      | 11/32 [00:10<00:20,  1.03it/s] 53%|█████▎    | 17/32 [00:17<00:15,  1.02s/it] 81%|████████▏ | 26/32 [00:25<00:05,  1.05it/s] 47%|████▋     | 15/32 [00:14<00:16,  1.03it/s] 38%|███▊      | 12/32 [00:11<00:19,  1.03it/s] 56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it] 84%|████████▍ | 27/32 [00:26<00:04,  1.05it/s] 50%|█████     | 16/32 [00:15<00:15,  1.03it/s] 41%|████      | 13/32 [00:12<00:18,  1.02it/s] 59%|█████▉    | 19/32 [00:19<00:13,  1.03s/it] 88%|████████▊ | 28/32 [00:27<00:03,  1.04it/s] 53%|█████▎    | 17/32 [00:16<00:14,  1.02it/s] 44%|████▍     | 14/32 [00:13<00:17,  1.00it/s] 91%|█████████ | 29/32 [00:28<00:02,  1.03it/s] 62%|██████▎   | 20/32 [00:20<00:12,  1.04s/it] 56%|█████▋    | 18/32 [00:17<00:13,  1.02it/s] 47%|████▋     | 15/32 [00:14<00:16,  1.00it/s] 94%|█████████▍| 30/32 [00:29<00:01,  1.02it/s] 66%|██████▌   | 21/32 [00:21<00:11,  1.04s/it] 59%|█████▉    | 19/32 [00:18<00:12,  1.01it/s] 50%|█████     | 16/32 [00:15<00:15,  1.00it/s] 97%|█████████▋| 31/32 [00:30<00:00,  1.01it/s] 69%|██████▉   | 22/32 [00:23<00:10,  1.05s/it] 62%|██████▎   | 20/32 [00:19<00:11,  1.00it/s] 53%|█████▎    | 17/32 [00:16<00:15,  1.01s/it]100%|██████████| 32/32 [00:31<00:00,  1.01it/s]100%|██████████| 32/32 [00:31<00:00,  1.02it/s]
 72%|███████▏  | 23/32 [00:24<00:09,  1.06s/it] 66%|██████▌   | 21/32 [00:20<00:11,  1.00s/it] 56%|█████▋    | 18/32 [00:17<00:14,  1.01s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.06s/it] 69%|██████▉   | 22/32 [00:21<00:10,  1.00s/it] 59%|█████▉    | 19/32 [00:18<00:12,  1.00it/s] 78%|███████▊  | 25/32 [00:26<00:07,  1.05s/it] 72%|███████▏  | 23/32 [00:22<00:09,  1.01s/it] 62%|██████▎   | 20/32 [00:19<00:12,  1.01s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.05s/it] 75%|███████▌  | 24/32 [00:23<00:07,  1.00it/s] 66%|██████▌   | 21/32 [00:20<00:11,  1.00s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.05s/it] 78%|███████▊  | 25/32 [00:24<00:06,  1.00it/s] 69%|██████▉   | 22/32 [00:21<00:10,  1.01s/it] 81%|████████▏ | 26/32 [00:25<00:06,  1.01s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.06s/it] 72%|███████▏  | 23/32 [00:22<00:09,  1.01s/it] 84%|████████▍ | 27/32 [00:26<00:05,  1.00s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.05s/it]W0318 07:00:49.199000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.199000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.199000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.199000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.199000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.199000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.200000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.229000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.229000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.229000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.229000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.229000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.245000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.245000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.245000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.245000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.245000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.407000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.407000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.407000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.407000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.407000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:23<00:08,  1.01s/it]W0318 07:00:49.625000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.625000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.626000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.626000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.626000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.626000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.626000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.646000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.647000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.647000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.647000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.647000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.709000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.709000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.709000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.709000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:00:49.709000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:27<00:03,  1.01it/s] 94%|█████████▍| 30/32 [00:31<00:02,  1.04s/it] 78%|███████▊  | 25/32 [00:24<00:06,  1.01it/s]W0318 07:00:50.555000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 07:00:50.854000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:00:50.854000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:00:50.854000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 07:00:50.854000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:00:50.854000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 07:00:50.854000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:00:50.854000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:00:50.875000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:00:50.875000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:00:50.875000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:00:50.875000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:00:50.875000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:28<00:02,  1.02it/s]W0318 07:00:51.129000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:00:51.129000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:00:51.129000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:00:51.129000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:00:51.129000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:32<00:01,  1.02s/it]W0318 07:00:51.380000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:25<00:05,  1.02it/s] 94%|█████████▍| 30/32 [00:29<00:01,  1.02it/s]100%|██████████| 32/32 [00:33<00:00,  1.03s/it]100%|██████████| 32/32 [00:33<00:00,  1.04s/it]
 84%|████████▍ | 27/32 [00:26<00:05,  1.01s/it] 97%|█████████▋| 31/32 [00:30<00:01,  1.02s/it] 88%|████████▊ | 28/32 [00:28<00:04,  1.04s/it]100%|██████████| 32/32 [00:31<00:00,  1.03s/it]100%|██████████| 32/32 [00:31<00:00,  1.01it/s]
 91%|█████████ | 29/32 [00:29<00:03,  1.05s/it] 94%|█████████▍| 30/32 [00:30<00:02,  1.05s/it] 97%|█████████▋| 31/32 [00:31<00:01,  1.05s/it]100%|██████████| 32/32 [00:32<00:00,  1.04s/it]100%|██████████| 32/32 [00:32<00:00,  1.01s/it]
I0318 07:00:58.288633 1129876 finetune.py:45] layer 28_up initial loss 0.4769783914089203
W0318 07:00:58.289002 1129876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 07:00:59.244617 1129876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0318 07:00:59.426000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.426000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.426000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.427000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.427000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.427000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.427000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.455000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.455000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.455000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.455000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.455000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.470000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.470000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.470000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.470000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.470000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.623000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.623000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.623000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.623000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.623000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.856000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.856000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.856000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.856000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.856000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.856000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.856000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.877000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.877000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.878000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.878000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.878000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.939000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.940000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.940000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.940000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:00:59.940000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
28_up proxy err 0.009223250672221184 tr(WHW.T) 4661.265625
  0%|          | 0/32 [00:00<?, ?it/s]W0318 07:01:00.822000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.128000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.129000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.129000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.129000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.129000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.129000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.129000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.150000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.150000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.150000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.150000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.150000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.306000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.307000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.307000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.307000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.307000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.307000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.307000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.336000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.336000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.336000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.336000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.336000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.351000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.351000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.351000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.351000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.352000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.395000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.395000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.395000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.395000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.395000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.504000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.504000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.504000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.505000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.505000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.643000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.725000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.725000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.725000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.725000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.725000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.725000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.725000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.745000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.745000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.745000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.745000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.746000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.812000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.812000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.812000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.812000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:01.812000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:38,  1.25s/it]W0318 07:01:02.652000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:02<00:32,  1.07s/it]W0318 07:01:02.951000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:02.952000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:02.952000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:01:02.952000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 07:01:02.952000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 07:01:02.952000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:02.952000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:02.974000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:02.974000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:02.974000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:02.974000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:01:02.974000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:03.231000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:03.232000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:03.232000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:03.232000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:01:03.232000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:03.501000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:03<00:29,  1.03s/it]W0318 07:01:04.405000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.406000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.406000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.406000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.406000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.406000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.406000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.436000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.436000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.436000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.436000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.436000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.452000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.453000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.453000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.453000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.453000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.614000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.614000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.614000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.614000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.614000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:04<00:28,  1.02s/it]W0318 07:01:04.847000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.847000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.847000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.847000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.847000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.848000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.848000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.869000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.870000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.870000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.870000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.870000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.937000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.937000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.937000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.937000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:01:04.937000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:05<00:27,  1.01s/it]W0318 07:01:05.829000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0318 07:01:06.147000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:06.147000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:06.147000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:01:06.147000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0318 07:01:06.147000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:06.148000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:06.148000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0318 07:01:06.168000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:06.169000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:06.169000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:06.169000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:06.169000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:01:06.433000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:01:06.433000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:01:06.433000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:01:06.433000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:01:06.433000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:01:06.703000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:06<00:26,  1.01s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.01s/it]I0318 07:01:08.536960 1130431 finetune.py:45] layer 29_up initial loss 0.6455608010292053
W0318 07:01:08.537310 1130431 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:08<00:24,  1.01s/it]W0318 07:01:09.651163 1130431 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:09<00:23,  1.03s/it]I0318 07:01:10.249978 1131058 finetune.py:45] layer 31_up initial loss 30.256650924682617
W0318 07:01:10.250422 1131058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:10<00:22,  1.04s/it]29_up proxy err 0.007379317190498114 tr(WHW.T) 6070.048828125
  0%|          | 0/32 [00:00<?, ?it/s]W0318 07:01:11.470237 1131058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:11<00:21,  1.05s/it]  3%|▎         | 1/32 [00:01<00:42,  1.36s/it]31_up proxy err 0.0025451662950217724 tr(WHW.T) 14563.8876953125
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:12<00:20,  1.05s/it]I0318 07:01:13.281828 1130615 finetune.py:45] layer 30_up initial loss 243.17410278320312
W0318 07:01:13.282117 1130615 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:02<00:35,  1.19s/it]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it] 41%|████      | 13/32 [00:13<00:19,  1.05s/it]W0318 07:01:14.112768 1130615 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:03<00:33,  1.14s/it]  6%|▋         | 2/32 [00:02<00:32,  1.09s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.04s/it]30_up proxy err 0.004510719794780016 tr(WHW.T) 10016.375
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:04<00:30,  1.10s/it]  9%|▉         | 3/32 [00:03<00:29,  1.03s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.03s/it] 16%|█▌        | 5/32 [00:05<00:29,  1.07s/it]  3%|▎         | 1/32 [00:01<00:38,  1.25s/it] 12%|█▎        | 4/32 [00:04<00:28,  1.01s/it] 50%|█████     | 16/32 [00:16<00:16,  1.04s/it]  6%|▋         | 2/32 [00:02<00:34,  1.14s/it] 19%|█▉        | 6/32 [00:06<00:28,  1.09s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.02s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.04s/it]  9%|▉         | 3/32 [00:03<00:31,  1.08s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.08s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.00s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.04s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.06s/it] 22%|██▏       | 7/32 [00:07<00:24,  1.02it/s] 59%|█████▉    | 19/32 [00:19<00:13,  1.00s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.01s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.04s/it] 25%|██▌       | 8/32 [00:08<00:23,  1.03it/s] 62%|██████▎   | 20/32 [00:20<00:12,  1.00s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.01s/it] 28%|██▊       | 9/32 [00:09<00:22,  1.03it/s] 31%|███▏      | 10/32 [00:10<00:22,  1.04s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.01s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.02s/it] 31%|███▏      | 10/32 [00:10<00:21,  1.01it/s] 34%|███▍      | 11/32 [00:11<00:22,  1.06s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.01s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.01s/it] 34%|███▍      | 11/32 [00:11<00:20,  1.02it/s] 38%|███▊      | 12/32 [00:12<00:20,  1.05s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.01s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.01s/it] 38%|███▊      | 12/32 [00:12<00:19,  1.01it/s] 41%|████      | 13/32 [00:13<00:19,  1.04s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.01s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.02s/it] 41%|████      | 13/32 [00:13<00:18,  1.00it/s] 44%|████▍     | 14/32 [00:15<00:19,  1.06s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.02s/it] 44%|████▍     | 14/32 [00:14<00:17,  1.01it/s] 34%|███▍      | 11/32 [00:11<00:21,  1.01s/it] 47%|████▋     | 15/32 [00:16<00:17,  1.05s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.01s/it] 47%|████▋     | 15/32 [00:15<00:16,  1.01it/s] 38%|███▊      | 12/32 [00:12<00:20,  1.01s/it] 50%|█████     | 16/32 [00:17<00:16,  1.05s/it] 84%|████████▍ | 27/32 [00:27<00:05,  1.01s/it] 50%|█████     | 16/32 [00:16<00:16,  1.00s/it] 41%|████      | 13/32 [00:13<00:19,  1.02s/it] 88%|████████▊ | 28/32 [00:28<00:04,  1.02s/it] 53%|█████▎    | 17/32 [00:18<00:15,  1.06s/it] 53%|█████▎    | 17/32 [00:17<00:14,  1.01it/s] 44%|████▍     | 14/32 [00:14<00:18,  1.01s/it] 91%|█████████ | 29/32 [00:29<00:03,  1.02s/it] 56%|█████▋    | 18/32 [00:19<00:14,  1.06s/it] 56%|█████▋    | 18/32 [00:18<00:13,  1.01it/s] 47%|████▋     | 15/32 [00:15<00:17,  1.01s/it] 94%|█████████▍| 30/32 [00:30<00:02,  1.02s/it] 59%|█████▉    | 19/32 [00:20<00:13,  1.06s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.00s/it] 50%|█████     | 16/32 [00:16<00:16,  1.02s/it] 97%|█████████▋| 31/32 [00:31<00:01,  1.02s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.06s/it] 62%|██████▎   | 20/32 [00:20<00:11,  1.00it/s] 53%|█████▎    | 17/32 [00:17<00:15,  1.02s/it]100%|██████████| 32/32 [00:32<00:00,  1.02s/it]100%|██████████| 32/32 [00:32<00:00,  1.02s/it]
 66%|██████▌   | 21/32 [00:22<00:11,  1.05s/it] 66%|██████▌   | 21/32 [00:21<00:10,  1.01it/s] 56%|█████▋    | 18/32 [00:18<00:14,  1.01s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.03s/it] 69%|██████▉   | 22/32 [00:22<00:09,  1.00it/s] 59%|█████▉    | 19/32 [00:19<00:12,  1.01it/s] 72%|███████▏  | 23/32 [00:24<00:09,  1.03s/it] 72%|███████▏  | 23/32 [00:22<00:08,  1.01it/s] 62%|██████▎   | 20/32 [00:20<00:11,  1.01it/s] 75%|███████▌  | 24/32 [00:25<00:08,  1.05s/it] 75%|███████▌  | 24/32 [00:24<00:07,  1.00it/s] 66%|██████▌   | 21/32 [00:21<00:11,  1.01s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.06s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.01s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.02s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.07s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.02s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.03s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.07s/it] 84%|████████▍ | 27/32 [00:27<00:05,  1.02s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.04s/it] 88%|████████▊ | 28/32 [00:28<00:04,  1.02s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.07s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.03s/it] 91%|█████████ | 29/32 [00:29<00:02,  1.00it/s] 91%|█████████ | 29/32 [00:30<00:03,  1.07s/it]I0318 07:01:42.012686 1129876 finetune.py:45] layer 28_gate initial loss 0.4685996174812317
W0318 07:01:42.013225 1129876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:26<00:06,  1.03s/it] 94%|█████████▍| 30/32 [00:30<00:01,  1.02it/s]W0318 07:01:42.843863 1129876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:31<00:02,  1.06s/it] 84%|████████▍ | 27/32 [00:27<00:05,  1.01s/it] 97%|█████████▋| 31/32 [00:31<00:00,  1.01it/s] 88%|████████▊ | 28/32 [00:28<00:04,  1.02s/it] 97%|█████████▋| 31/32 [00:33<00:01,  1.07s/it]100%|██████████| 32/32 [00:31<00:00,  1.02it/s]100%|██████████| 32/32 [00:31<00:00,  1.00it/s]
 91%|█████████ | 29/32 [00:29<00:03,  1.01s/it]100%|██████████| 32/32 [00:34<00:00,  1.06s/it]100%|██████████| 32/32 [00:34<00:00,  1.06s/it]
28_gate proxy err 0.007327998988330364 tr(WHW.T) 6547.48095703125
  0%|          | 0/86 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:30<00:02,  1.02s/it]  1%|          | 1/86 [00:00<00:51,  1.64it/s]  2%|▏         | 2/86 [00:00<00:38,  2.21it/s] 97%|█████████▋| 31/32 [00:31<00:01,  1.03s/it]  3%|▎         | 3/86 [00:01<00:34,  2.43it/s]  5%|▍         | 4/86 [00:01<00:31,  2.60it/s]  6%|▌         | 5/86 [00:02<00:29,  2.70it/s]100%|██████████| 32/32 [00:32<00:00,  1.03s/it]100%|██████████| 32/32 [00:32<00:00,  1.02s/it]
  7%|▋         | 6/86 [00:02<00:29,  2.75it/s]  8%|▊         | 7/86 [00:02<00:28,  2.79it/s]  9%|▉         | 8/86 [00:03<00:27,  2.81it/s] 10%|█         | 9/86 [00:03<00:27,  2.84it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.85it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.86it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.88it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.86it/s] 16%|█▋        | 14/86 [00:05<00:24,  2.88it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.89it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.89it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.87it/s] 21%|██        | 18/86 [00:06<00:23,  2.88it/s]I0318 07:01:52.702205 1131058 finetune.py:45] layer 31_gate initial loss 29.91384506225586
W0318 07:01:52.702759 1131058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0318 07:01:52.890184 1130431 finetune.py:45] layer 29_gate initial loss 0.6314725279808044
W0318 07:01:52.890526 1130431 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 19/86 [00:06<00:23,  2.88it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.89it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.90it/s]W0318 07:01:53.783137 1130431 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 26%|██▌       | 22/86 [00:07<00:22,  2.90it/s]W0318 07:01:54.055286 1131058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 27%|██▋       | 23/86 [00:08<00:21,  2.89it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.88it/s] 29%|██▉       | 25/86 [00:08<00:21,  2.89it/s] 30%|███       | 26/86 [00:09<00:20,  2.89it/s] 31%|███▏      | 27/86 [00:09<00:21,  2.78it/s]I0318 07:01:55.893493 1130615 finetune.py:45] layer 30_gate initial loss 231.83004760742188
W0318 07:01:55.893792 1130615 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 33%|███▎      | 28/86 [00:10<00:21,  2.75it/s] 34%|███▎      | 29/86 [00:10<00:20,  2.76it/s]W0318 07:01:56.737630 1130615 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 35%|███▍      | 30/86 [00:10<00:20,  2.79it/s]29_gate proxy err 0.006696565076708794 tr(WHW.T) 7369.61328125
  0%|          | 0/86 [00:00<?, ?it/s] 36%|███▌      | 31/86 [00:11<00:19,  2.81it/s] 37%|███▋      | 32/86 [00:11<00:19,  2.84it/s]  1%|          | 1/86 [00:00<00:52,  1.61it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.86it/s]31_gate proxy err 0.0028080183546990156 tr(WHW.T) 14836.291015625
  0%|          | 0/86 [00:00<?, ?it/s]  2%|▏         | 2/86 [00:00<00:38,  2.17it/s] 40%|███▉      | 34/86 [00:12<00:18,  2.87it/s]  3%|▎         | 3/86 [00:01<00:34,  2.44it/s]  1%|          | 1/86 [00:00<00:51,  1.65it/s] 41%|████      | 35/86 [00:12<00:17,  2.89it/s]  5%|▍         | 4/86 [00:01<00:31,  2.57it/s]  2%|▏         | 2/86 [00:00<00:38,  2.19it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.88it/s]  6%|▌         | 5/86 [00:02<00:30,  2.63it/s]  3%|▎         | 3/86 [00:01<00:34,  2.42it/s] 43%|████▎     | 37/86 [00:13<00:17,  2.88it/s]  7%|▋         | 6/86 [00:02<00:29,  2.71it/s]30_gate proxy err 0.004576165694743395 tr(WHW.T) 11001.1171875
  0%|          | 0/86 [00:00<?, ?it/s]  5%|▍         | 4/86 [00:01<00:31,  2.58it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.91it/s]  8%|▊         | 7/86 [00:02<00:28,  2.76it/s]  6%|▌         | 5/86 [00:02<00:30,  2.66it/s] 45%|████▌     | 39/86 [00:13<00:16,  2.92it/s]  1%|          | 1/86 [00:00<00:49,  1.71it/s]  9%|▉         | 8/86 [00:03<00:28,  2.75it/s] 47%|████▋     | 40/86 [00:14<00:15,  2.89it/s]  7%|▋         | 6/86 [00:02<00:30,  2.66it/s]  2%|▏         | 2/86 [00:00<00:37,  2.25it/s] 10%|█         | 9/86 [00:03<00:27,  2.78it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.92it/s]  8%|▊         | 7/86 [00:02<00:28,  2.73it/s]  3%|▎         | 3/86 [00:01<00:32,  2.55it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.82it/s] 49%|████▉     | 42/86 [00:14<00:14,  2.95it/s]  9%|▉         | 8/86 [00:03<00:28,  2.78it/s]  5%|▍         | 4/86 [00:01<00:30,  2.71it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.85it/s] 50%|█████     | 43/86 [00:15<00:14,  2.98it/s] 10%|█         | 9/86 [00:03<00:27,  2.83it/s]  6%|▌         | 5/86 [00:01<00:28,  2.83it/s] 51%|█████     | 44/86 [00:15<00:14,  3.00it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.87it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.86it/s]  7%|▋         | 6/86 [00:02<00:27,  2.89it/s] 52%|█████▏    | 45/86 [00:15<00:13,  2.98it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.86it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.86it/s]  8%|▊         | 7/86 [00:02<00:26,  2.93it/s] 53%|█████▎    | 46/86 [00:16<00:13,  2.98it/s] 16%|█▋        | 14/86 [00:05<00:25,  2.87it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.88it/s]  9%|▉         | 8/86 [00:02<00:26,  2.95it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.98it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.88it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.89it/s] 10%|█         | 9/86 [00:03<00:26,  2.95it/s] 56%|█████▌    | 48/86 [00:16<00:12,  2.92it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.83it/s] 16%|█▋        | 14/86 [00:05<00:25,  2.86it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.89it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.86it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.78it/s] 17%|█▋        | 15/86 [00:05<00:25,  2.73it/s] 13%|█▎        | 11/86 [00:03<00:26,  2.86it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.85it/s] 21%|██        | 18/86 [00:06<00:24,  2.77it/s] 19%|█▊        | 16/86 [00:05<00:25,  2.74it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.86it/s] 59%|█████▉    | 51/86 [00:17<00:12,  2.85it/s] 22%|██▏       | 19/86 [00:06<00:24,  2.76it/s] 20%|█▉        | 17/86 [00:06<00:25,  2.72it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.85it/s] 60%|██████    | 52/86 [00:18<00:11,  2.85it/s] 23%|██▎       | 20/86 [00:07<00:23,  2.76it/s] 21%|██        | 18/86 [00:06<00:24,  2.73it/s] 16%|█▋        | 14/86 [00:05<00:25,  2.86it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.85it/s] 24%|██▍       | 21/86 [00:07<00:23,  2.77it/s] 22%|██▏       | 19/86 [00:07<00:24,  2.76it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.88it/s] 63%|██████▎   | 54/86 [00:18<00:10,  2.92it/s] 26%|██▌       | 22/86 [00:08<00:22,  2.82it/s] 19%|█▊        | 16/86 [00:05<00:23,  2.93it/s] 23%|██▎       | 20/86 [00:07<00:23,  2.79it/s] 64%|██████▍   | 55/86 [00:19<00:10,  2.94it/s] 27%|██▋       | 23/86 [00:08<00:22,  2.83it/s] 20%|█▉        | 17/86 [00:06<00:23,  2.96it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.83it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.97it/s] 28%|██▊       | 24/86 [00:08<00:22,  2.81it/s] 21%|██        | 18/86 [00:06<00:23,  2.95it/s] 26%|██▌       | 22/86 [00:08<00:22,  2.86it/s] 66%|██████▋   | 57/86 [00:19<00:09,  2.98it/s] 29%|██▉       | 25/86 [00:09<00:21,  2.86it/s] 22%|██▏       | 19/86 [00:06<00:22,  2.99it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.89it/s] 67%|██████▋   | 58/86 [00:20<00:09,  3.02it/s] 30%|███       | 26/86 [00:09<00:20,  2.90it/s] 23%|██▎       | 20/86 [00:07<00:21,  3.02it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.92it/s] 69%|██████▊   | 59/86 [00:20<00:08,  3.05it/s] 24%|██▍       | 21/86 [00:07<00:21,  3.03it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.90it/s] 29%|██▉       | 25/86 [00:09<00:20,  2.94it/s] 70%|██████▉   | 60/86 [00:20<00:08,  3.02it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.98it/s] 33%|███▎      | 28/86 [00:10<00:20,  2.84it/s] 30%|███       | 26/86 [00:09<00:20,  2.90it/s] 71%|███████   | 61/86 [00:21<00:08,  2.98it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.95it/s] 34%|███▎      | 29/86 [00:10<00:20,  2.83it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.89it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.96it/s] 28%|██▊       | 24/86 [00:08<00:20,  2.96it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.84it/s] 33%|███▎      | 28/86 [00:10<00:20,  2.89it/s] 73%|███████▎  | 63/86 [00:21<00:07,  2.95it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.97it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.93it/s] 36%|███▌      | 31/86 [00:11<00:19,  2.83it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.91it/s] 30%|███       | 26/86 [00:09<00:20,  2.89it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.87it/s] 37%|███▋      | 32/86 [00:11<00:19,  2.77it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.89it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.87it/s] 36%|███▌      | 31/86 [00:11<00:19,  2.84it/s] 38%|███▊      | 33/86 [00:11<00:19,  2.76it/s] 77%|███████▋  | 66/86 [00:23<00:06,  2.86it/s] 33%|███▎      | 28/86 [00:09<00:20,  2.87it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.85it/s] 40%|███▉      | 34/86 [00:12<00:18,  2.77it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.87it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.90it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.90it/s] 41%|████      | 35/86 [00:12<00:18,  2.81it/s] 79%|███████▉  | 68/86 [00:23<00:06,  2.93it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.94it/s] 40%|███▉      | 34/86 [00:12<00:17,  2.95it/s] 42%|████▏     | 36/86 [00:13<00:17,  2.83it/s] 80%|████████  | 69/86 [00:24<00:05,  2.96it/s] 36%|███▌      | 31/86 [00:10<00:18,  2.93it/s] 41%|████      | 35/86 [00:12<00:17,  2.93it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.94it/s] 43%|████▎     | 37/86 [00:13<00:17,  2.81it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.91it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.90it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.91it/s] 44%|████▍     | 38/86 [00:13<00:17,  2.80it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.92it/s] 43%|████▎     | 37/86 [00:13<00:16,  2.92it/s] 84%|████████▎ | 72/86 [00:25<00:04,  2.93it/s] 45%|████▌     | 39/86 [00:14<00:16,  2.82it/s] 40%|███▉      | 34/86 [00:11<00:17,  2.95it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.94it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.92it/s] 47%|████▋     | 40/86 [00:14<00:16,  2.80it/s] 41%|████      | 35/86 [00:12<00:17,  2.90it/s] 45%|████▌     | 39/86 [00:13<00:16,  2.90it/s] 86%|████████▌ | 74/86 [00:25<00:04,  2.90it/s] 48%|████▊     | 41/86 [00:14<00:16,  2.77it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.89it/s] 47%|████▋     | 40/86 [00:14<00:16,  2.85it/s] 87%|████████▋ | 75/86 [00:26<00:03,  2.87it/s] 49%|████▉     | 42/86 [00:15<00:15,  2.77it/s] 43%|████▎     | 37/86 [00:12<00:17,  2.87it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.86it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.88it/s] 50%|█████     | 43/86 [00:15<00:15,  2.77it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.89it/s] 49%|████▉     | 42/86 [00:14<00:15,  2.88it/s] 90%|████████▉ | 77/86 [00:26<00:03,  2.91it/s] 51%|█████     | 44/86 [00:15<00:14,  2.80it/s] 45%|████▌     | 39/86 [00:13<00:16,  2.91it/s] 50%|█████     | 43/86 [00:15<00:14,  2.93it/s] 91%|█████████ | 78/86 [00:27<00:02,  2.94it/s] 52%|█████▏    | 45/86 [00:16<00:14,  2.80it/s] 47%|████▋     | 40/86 [00:13<00:15,  2.91it/s] 51%|█████     | 44/86 [00:15<00:14,  2.93it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.92it/s] 53%|█████▎    | 46/86 [00:16<00:14,  2.78it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.90it/s] 52%|█████▏    | 45/86 [00:15<00:14,  2.92it/s] 93%|█████████▎| 80/86 [00:27<00:02,  2.90it/s] 55%|█████▍    | 47/86 [00:16<00:14,  2.78it/s] 49%|████▉     | 42/86 [00:14<00:15,  2.90it/s] 53%|█████▎    | 46/86 [00:16<00:13,  2.95it/s] 94%|█████████▍| 81/86 [00:28<00:01,  2.92it/s] 56%|█████▌    | 48/86 [00:17<00:13,  2.80it/s] 50%|█████     | 43/86 [00:14<00:14,  2.93it/s] 55%|█████▍    | 47/86 [00:16<00:12,  3.01it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.95it/s] 57%|█████▋    | 49/86 [00:17<00:13,  2.84it/s] 51%|█████     | 44/86 [00:15<00:14,  2.96it/s] 56%|█████▌    | 48/86 [00:16<00:12,  3.03it/s] 97%|█████████▋| 83/86 [00:28<00:01,  2.95it/s] 58%|█████▊    | 50/86 [00:18<00:12,  2.79it/s] 52%|█████▏    | 45/86 [00:15<00:14,  2.90it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.97it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.92it/s] 53%|█████▎    | 46/86 [00:15<00:13,  2.87it/s] 59%|█████▉    | 51/86 [00:18<00:12,  2.76it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.94it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.90it/s] 59%|█████▉    | 51/86 [00:17<00:11,  2.93it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.85it/s] 60%|██████    | 52/86 [00:18<00:12,  2.76it/s]100%|██████████| 86/86 [00:29<00:00,  2.89it/s]100%|██████████| 86/86 [00:29<00:00,  2.88it/s]
 60%|██████    | 52/86 [00:18<00:11,  2.96it/s] 56%|█████▌    | 48/86 [00:16<00:13,  2.88it/s] 62%|██████▏   | 53/86 [00:19<00:11,  2.79it/s] 62%|██████▏   | 53/86 [00:18<00:10,  3.01it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.88it/s] 63%|██████▎   | 54/86 [00:19<00:11,  2.78it/s] 63%|██████▎   | 54/86 [00:18<00:10,  2.98it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.86it/s] 64%|██████▍   | 55/86 [00:19<00:11,  2.76it/s] 64%|██████▍   | 55/86 [00:19<00:10,  2.95it/s] 59%|█████▉    | 51/86 [00:17<00:12,  2.85it/s] 65%|██████▌   | 56/86 [00:20<00:10,  2.77it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.99it/s] 60%|██████    | 52/86 [00:18<00:11,  2.91it/s] 66%|██████▋   | 57/86 [00:20<00:10,  2.78it/s] 66%|██████▋   | 57/86 [00:19<00:09,  3.00it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.88it/s] 67%|██████▋   | 58/86 [00:20<00:10,  2.76it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.96it/s] 63%|██████▎   | 54/86 [00:18<00:11,  2.85it/s] 69%|██████▊   | 59/86 [00:21<00:09,  2.75it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.93it/s] 64%|██████▍   | 55/86 [00:19<00:10,  2.85it/s] 70%|██████▉   | 60/86 [00:21<00:09,  2.75it/s] 70%|██████▉   | 60/86 [00:21<00:08,  2.93it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.85it/s] 71%|███████   | 61/86 [00:22<00:09,  2.77it/s] 71%|███████   | 61/86 [00:21<00:08,  2.96it/s] 66%|██████▋   | 57/86 [00:19<00:10,  2.90it/s] 72%|███████▏  | 62/86 [00:22<00:08,  2.81it/s] 72%|███████▏  | 62/86 [00:21<00:07,  3.00it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.91it/s] 73%|███████▎  | 63/86 [00:22<00:08,  2.80it/s] 73%|███████▎  | 63/86 [00:22<00:07,  2.97it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.89it/s] 74%|███████▍  | 64/86 [00:23<00:07,  2.78it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.92it/s] 70%|██████▉   | 60/86 [00:20<00:08,  2.89it/s] 76%|███████▌  | 65/86 [00:23<00:07,  2.81it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.97it/s] 71%|███████   | 61/86 [00:21<00:08,  2.94it/s] 77%|███████▋  | 66/86 [00:23<00:07,  2.84it/s] 77%|███████▋  | 66/86 [00:23<00:06,  3.02it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.97it/s] 78%|███████▊  | 67/86 [00:23<00:06,  3.05it/s] 78%|███████▊  | 67/86 [00:24<00:06,  2.88it/s] 73%|███████▎  | 63/86 [00:21<00:07,  3.00it/s] 79%|███████▉  | 68/86 [00:23<00:05,  3.07it/s] 79%|███████▉  | 68/86 [00:24<00:06,  2.89it/s] 74%|███████▍  | 64/86 [00:22<00:07,  3.01it/s] 80%|████████  | 69/86 [00:23<00:05,  3.07it/s] 80%|████████  | 69/86 [00:24<00:05,  2.88it/s] 76%|███████▌  | 65/86 [00:22<00:07,  3.00it/s] 81%|████████▏ | 70/86 [00:24<00:05,  3.08it/s] 81%|████████▏ | 70/86 [00:25<00:05,  2.87it/s] 77%|███████▋  | 66/86 [00:22<00:06,  3.00it/s] 83%|████████▎ | 71/86 [00:24<00:04,  3.09it/s] 83%|████████▎ | 71/86 [00:25<00:05,  2.87it/s]W0318 07:02:22.682000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.682000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.683000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.683000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.683000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.683000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.683000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.726000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.726000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.726000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.726000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.726000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 67/86 [00:23<00:06,  2.98it/s]W0318 07:02:22.742000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.742000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.742000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.742000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.742000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 84%|████████▎ | 72/86 [00:24<00:04,  3.02it/s]W0318 07:02:22.918000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.918000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.918000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.918000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:22.918000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 84%|████████▎ | 72/86 [00:25<00:04,  2.80it/s] 79%|███████▉  | 68/86 [00:23<00:06,  2.92it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.97it/s]W0318 07:02:23.253000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:23.253000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:23.253000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:23.253000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:23.253000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:23.254000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 07:02:23.254000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:23.290000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:23.290000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:23.290000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:23.290000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:23.291000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:23.364000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:23.365000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:23.365000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:23.365000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:23.365000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 85%|████████▍ | 73/86 [00:26<00:04,  2.65it/s] 80%|████████  | 69/86 [00:23<00:06,  2.72it/s] 86%|████████▌ | 74/86 [00:25<00:04,  2.88it/s] 86%|████████▌ | 74/86 [00:26<00:04,  2.48it/s] 87%|████████▋ | 75/86 [00:26<00:03,  2.86it/s] 81%|████████▏ | 70/86 [00:24<00:06,  2.53it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.83it/s] 87%|████████▋ | 75/86 [00:27<00:04,  2.40it/s] 83%|████████▎ | 71/86 [00:24<00:06,  2.36it/s]W0318 07:02:24.615000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:24.626000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:24.635000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:24.635000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 90%|████████▉ | 77/86 [00:26<00:03,  2.79it/s] 88%|████████▊ | 76/86 [00:27<00:04,  2.34it/s] 84%|████████▎ | 72/86 [00:25<00:06,  2.21it/s] 91%|█████████ | 78/86 [00:27<00:02,  2.73it/s]W0318 07:02:25.107000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.107000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.107000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.107000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.107000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.108000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.108000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.143000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.143000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.143000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.143000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.143000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 90%|████████▉ | 77/86 [00:28<00:03,  2.30it/s] 85%|████████▍ | 73/86 [00:25<00:05,  2.26it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.75it/s]W0318 07:02:25.501000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.501000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.501000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.501000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.501000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.501000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.502000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.502000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 78/86 [00:28<00:03,  2.41it/s] 86%|████████▌ | 74/86 [00:26<00:05,  2.39it/s] 93%|█████████▎| 80/86 [00:27<00:02,  2.77it/s]W0318 07:02:25.816000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.817000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.817000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.817000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:25.817000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 92%|█████████▏| 79/86 [00:28<00:02,  2.50it/s] 87%|████████▋ | 75/86 [00:26<00:04,  2.51it/s] 94%|█████████▍| 81/86 [00:28<00:01,  2.80it/s]W0318 07:02:26.160000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:26.166000 139872282695488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 93%|█████████▎| 80/86 [00:29<00:02,  2.57it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.62it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.81it/s] 94%|█████████▍| 81/86 [00:29<00:01,  2.63it/s] 90%|████████▉ | 77/86 [00:27<00:03,  2.71it/s] 97%|█████████▋| 83/86 [00:28<00:01,  2.85it/s] 95%|█████████▌| 82/86 [00:29<00:01,  2.72it/s] 91%|█████████ | 78/86 [00:27<00:02,  2.81it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.92it/s] 97%|█████████▋| 83/86 [00:30<00:01,  2.79it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.88it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.98it/s] 98%|█████████▊| 84/86 [00:30<00:00,  2.84it/s] 93%|█████████▎| 80/86 [00:28<00:02,  2.94it/s]100%|██████████| 86/86 [00:29<00:00,  3.01it/s]100%|██████████| 86/86 [00:29<00:00,  2.88it/s]
 99%|█████████▉| 85/86 [00:30<00:00,  2.85it/s] 94%|█████████▍| 81/86 [00:28<00:01,  2.95it/s]100%|██████████| 86/86 [00:31<00:00,  2.83it/s]100%|██████████| 86/86 [00:31<00:00,  2.75it/s]
 95%|█████████▌| 82/86 [00:28<00:01,  2.93it/s] 97%|█████████▋| 83/86 [00:29<00:01,  2.91it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.89it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.88it/s]100%|██████████| 86/86 [00:30<00:00,  2.88it/s]100%|██████████| 86/86 [00:30<00:00,  2.84it/s]
I0318 07:02:33.237293 1129876 finetune.py:45] layer 28_down initial loss 0.4629511833190918
W0318 07:02:33.237761 1129876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 07:02:33.807008 1129876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

28_down proxy err 0.012531707994639874 tr(WHW.T) 603.8402099609375
W0318 07:02:34.831000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 07:02:34.832000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:34.832000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:34.832000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:34.832000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:34.832000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:34.832000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:34.876000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:34.876000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:34.876000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:34.876000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:34.876000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:34.892000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:34.892000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:34.892000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:34.892000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:34.892000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.070000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.070000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.070000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.071000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.071000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.415000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.415000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.415000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.415000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.415000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.416000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.416000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.451000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.451000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.451000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.451000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.451000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.524000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.524000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.524000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.524000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.524000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.553000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.553000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.553000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.554000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.554000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.554000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.554000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.599000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.599000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.599000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.599000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.599000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.615000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.616000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.616000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.616000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.616000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.789000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.789000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.789000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.789000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:35.789000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.119000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.119000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.120000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.120000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.120000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.120000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.120000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.154000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.154000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.154000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.154000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.154000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.227000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.227000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.227000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.227000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.227000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.757000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.762000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.762000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.762000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.762000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.762000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.762000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.763000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.768000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.775000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.775000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.802000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.803000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.803000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.803000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.803000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.817000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.818000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.818000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.818000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.818000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.980000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.980000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.980000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.980000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:36.980000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.203000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.204000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.204000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.204000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.204000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.204000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.204000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.232000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.232000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.232000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.232000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.232000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.285000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.285000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.285000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.285000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.285000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.285000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.286000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.315000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.315000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.315000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.315000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.315000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.372000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.383000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.383000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.383000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.383000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.383000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.384000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.392000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.392000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.563000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.563000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.563000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.563000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.564000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.564000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.564000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.564000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.821000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.821000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.821000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.821000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.821000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.821000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.821000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.845000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.845000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.845000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.846000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.846000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.852000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.852000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.852000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.852000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:37.852000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.167000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.172000 140604655187776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.179000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.179000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.180000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.180000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.180000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.180000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.180000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.180000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.472000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.473000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.473000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.473000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.473000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.524000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.530000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.537000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.537000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.822000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.828000 140213065697088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.997000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.997000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.997000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.997000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.997000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.997000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:38.997000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.030000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.030000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.030000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.030000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.030000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.366000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.366000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.366000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.367000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.367000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.367000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.367000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.367000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.652000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.652000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.652000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.652000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.652000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.968000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0318 07:02:39.973000 140460833503040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0318 07:02:44.839276 1131058 finetune.py:45] layer 31_down initial loss 29.93825340270996
W0318 07:02:44.839826 1131058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 07:02:45.378151 1131058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

31_down proxy err 0.0009570497204549611 tr(WHW.T) 17873.55078125
I0318 07:02:46.002163 1130431 finetune.py:45] layer 29_down initial loss 0.6262338757514954
W0318 07:02:46.002663 1130431 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 07:02:46.666386 1130431 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

29_down proxy err 0.011196334846317768 tr(WHW.T) 782.2445678710938
I0318 07:02:47.084868 1130615 finetune.py:45] layer 30_down initial loss 231.71273803710938
W0318 07:02:47.085191 1130615 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0318 07:02:47.625261 1130615 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

30_down proxy err 0.0030585923232138157 tr(WHW.T) 3582.61669921875
