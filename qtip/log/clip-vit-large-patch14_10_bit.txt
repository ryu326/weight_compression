I0416 08:14:26.315322 3179534 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:14:26.315477 3179534 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:14:26.315521 3179534 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:14:26.439790 3179534 config.py:58] PyTorch version 2.4.0 available.
W0416 08:14:28.763707 3179534 warnings.py:110] /workspace/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

I0416 08:14:30.012056 3179534 quantize_finetune_clip.py:141] loaded model
I0416 08:14:30.012174 3179534 quantize_finetune_clip.py:143] loaded dataset and devset
I0416 08:14:30.106884 3179534 quantize_finetune_clip.py:151] vision layer 0 gpu 0
I0416 08:14:31.183479 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 0 in 0.90s
I0416 08:14:31.921235 3179534 quantize_finetune_clip.py:151] vision layer 1 gpu 0
I0416 08:14:34.192345 3182627 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:14:34.192525 3182627 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:14:34.192588 3182627 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:14:34.308312 3182627 config.py:58] PyTorch version 2.4.0 available.
W0416 08:14:37.097301 3182627 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-1:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:14:40.066931 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 1 in 0.19s
I0416 08:14:40.198807 3179534 quantize_finetune_clip.py:151] vision layer 2 gpu 0
I0416 08:14:42.494735 3185203 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:14:42.494861 3185203 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:14:42.494924 3185203 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:14:42.613101 3185203 config.py:58] PyTorch version 2.4.0 available.
W0416 08:14:44.934861 3185203 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-2:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:14:47.131614 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 2 in 0.27s
I0416 08:14:47.360532 3179534 quantize_finetune_clip.py:151] vision layer 3 gpu 0
I0416 08:14:49.691373 3186396 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:14:49.691495 3186396 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:14:49.691554 3186396 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:14:49.807428 3186396 config.py:58] PyTorch version 2.4.0 available.
W0416 08:14:52.374592 3186396 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-3:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:14:54.511247 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 3 in 0.19s
I0416 08:14:54.643813 3179534 quantize_finetune_clip.py:151] vision layer 4 gpu 0
I0416 08:14:56.935012 3187841 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:14:56.935134 3187841 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:14:56.935193 3187841 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:14:57.053211 3187841 config.py:58] PyTorch version 2.4.0 available.
W0416 08:14:59.389967 3187841 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-4:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:15:01.401050 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 4 in 0.20s
I0416 08:15:01.525716 3179534 quantize_finetune_clip.py:151] vision layer 5 gpu 0
I0416 08:15:03.772913 3189331 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:03.773060 3189331 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:03.773127 3189331 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:03.892915 3189331 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:06.310151 3189331 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-5:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:15:08.365395 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 5 in 0.23s
I0416 08:15:08.538515 3179534 quantize_finetune_clip.py:151] vision layer 6 gpu 0
I0416 08:15:10.936237 3190813 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:10.936366 3190813 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:10.936435 3190813 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:11.057984 3190813 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:13.472509 3190813 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-6:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:15:15.586258 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 6 in 0.17s
I0416 08:15:15.715070 3179534 quantize_finetune_clip.py:151] vision layer 7 gpu 0
I0416 08:15:18.061845 3192323 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:18.061971 3192323 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:18.062039 3192323 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:18.182293 3192323 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:20.570257 3192323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-7:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:15:22.899564 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 7 in 0.31s
I0416 08:15:23.014319 3179534 quantize_finetune_clip.py:151] vision layer 8 gpu 0
I0416 08:15:25.395607 3193890 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:25.395808 3193890 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:25.395900 3193890 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:25.537491 3193890 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:28.138420 3193890 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-8:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:15:30.436944 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 8 in 0.18s
I0416 08:15:30.565773 3179534 quantize_finetune_clip.py:151] vision layer 9 gpu 0
I0416 08:15:32.919386 3196056 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:32.919514 3196056 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:32.919597 3196056 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:33.040339 3196056 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:35.434520 3196056 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-9:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:15:37.409687 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 9 in 0.16s
I0416 08:15:37.542027 3179534 quantize_finetune_clip.py:151] vision layer 10 gpu 0
I0416 08:15:39.899121 3197795 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:39.899241 3197795 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:39.899304 3197795 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:40.020053 3197795 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:42.354828 3197795 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-10:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:15:44.463749 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 10 in 0.14s
I0416 08:15:44.591926 3179534 quantize_finetune_clip.py:151] vision layer 11 gpu 0
I0416 08:15:46.911785 3199184 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:46.911946 3199184 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:46.912004 3199184 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:47.029366 3199184 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:49.815329 3199184 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-11:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:15:52.733141 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 11 in 0.14s
I0416 08:15:52.867490 3179534 quantize_finetune_clip.py:151] vision layer 12 gpu 0
I0416 08:15:55.185723 3201256 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:55.185858 3201256 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:55.185922 3201256 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:55.302237 3201256 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:57.675362 3201256 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-12:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:15:59.661887 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 12 in 0.18s
I0416 08:15:59.791052 3179534 quantize_finetune_clip.py:151] vision layer 13 gpu 0
I0416 08:16:02.277527 3202962 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:02.277689 3202962 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:02.277768 3202962 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:02.404621 3202962 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:05.446442 3202962 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-13:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:16:09.096021 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 13 in 0.30s
I0416 08:16:09.220763 3179534 quantize_finetune_clip.py:151] vision layer 14 gpu 0
I0416 08:16:11.663520 3205569 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:11.663642 3205569 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:11.663707 3205569 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:11.788535 3205569 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:14.085236 3205569 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-14:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:16:16.379607 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 14 in 0.21s
I0416 08:16:16.506421 3179534 quantize_finetune_clip.py:151] vision layer 15 gpu 0
I0416 08:16:18.904232 3207889 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:18.904361 3207889 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:18.904426 3207889 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:19.027984 3207889 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:21.591575 3207889 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-15:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:16:23.913054 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 15 in 0.26s
I0416 08:16:24.037393 3179534 quantize_finetune_clip.py:151] vision layer 16 gpu 0
I0416 08:16:26.429996 3209528 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:26.430161 3209528 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:26.430227 3209528 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:26.555244 3209528 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:29.333738 3209528 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-16:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:16:32.390051 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 16 in 0.15s
I0416 08:16:32.510883 3179534 quantize_finetune_clip.py:151] vision layer 17 gpu 0
I0416 08:16:34.810536 3211385 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:34.810660 3211385 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:34.810729 3211385 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:34.930791 3211385 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:37.284810 3211385 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-17:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:16:39.143419 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 17 in 0.13s
I0416 08:16:39.265399 3179534 quantize_finetune_clip.py:151] vision layer 18 gpu 0
I0416 08:16:41.568154 3212760 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:41.568337 3212760 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:41.568433 3212760 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:41.691077 3212760 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:44.058900 3212760 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-18:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:16:47.856039 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 18 in 0.14s
I0416 08:16:47.982313 3179534 quantize_finetune_clip.py:151] vision layer 19 gpu 0
I0416 08:16:50.405692 3214891 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:50.405944 3214891 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:50.406066 3214891 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:50.529940 3214891 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:53.309136 3214891 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-19:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:16:55.755644 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 19 in 0.15s
I0416 08:16:55.937959 3179534 quantize_finetune_clip.py:151] vision layer 20 gpu 0
I0416 08:16:58.284953 3217456 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:58.285088 3217456 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:58.285157 3217456 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:58.409317 3217456 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:00.763771 3217456 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-20:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:17:02.965059 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 20 in 0.33s
I0416 08:17:03.121634 3179534 quantize_finetune_clip.py:151] vision layer 21 gpu 0
I0416 08:17:05.588627 3219209 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:05.588744 3219209 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:05.588811 3219209 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:05.708714 3219209 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:08.282707 3219209 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-21:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:17:10.455105 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 21 in 0.17s
I0416 08:17:10.582312 3179534 quantize_finetune_clip.py:151] vision layer 22 gpu 0
I0416 08:17:12.976773 3221083 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:12.976900 3221083 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:12.976965 3221083 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:13.096040 3221083 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:15.775948 3221083 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-22:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:17:18.602005 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 22 in 0.15s
I0416 08:17:18.741225 3179534 quantize_finetune_clip.py:151] vision layer 23 gpu 0
I0416 08:17:21.066269 3223096 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:21.066395 3223096 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:21.066458 3223096 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:21.190710 3223096 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:23.674920 3223096 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-23:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:17:26.735631 3179534 quantize_finetune_clip.py:168] computed original embedding for vision layer 23 in 0.16s
I0416 08:17:29.308176 3224946 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:29.308321 3224946 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:29.308396 3224946 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:29.435016 3224946 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:31.797164 3224946 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s]  0%|          | 0/8 [00:00<?, ?it/s]
Process Process-24:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:17:33.295704 3179534 quantize_finetune_clip.py:151] text layer 0 gpu 0
I0416 08:17:33.593250 3179534 quantize_finetune_clip.py:168] computed original embedding for text layer 0 in 0.13s
I0416 08:17:33.705750 3179534 quantize_finetune_clip.py:151] text layer 1 gpu 0
I0416 08:17:36.065954 3226584 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:36.066138 3226584 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:36.066248 3226584 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:36.186533 3226584 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:38.883409 3226584 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-25:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:17:41.723067 3179534 quantize_finetune_clip.py:168] computed original embedding for text layer 1 in 0.15s
I0416 08:17:41.835034 3179534 quantize_finetune_clip.py:151] text layer 2 gpu 0
I0416 08:17:44.185253 3229038 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:44.185523 3229038 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:44.185587 3229038 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:44.315928 3229038 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:47.003623 3229038 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-26:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:17:49.902104 3179534 quantize_finetune_clip.py:168] computed original embedding for text layer 2 in 0.12s
I0416 08:17:50.018344 3179534 quantize_finetune_clip.py:151] text layer 3 gpu 0
I0416 08:17:52.523225 3231151 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:52.523519 3231151 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:52.523638 3231151 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:52.654879 3231151 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:55.389142 3231151 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-27:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:17:58.243890 3179534 quantize_finetune_clip.py:168] computed original embedding for text layer 3 in 0.15s
I0416 08:17:58.372835 3179534 quantize_finetune_clip.py:151] text layer 4 gpu 0
I0416 08:18:00.842565 3233060 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:00.842724 3233060 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:00.842788 3233060 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:00.967437 3233060 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:03.344939 3233060 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-28:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:18:06.274785 3179534 quantize_finetune_clip.py:168] computed original embedding for text layer 4 in 0.15s
I0416 08:18:06.402585 3179534 quantize_finetune_clip.py:151] text layer 5 gpu 0
I0416 08:18:08.938330 3235116 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:08.938457 3235116 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:08.938519 3235116 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:09.060518 3235116 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:11.482071 3235116 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-29:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:18:13.451228 3179534 quantize_finetune_clip.py:168] computed original embedding for text layer 5 in 0.13s
I0416 08:18:13.580577 3179534 quantize_finetune_clip.py:151] text layer 6 gpu 0
I0416 08:18:15.986364 3236557 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:15.986562 3236557 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:15.986621 3236557 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:16.118546 3236557 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:18.985701 3236557 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-30:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:18:21.954981 3179534 quantize_finetune_clip.py:168] computed original embedding for text layer 6 in 0.16s
I0416 08:18:22.066412 3179534 quantize_finetune_clip.py:151] text layer 7 gpu 0
I0416 08:18:24.801067 3238458 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:24.801228 3238458 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:24.801292 3238458 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:24.924558 3238458 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:27.947744 3238458 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-31:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:18:30.759927 3179534 quantize_finetune_clip.py:168] computed original embedding for text layer 7 in 0.14s
I0416 08:18:30.878528 3179534 quantize_finetune_clip.py:151] text layer 8 gpu 0
I0416 08:18:33.182482 3241335 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:33.182640 3241335 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:33.182711 3241335 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:33.298613 3241335 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:35.802040 3241335 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-32:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:18:37.942626 3179534 quantize_finetune_clip.py:168] computed original embedding for text layer 8 in 0.17s
I0416 08:18:38.061280 3179534 quantize_finetune_clip.py:151] text layer 9 gpu 0
I0416 08:18:40.306982 3243314 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:40.307099 3243314 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:40.307158 3243314 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:40.422458 3243314 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:42.690016 3243314 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-33:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:18:44.684034 3179534 quantize_finetune_clip.py:168] computed original embedding for text layer 9 in 0.14s
I0416 08:18:44.802971 3179534 quantize_finetune_clip.py:151] text layer 10 gpu 0
I0416 08:18:47.132654 3244602 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:47.132780 3244602 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:47.132844 3244602 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:47.253706 3244602 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:49.652609 3244602 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-34:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:18:51.593080 3179534 quantize_finetune_clip.py:168] computed original embedding for text layer 10 in 0.13s
I0416 08:18:51.709653 3179534 quantize_finetune_clip.py:151] text layer 11 gpu 0
I0416 08:18:54.058594 3246631 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:54.058816 3246631 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:54.058877 3246631 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:54.200717 3246631 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:56.901968 3246631 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-35:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:18:59.628738 3179534 quantize_finetune_clip.py:168] computed original embedding for text layer 11 in 0.23s
I0416 08:19:02.060043 3248157 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:19:02.060238 3248157 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:19:02.060303 3248157 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:19:02.176116 3248157 config.py:58] PyTorch version 2.4.0 available.
W0416 08:19:04.574787 3248157 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s]  0%|          | 0/6 [00:00<?, ?it/s]
Process Process-36:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 127, in quantize_finetune_decoder_layer_clip
    hatWr, Qidxs = ldlq.LDLQ(Wr, LRr, cb, args, for_kernel=has_kernel)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/algo/ldlq.py", line 58, in LDLQ
    q_out = cb.quantize(thing)
            ^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 287, in quantize
    state = self.quantize_seq(roll_X, overlap=None)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 279, in quantize_seq
    Qidxs[i] = self.viterbi(X[i], overlap=b_overlap)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 233, in viterbi
    from_state = torch.zeros(T // self.V,
                 ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: zeros(): argument 'size' failed to unpack the object at pos 3 with error "type must be tuple of ints,but got float"
I0416 08:19:12.175601 3250180 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:19:12.175760 3250180 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:19:12.175804 3250180 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:19:12.298894 3250180 config.py:58] PyTorch version 2.4.0 available.
W0416 08:19:13.942263 3250180 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0416 08:19:13.943046 3250180 hfize_clip.py:43] CLIPConfig {
  "_name_or_path": "../Wparam_dataset/hf_model/openai--clip-vit-large-patch14",
  "architectures": [
    "CLIPModel"
  ],
  "initializer_factor": 1.0,
  "logit_scale_init_value": 2.6592,
  "model_type": "clip",
  "projection_dim": 768,
  "quip_params": {
    "K": 10,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "text_config": {
    "dropout": 0.0,
    "hidden_size": 768,
    "intermediate_size": 3072,
    "model_type": "clip_text_model",
    "num_attention_heads": 12,
    "projection_dim": 768,
    "torch_dtype": "float32"
  },
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "vision_config": {
    "dropout": 0.0,
    "hidden_size": 1024,
    "intermediate_size": 4096,
    "model_type": "clip_vision_model",
    "num_attention_heads": 16,
    "num_hidden_layers": 24,
    "patch_size": 14,
    "projection_dim": 768,
    "torch_dtype": "float32"
  }
}

Some weights of the model checkpoint at ../Wparam_dataset/hf_model/openai--clip-vit-large-patch14 were not used when initializing CLIPModel: ['text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing CLIPModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of CLIPModel were not initialized from the model checkpoint at ../Wparam_dataset/hf_model/openai--clip-vit-large-patch14 and are newly initialized: ['text_model.encoder.layers.0.mlp.fc1.SU', 'text_model.encoder.layers.0.mlp.fc1.SV', 'text_model.encoder.layers.0.mlp.fc1.rcp', 'text_model.encoder.layers.0.mlp.fc1.tlut', 'text_model.encoder.layers.0.mlp.fc1.tp_rank', 'text_model.encoder.layers.0.mlp.fc1.trellis', 'text_model.encoder.layers.0.mlp.fc2.SU', 'text_model.encoder.layers.0.mlp.fc2.SV', 'text_model.encoder.layers.0.mlp.fc2.rcp', 'text_model.encoder.layers.0.mlp.fc2.tlut', 'text_model.encoder.layers.0.mlp.fc2.tp_rank', 'text_model.encoder.layers.0.mlp.fc2.trellis', 'text_model.encoder.layers.0.self_attn.k_proj.SU', 'text_model.encoder.layers.0.self_attn.k_proj.SV', 'text_model.encoder.layers.0.self_attn.k_proj.rcp', 'text_model.encoder.layers.0.self_attn.k_proj.tlut', 'text_model.encoder.layers.0.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.k_proj.trellis', 'text_model.encoder.layers.0.self_attn.out_proj.SU', 'text_model.encoder.layers.0.self_attn.out_proj.SV', 'text_model.encoder.layers.0.self_attn.out_proj.rcp', 'text_model.encoder.layers.0.self_attn.out_proj.tlut', 'text_model.encoder.layers.0.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.out_proj.trellis', 'text_model.encoder.layers.0.self_attn.q_proj.SU', 'text_model.encoder.layers.0.self_attn.q_proj.SV', 'text_model.encoder.layers.0.self_attn.q_proj.rcp', 'text_model.encoder.layers.0.self_attn.q_proj.tlut', 'text_model.encoder.layers.0.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.q_proj.trellis', 'text_model.encoder.layers.0.self_attn.v_proj.SU', 'text_model.encoder.layers.0.self_attn.v_proj.SV', 'text_model.encoder.layers.0.self_attn.v_proj.rcp', 'text_model.encoder.layers.0.self_attn.v_proj.tlut', 'text_model.encoder.layers.0.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.v_proj.trellis', 'text_model.encoder.layers.1.mlp.fc1.SU', 'text_model.encoder.layers.1.mlp.fc1.SV', 'text_model.encoder.layers.1.mlp.fc1.rcp', 'text_model.encoder.layers.1.mlp.fc1.tlut', 'text_model.encoder.layers.1.mlp.fc1.tp_rank', 'text_model.encoder.layers.1.mlp.fc1.trellis', 'text_model.encoder.layers.1.mlp.fc2.SU', 'text_model.encoder.layers.1.mlp.fc2.SV', 'text_model.encoder.layers.1.mlp.fc2.rcp', 'text_model.encoder.layers.1.mlp.fc2.tlut', 'text_model.encoder.layers.1.mlp.fc2.tp_rank', 'text_model.encoder.layers.1.mlp.fc2.trellis', 'text_model.encoder.layers.1.self_attn.k_proj.SU', 'text_model.encoder.layers.1.self_attn.k_proj.SV', 'text_model.encoder.layers.1.self_attn.k_proj.rcp', 'text_model.encoder.layers.1.self_attn.k_proj.tlut', 'text_model.encoder.layers.1.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.k_proj.trellis', 'text_model.encoder.layers.1.self_attn.out_proj.SU', 'text_model.encoder.layers.1.self_attn.out_proj.SV', 'text_model.encoder.layers.1.self_attn.out_proj.rcp', 'text_model.encoder.layers.1.self_attn.out_proj.tlut', 'text_model.encoder.layers.1.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.out_proj.trellis', 'text_model.encoder.layers.1.self_attn.q_proj.SU', 'text_model.encoder.layers.1.self_attn.q_proj.SV', 'text_model.encoder.layers.1.self_attn.q_proj.rcp', 'text_model.encoder.layers.1.self_attn.q_proj.tlut', 'text_model.encoder.layers.1.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.q_proj.trellis', 'text_model.encoder.layers.1.self_attn.v_proj.SU', 'text_model.encoder.layers.1.self_attn.v_proj.SV', 'text_model.encoder.layers.1.self_attn.v_proj.rcp', 'text_model.encoder.layers.1.self_attn.v_proj.tlut', 'text_model.encoder.layers.1.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.v_proj.trellis', 'text_model.encoder.layers.10.mlp.fc1.SU', 'text_model.encoder.layers.10.mlp.fc1.SV', 'text_model.encoder.layers.10.mlp.fc1.rcp', 'text_model.encoder.layers.10.mlp.fc1.tlut', 'text_model.encoder.layers.10.mlp.fc1.tp_rank', 'text_model.encoder.layers.10.mlp.fc1.trellis', 'text_model.encoder.layers.10.mlp.fc2.SU', 'text_model.encoder.layers.10.mlp.fc2.SV', 'text_model.encoder.layers.10.mlp.fc2.rcp', 'text_model.encoder.layers.10.mlp.fc2.tlut', 'text_model.encoder.layers.10.mlp.fc2.tp_rank', 'text_model.encoder.layers.10.mlp.fc2.trellis', 'text_model.encoder.layers.10.self_attn.k_proj.SU', 'text_model.encoder.layers.10.self_attn.k_proj.SV', 'text_model.encoder.layers.10.self_attn.k_proj.rcp', 'text_model.encoder.layers.10.self_attn.k_proj.tlut', 'text_model.encoder.layers.10.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.k_proj.trellis', 'text_model.encoder.layers.10.self_attn.out_proj.SU', 'text_model.encoder.layers.10.self_attn.out_proj.SV', 'text_model.encoder.layers.10.self_attn.out_proj.rcp', 'text_model.encoder.layers.10.self_attn.out_proj.tlut', 'text_model.encoder.layers.10.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.out_proj.trellis', 'text_model.encoder.layers.10.self_attn.q_proj.SU', 'text_model.encoder.layers.10.self_attn.q_proj.SV', 'text_model.encoder.layers.10.self_attn.q_proj.rcp', 'text_model.encoder.layers.10.self_attn.q_proj.tlut', 'text_model.encoder.layers.10.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.q_proj.trellis', 'text_model.encoder.layers.10.self_attn.v_proj.SU', 'text_model.encoder.layers.10.self_attn.v_proj.SV', 'text_model.encoder.layers.10.self_attn.v_proj.rcp', 'text_model.encoder.layers.10.self_attn.v_proj.tlut', 'text_model.encoder.layers.10.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.v_proj.trellis', 'text_model.encoder.layers.11.mlp.fc1.SU', 'text_model.encoder.layers.11.mlp.fc1.SV', 'text_model.encoder.layers.11.mlp.fc1.rcp', 'text_model.encoder.layers.11.mlp.fc1.tlut', 'text_model.encoder.layers.11.mlp.fc1.tp_rank', 'text_model.encoder.layers.11.mlp.fc1.trellis', 'text_model.encoder.layers.11.mlp.fc2.SU', 'text_model.encoder.layers.11.mlp.fc2.SV', 'text_model.encoder.layers.11.mlp.fc2.rcp', 'text_model.encoder.layers.11.mlp.fc2.tlut', 'text_model.encoder.layers.11.mlp.fc2.tp_rank', 'text_model.encoder.layers.11.mlp.fc2.trellis', 'text_model.encoder.layers.11.self_attn.k_proj.SU', 'text_model.encoder.layers.11.self_attn.k_proj.SV', 'text_model.encoder.layers.11.self_attn.k_proj.rcp', 'text_model.encoder.layers.11.self_attn.k_proj.tlut', 'text_model.encoder.layers.11.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.k_proj.trellis', 'text_model.encoder.layers.11.self_attn.out_proj.SU', 'text_model.encoder.layers.11.self_attn.out_proj.SV', 'text_model.encoder.layers.11.self_attn.out_proj.rcp', 'text_model.encoder.layers.11.self_attn.out_proj.tlut', 'text_model.encoder.layers.11.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.out_proj.trellis', 'text_model.encoder.layers.11.self_attn.q_proj.SU', 'text_model.encoder.layers.11.self_attn.q_proj.SV', 'text_model.encoder.layers.11.self_attn.q_proj.rcp', 'text_model.encoder.layers.11.self_attn.q_proj.tlut', 'text_model.encoder.layers.11.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.q_proj.trellis', 'text_model.encoder.layers.11.self_attn.v_proj.SU', 'text_model.encoder.layers.11.self_attn.v_proj.SV', 'text_model.encoder.layers.11.self_attn.v_proj.rcp', 'text_model.encoder.layers.11.self_attn.v_proj.tlut', 'text_model.encoder.layers.11.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.v_proj.trellis', 'text_model.encoder.layers.2.mlp.fc1.SU', 'text_model.encoder.layers.2.mlp.fc1.SV', 'text_model.encoder.layers.2.mlp.fc1.rcp', 'text_model.encoder.layers.2.mlp.fc1.tlut', 'text_model.encoder.layers.2.mlp.fc1.tp_rank', 'text_model.encoder.layers.2.mlp.fc1.trellis', 'text_model.encoder.layers.2.mlp.fc2.SU', 'text_model.encoder.layers.2.mlp.fc2.SV', 'text_model.encoder.layers.2.mlp.fc2.rcp', 'text_model.encoder.layers.2.mlp.fc2.tlut', 'text_model.encoder.layers.2.mlp.fc2.tp_rank', 'text_model.encoder.layers.2.mlp.fc2.trellis', 'text_model.encoder.layers.2.self_attn.k_proj.SU', 'text_model.encoder.layers.2.self_attn.k_proj.SV', 'text_model.encoder.layers.2.self_attn.k_proj.rcp', 'text_model.encoder.layers.2.self_attn.k_proj.tlut', 'text_model.encoder.layers.2.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.k_proj.trellis', 'text_model.encoder.layers.2.self_attn.out_proj.SU', 'text_model.encoder.layers.2.self_attn.out_proj.SV', 'text_model.encoder.layers.2.self_attn.out_proj.rcp', 'text_model.encoder.layers.2.self_attn.out_proj.tlut', 'text_model.encoder.layers.2.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.out_proj.trellis', 'text_model.encoder.layers.2.self_attn.q_proj.SU', 'text_model.encoder.layers.2.self_attn.q_proj.SV', 'text_model.encoder.layers.2.self_attn.q_proj.rcp', 'text_model.encoder.layers.2.self_attn.q_proj.tlut', 'text_model.encoder.layers.2.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.q_proj.trellis', 'text_model.encoder.layers.2.self_attn.v_proj.SU', 'text_model.encoder.layers.2.self_attn.v_proj.SV', 'text_model.encoder.layers.2.self_attn.v_proj.rcp', 'text_model.encoder.layers.2.self_attn.v_proj.tlut', 'text_model.encoder.layers.2.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.v_proj.trellis', 'text_model.encoder.layers.3.mlp.fc1.SU', 'text_model.encoder.layers.3.mlp.fc1.SV', 'text_model.encoder.layers.3.mlp.fc1.rcp', 'text_model.encoder.layers.3.mlp.fc1.tlut', 'text_model.encoder.layers.3.mlp.fc1.tp_rank', 'text_model.encoder.layers.3.mlp.fc1.trellis', 'text_model.encoder.layers.3.mlp.fc2.SU', 'text_model.encoder.layers.3.mlp.fc2.SV', 'text_model.encoder.layers.3.mlp.fc2.rcp', 'text_model.encoder.layers.3.mlp.fc2.tlut', 'text_model.encoder.layers.3.mlp.fc2.tp_rank', 'text_model.encoder.layers.3.mlp.fc2.trellis', 'text_model.encoder.layers.3.self_attn.k_proj.SU', 'text_model.encoder.layers.3.self_attn.k_proj.SV', 'text_model.encoder.layers.3.self_attn.k_proj.rcp', 'text_model.encoder.layers.3.self_attn.k_proj.tlut', 'text_model.encoder.layers.3.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.k_proj.trellis', 'text_model.encoder.layers.3.self_attn.out_proj.SU', 'text_model.encoder.layers.3.self_attn.out_proj.SV', 'text_model.encoder.layers.3.self_attn.out_proj.rcp', 'text_model.encoder.layers.3.self_attn.out_proj.tlut', 'text_model.encoder.layers.3.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.out_proj.trellis', 'text_model.encoder.layers.3.self_attn.q_proj.SU', 'text_model.encoder.layers.3.self_attn.q_proj.SV', 'text_model.encoder.layers.3.self_attn.q_proj.rcp', 'text_model.encoder.layers.3.self_attn.q_proj.tlut', 'text_model.encoder.layers.3.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.q_proj.trellis', 'text_model.encoder.layers.3.self_attn.v_proj.SU', 'text_model.encoder.layers.3.self_attn.v_proj.SV', 'text_model.encoder.layers.3.self_attn.v_proj.rcp', 'text_model.encoder.layers.3.self_attn.v_proj.tlut', 'text_model.encoder.layers.3.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.v_proj.trellis', 'text_model.encoder.layers.4.mlp.fc1.SU', 'text_model.encoder.layers.4.mlp.fc1.SV', 'text_model.encoder.layers.4.mlp.fc1.rcp', 'text_model.encoder.layers.4.mlp.fc1.tlut', 'text_model.encoder.layers.4.mlp.fc1.tp_rank', 'text_model.encoder.layers.4.mlp.fc1.trellis', 'text_model.encoder.layers.4.mlp.fc2.SU', 'text_model.encoder.layers.4.mlp.fc2.SV', 'text_model.encoder.layers.4.mlp.fc2.rcp', 'text_model.encoder.layers.4.mlp.fc2.tlut', 'text_model.encoder.layers.4.mlp.fc2.tp_rank', 'text_model.encoder.layers.4.mlp.fc2.trellis', 'text_model.encoder.layers.4.self_attn.k_proj.SU', 'text_model.encoder.layers.4.self_attn.k_proj.SV', 'text_model.encoder.layers.4.self_attn.k_proj.rcp', 'text_model.encoder.layers.4.self_attn.k_proj.tlut', 'text_model.encoder.layers.4.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.k_proj.trellis', 'text_model.encoder.layers.4.self_attn.out_proj.SU', 'text_model.encoder.layers.4.self_attn.out_proj.SV', 'text_model.encoder.layers.4.self_attn.out_proj.rcp', 'text_model.encoder.layers.4.self_attn.out_proj.tlut', 'text_model.encoder.layers.4.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.out_proj.trellis', 'text_model.encoder.layers.4.self_attn.q_proj.SU', 'text_model.encoder.layers.4.self_attn.q_proj.SV', 'text_model.encoder.layers.4.self_attn.q_proj.rcp', 'text_model.encoder.layers.4.self_attn.q_proj.tlut', 'text_model.encoder.layers.4.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.q_proj.trellis', 'text_model.encoder.layers.4.self_attn.v_proj.SU', 'text_model.encoder.layers.4.self_attn.v_proj.SV', 'text_model.encoder.layers.4.self_attn.v_proj.rcp', 'text_model.encoder.layers.4.self_attn.v_proj.tlut', 'text_model.encoder.layers.4.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.v_proj.trellis', 'text_model.encoder.layers.5.mlp.fc1.SU', 'text_model.encoder.layers.5.mlp.fc1.SV', 'text_model.encoder.layers.5.mlp.fc1.rcp', 'text_model.encoder.layers.5.mlp.fc1.tlut', 'text_model.encoder.layers.5.mlp.fc1.tp_rank', 'text_model.encoder.layers.5.mlp.fc1.trellis', 'text_model.encoder.layers.5.mlp.fc2.SU', 'text_model.encoder.layers.5.mlp.fc2.SV', 'text_model.encoder.layers.5.mlp.fc2.rcp', 'text_model.encoder.layers.5.mlp.fc2.tlut', 'text_model.encoder.layers.5.mlp.fc2.tp_rank', 'text_model.encoder.layers.5.mlp.fc2.trellis', 'text_model.encoder.layers.5.self_attn.k_proj.SU', 'text_model.encoder.layers.5.self_attn.k_proj.SV', 'text_model.encoder.layers.5.self_attn.k_proj.rcp', 'text_model.encoder.layers.5.self_attn.k_proj.tlut', 'text_model.encoder.layers.5.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.k_proj.trellis', 'text_model.encoder.layers.5.self_attn.out_proj.SU', 'text_model.encoder.layers.5.self_attn.out_proj.SV', 'text_model.encoder.layers.5.self_attn.out_proj.rcp', 'text_model.encoder.layers.5.self_attn.out_proj.tlut', 'text_model.encoder.layers.5.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.out_proj.trellis', 'text_model.encoder.layers.5.self_attn.q_proj.SU', 'text_model.encoder.layers.5.self_attn.q_proj.SV', 'text_model.encoder.layers.5.self_attn.q_proj.rcp', 'text_model.encoder.layers.5.self_attn.q_proj.tlut', 'text_model.encoder.layers.5.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.q_proj.trellis', 'text_model.encoder.layers.5.self_attn.v_proj.SU', 'text_model.encoder.layers.5.self_attn.v_proj.SV', 'text_model.encoder.layers.5.self_attn.v_proj.rcp', 'text_model.encoder.layers.5.self_attn.v_proj.tlut', 'text_model.encoder.layers.5.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.v_proj.trellis', 'text_model.encoder.layers.6.mlp.fc1.SU', 'text_model.encoder.layers.6.mlp.fc1.SV', 'text_model.encoder.layers.6.mlp.fc1.rcp', 'text_model.encoder.layers.6.mlp.fc1.tlut', 'text_model.encoder.layers.6.mlp.fc1.tp_rank', 'text_model.encoder.layers.6.mlp.fc1.trellis', 'text_model.encoder.layers.6.mlp.fc2.SU', 'text_model.encoder.layers.6.mlp.fc2.SV', 'text_model.encoder.layers.6.mlp.fc2.rcp', 'text_model.encoder.layers.6.mlp.fc2.tlut', 'text_model.encoder.layers.6.mlp.fc2.tp_rank', 'text_model.encoder.layers.6.mlp.fc2.trellis', 'text_model.encoder.layers.6.self_attn.k_proj.SU', 'text_model.encoder.layers.6.self_attn.k_proj.SV', 'text_model.encoder.layers.6.self_attn.k_proj.rcp', 'text_model.encoder.layers.6.self_attn.k_proj.tlut', 'text_model.encoder.layers.6.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.k_proj.trellis', 'text_model.encoder.layers.6.self_attn.out_proj.SU', 'text_model.encoder.layers.6.self_attn.out_proj.SV', 'text_model.encoder.layers.6.self_attn.out_proj.rcp', 'text_model.encoder.layers.6.self_attn.out_proj.tlut', 'text_model.encoder.layers.6.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.out_proj.trellis', 'text_model.encoder.layers.6.self_attn.q_proj.SU', 'text_model.encoder.layers.6.self_attn.q_proj.SV', 'text_model.encoder.layers.6.self_attn.q_proj.rcp', 'text_model.encoder.layers.6.self_attn.q_proj.tlut', 'text_model.encoder.layers.6.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.q_proj.trellis', 'text_model.encoder.layers.6.self_attn.v_proj.SU', 'text_model.encoder.layers.6.self_attn.v_proj.SV', 'text_model.encoder.layers.6.self_attn.v_proj.rcp', 'text_model.encoder.layers.6.self_attn.v_proj.tlut', 'text_model.encoder.layers.6.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.v_proj.trellis', 'text_model.encoder.layers.7.mlp.fc1.SU', 'text_model.encoder.layers.7.mlp.fc1.SV', 'text_model.encoder.layers.7.mlp.fc1.rcp', 'text_model.encoder.layers.7.mlp.fc1.tlut', 'text_model.encoder.layers.7.mlp.fc1.tp_rank', 'text_model.encoder.layers.7.mlp.fc1.trellis', 'text_model.encoder.layers.7.mlp.fc2.SU', 'text_model.encoder.layers.7.mlp.fc2.SV', 'text_model.encoder.layers.7.mlp.fc2.rcp', 'text_model.encoder.layers.7.mlp.fc2.tlut', 'text_model.encoder.layers.7.mlp.fc2.tp_rank', 'text_model.encoder.layers.7.mlp.fc2.trellis', 'text_model.encoder.layers.7.self_attn.k_proj.SU', 'text_model.encoder.layers.7.self_attn.k_proj.SV', 'text_model.encoder.layers.7.self_attn.k_proj.rcp', 'text_model.encoder.layers.7.self_attn.k_proj.tlut', 'text_model.encoder.layers.7.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.k_proj.trellis', 'text_model.encoder.layers.7.self_attn.out_proj.SU', 'text_model.encoder.layers.7.self_attn.out_proj.SV', 'text_model.encoder.layers.7.self_attn.out_proj.rcp', 'text_model.encoder.layers.7.self_attn.out_proj.tlut', 'text_model.encoder.layers.7.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.out_proj.trellis', 'text_model.encoder.layers.7.self_attn.q_proj.SU', 'text_model.encoder.layers.7.self_attn.q_proj.SV', 'text_model.encoder.layers.7.self_attn.q_proj.rcp', 'text_model.encoder.layers.7.self_attn.q_proj.tlut', 'text_model.encoder.layers.7.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.q_proj.trellis', 'text_model.encoder.layers.7.self_attn.v_proj.SU', 'text_model.encoder.layers.7.self_attn.v_proj.SV', 'text_model.encoder.layers.7.self_attn.v_proj.rcp', 'text_model.encoder.layers.7.self_attn.v_proj.tlut', 'text_model.encoder.layers.7.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.v_proj.trellis', 'text_model.encoder.layers.8.mlp.fc1.SU', 'text_model.encoder.layers.8.mlp.fc1.SV', 'text_model.encoder.layers.8.mlp.fc1.rcp', 'text_model.encoder.layers.8.mlp.fc1.tlut', 'text_model.encoder.layers.8.mlp.fc1.tp_rank', 'text_model.encoder.layers.8.mlp.fc1.trellis', 'text_model.encoder.layers.8.mlp.fc2.SU', 'text_model.encoder.layers.8.mlp.fc2.SV', 'text_model.encoder.layers.8.mlp.fc2.rcp', 'text_model.encoder.layers.8.mlp.fc2.tlut', 'text_model.encoder.layers.8.mlp.fc2.tp_rank', 'text_model.encoder.layers.8.mlp.fc2.trellis', 'text_model.encoder.layers.8.self_attn.k_proj.SU', 'text_model.encoder.layers.8.self_attn.k_proj.SV', 'text_model.encoder.layers.8.self_attn.k_proj.rcp', 'text_model.encoder.layers.8.self_attn.k_proj.tlut', 'text_model.encoder.layers.8.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.k_proj.trellis', 'text_model.encoder.layers.8.self_attn.out_proj.SU', 'text_model.encoder.layers.8.self_attn.out_proj.SV', 'text_model.encoder.layers.8.self_attn.out_proj.rcp', 'text_model.encoder.layers.8.self_attn.out_proj.tlut', 'text_model.encoder.layers.8.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.out_proj.trellis', 'text_model.encoder.layers.8.self_attn.q_proj.SU', 'text_model.encoder.layers.8.self_attn.q_proj.SV', 'text_model.encoder.layers.8.self_attn.q_proj.rcp', 'text_model.encoder.layers.8.self_attn.q_proj.tlut', 'text_model.encoder.layers.8.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.q_proj.trellis', 'text_model.encoder.layers.8.self_attn.v_proj.SU', 'text_model.encoder.layers.8.self_attn.v_proj.SV', 'text_model.encoder.layers.8.self_attn.v_proj.rcp', 'text_model.encoder.layers.8.self_attn.v_proj.tlut', 'text_model.encoder.layers.8.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.v_proj.trellis', 'text_model.encoder.layers.9.mlp.fc1.SU', 'text_model.encoder.layers.9.mlp.fc1.SV', 'text_model.encoder.layers.9.mlp.fc1.rcp', 'text_model.encoder.layers.9.mlp.fc1.tlut', 'text_model.encoder.layers.9.mlp.fc1.tp_rank', 'text_model.encoder.layers.9.mlp.fc1.trellis', 'text_model.encoder.layers.9.mlp.fc2.SU', 'text_model.encoder.layers.9.mlp.fc2.SV', 'text_model.encoder.layers.9.mlp.fc2.rcp', 'text_model.encoder.layers.9.mlp.fc2.tlut', 'text_model.encoder.layers.9.mlp.fc2.tp_rank', 'text_model.encoder.layers.9.mlp.fc2.trellis', 'text_model.encoder.layers.9.self_attn.k_proj.SU', 'text_model.encoder.layers.9.self_attn.k_proj.SV', 'text_model.encoder.layers.9.self_attn.k_proj.rcp', 'text_model.encoder.layers.9.self_attn.k_proj.tlut', 'text_model.encoder.layers.9.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.k_proj.trellis', 'text_model.encoder.layers.9.self_attn.out_proj.SU', 'text_model.encoder.layers.9.self_attn.out_proj.SV', 'text_model.encoder.layers.9.self_attn.out_proj.rcp', 'text_model.encoder.layers.9.self_attn.out_proj.tlut', 'text_model.encoder.layers.9.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.out_proj.trellis', 'text_model.encoder.layers.9.self_attn.q_proj.SU', 'text_model.encoder.layers.9.self_attn.q_proj.SV', 'text_model.encoder.layers.9.self_attn.q_proj.rcp', 'text_model.encoder.layers.9.self_attn.q_proj.tlut', 'text_model.encoder.layers.9.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.q_proj.trellis', 'text_model.encoder.layers.9.self_attn.v_proj.SU', 'text_model.encoder.layers.9.self_attn.v_proj.SV', 'text_model.encoder.layers.9.self_attn.v_proj.rcp', 'text_model.encoder.layers.9.self_attn.v_proj.tlut', 'text_model.encoder.layers.9.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.v_proj.trellis', 'vision_model.encoder.layers.0.mlp.fc1.SU', 'vision_model.encoder.layers.0.mlp.fc1.SV', 'vision_model.encoder.layers.0.mlp.fc1.rcp', 'vision_model.encoder.layers.0.mlp.fc1.tlut', 'vision_model.encoder.layers.0.mlp.fc1.tp_rank', 'vision_model.encoder.layers.0.mlp.fc1.trellis', 'vision_model.encoder.layers.0.mlp.fc2.SU', 'vision_model.encoder.layers.0.mlp.fc2.SV', 'vision_model.encoder.layers.0.mlp.fc2.rcp', 'vision_model.encoder.layers.0.mlp.fc2.tlut', 'vision_model.encoder.layers.0.mlp.fc2.tp_rank', 'vision_model.encoder.layers.0.mlp.fc2.trellis', 'vision_model.encoder.layers.0.self_attn.k_proj.SU', 'vision_model.encoder.layers.0.self_attn.k_proj.SV', 'vision_model.encoder.layers.0.self_attn.k_proj.rcp', 'vision_model.encoder.layers.0.self_attn.k_proj.tlut', 'vision_model.encoder.layers.0.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.k_proj.trellis', 'vision_model.encoder.layers.0.self_attn.out_proj.SU', 'vision_model.encoder.layers.0.self_attn.out_proj.SV', 'vision_model.encoder.layers.0.self_attn.out_proj.rcp', 'vision_model.encoder.layers.0.self_attn.out_proj.tlut', 'vision_model.encoder.layers.0.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.out_proj.trellis', 'vision_model.encoder.layers.0.self_attn.q_proj.SU', 'vision_model.encoder.layers.0.self_attn.q_proj.SV', 'vision_model.encoder.layers.0.self_attn.q_proj.rcp', 'vision_model.encoder.layers.0.self_attn.q_proj.tlut', 'vision_model.encoder.layers.0.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.q_proj.trellis', 'vision_model.encoder.layers.0.self_attn.v_proj.SU', 'vision_model.encoder.layers.0.self_attn.v_proj.SV', 'vision_model.encoder.layers.0.self_attn.v_proj.rcp', 'vision_model.encoder.layers.0.self_attn.v_proj.tlut', 'vision_model.encoder.layers.0.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.v_proj.trellis', 'vision_model.encoder.layers.1.mlp.fc1.SU', 'vision_model.encoder.layers.1.mlp.fc1.SV', 'vision_model.encoder.layers.1.mlp.fc1.rcp', 'vision_model.encoder.layers.1.mlp.fc1.tlut', 'vision_model.encoder.layers.1.mlp.fc1.tp_rank', 'vision_model.encoder.layers.1.mlp.fc1.trellis', 'vision_model.encoder.layers.1.mlp.fc2.SU', 'vision_model.encoder.layers.1.mlp.fc2.SV', 'vision_model.encoder.layers.1.mlp.fc2.rcp', 'vision_model.encoder.layers.1.mlp.fc2.tlut', 'vision_model.encoder.layers.1.mlp.fc2.tp_rank', 'vision_model.encoder.layers.1.mlp.fc2.trellis', 'vision_model.encoder.layers.1.self_attn.k_proj.SU', 'vision_model.encoder.layers.1.self_attn.k_proj.SV', 'vision_model.encoder.layers.1.self_attn.k_proj.rcp', 'vision_model.encoder.layers.1.self_attn.k_proj.tlut', 'vision_model.encoder.layers.1.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.k_proj.trellis', 'vision_model.encoder.layers.1.self_attn.out_proj.SU', 'vision_model.encoder.layers.1.self_attn.out_proj.SV', 'vision_model.encoder.layers.1.self_attn.out_proj.rcp', 'vision_model.encoder.layers.1.self_attn.out_proj.tlut', 'vision_model.encoder.layers.1.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.out_proj.trellis', 'vision_model.encoder.layers.1.self_attn.q_proj.SU', 'vision_model.encoder.layers.1.self_attn.q_proj.SV', 'vision_model.encoder.layers.1.self_attn.q_proj.rcp', 'vision_model.encoder.layers.1.self_attn.q_proj.tlut', 'vision_model.encoder.layers.1.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.q_proj.trellis', 'vision_model.encoder.layers.1.self_attn.v_proj.SU', 'vision_model.encoder.layers.1.self_attn.v_proj.SV', 'vision_model.encoder.layers.1.self_attn.v_proj.rcp', 'vision_model.encoder.layers.1.self_attn.v_proj.tlut', 'vision_model.encoder.layers.1.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.v_proj.trellis', 'vision_model.encoder.layers.10.mlp.fc1.SU', 'vision_model.encoder.layers.10.mlp.fc1.SV', 'vision_model.encoder.layers.10.mlp.fc1.rcp', 'vision_model.encoder.layers.10.mlp.fc1.tlut', 'vision_model.encoder.layers.10.mlp.fc1.tp_rank', 'vision_model.encoder.layers.10.mlp.fc1.trellis', 'vision_model.encoder.layers.10.mlp.fc2.SU', 'vision_model.encoder.layers.10.mlp.fc2.SV', 'vision_model.encoder.layers.10.mlp.fc2.rcp', 'vision_model.encoder.layers.10.mlp.fc2.tlut', 'vision_model.encoder.layers.10.mlp.fc2.tp_rank', 'vision_model.encoder.layers.10.mlp.fc2.trellis', 'vision_model.encoder.layers.10.self_attn.k_proj.SU', 'vision_model.encoder.layers.10.self_attn.k_proj.SV', 'vision_model.encoder.layers.10.self_attn.k_proj.rcp', 'vision_model.encoder.layers.10.self_attn.k_proj.tlut', 'vision_model.encoder.layers.10.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.k_proj.trellis', 'vision_model.encoder.layers.10.self_attn.out_proj.SU', 'vision_model.encoder.layers.10.self_attn.out_proj.SV', 'vision_model.encoder.layers.10.self_attn.out_proj.rcp', 'vision_model.encoder.layers.10.self_attn.out_proj.tlut', 'vision_model.encoder.layers.10.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.out_proj.trellis', 'vision_model.encoder.layers.10.self_attn.q_proj.SU', 'vision_model.encoder.layers.10.self_attn.q_proj.SV', 'vision_model.encoder.layers.10.self_attn.q_proj.rcp', 'vision_model.encoder.layers.10.self_attn.q_proj.tlut', 'vision_model.encoder.layers.10.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.q_proj.trellis', 'vision_model.encoder.layers.10.self_attn.v_proj.SU', 'vision_model.encoder.layers.10.self_attn.v_proj.SV', 'vision_model.encoder.layers.10.self_attn.v_proj.rcp', 'vision_model.encoder.layers.10.self_attn.v_proj.tlut', 'vision_model.encoder.layers.10.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.v_proj.trellis', 'vision_model.encoder.layers.11.mlp.fc1.SU', 'vision_model.encoder.layers.11.mlp.fc1.SV', 'vision_model.encoder.layers.11.mlp.fc1.rcp', 'vision_model.encoder.layers.11.mlp.fc1.tlut', 'vision_model.encoder.layers.11.mlp.fc1.tp_rank', 'vision_model.encoder.layers.11.mlp.fc1.trellis', 'vision_model.encoder.layers.11.mlp.fc2.SU', 'vision_model.encoder.layers.11.mlp.fc2.SV', 'vision_model.encoder.layers.11.mlp.fc2.rcp', 'vision_model.encoder.layers.11.mlp.fc2.tlut', 'vision_model.encoder.layers.11.mlp.fc2.tp_rank', 'vision_model.encoder.layers.11.mlp.fc2.trellis', 'vision_model.encoder.layers.11.self_attn.k_proj.SU', 'vision_model.encoder.layers.11.self_attn.k_proj.SV', 'vision_model.encoder.layers.11.self_attn.k_proj.rcp', 'vision_model.encoder.layers.11.self_attn.k_proj.tlut', 'vision_model.encoder.layers.11.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.k_proj.trellis', 'vision_model.encoder.layers.11.self_attn.out_proj.SU', 'vision_model.encoder.layers.11.self_attn.out_proj.SV', 'vision_model.encoder.layers.11.self_attn.out_proj.rcp', 'vision_model.encoder.layers.11.self_attn.out_proj.tlut', 'vision_model.encoder.layers.11.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.out_proj.trellis', 'vision_model.encoder.layers.11.self_attn.q_proj.SU', 'vision_model.encoder.layers.11.self_attn.q_proj.SV', 'vision_model.encoder.layers.11.self_attn.q_proj.rcp', 'vision_model.encoder.layers.11.self_attn.q_proj.tlut', 'vision_model.encoder.layers.11.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.q_proj.trellis', 'vision_model.encoder.layers.11.self_attn.v_proj.SU', 'vision_model.encoder.layers.11.self_attn.v_proj.SV', 'vision_model.encoder.layers.11.self_attn.v_proj.rcp', 'vision_model.encoder.layers.11.self_attn.v_proj.tlut', 'vision_model.encoder.layers.11.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.v_proj.trellis', 'vision_model.encoder.layers.12.mlp.fc1.SU', 'vision_model.encoder.layers.12.mlp.fc1.SV', 'vision_model.encoder.layers.12.mlp.fc1.rcp', 'vision_model.encoder.layers.12.mlp.fc1.tlut', 'vision_model.encoder.layers.12.mlp.fc1.tp_rank', 'vision_model.encoder.layers.12.mlp.fc1.trellis', 'vision_model.encoder.layers.12.mlp.fc2.SU', 'vision_model.encoder.layers.12.mlp.fc2.SV', 'vision_model.encoder.layers.12.mlp.fc2.rcp', 'vision_model.encoder.layers.12.mlp.fc2.tlut', 'vision_model.encoder.layers.12.mlp.fc2.tp_rank', 'vision_model.encoder.layers.12.mlp.fc2.trellis', 'vision_model.encoder.layers.12.self_attn.k_proj.SU', 'vision_model.encoder.layers.12.self_attn.k_proj.SV', 'vision_model.encoder.layers.12.self_attn.k_proj.rcp', 'vision_model.encoder.layers.12.self_attn.k_proj.tlut', 'vision_model.encoder.layers.12.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.k_proj.trellis', 'vision_model.encoder.layers.12.self_attn.out_proj.SU', 'vision_model.encoder.layers.12.self_attn.out_proj.SV', 'vision_model.encoder.layers.12.self_attn.out_proj.rcp', 'vision_model.encoder.layers.12.self_attn.out_proj.tlut', 'vision_model.encoder.layers.12.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.out_proj.trellis', 'vision_model.encoder.layers.12.self_attn.q_proj.SU', 'vision_model.encoder.layers.12.self_attn.q_proj.SV', 'vision_model.encoder.layers.12.self_attn.q_proj.rcp', 'vision_model.encoder.layers.12.self_attn.q_proj.tlut', 'vision_model.encoder.layers.12.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.q_proj.trellis', 'vision_model.encoder.layers.12.self_attn.v_proj.SU', 'vision_model.encoder.layers.12.self_attn.v_proj.SV', 'vision_model.encoder.layers.12.self_attn.v_proj.rcp', 'vision_model.encoder.layers.12.self_attn.v_proj.tlut', 'vision_model.encoder.layers.12.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.v_proj.trellis', 'vision_model.encoder.layers.13.mlp.fc1.SU', 'vision_model.encoder.layers.13.mlp.fc1.SV', 'vision_model.encoder.layers.13.mlp.fc1.rcp', 'vision_model.encoder.layers.13.mlp.fc1.tlut', 'vision_model.encoder.layers.13.mlp.fc1.tp_rank', 'vision_model.encoder.layers.13.mlp.fc1.trellis', 'vision_model.encoder.layers.13.mlp.fc2.SU', 'vision_model.encoder.layers.13.mlp.fc2.SV', 'vision_model.encoder.layers.13.mlp.fc2.rcp', 'vision_model.encoder.layers.13.mlp.fc2.tlut', 'vision_model.encoder.layers.13.mlp.fc2.tp_rank', 'vision_model.encoder.layers.13.mlp.fc2.trellis', 'vision_model.encoder.layers.13.self_attn.k_proj.SU', 'vision_model.encoder.layers.13.self_attn.k_proj.SV', 'vision_model.encoder.layers.13.self_attn.k_proj.rcp', 'vision_model.encoder.layers.13.self_attn.k_proj.tlut', 'vision_model.encoder.layers.13.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.k_proj.trellis', 'vision_model.encoder.layers.13.self_attn.out_proj.SU', 'vision_model.encoder.layers.13.self_attn.out_proj.SV', 'vision_model.encoder.layers.13.self_attn.out_proj.rcp', 'vision_model.encoder.layers.13.self_attn.out_proj.tlut', 'vision_model.encoder.layers.13.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.out_proj.trellis', 'vision_model.encoder.layers.13.self_attn.q_proj.SU', 'vision_model.encoder.layers.13.self_attn.q_proj.SV', 'vision_model.encoder.layers.13.self_attn.q_proj.rcp', 'vision_model.encoder.layers.13.self_attn.q_proj.tlut', 'vision_model.encoder.layers.13.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.q_proj.trellis', 'vision_model.encoder.layers.13.self_attn.v_proj.SU', 'vision_model.encoder.layers.13.self_attn.v_proj.SV', 'vision_model.encoder.layers.13.self_attn.v_proj.rcp', 'vision_model.encoder.layers.13.self_attn.v_proj.tlut', 'vision_model.encoder.layers.13.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.v_proj.trellis', 'vision_model.encoder.layers.14.mlp.fc1.SU', 'vision_model.encoder.layers.14.mlp.fc1.SV', 'vision_model.encoder.layers.14.mlp.fc1.rcp', 'vision_model.encoder.layers.14.mlp.fc1.tlut', 'vision_model.encoder.layers.14.mlp.fc1.tp_rank', 'vision_model.encoder.layers.14.mlp.fc1.trellis', 'vision_model.encoder.layers.14.mlp.fc2.SU', 'vision_model.encoder.layers.14.mlp.fc2.SV', 'vision_model.encoder.layers.14.mlp.fc2.rcp', 'vision_model.encoder.layers.14.mlp.fc2.tlut', 'vision_model.encoder.layers.14.mlp.fc2.tp_rank', 'vision_model.encoder.layers.14.mlp.fc2.trellis', 'vision_model.encoder.layers.14.self_attn.k_proj.SU', 'vision_model.encoder.layers.14.self_attn.k_proj.SV', 'vision_model.encoder.layers.14.self_attn.k_proj.rcp', 'vision_model.encoder.layers.14.self_attn.k_proj.tlut', 'vision_model.encoder.layers.14.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.k_proj.trellis', 'vision_model.encoder.layers.14.self_attn.out_proj.SU', 'vision_model.encoder.layers.14.self_attn.out_proj.SV', 'vision_model.encoder.layers.14.self_attn.out_proj.rcp', 'vision_model.encoder.layers.14.self_attn.out_proj.tlut', 'vision_model.encoder.layers.14.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.out_proj.trellis', 'vision_model.encoder.layers.14.self_attn.q_proj.SU', 'vision_model.encoder.layers.14.self_attn.q_proj.SV', 'vision_model.encoder.layers.14.self_attn.q_proj.rcp', 'vision_model.encoder.layers.14.self_attn.q_proj.tlut', 'vision_model.encoder.layers.14.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.q_proj.trellis', 'vision_model.encoder.layers.14.self_attn.v_proj.SU', 'vision_model.encoder.layers.14.self_attn.v_proj.SV', 'vision_model.encoder.layers.14.self_attn.v_proj.rcp', 'vision_model.encoder.layers.14.self_attn.v_proj.tlut', 'vision_model.encoder.layers.14.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.v_proj.trellis', 'vision_model.encoder.layers.15.mlp.fc1.SU', 'vision_model.encoder.layers.15.mlp.fc1.SV', 'vision_model.encoder.layers.15.mlp.fc1.rcp', 'vision_model.encoder.layers.15.mlp.fc1.tlut', 'vision_model.encoder.layers.15.mlp.fc1.tp_rank', 'vision_model.encoder.layers.15.mlp.fc1.trellis', 'vision_model.encoder.layers.15.mlp.fc2.SU', 'vision_model.encoder.layers.15.mlp.fc2.SV', 'vision_model.encoder.layers.15.mlp.fc2.rcp', 'vision_model.encoder.layers.15.mlp.fc2.tlut', 'vision_model.encoder.layers.15.mlp.fc2.tp_rank', 'vision_model.encoder.layers.15.mlp.fc2.trellis', 'vision_model.encoder.layers.15.self_attn.k_proj.SU', 'vision_model.encoder.layers.15.self_attn.k_proj.SV', 'vision_model.encoder.layers.15.self_attn.k_proj.rcp', 'vision_model.encoder.layers.15.self_attn.k_proj.tlut', 'vision_model.encoder.layers.15.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.k_proj.trellis', 'vision_model.encoder.layers.15.self_attn.out_proj.SU', 'vision_model.encoder.layers.15.self_attn.out_proj.SV', 'vision_model.encoder.layers.15.self_attn.out_proj.rcp', 'vision_model.encoder.layers.15.self_attn.out_proj.tlut', 'vision_model.encoder.layers.15.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.out_proj.trellis', 'vision_model.encoder.layers.15.self_attn.q_proj.SU', 'vision_model.encoder.layers.15.self_attn.q_proj.SV', 'vision_model.encoder.layers.15.self_attn.q_proj.rcp', 'vision_model.encoder.layers.15.self_attn.q_proj.tlut', 'vision_model.encoder.layers.15.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.q_proj.trellis', 'vision_model.encoder.layers.15.self_attn.v_proj.SU', 'vision_model.encoder.layers.15.self_attn.v_proj.SV', 'vision_model.encoder.layers.15.self_attn.v_proj.rcp', 'vision_model.encoder.layers.15.self_attn.v_proj.tlut', 'vision_model.encoder.layers.15.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.v_proj.trellis', 'vision_model.encoder.layers.16.mlp.fc1.SU', 'vision_model.encoder.layers.16.mlp.fc1.SV', 'vision_model.encoder.layers.16.mlp.fc1.rcp', 'vision_model.encoder.layers.16.mlp.fc1.tlut', 'vision_model.encoder.layers.16.mlp.fc1.tp_rank', 'vision_model.encoder.layers.16.mlp.fc1.trellis', 'vision_model.encoder.layers.16.mlp.fc2.SU', 'vision_model.encoder.layers.16.mlp.fc2.SV', 'vision_model.encoder.layers.16.mlp.fc2.rcp', 'vision_model.encoder.layers.16.mlp.fc2.tlut', 'vision_model.encoder.layers.16.mlp.fc2.tp_rank', 'vision_model.encoder.layers.16.mlp.fc2.trellis', 'vision_model.encoder.layers.16.self_attn.k_proj.SU', 'vision_model.encoder.layers.16.self_attn.k_proj.SV', 'vision_model.encoder.layers.16.self_attn.k_proj.rcp', 'vision_model.encoder.layers.16.self_attn.k_proj.tlut', 'vision_model.encoder.layers.16.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.k_proj.trellis', 'vision_model.encoder.layers.16.self_attn.out_proj.SU', 'vision_model.encoder.layers.16.self_attn.out_proj.SV', 'vision_model.encoder.layers.16.self_attn.out_proj.rcp', 'vision_model.encoder.layers.16.self_attn.out_proj.tlut', 'vision_model.encoder.layers.16.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.out_proj.trellis', 'vision_model.encoder.layers.16.self_attn.q_proj.SU', 'vision_model.encoder.layers.16.self_attn.q_proj.SV', 'vision_model.encoder.layers.16.self_attn.q_proj.rcp', 'vision_model.encoder.layers.16.self_attn.q_proj.tlut', 'vision_model.encoder.layers.16.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.q_proj.trellis', 'vision_model.encoder.layers.16.self_attn.v_proj.SU', 'vision_model.encoder.layers.16.self_attn.v_proj.SV', 'vision_model.encoder.layers.16.self_attn.v_proj.rcp', 'vision_model.encoder.layers.16.self_attn.v_proj.tlut', 'vision_model.encoder.layers.16.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.v_proj.trellis', 'vision_model.encoder.layers.17.mlp.fc1.SU', 'vision_model.encoder.layers.17.mlp.fc1.SV', 'vision_model.encoder.layers.17.mlp.fc1.rcp', 'vision_model.encoder.layers.17.mlp.fc1.tlut', 'vision_model.encoder.layers.17.mlp.fc1.tp_rank', 'vision_model.encoder.layers.17.mlp.fc1.trellis', 'vision_model.encoder.layers.17.mlp.fc2.SU', 'vision_model.encoder.layers.17.mlp.fc2.SV', 'vision_model.encoder.layers.17.mlp.fc2.rcp', 'vision_model.encoder.layers.17.mlp.fc2.tlut', 'vision_model.encoder.layers.17.mlp.fc2.tp_rank', 'vision_model.encoder.layers.17.mlp.fc2.trellis', 'vision_model.encoder.layers.17.self_attn.k_proj.SU', 'vision_model.encoder.layers.17.self_attn.k_proj.SV', 'vision_model.encoder.layers.17.self_attn.k_proj.rcp', 'vision_model.encoder.layers.17.self_attn.k_proj.tlut', 'vision_model.encoder.layers.17.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.k_proj.trellis', 'vision_model.encoder.layers.17.self_attn.out_proj.SU', 'vision_model.encoder.layers.17.self_attn.out_proj.SV', 'vision_model.encoder.layers.17.self_attn.out_proj.rcp', 'vision_model.encoder.layers.17.self_attn.out_proj.tlut', 'vision_model.encoder.layers.17.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.out_proj.trellis', 'vision_model.encoder.layers.17.self_attn.q_proj.SU', 'vision_model.encoder.layers.17.self_attn.q_proj.SV', 'vision_model.encoder.layers.17.self_attn.q_proj.rcp', 'vision_model.encoder.layers.17.self_attn.q_proj.tlut', 'vision_model.encoder.layers.17.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.q_proj.trellis', 'vision_model.encoder.layers.17.self_attn.v_proj.SU', 'vision_model.encoder.layers.17.self_attn.v_proj.SV', 'vision_model.encoder.layers.17.self_attn.v_proj.rcp', 'vision_model.encoder.layers.17.self_attn.v_proj.tlut', 'vision_model.encoder.layers.17.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.v_proj.trellis', 'vision_model.encoder.layers.18.mlp.fc1.SU', 'vision_model.encoder.layers.18.mlp.fc1.SV', 'vision_model.encoder.layers.18.mlp.fc1.rcp', 'vision_model.encoder.layers.18.mlp.fc1.tlut', 'vision_model.encoder.layers.18.mlp.fc1.tp_rank', 'vision_model.encoder.layers.18.mlp.fc1.trellis', 'vision_model.encoder.layers.18.mlp.fc2.SU', 'vision_model.encoder.layers.18.mlp.fc2.SV', 'vision_model.encoder.layers.18.mlp.fc2.rcp', 'vision_model.encoder.layers.18.mlp.fc2.tlut', 'vision_model.encoder.layers.18.mlp.fc2.tp_rank', 'vision_model.encoder.layers.18.mlp.fc2.trellis', 'vision_model.encoder.layers.18.self_attn.k_proj.SU', 'vision_model.encoder.layers.18.self_attn.k_proj.SV', 'vision_model.encoder.layers.18.self_attn.k_proj.rcp', 'vision_model.encoder.layers.18.self_attn.k_proj.tlut', 'vision_model.encoder.layers.18.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.k_proj.trellis', 'vision_model.encoder.layers.18.self_attn.out_proj.SU', 'vision_model.encoder.layers.18.self_attn.out_proj.SV', 'vision_model.encoder.layers.18.self_attn.out_proj.rcp', 'vision_model.encoder.layers.18.self_attn.out_proj.tlut', 'vision_model.encoder.layers.18.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.out_proj.trellis', 'vision_model.encoder.layers.18.self_attn.q_proj.SU', 'vision_model.encoder.layers.18.self_attn.q_proj.SV', 'vision_model.encoder.layers.18.self_attn.q_proj.rcp', 'vision_model.encoder.layers.18.self_attn.q_proj.tlut', 'vision_model.encoder.layers.18.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.q_proj.trellis', 'vision_model.encoder.layers.18.self_attn.v_proj.SU', 'vision_model.encoder.layers.18.self_attn.v_proj.SV', 'vision_model.encoder.layers.18.self_attn.v_proj.rcp', 'vision_model.encoder.layers.18.self_attn.v_proj.tlut', 'vision_model.encoder.layers.18.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.v_proj.trellis', 'vision_model.encoder.layers.19.mlp.fc1.SU', 'vision_model.encoder.layers.19.mlp.fc1.SV', 'vision_model.encoder.layers.19.mlp.fc1.rcp', 'vision_model.encoder.layers.19.mlp.fc1.tlut', 'vision_model.encoder.layers.19.mlp.fc1.tp_rank', 'vision_model.encoder.layers.19.mlp.fc1.trellis', 'vision_model.encoder.layers.19.mlp.fc2.SU', 'vision_model.encoder.layers.19.mlp.fc2.SV', 'vision_model.encoder.layers.19.mlp.fc2.rcp', 'vision_model.encoder.layers.19.mlp.fc2.tlut', 'vision_model.encoder.layers.19.mlp.fc2.tp_rank', 'vision_model.encoder.layers.19.mlp.fc2.trellis', 'vision_model.encoder.layers.19.self_attn.k_proj.SU', 'vision_model.encoder.layers.19.self_attn.k_proj.SV', 'vision_model.encoder.layers.19.self_attn.k_proj.rcp', 'vision_model.encoder.layers.19.self_attn.k_proj.tlut', 'vision_model.encoder.layers.19.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.k_proj.trellis', 'vision_model.encoder.layers.19.self_attn.out_proj.SU', 'vision_model.encoder.layers.19.self_attn.out_proj.SV', 'vision_model.encoder.layers.19.self_attn.out_proj.rcp', 'vision_model.encoder.layers.19.self_attn.out_proj.tlut', 'vision_model.encoder.layers.19.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.out_proj.trellis', 'vision_model.encoder.layers.19.self_attn.q_proj.SU', 'vision_model.encoder.layers.19.self_attn.q_proj.SV', 'vision_model.encoder.layers.19.self_attn.q_proj.rcp', 'vision_model.encoder.layers.19.self_attn.q_proj.tlut', 'vision_model.encoder.layers.19.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.q_proj.trellis', 'vision_model.encoder.layers.19.self_attn.v_proj.SU', 'vision_model.encoder.layers.19.self_attn.v_proj.SV', 'vision_model.encoder.layers.19.self_attn.v_proj.rcp', 'vision_model.encoder.layers.19.self_attn.v_proj.tlut', 'vision_model.encoder.layers.19.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.v_proj.trellis', 'vision_model.encoder.layers.2.mlp.fc1.SU', 'vision_model.encoder.layers.2.mlp.fc1.SV', 'vision_model.encoder.layers.2.mlp.fc1.rcp', 'vision_model.encoder.layers.2.mlp.fc1.tlut', 'vision_model.encoder.layers.2.mlp.fc1.tp_rank', 'vision_model.encoder.layers.2.mlp.fc1.trellis', 'vision_model.encoder.layers.2.mlp.fc2.SU', 'vision_model.encoder.layers.2.mlp.fc2.SV', 'vision_model.encoder.layers.2.mlp.fc2.rcp', 'vision_model.encoder.layers.2.mlp.fc2.tlut', 'vision_model.encoder.layers.2.mlp.fc2.tp_rank', 'vision_model.encoder.layers.2.mlp.fc2.trellis', 'vision_model.encoder.layers.2.self_attn.k_proj.SU', 'vision_model.encoder.layers.2.self_attn.k_proj.SV', 'vision_model.encoder.layers.2.self_attn.k_proj.rcp', 'vision_model.encoder.layers.2.self_attn.k_proj.tlut', 'vision_model.encoder.layers.2.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.k_proj.trellis', 'vision_model.encoder.layers.2.self_attn.out_proj.SU', 'vision_model.encoder.layers.2.self_attn.out_proj.SV', 'vision_model.encoder.layers.2.self_attn.out_proj.rcp', 'vision_model.encoder.layers.2.self_attn.out_proj.tlut', 'vision_model.encoder.layers.2.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.out_proj.trellis', 'vision_model.encoder.layers.2.self_attn.q_proj.SU', 'vision_model.encoder.layers.2.self_attn.q_proj.SV', 'vision_model.encoder.layers.2.self_attn.q_proj.rcp', 'vision_model.encoder.layers.2.self_attn.q_proj.tlut', 'vision_model.encoder.layers.2.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.q_proj.trellis', 'vision_model.encoder.layers.2.self_attn.v_proj.SU', 'vision_model.encoder.layers.2.self_attn.v_proj.SV', 'vision_model.encoder.layers.2.self_attn.v_proj.rcp', 'vision_model.encoder.layers.2.self_attn.v_proj.tlut', 'vision_model.encoder.layers.2.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.v_proj.trellis', 'vision_model.encoder.layers.20.mlp.fc1.SU', 'vision_model.encoder.layers.20.mlp.fc1.SV', 'vision_model.encoder.layers.20.mlp.fc1.rcp', 'vision_model.encoder.layers.20.mlp.fc1.tlut', 'vision_model.encoder.layers.20.mlp.fc1.tp_rank', 'vision_model.encoder.layers.20.mlp.fc1.trellis', 'vision_model.encoder.layers.20.mlp.fc2.SU', 'vision_model.encoder.layers.20.mlp.fc2.SV', 'vision_model.encoder.layers.20.mlp.fc2.rcp', 'vision_model.encoder.layers.20.mlp.fc2.tlut', 'vision_model.encoder.layers.20.mlp.fc2.tp_rank', 'vision_model.encoder.layers.20.mlp.fc2.trellis', 'vision_model.encoder.layers.20.self_attn.k_proj.SU', 'vision_model.encoder.layers.20.self_attn.k_proj.SV', 'vision_model.encoder.layers.20.self_attn.k_proj.rcp', 'vision_model.encoder.layers.20.self_attn.k_proj.tlut', 'vision_model.encoder.layers.20.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.k_proj.trellis', 'vision_model.encoder.layers.20.self_attn.out_proj.SU', 'vision_model.encoder.layers.20.self_attn.out_proj.SV', 'vision_model.encoder.layers.20.self_attn.out_proj.rcp', 'vision_model.encoder.layers.20.self_attn.out_proj.tlut', 'vision_model.encoder.layers.20.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.out_proj.trellis', 'vision_model.encoder.layers.20.self_attn.q_proj.SU', 'vision_model.encoder.layers.20.self_attn.q_proj.SV', 'vision_model.encoder.layers.20.self_attn.q_proj.rcp', 'vision_model.encoder.layers.20.self_attn.q_proj.tlut', 'vision_model.encoder.layers.20.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.q_proj.trellis', 'vision_model.encoder.layers.20.self_attn.v_proj.SU', 'vision_model.encoder.layers.20.self_attn.v_proj.SV', 'vision_model.encoder.layers.20.self_attn.v_proj.rcp', 'vision_model.encoder.layers.20.self_attn.v_proj.tlut', 'vision_model.encoder.layers.20.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.v_proj.trellis', 'vision_model.encoder.layers.21.mlp.fc1.SU', 'vision_model.encoder.layers.21.mlp.fc1.SV', 'vision_model.encoder.layers.21.mlp.fc1.rcp', 'vision_model.encoder.layers.21.mlp.fc1.tlut', 'vision_model.encoder.layers.21.mlp.fc1.tp_rank', 'vision_model.encoder.layers.21.mlp.fc1.trellis', 'vision_model.encoder.layers.21.mlp.fc2.SU', 'vision_model.encoder.layers.21.mlp.fc2.SV', 'vision_model.encoder.layers.21.mlp.fc2.rcp', 'vision_model.encoder.layers.21.mlp.fc2.tlut', 'vision_model.encoder.layers.21.mlp.fc2.tp_rank', 'vision_model.encoder.layers.21.mlp.fc2.trellis', 'vision_model.encoder.layers.21.self_attn.k_proj.SU', 'vision_model.encoder.layers.21.self_attn.k_proj.SV', 'vision_model.encoder.layers.21.self_attn.k_proj.rcp', 'vision_model.encoder.layers.21.self_attn.k_proj.tlut', 'vision_model.encoder.layers.21.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.k_proj.trellis', 'vision_model.encoder.layers.21.self_attn.out_proj.SU', 'vision_model.encoder.layers.21.self_attn.out_proj.SV', 'vision_model.encoder.layers.21.self_attn.out_proj.rcp', 'vision_model.encoder.layers.21.self_attn.out_proj.tlut', 'vision_model.encoder.layers.21.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.out_proj.trellis', 'vision_model.encoder.layers.21.self_attn.q_proj.SU', 'vision_model.encoder.layers.21.self_attn.q_proj.SV', 'vision_model.encoder.layers.21.self_attn.q_proj.rcp', 'vision_model.encoder.layers.21.self_attn.q_proj.tlut', 'vision_model.encoder.layers.21.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.q_proj.trellis', 'vision_model.encoder.layers.21.self_attn.v_proj.SU', 'vision_model.encoder.layers.21.self_attn.v_proj.SV', 'vision_model.encoder.layers.21.self_attn.v_proj.rcp', 'vision_model.encoder.layers.21.self_attn.v_proj.tlut', 'vision_model.encoder.layers.21.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.v_proj.trellis', 'vision_model.encoder.layers.22.mlp.fc1.SU', 'vision_model.encoder.layers.22.mlp.fc1.SV', 'vision_model.encoder.layers.22.mlp.fc1.rcp', 'vision_model.encoder.layers.22.mlp.fc1.tlut', 'vision_model.encoder.layers.22.mlp.fc1.tp_rank', 'vision_model.encoder.layers.22.mlp.fc1.trellis', 'vision_model.encoder.layers.22.mlp.fc2.SU', 'vision_model.encoder.layers.22.mlp.fc2.SV', 'vision_model.encoder.layers.22.mlp.fc2.rcp', 'vision_model.encoder.layers.22.mlp.fc2.tlut', 'vision_model.encoder.layers.22.mlp.fc2.tp_rank', 'vision_model.encoder.layers.22.mlp.fc2.trellis', 'vision_model.encoder.layers.22.self_attn.k_proj.SU', 'vision_model.encoder.layers.22.self_attn.k_proj.SV', 'vision_model.encoder.layers.22.self_attn.k_proj.rcp', 'vision_model.encoder.layers.22.self_attn.k_proj.tlut', 'vision_model.encoder.layers.22.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.k_proj.trellis', 'vision_model.encoder.layers.22.self_attn.out_proj.SU', 'vision_model.encoder.layers.22.self_attn.out_proj.SV', 'vision_model.encoder.layers.22.self_attn.out_proj.rcp', 'vision_model.encoder.layers.22.self_attn.out_proj.tlut', 'vision_model.encoder.layers.22.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.out_proj.trellis', 'vision_model.encoder.layers.22.self_attn.q_proj.SU', 'vision_model.encoder.layers.22.self_attn.q_proj.SV', 'vision_model.encoder.layers.22.self_attn.q_proj.rcp', 'vision_model.encoder.layers.22.self_attn.q_proj.tlut', 'vision_model.encoder.layers.22.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.q_proj.trellis', 'vision_model.encoder.layers.22.self_attn.v_proj.SU', 'vision_model.encoder.layers.22.self_attn.v_proj.SV', 'vision_model.encoder.layers.22.self_attn.v_proj.rcp', 'vision_model.encoder.layers.22.self_attn.v_proj.tlut', 'vision_model.encoder.layers.22.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.v_proj.trellis', 'vision_model.encoder.layers.23.mlp.fc1.SU', 'vision_model.encoder.layers.23.mlp.fc1.SV', 'vision_model.encoder.layers.23.mlp.fc1.rcp', 'vision_model.encoder.layers.23.mlp.fc1.tlut', 'vision_model.encoder.layers.23.mlp.fc1.tp_rank', 'vision_model.encoder.layers.23.mlp.fc1.trellis', 'vision_model.encoder.layers.23.mlp.fc2.SU', 'vision_model.encoder.layers.23.mlp.fc2.SV', 'vision_model.encoder.layers.23.mlp.fc2.rcp', 'vision_model.encoder.layers.23.mlp.fc2.tlut', 'vision_model.encoder.layers.23.mlp.fc2.tp_rank', 'vision_model.encoder.layers.23.mlp.fc2.trellis', 'vision_model.encoder.layers.23.self_attn.k_proj.SU', 'vision_model.encoder.layers.23.self_attn.k_proj.SV', 'vision_model.encoder.layers.23.self_attn.k_proj.rcp', 'vision_model.encoder.layers.23.self_attn.k_proj.tlut', 'vision_model.encoder.layers.23.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.k_proj.trellis', 'vision_model.encoder.layers.23.self_attn.out_proj.SU', 'vision_model.encoder.layers.23.self_attn.out_proj.SV', 'vision_model.encoder.layers.23.self_attn.out_proj.rcp', 'vision_model.encoder.layers.23.self_attn.out_proj.tlut', 'vision_model.encoder.layers.23.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.out_proj.trellis', 'vision_model.encoder.layers.23.self_attn.q_proj.SU', 'vision_model.encoder.layers.23.self_attn.q_proj.SV', 'vision_model.encoder.layers.23.self_attn.q_proj.rcp', 'vision_model.encoder.layers.23.self_attn.q_proj.tlut', 'vision_model.encoder.layers.23.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.q_proj.trellis', 'vision_model.encoder.layers.23.self_attn.v_proj.SU', 'vision_model.encoder.layers.23.self_attn.v_proj.SV', 'vision_model.encoder.layers.23.self_attn.v_proj.rcp', 'vision_model.encoder.layers.23.self_attn.v_proj.tlut', 'vision_model.encoder.layers.23.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.v_proj.trellis', 'vision_model.encoder.layers.3.mlp.fc1.SU', 'vision_model.encoder.layers.3.mlp.fc1.SV', 'vision_model.encoder.layers.3.mlp.fc1.rcp', 'vision_model.encoder.layers.3.mlp.fc1.tlut', 'vision_model.encoder.layers.3.mlp.fc1.tp_rank', 'vision_model.encoder.layers.3.mlp.fc1.trellis', 'vision_model.encoder.layers.3.mlp.fc2.SU', 'vision_model.encoder.layers.3.mlp.fc2.SV', 'vision_model.encoder.layers.3.mlp.fc2.rcp', 'vision_model.encoder.layers.3.mlp.fc2.tlut', 'vision_model.encoder.layers.3.mlp.fc2.tp_rank', 'vision_model.encoder.layers.3.mlp.fc2.trellis', 'vision_model.encoder.layers.3.self_attn.k_proj.SU', 'vision_model.encoder.layers.3.self_attn.k_proj.SV', 'vision_model.encoder.layers.3.self_attn.k_proj.rcp', 'vision_model.encoder.layers.3.self_attn.k_proj.tlut', 'vision_model.encoder.layers.3.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.k_proj.trellis', 'vision_model.encoder.layers.3.self_attn.out_proj.SU', 'vision_model.encoder.layers.3.self_attn.out_proj.SV', 'vision_model.encoder.layers.3.self_attn.out_proj.rcp', 'vision_model.encoder.layers.3.self_attn.out_proj.tlut', 'vision_model.encoder.layers.3.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.out_proj.trellis', 'vision_model.encoder.layers.3.self_attn.q_proj.SU', 'vision_model.encoder.layers.3.self_attn.q_proj.SV', 'vision_model.encoder.layers.3.self_attn.q_proj.rcp', 'vision_model.encoder.layers.3.self_attn.q_proj.tlut', 'vision_model.encoder.layers.3.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.q_proj.trellis', 'vision_model.encoder.layers.3.self_attn.v_proj.SU', 'vision_model.encoder.layers.3.self_attn.v_proj.SV', 'vision_model.encoder.layers.3.self_attn.v_proj.rcp', 'vision_model.encoder.layers.3.self_attn.v_proj.tlut', 'vision_model.encoder.layers.3.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.v_proj.trellis', 'vision_model.encoder.layers.4.mlp.fc1.SU', 'vision_model.encoder.layers.4.mlp.fc1.SV', 'vision_model.encoder.layers.4.mlp.fc1.rcp', 'vision_model.encoder.layers.4.mlp.fc1.tlut', 'vision_model.encoder.layers.4.mlp.fc1.tp_rank', 'vision_model.encoder.layers.4.mlp.fc1.trellis', 'vision_model.encoder.layers.4.mlp.fc2.SU', 'vision_model.encoder.layers.4.mlp.fc2.SV', 'vision_model.encoder.layers.4.mlp.fc2.rcp', 'vision_model.encoder.layers.4.mlp.fc2.tlut', 'vision_model.encoder.layers.4.mlp.fc2.tp_rank', 'vision_model.encoder.layers.4.mlp.fc2.trellis', 'vision_model.encoder.layers.4.self_attn.k_proj.SU', 'vision_model.encoder.layers.4.self_attn.k_proj.SV', 'vision_model.encoder.layers.4.self_attn.k_proj.rcp', 'vision_model.encoder.layers.4.self_attn.k_proj.tlut', 'vision_model.encoder.layers.4.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.k_proj.trellis', 'vision_model.encoder.layers.4.self_attn.out_proj.SU', 'vision_model.encoder.layers.4.self_attn.out_proj.SV', 'vision_model.encoder.layers.4.self_attn.out_proj.rcp', 'vision_model.encoder.layers.4.self_attn.out_proj.tlut', 'vision_model.encoder.layers.4.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.out_proj.trellis', 'vision_model.encoder.layers.4.self_attn.q_proj.SU', 'vision_model.encoder.layers.4.self_attn.q_proj.SV', 'vision_model.encoder.layers.4.self_attn.q_proj.rcp', 'vision_model.encoder.layers.4.self_attn.q_proj.tlut', 'vision_model.encoder.layers.4.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.q_proj.trellis', 'vision_model.encoder.layers.4.self_attn.v_proj.SU', 'vision_model.encoder.layers.4.self_attn.v_proj.SV', 'vision_model.encoder.layers.4.self_attn.v_proj.rcp', 'vision_model.encoder.layers.4.self_attn.v_proj.tlut', 'vision_model.encoder.layers.4.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.v_proj.trellis', 'vision_model.encoder.layers.5.mlp.fc1.SU', 'vision_model.encoder.layers.5.mlp.fc1.SV', 'vision_model.encoder.layers.5.mlp.fc1.rcp', 'vision_model.encoder.layers.5.mlp.fc1.tlut', 'vision_model.encoder.layers.5.mlp.fc1.tp_rank', 'vision_model.encoder.layers.5.mlp.fc1.trellis', 'vision_model.encoder.layers.5.mlp.fc2.SU', 'vision_model.encoder.layers.5.mlp.fc2.SV', 'vision_model.encoder.layers.5.mlp.fc2.rcp', 'vision_model.encoder.layers.5.mlp.fc2.tlut', 'vision_model.encoder.layers.5.mlp.fc2.tp_rank', 'vision_model.encoder.layers.5.mlp.fc2.trellis', 'vision_model.encoder.layers.5.self_attn.k_proj.SU', 'vision_model.encoder.layers.5.self_attn.k_proj.SV', 'vision_model.encoder.layers.5.self_attn.k_proj.rcp', 'vision_model.encoder.layers.5.self_attn.k_proj.tlut', 'vision_model.encoder.layers.5.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.k_proj.trellis', 'vision_model.encoder.layers.5.self_attn.out_proj.SU', 'vision_model.encoder.layers.5.self_attn.out_proj.SV', 'vision_model.encoder.layers.5.self_attn.out_proj.rcp', 'vision_model.encoder.layers.5.self_attn.out_proj.tlut', 'vision_model.encoder.layers.5.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.out_proj.trellis', 'vision_model.encoder.layers.5.self_attn.q_proj.SU', 'vision_model.encoder.layers.5.self_attn.q_proj.SV', 'vision_model.encoder.layers.5.self_attn.q_proj.rcp', 'vision_model.encoder.layers.5.self_attn.q_proj.tlut', 'vision_model.encoder.layers.5.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.q_proj.trellis', 'vision_model.encoder.layers.5.self_attn.v_proj.SU', 'vision_model.encoder.layers.5.self_attn.v_proj.SV', 'vision_model.encoder.layers.5.self_attn.v_proj.rcp', 'vision_model.encoder.layers.5.self_attn.v_proj.tlut', 'vision_model.encoder.layers.5.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.v_proj.trellis', 'vision_model.encoder.layers.6.mlp.fc1.SU', 'vision_model.encoder.layers.6.mlp.fc1.SV', 'vision_model.encoder.layers.6.mlp.fc1.rcp', 'vision_model.encoder.layers.6.mlp.fc1.tlut', 'vision_model.encoder.layers.6.mlp.fc1.tp_rank', 'vision_model.encoder.layers.6.mlp.fc1.trellis', 'vision_model.encoder.layers.6.mlp.fc2.SU', 'vision_model.encoder.layers.6.mlp.fc2.SV', 'vision_model.encoder.layers.6.mlp.fc2.rcp', 'vision_model.encoder.layers.6.mlp.fc2.tlut', 'vision_model.encoder.layers.6.mlp.fc2.tp_rank', 'vision_model.encoder.layers.6.mlp.fc2.trellis', 'vision_model.encoder.layers.6.self_attn.k_proj.SU', 'vision_model.encoder.layers.6.self_attn.k_proj.SV', 'vision_model.encoder.layers.6.self_attn.k_proj.rcp', 'vision_model.encoder.layers.6.self_attn.k_proj.tlut', 'vision_model.encoder.layers.6.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.k_proj.trellis', 'vision_model.encoder.layers.6.self_attn.out_proj.SU', 'vision_model.encoder.layers.6.self_attn.out_proj.SV', 'vision_model.encoder.layers.6.self_attn.out_proj.rcp', 'vision_model.encoder.layers.6.self_attn.out_proj.tlut', 'vision_model.encoder.layers.6.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.out_proj.trellis', 'vision_model.encoder.layers.6.self_attn.q_proj.SU', 'vision_model.encoder.layers.6.self_attn.q_proj.SV', 'vision_model.encoder.layers.6.self_attn.q_proj.rcp', 'vision_model.encoder.layers.6.self_attn.q_proj.tlut', 'vision_model.encoder.layers.6.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.q_proj.trellis', 'vision_model.encoder.layers.6.self_attn.v_proj.SU', 'vision_model.encoder.layers.6.self_attn.v_proj.SV', 'vision_model.encoder.layers.6.self_attn.v_proj.rcp', 'vision_model.encoder.layers.6.self_attn.v_proj.tlut', 'vision_model.encoder.layers.6.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.v_proj.trellis', 'vision_model.encoder.layers.7.mlp.fc1.SU', 'vision_model.encoder.layers.7.mlp.fc1.SV', 'vision_model.encoder.layers.7.mlp.fc1.rcp', 'vision_model.encoder.layers.7.mlp.fc1.tlut', 'vision_model.encoder.layers.7.mlp.fc1.tp_rank', 'vision_model.encoder.layers.7.mlp.fc1.trellis', 'vision_model.encoder.layers.7.mlp.fc2.SU', 'vision_model.encoder.layers.7.mlp.fc2.SV', 'vision_model.encoder.layers.7.mlp.fc2.rcp', 'vision_model.encoder.layers.7.mlp.fc2.tlut', 'vision_model.encoder.layers.7.mlp.fc2.tp_rank', 'vision_model.encoder.layers.7.mlp.fc2.trellis', 'vision_model.encoder.layers.7.self_attn.k_proj.SU', 'vision_model.encoder.layers.7.self_attn.k_proj.SV', 'vision_model.encoder.layers.7.self_attn.k_proj.rcp', 'vision_model.encoder.layers.7.self_attn.k_proj.tlut', 'vision_model.encoder.layers.7.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.k_proj.trellis', 'vision_model.encoder.layers.7.self_attn.out_proj.SU', 'vision_model.encoder.layers.7.self_attn.out_proj.SV', 'vision_model.encoder.layers.7.self_attn.out_proj.rcp', 'vision_model.encoder.layers.7.self_attn.out_proj.tlut', 'vision_model.encoder.layers.7.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.out_proj.trellis', 'vision_model.encoder.layers.7.self_attn.q_proj.SU', 'vision_model.encoder.layers.7.self_attn.q_proj.SV', 'vision_model.encoder.layers.7.self_attn.q_proj.rcp', 'vision_model.encoder.layers.7.self_attn.q_proj.tlut', 'vision_model.encoder.layers.7.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.q_proj.trellis', 'vision_model.encoder.layers.7.self_attn.v_proj.SU', 'vision_model.encoder.layers.7.self_attn.v_proj.SV', 'vision_model.encoder.layers.7.self_attn.v_proj.rcp', 'vision_model.encoder.layers.7.self_attn.v_proj.tlut', 'vision_model.encoder.layers.7.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.v_proj.trellis', 'vision_model.encoder.layers.8.mlp.fc1.SU', 'vision_model.encoder.layers.8.mlp.fc1.SV', 'vision_model.encoder.layers.8.mlp.fc1.rcp', 'vision_model.encoder.layers.8.mlp.fc1.tlut', 'vision_model.encoder.layers.8.mlp.fc1.tp_rank', 'vision_model.encoder.layers.8.mlp.fc1.trellis', 'vision_model.encoder.layers.8.mlp.fc2.SU', 'vision_model.encoder.layers.8.mlp.fc2.SV', 'vision_model.encoder.layers.8.mlp.fc2.rcp', 'vision_model.encoder.layers.8.mlp.fc2.tlut', 'vision_model.encoder.layers.8.mlp.fc2.tp_rank', 'vision_model.encoder.layers.8.mlp.fc2.trellis', 'vision_model.encoder.layers.8.self_attn.k_proj.SU', 'vision_model.encoder.layers.8.self_attn.k_proj.SV', 'vision_model.encoder.layers.8.self_attn.k_proj.rcp', 'vision_model.encoder.layers.8.self_attn.k_proj.tlut', 'vision_model.encoder.layers.8.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.k_proj.trellis', 'vision_model.encoder.layers.8.self_attn.out_proj.SU', 'vision_model.encoder.layers.8.self_attn.out_proj.SV', 'vision_model.encoder.layers.8.self_attn.out_proj.rcp', 'vision_model.encoder.layers.8.self_attn.out_proj.tlut', 'vision_model.encoder.layers.8.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.out_proj.trellis', 'vision_model.encoder.layers.8.self_attn.q_proj.SU', 'vision_model.encoder.layers.8.self_attn.q_proj.SV', 'vision_model.encoder.layers.8.self_attn.q_proj.rcp', 'vision_model.encoder.layers.8.self_attn.q_proj.tlut', 'vision_model.encoder.layers.8.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.q_proj.trellis', 'vision_model.encoder.layers.8.self_attn.v_proj.SU', 'vision_model.encoder.layers.8.self_attn.v_proj.SV', 'vision_model.encoder.layers.8.self_attn.v_proj.rcp', 'vision_model.encoder.layers.8.self_attn.v_proj.tlut', 'vision_model.encoder.layers.8.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.v_proj.trellis', 'vision_model.encoder.layers.9.mlp.fc1.SU', 'vision_model.encoder.layers.9.mlp.fc1.SV', 'vision_model.encoder.layers.9.mlp.fc1.rcp', 'vision_model.encoder.layers.9.mlp.fc1.tlut', 'vision_model.encoder.layers.9.mlp.fc1.tp_rank', 'vision_model.encoder.layers.9.mlp.fc1.trellis', 'vision_model.encoder.layers.9.mlp.fc2.SU', 'vision_model.encoder.layers.9.mlp.fc2.SV', 'vision_model.encoder.layers.9.mlp.fc2.rcp', 'vision_model.encoder.layers.9.mlp.fc2.tlut', 'vision_model.encoder.layers.9.mlp.fc2.tp_rank', 'vision_model.encoder.layers.9.mlp.fc2.trellis', 'vision_model.encoder.layers.9.self_attn.k_proj.SU', 'vision_model.encoder.layers.9.self_attn.k_proj.SV', 'vision_model.encoder.layers.9.self_attn.k_proj.rcp', 'vision_model.encoder.layers.9.self_attn.k_proj.tlut', 'vision_model.encoder.layers.9.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.k_proj.trellis', 'vision_model.encoder.layers.9.self_attn.out_proj.SU', 'vision_model.encoder.layers.9.self_attn.out_proj.SV', 'vision_model.encoder.layers.9.self_attn.out_proj.rcp', 'vision_model.encoder.layers.9.self_attn.out_proj.tlut', 'vision_model.encoder.layers.9.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.out_proj.trellis', 'vision_model.encoder.layers.9.self_attn.q_proj.SU', 'vision_model.encoder.layers.9.self_attn.q_proj.SV', 'vision_model.encoder.layers.9.self_attn.q_proj.rcp', 'vision_model.encoder.layers.9.self_attn.q_proj.tlut', 'vision_model.encoder.layers.9.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.q_proj.trellis', 'vision_model.encoder.layers.9.self_attn.v_proj.SU', 'vision_model.encoder.layers.9.self_attn.v_proj.SV', 'vision_model.encoder.layers.9.self_attn.v_proj.rcp', 'vision_model.encoder.layers.9.self_attn.v_proj.tlut', 'vision_model.encoder.layers.9.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
I0416 08:19:17.684190 3250180 hfize_clip.py:65] Loading text layer 0
W0416 08:19:17.684519 3250180 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved = torch.load(f'{path_prefix}/{full_key}.pt', map_location='cpu')

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py", line 133, in <module>
    main(args)
  File "/workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py", line 80, in main
    load_clip_block('text', model.text_model.encoder.layers, orig_model.text_model.encoder.layers)
  File "/workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py", line 71, in load_clip_block
    load_proj_or_restore(layer.self_attn, 'q_proj', f'{prefix}_{i}', 'q', orig.self_attn, args.quantized_path, skip_list)
  File "/workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py", line 32, in load_proj_or_restore
    saved = torch.load(f'{path_prefix}/{full_key}.pt', map_location='cpu')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 1065, in load
    with _open_file_like(f, 'rb') as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 468, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 449, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '../hf_model_comp/qtip/ckpt/clip-vit-large-patch14_10bit/text_0_q.pt'
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../hf_model_comp/qtip/hf/clip-vit-large-patch14_10bit'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/qtip/eval/eval_clip_imagenet.py", line 267, in <module>
    main(args)
  File "/workspace/Weight_compression/qtip/eval/eval_clip_imagenet.py", line 176, in main
    model = model_from_hf_path_clip(args.hf_path).to('cuda')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/eval/eval_clip_imagenet.py", line 156, in model_from_hf_path_clip
    bad_config = transformers.AutoConfig.from_pretrained(path)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1006, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 570, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 629, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 469, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: '../hf_model_comp/qtip/hf/clip-vit-large-patch14_10bit'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
