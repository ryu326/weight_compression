Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './hf/3_8b_4bit'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/qtip/eval/eval_ppl.py", line 74, in <module>
    main(args)
  File "/workspace/Weight_compression/qtip/eval/eval_ppl.py", line 29, in main
    model, model_str = model_from_hf_path(args.hf_path, max_mem_ratio=args.max_mem_ratio)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/utils/unsafe_import.py", line 16, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1006, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 570, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 629, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 469, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: './hf/3_8b_4bit'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
I0315 06:57:55.235115 433724 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 06:57:55.235258 433724 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 06:57:55.235301 433724 utils.py:162] NumExpr defaulting to 16 threads.
I0315 06:57:55.433040 433724 config.py:58] PyTorch version 2.4.0 available.
W0315 06:57:57.615684 433724 warnings.py:110] /workspace/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.64it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.12it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.35it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.48it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.54it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.58it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.64it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.48it/s]
I0315 06:57:59.264518 433724 quantize_finetune_llama.py:135] loaded model
I0315 06:58:15.732891 434745 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 06:58:15.733025 434745 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 06:58:15.733072 434745 utils.py:162] NumExpr defaulting to 16 threads.
I0315 06:58:15.918643 434745 config.py:58] PyTorch version 2.4.0 available.
W0315 06:58:17.704023 434745 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0315 06:58:17.704533 434745 hfize_llama.py:25] LlamaConfig {
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {
    "K": 4,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  3.97it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  4.83it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  5.71it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  5.93it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  6.42it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  6.69it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  6.67it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  6.12it/s]
Some weights of the model checkpoint at ../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B and are newly initialized: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.tp_rank', 'model.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.33it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.53it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.56it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.45it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.53it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.44it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.58it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.52it/s]
W0315 06:58:23.412372 434745 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py", line 129, in <module>
    main(args)
  File "/workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py", line 65, in main
    saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 1065, in load
    with _open_file_like(f, 'rb') as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 468, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 449, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: './ckpt/3_8b_4bit/0_q.pt'
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './hf/3_8b_4bit'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/qtip/eval/eval_ppl.py", line 74, in <module>
    main(args)
  File "/workspace/Weight_compression/qtip/eval/eval_ppl.py", line 29, in main
    model, model_str = model_from_hf_path(args.hf_path, max_mem_ratio=args.max_mem_ratio)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/utils/unsafe_import.py", line 16, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1006, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 570, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 629, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 469, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: './hf/3_8b_4bit'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
I0316 08:42:49.367674 460347 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 08:42:49.367937 460347 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 08:42:49.367982 460347 utils.py:162] NumExpr defaulting to 16 threads.
I0316 08:42:49.572345 460347 config.py:58] PyTorch version 2.4.0 available.
W0316 08:42:51.758958 460347 warnings.py:110] /workspace/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.44it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.85it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.92it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.07it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.11it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.17it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.26it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.10it/s]
I0316 08:42:53.451889 460347 quantize_finetune_llama.py:135] loaded model
I0316 08:43:13.526435 460347 quantize_finetune_llama.py:139] loaded dataset and devset
I0316 08:43:22.376458 460347 quantize_finetune_llama.py:159] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 08:44:23.100983 460347 quantize_finetune_llama.py:186] computed original embedding for layer 0 in 60.57680416107178s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0316 08:44:47.155362 460347 quantize_finetune_llama.py:159] layer 1 gpu 1
I0316 08:44:49.334211 462198 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 08:44:49.334378 462198 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 08:44:49.334440 462198 utils.py:162] NumExpr defaulting to 16 threads.
I0316 08:44:49.577863 462198 config.py:58] PyTorch version 2.4.0 available.
I0316 08:44:51.778383 462198 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 08:44:52.393893 462198 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:03<01:50,  3.57s/it]  6%|▋         | 2/32 [00:03<00:49,  1.66s/it]  9%|▉         | 3/32 [00:04<00:30,  1.06s/it] 12%|█▎        | 4/32 [00:04<00:21,  1.29it/s] 16%|█▌        | 5/32 [00:04<00:16,  1.62it/s] 19%|█▉        | 6/32 [00:05<00:13,  1.93it/s] 22%|██▏       | 7/32 [00:05<00:11,  2.20it/s] 25%|██▌       | 8/32 [00:05<00:09,  2.41it/s] 28%|██▊       | 9/32 [00:06<00:08,  2.59it/s] 31%|███▏      | 10/32 [00:06<00:08,  2.69it/s] 34%|███▍      | 11/32 [00:06<00:07,  2.74it/s] 38%|███▊      | 12/32 [00:07<00:07,  2.78it/s] 41%|████      | 13/32 [00:07<00:06,  2.80it/s] 44%|████▍     | 14/32 [00:07<00:06,  2.86it/s] 47%|████▋     | 15/32 [00:08<00:05,  2.91it/s] 50%|█████     | 16/32 [00:08<00:05,  2.93it/s] 53%|█████▎    | 17/32 [00:08<00:05,  2.92it/s] 56%|█████▋    | 18/32 [00:09<00:04,  2.90it/s] 59%|█████▉    | 19/32 [00:09<00:04,  2.92it/s] 62%|██████▎   | 20/32 [00:09<00:04,  2.95it/s] 66%|██████▌   | 21/32 [00:10<00:03,  2.98it/s] 69%|██████▉   | 22/32 [00:10<00:03,  2.98it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.96it/s] 75%|███████▌  | 24/32 [00:11<00:02,  3.00it/s] 78%|███████▊  | 25/32 [00:11<00:02,  2.96it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.97it/s] 84%|████████▍ | 27/32 [00:12<00:01,  2.98it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.96it/s] 91%|█████████ | 29/32 [00:13<00:01,  2.93it/s] 94%|█████████▍| 30/32 [00:13<00:00,  2.95it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.96it/s]100%|██████████| 32/32 [00:14<00:00,  2.97it/s]100%|██████████| 32/32 [00:14<00:00,  2.28it/s]
W0316 08:45:10.282000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:45:10.282000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:45:10.283000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:45:10.283000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:45:10.283000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 08:45:10.283000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:45:10.283000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 08:45:10.309000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:45:10.309000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:45:10.310000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:45:10.310000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:45:10.310000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:45:10.600000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:45:10.600000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:45:10.601000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:45:10.601000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:45:10.601000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:45:11.201000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:45:11.201000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:45:11.201000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:45:11.201000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 08:45:11.201000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:45:11.202000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 08:45:11.202000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:45:11.219000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:45:11.219000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:45:11.219000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:45:11.219000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:45:11.219000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:45:11.416000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:45:11.417000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:45:11.417000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:45:11.417000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:45:11.417000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:45:12.549000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:45:12.550000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:45:12.550000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:45:12.550000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 08:45:12.550000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:45:12.550000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 08:45:12.550000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:45:12.569000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:45:12.569000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:45:12.569000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:45:12.569000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:45:12.569000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:45:13.188000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:45:13.189000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:45:13.189000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:45:13.189000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:45:13.189000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 08:45:19.908560 462198 finetune.py:45] layer 0_v initial loss 3.3836087709460116e-08
W0316 08:45:19.908779 462198 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 08:45:51.240179 460347 quantize_finetune_llama.py:186] computed original embedding for layer 1 in 63.9334831237793s
I0316 08:45:55.115648 462198 finetune.py:68] layer 0_v @ epoch 0 new loss 2.772348040025463e-08 old loss 3.3836087709460116e-08 BETTER
I0316 08:46:00.335999 460347 quantize_finetune_llama.py:159] layer 2 gpu 2
I0316 08:46:02.338033 463610 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 08:46:02.338142 463610 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 08:46:02.338203 463610 utils.py:162] NumExpr defaulting to 16 threads.
I0316 08:46:02.527685 463610 config.py:58] PyTorch version 2.4.0 available.
I0316 08:46:04.713019 463610 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 08:46:05.184825 463610 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:04<02:05,  4.04s/it]  6%|▋         | 2/32 [00:04<00:56,  1.87s/it]  9%|▉         | 3/32 [00:04<00:34,  1.18s/it] 12%|█▎        | 4/32 [00:05<00:23,  1.18it/s] 16%|█▌        | 5/32 [00:05<00:18,  1.49it/s] 19%|█▉        | 6/32 [00:05<00:14,  1.77it/s] 22%|██▏       | 7/32 [00:06<00:12,  2.01it/s] 25%|██▌       | 8/32 [00:06<00:10,  2.20it/s] 28%|██▊       | 9/32 [00:06<00:09,  2.37it/s] 31%|███▏      | 10/32 [00:07<00:08,  2.49it/s] 34%|███▍      | 11/32 [00:07<00:08,  2.57it/s] 38%|███▊      | 12/32 [00:07<00:07,  2.65it/s] 41%|████      | 13/32 [00:08<00:06,  2.72it/s] 44%|████▍     | 14/32 [00:08<00:06,  2.75it/s] 47%|████▋     | 15/32 [00:09<00:06,  2.76it/s] 50%|█████     | 16/32 [00:09<00:05,  2.79it/s] 53%|█████▎    | 17/32 [00:09<00:05,  2.79it/s] 56%|█████▋    | 18/32 [00:10<00:05,  2.78it/s] 59%|█████▉    | 19/32 [00:10<00:04,  2.82it/s] 62%|██████▎   | 20/32 [00:10<00:04,  2.83it/s] 66%|██████▌   | 21/32 [00:11<00:03,  2.82it/s] 69%|██████▉   | 22/32 [00:11<00:03,  2.81it/s] 72%|███████▏  | 23/32 [00:11<00:03,  2.80it/s] 75%|███████▌  | 24/32 [00:12<00:02,  2.80it/s] 78%|███████▊  | 25/32 [00:12<00:02,  2.81it/s] 81%|████████▏ | 26/32 [00:12<00:02,  2.84it/s] 84%|████████▍ | 27/32 [00:13<00:01,  2.84it/s] 88%|████████▊ | 28/32 [00:13<00:01,  2.83it/s] 91%|█████████ | 29/32 [00:13<00:01,  2.83it/s] 94%|█████████▍| 30/32 [00:14<00:00,  2.83it/s] 97%|█████████▋| 31/32 [00:14<00:00,  2.86it/s]100%|██████████| 32/32 [00:15<00:00,  2.85it/s]100%|██████████| 32/32 [00:15<00:00,  2.13it/s]
W0316 08:46:23.944000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:46:23.944000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:46:23.944000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:46:23.944000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 08:46:23.944000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:46:23.944000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 08:46:23.944000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:46:23.972000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:46:23.972000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:46:23.972000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:46:23.972000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:46:23.972000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:46:24.270000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:46:24.270000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:46:24.270000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:46:24.270000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:46:24.270000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:46:24.883000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 08:46:24.883000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:46:24.883000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:46:24.883000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:46:24.883000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:46:24.884000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:46:24.884000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 08:46:24.901000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:46:24.901000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:46:24.902000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:46:24.902000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:46:24.902000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:46:25.099000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:46:25.099000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:46:25.099000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:46:25.099000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:46:25.099000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:46:26.254000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:46:26.255000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 08:46:26.255000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:46:26.255000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:46:26.255000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 08:46:26.255000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:46:26.255000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:46:26.273000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:46:26.273000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:46:26.273000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:46:26.274000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:46:26.274000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:46:26.899000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:46:26.899000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:46:26.899000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:46:26.899000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:46:26.899000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 08:46:31.895644 462198 finetune.py:68] layer 0_v @ epoch 1 new loss 2.5968933670128536e-08 old loss 2.772348040025463e-08 BETTER
I0316 08:46:33.683870 463610 finetune.py:45] layer 1_v initial loss 3.972537570007262e-07
W0316 08:46:33.684039 463610 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 08:47:07.246473 463610 finetune.py:68] layer 1_v @ epoch 0 new loss 2.009504385114269e-07 old loss 3.972537570007262e-07 BETTER
I0316 08:47:08.017778 460347 quantize_finetune_llama.py:186] computed original embedding for layer 2 in 67.49203681945801s
I0316 08:47:08.579222 462198 finetune.py:68] layer 0_v @ epoch 2 new loss 2.5172358419922602e-08 old loss 2.5968933670128536e-08 BETTER
I0316 08:47:18.635612 460347 quantize_finetune_llama.py:159] layer 3 gpu 3
I0316 08:47:20.651354 465058 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 08:47:20.651476 465058 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 08:47:20.651554 465058 utils.py:162] NumExpr defaulting to 16 threads.
I0316 08:47:20.857831 465058 config.py:58] PyTorch version 2.4.0 available.
I0316 08:47:23.170254 465058 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 08:47:23.538620 465058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:03<01:58,  3.83s/it]  6%|▋         | 2/32 [00:04<00:53,  1.79s/it]  9%|▉         | 3/32 [00:04<00:32,  1.13s/it] 12%|█▎        | 4/32 [00:04<00:23,  1.21it/s] 16%|█▌        | 5/32 [00:05<00:17,  1.52it/s] 19%|█▉        | 6/32 [00:05<00:14,  1.80it/s] 22%|██▏       | 7/32 [00:05<00:12,  2.04it/s] 25%|██▌       | 8/32 [00:06<00:10,  2.23it/s] 28%|██▊       | 9/32 [00:06<00:09,  2.38it/s] 31%|███▏      | 10/32 [00:07<00:08,  2.50it/s] 34%|███▍      | 11/32 [00:07<00:08,  2.57it/s] 38%|███▊      | 12/32 [00:07<00:07,  2.63it/s] 41%|████      | 13/32 [00:08<00:07,  2.68it/s] 44%|████▍     | 14/32 [00:08<00:06,  2.72it/s] 47%|████▋     | 15/32 [00:08<00:06,  2.74it/s] 50%|█████     | 16/32 [00:09<00:05,  2.76it/s] 53%|█████▎    | 17/32 [00:09<00:05,  2.78it/s] 56%|█████▋    | 18/32 [00:09<00:05,  2.80it/s] 59%|█████▉    | 19/32 [00:10<00:04,  2.80it/s] 62%|██████▎   | 20/32 [00:10<00:04,  2.79it/s] 66%|██████▌   | 21/32 [00:10<00:03,  2.79it/s] 69%|██████▉   | 22/32 [00:11<00:03,  2.81it/s] 72%|███████▏  | 23/32 [00:11<00:03,  2.81it/s] 75%|███████▌  | 24/32 [00:12<00:02,  2.81it/s] 78%|███████▊  | 25/32 [00:12<00:02,  2.81it/s] 81%|████████▏ | 26/32 [00:12<00:02,  2.83it/s] 84%|████████▍ | 27/32 [00:13<00:01,  2.82it/s] 88%|████████▊ | 28/32 [00:13<00:01,  2.81it/s] 91%|█████████ | 29/32 [00:13<00:01,  2.81it/s] 94%|█████████▍| 30/32 [00:14<00:00,  2.78it/s] 97%|█████████▋| 31/32 [00:14<00:00,  2.76it/s]100%|██████████| 32/32 [00:14<00:00,  2.75it/s]100%|██████████| 32/32 [00:14<00:00,  2.15it/s]
I0316 08:47:42.362047 463610 finetune.py:68] layer 1_v @ epoch 1 new loss 1.3848946878169954e-07 old loss 2.009504385114269e-07 BETTER
W0316 08:47:42.624000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 08:47:42.624000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:47:42.624000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:47:42.624000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:47:42.624000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:47:42.625000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 08:47:42.625000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:47:42.652000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:47:42.652000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:47:42.652000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:47:42.652000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:47:42.653000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:47:42.954000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:47:42.954000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:47:42.954000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:47:42.954000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:47:42.954000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:47:43.597000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:47:43.597000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 08:47:43.597000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:47:43.597000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 08:47:43.598000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:47:43.598000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:47:43.598000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:47:43.616000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:47:43.616000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:47:43.616000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:47:43.616000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:47:43.617000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:47:43.827000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:47:43.827000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:47:43.827000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:47:43.827000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:47:43.827000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:47:45.017000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:47:45.017000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:47:45.017000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 08:47:45.017000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:47:45.017000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 08:47:45.017000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:47:45.017000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:47:45.035000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:47:45.036000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:47:45.036000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:47:45.036000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:47:45.036000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
I0316 08:47:45.639415 462198 finetune.py:68] layer 0_v @ epoch 3 new loss 2.4725055780550065e-08 old loss 2.5172358419922602e-08 BETTER
W0316 08:47:45.683000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:47:45.683000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:47:45.683000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:47:45.683000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:47:45.683000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 08:47:52.467892 465058 finetune.py:45] layer 2_v initial loss 2.2239260033529717e-06
W0316 08:47:52.468147 465058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 08:48:17.813719 463610 finetune.py:68] layer 1_v @ epoch 2 new loss 1.1273644417997275e-07 old loss 1.3848946878169954e-07 BETTER
I0316 08:48:22.836098 462198 finetune.py:68] layer 0_v @ epoch 4 new loss 2.4399394504825977e-08 old loss 2.4725055780550065e-08 BETTER
I0316 08:48:24.673731 460347 quantize_finetune_llama.py:186] computed original embedding for layer 3 in 65.82447838783264s
W0316 08:48:24.748012 462198 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

0_v proxy err 0.0013809925876557827 tr(WHW.T) 1.3175290822982788
  0%|          | 0/32 [00:00<?, ?it/s]I0316 08:48:26.741060 465058 finetune.py:68] layer 2_v @ epoch 0 new loss 5.831296903124894e-07 old loss 2.2239260033529717e-06 BETTER
  3%|▎         | 1/32 [00:04<02:13,  4.32s/it]  6%|▋         | 2/32 [00:04<00:59,  2.00s/it]  9%|▉         | 3/32 [00:05<00:36,  1.26s/it] 12%|█▎        | 4/32 [00:05<00:25,  1.10it/s] 16%|█▌        | 5/32 [00:05<00:19,  1.40it/s] 19%|█▉        | 6/32 [00:06<00:15,  1.66it/s] 22%|██▏       | 7/32 [00:06<00:13,  1.90it/s] 25%|██▌       | 8/32 [00:06<00:11,  2.09it/s] 28%|██▊       | 9/32 [00:07<00:10,  2.25it/s] 31%|███▏      | 10/32 [00:07<00:09,  2.36it/s] 34%|███▍      | 11/32 [00:08<00:08,  2.44it/s]I0316 08:48:34.418936 460347 quantize_finetune_llama.py:159] layer 4 gpu 0
 38%|███▊      | 12/32 [00:08<00:08,  2.50it/s] 41%|████      | 13/32 [00:08<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:09<00:06,  2.58it/s] 47%|████▋     | 15/32 [00:09<00:06,  2.60it/s] 50%|█████     | 16/32 [00:09<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:10<00:05,  2.62it/s]I0316 08:48:36.457284 466629 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 08:48:36.457395 466629 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 08:48:36.457459 466629 utils.py:162] NumExpr defaulting to 16 threads.
I0316 08:48:36.683176 466629 config.py:58] PyTorch version 2.4.0 available.
 56%|█████▋    | 18/32 [00:10<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:11<00:04,  2.66it/s] 62%|██████▎   | 20/32 [00:11<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:11<00:04,  2.63it/s] 69%|██████▉   | 22/32 [00:12<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:12<00:03,  2.65it/s]I0316 08:48:38.910759 466629 data_utils.py:336] using 256 training seqs, 128 validation seqs
 75%|███████▌  | 24/32 [00:12<00:03,  2.65it/s]W0316 08:48:39.268049 466629 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 78%|███████▊  | 25/32 [00:13<00:02,  2.65it/s] 81%|████████▏ | 26/32 [00:13<00:02,  2.64it/s] 84%|████████▍ | 27/32 [00:14<00:01,  2.65it/s]  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:14<00:01,  2.65it/s] 91%|█████████ | 29/32 [00:14<00:01,  2.66it/s] 94%|█████████▍| 30/32 [00:15<00:00,  2.68it/s] 97%|█████████▋| 31/32 [00:15<00:00,  2.66it/s]100%|██████████| 32/32 [00:15<00:00,  2.66it/s]100%|██████████| 32/32 [00:15<00:00,  2.00it/s]
  3%|▎         | 1/32 [00:03<01:56,  3.77s/it]  6%|▋         | 2/32 [00:04<00:52,  1.76s/it]  9%|▉         | 3/32 [00:04<00:32,  1.12s/it] 12%|█▎        | 4/32 [00:04<00:22,  1.22it/s] 16%|█▌        | 5/32 [00:05<00:17,  1.55it/s] 19%|█▉        | 6/32 [00:05<00:14,  1.83it/s] 22%|██▏       | 7/32 [00:05<00:12,  2.07it/s] 25%|██▌       | 8/32 [00:06<00:10,  2.26it/s] 28%|██▊       | 9/32 [00:06<00:09,  2.42it/s] 31%|███▏      | 10/32 [00:06<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:07<00:07,  2.63it/s]W0316 08:48:47.792000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:48:47.792000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:48:47.792000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:48:47.792000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:48:47.792000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 08:48:47.792000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:48:47.792000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 08:48:47.822000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:48:47.822000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:48:47.822000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:48:47.822000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:48:47.822000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:07<00:07,  2.70it/s]W0316 08:48:47.990000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:48:47.990000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:48:47.990000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:48:47.990000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:48:47.990000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:48:48.222000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:48:48.222000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:48:48.222000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 08:48:48.222000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:48:48.222000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:48:48.222000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:48:48.223000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 08:48:48.243000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:48:48.243000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:48:48.243000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:48:48.243000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:48:48.243000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:07<00:06,  2.74it/s]W0316 08:48:48.309000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:48:48.309000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:48:48.309000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:48:48.310000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:48:48.310000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:08<00:06,  2.77it/s] 47%|████▋     | 15/32 [00:08<00:06,  2.79it/s] 50%|█████     | 16/32 [00:09<00:05,  2.80it/s]W0316 08:48:49.380000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:09<00:05,  2.81it/s]W0316 08:48:49.699000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:48:49.699000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:48:49.699000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:48:49.700000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 08:48:49.700000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:48:49.700000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 08:48:49.700000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:48:49.722000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:48:49.722000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:48:49.722000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:48:49.722000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:48:49.722000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:48:49.976000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:48:49.976000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:48:49.976000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:48:49.976000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:48:49.976000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:09<00:04,  2.82it/s] 59%|█████▉    | 19/32 [00:10<00:04,  2.82it/s]W0316 08:48:50.438000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:10<00:04,  2.83it/s] 66%|██████▌   | 21/32 [00:10<00:03,  2.83it/s] 69%|██████▉   | 22/32 [00:11<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:11<00:03,  2.84it/s] 75%|███████▌  | 24/32 [00:11<00:02,  2.84it/s] 78%|███████▊  | 25/32 [00:12<00:02,  2.84it/s] 81%|████████▏ | 26/32 [00:12<00:02,  2.84it/s] 84%|████████▍ | 27/32 [00:12<00:01,  2.83it/s] 88%|████████▊ | 28/32 [00:13<00:01,  2.83it/s]I0316 08:48:53.608646 463610 finetune.py:68] layer 1_v @ epoch 3 new loss 1.0004981731981388e-07 old loss 1.1273644417997275e-07 BETTER
 91%|█████████ | 29/32 [00:13<00:01,  2.83it/s] 94%|█████████▍| 30/32 [00:13<00:00,  2.83it/s] 97%|█████████▋| 31/32 [00:14<00:00,  2.83it/s]100%|██████████| 32/32 [00:14<00:00,  2.82it/s]100%|██████████| 32/32 [00:14<00:00,  2.18it/s]
I0316 08:48:57.884539 462198 finetune.py:45] layer 0_q initial loss 2.6158408772403163e-08
W0316 08:48:57.884946 462198 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0316 08:48:57.903000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:48:57.904000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 08:48:57.904000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:48:57.904000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:48:57.904000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:48:57.904000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 08:48:57.904000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:48:57.932000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:48:57.932000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:48:57.932000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:48:57.932000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:48:57.932000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:48:58.231000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:48:58.231000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:48:58.231000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:48:58.231000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:48:58.231000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:48:58.858000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:48:58.859000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 08:48:58.859000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 08:48:58.859000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:48:58.859000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:48:58.859000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:48:58.859000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:48:58.877000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:48:58.877000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:48:58.877000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:48:58.877000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:48:58.877000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:48:59.083000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:48:59.083000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:48:59.083000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:48:59.083000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:48:59.083000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:49:00.255000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:49:00.256000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 08:49:00.256000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:49:00.256000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:49:00.256000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:49:00.256000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 08:49:00.256000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:49:00.274000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:49:00.274000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:49:00.274000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:49:00.274000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:49:00.274000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:49:00.913000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:49:00.913000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:49:00.914000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:49:00.914000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:49:00.914000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 08:49:02.803687 465058 finetune.py:68] layer 2_v @ epoch 1 new loss 3.0823761676401773e-07 old loss 5.831296903124894e-07 BETTER
I0316 08:49:07.764119 466629 finetune.py:45] layer 3_v initial loss 5.104117462906288e-06
W0316 08:49:07.764542 466629 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 08:49:29.810862 463610 finetune.py:68] layer 1_v @ epoch 4 new loss 9.260285338541507e-08 old loss 1.0004981731981388e-07 BETTER
W0316 08:49:31.950946 463610 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

1_v proxy err 0.001383635331876576 tr(WHW.T) 5.699800491333008
  0%|          | 0/32 [00:00<?, ?it/s]I0316 08:49:34.023963 462198 finetune.py:68] layer 0_q @ epoch 0 new loss 2.4936024800581436e-08 old loss 2.6158408772403163e-08 BETTER
  3%|▎         | 1/32 [00:03<01:43,  3.35s/it]  6%|▋         | 2/32 [00:03<00:48,  1.62s/it]  9%|▉         | 3/32 [00:04<00:30,  1.06s/it] 12%|█▎        | 4/32 [00:04<00:22,  1.24it/s] 16%|█▌        | 5/32 [00:04<00:17,  1.52it/s] 19%|█▉        | 6/32 [00:05<00:14,  1.74it/s] 22%|██▏       | 7/32 [00:05<00:13,  1.92it/s]I0316 08:49:39.275530 465058 finetune.py:68] layer 2_v @ epoch 2 new loss 2.3437615936927614e-07 old loss 3.0823761676401773e-07 BETTER
 25%|██▌       | 8/32 [00:06<00:11,  2.08it/s] 28%|██▊       | 9/32 [00:06<00:10,  2.19it/s] 31%|███▏      | 10/32 [00:07<00:09,  2.25it/s] 34%|███▍      | 11/32 [00:07<00:09,  2.29it/s] 38%|███▊      | 12/32 [00:07<00:08,  2.31it/s] 41%|████      | 13/32 [00:08<00:08,  2.36it/s] 44%|████▍     | 14/32 [00:08<00:07,  2.41it/s]I0316 08:49:41.866817 466629 finetune.py:68] layer 3_v @ epoch 0 new loss 1.1829863524326356e-06 old loss 5.104117462906288e-06 BETTER
 47%|████▋     | 15/32 [00:09<00:06,  2.44it/s] 50%|█████     | 16/32 [00:09<00:06,  2.46it/s] 53%|█████▎    | 17/32 [00:09<00:06,  2.47it/s] 56%|█████▋    | 18/32 [00:10<00:05,  2.47it/s] 59%|█████▉    | 19/32 [00:10<00:05,  2.46it/s] 62%|██████▎   | 20/32 [00:11<00:04,  2.45it/s] 66%|██████▌   | 21/32 [00:11<00:04,  2.45it/s] 69%|██████▉   | 22/32 [00:11<00:04,  2.45it/s] 72%|███████▏  | 23/32 [00:12<00:03,  2.45it/s] 75%|███████▌  | 24/32 [00:12<00:03,  2.45it/s] 78%|███████▊  | 25/32 [00:13<00:02,  2.45it/s] 81%|████████▏ | 26/32 [00:13<00:02,  2.46it/s] 84%|████████▍ | 27/32 [00:13<00:02,  2.47it/s] 88%|████████▊ | 28/32 [00:14<00:01,  2.47it/s] 91%|█████████ | 29/32 [00:14<00:01,  2.46it/s] 94%|█████████▍| 30/32 [00:15<00:00,  2.45it/s] 97%|█████████▋| 31/32 [00:15<00:00,  2.46it/s]100%|██████████| 32/32 [00:15<00:00,  2.47it/s]100%|██████████| 32/32 [00:15<00:00,  2.01it/s]
W0316 08:49:55.025000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.025000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.025000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.025000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.025000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.026000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.026000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.056000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.056000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.056000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.056000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.056000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.219000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.219000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.219000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.219000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.219000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.445000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.445000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.445000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.445000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.446000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.446000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.446000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.466000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.466000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.466000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.466000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.466000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.531000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.531000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.532000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.532000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:49:55.532000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:49:56.590000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 08:49:56.917000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:49:56.917000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 08:49:56.918000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 08:49:56.918000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:49:56.918000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:49:56.918000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:49:56.918000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:49:56.942000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:49:56.942000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:49:56.942000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:49:56.942000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:49:56.942000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:49:57.201000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:49:57.201000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:49:57.201000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:49:57.201000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:49:57.201000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:49:57.675000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 08:50:05.027590 463610 finetune.py:45] layer 1_q initial loss 1.0009345174921691e-07
W0316 08:50:05.027999 463610 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 08:50:11.583120 462198 finetune.py:68] layer 0_q @ epoch 1 new loss 2.4615289362373005e-08 old loss 2.4936024800581436e-08 BETTER
I0316 08:50:15.753079 465058 finetune.py:68] layer 2_v @ epoch 3 new loss 2.0182412185931753e-07 old loss 2.3437615936927614e-07 BETTER
I0316 08:50:17.348524 466629 finetune.py:68] layer 3_v @ epoch 1 new loss 6.810300874349196e-07 old loss 1.1829863524326356e-06 BETTER
I0316 08:50:39.698371 463610 finetune.py:68] layer 1_q @ epoch 0 new loss 9.58753787472233e-08 old loss 1.0009345174921691e-07 BETTER
I0316 08:50:49.088944 462198 finetune.py:68] layer 0_q @ epoch 2 new loss 2.440019031269003e-08 old loss 2.4615289362373005e-08 BETTER
I0316 08:50:52.181533 465058 finetune.py:68] layer 2_v @ epoch 4 new loss 1.8286665692812676e-07 old loss 2.0182412185931753e-07 BETTER
I0316 08:50:53.007626 466629 finetune.py:68] layer 3_v @ epoch 2 new loss 5.312134021551174e-07 old loss 6.810300874349196e-07 BETTER
W0316 08:50:53.852002 465058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_v proxy err 0.0019992440938949585 tr(WHW.T) 26.839576721191406
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:03<01:45,  3.39s/it]  6%|▋         | 2/32 [00:03<00:48,  1.63s/it]  9%|▉         | 3/32 [00:04<00:30,  1.07s/it] 12%|█▎        | 4/32 [00:04<00:22,  1.25it/s] 16%|█▌        | 5/32 [00:04<00:17,  1.53it/s] 19%|█▉        | 6/32 [00:05<00:14,  1.78it/s] 22%|██▏       | 7/32 [00:05<00:12,  1.98it/s] 25%|██▌       | 8/32 [00:06<00:11,  2.13it/s] 28%|██▊       | 9/32 [00:06<00:10,  2.25it/s] 31%|███▏      | 10/32 [00:06<00:09,  2.32it/s] 34%|███▍      | 11/32 [00:07<00:08,  2.40it/s] 38%|███▊      | 12/32 [00:07<00:08,  2.45it/s] 41%|████      | 13/32 [00:08<00:07,  2.49it/s] 44%|████▍     | 14/32 [00:08<00:07,  2.51it/s] 47%|████▋     | 15/32 [00:08<00:06,  2.54it/s] 50%|█████     | 16/32 [00:09<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:09<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:10<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:10<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:10<00:04,  2.60it/s] 66%|██████▌   | 21/32 [00:11<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:11<00:03,  2.59it/s] 72%|███████▏  | 23/32 [00:11<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:12<00:03,  2.58it/s] 78%|███████▊  | 25/32 [00:12<00:02,  2.57it/s] 81%|████████▏ | 26/32 [00:13<00:02,  2.57it/s] 84%|████████▍ | 27/32 [00:13<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:13<00:01,  2.58it/s] 91%|█████████ | 29/32 [00:14<00:01,  2.58it/s] 94%|█████████▍| 30/32 [00:14<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:15<00:00,  2.59it/s]100%|██████████| 32/32 [00:15<00:00,  2.59it/s]100%|██████████| 32/32 [00:15<00:00,  2.07it/s]
I0316 08:51:15.537708 463610 finetune.py:68] layer 1_q @ epoch 1 new loss 9.053124472302443e-08 old loss 9.58753787472233e-08 BETTER
W0316 08:51:16.456000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.456000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.456000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.457000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.457000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.457000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.457000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.487000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.488000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.488000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.488000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.488000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.658000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.659000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.659000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.659000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.659000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.891000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.891000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.891000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.891000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.891000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.891000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.891000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.913000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.913000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.913000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.914000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.914000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.980000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.980000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.980000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.980000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:51:16.980000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:51:18.066000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 08:51:18.394000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:51:18.394000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 08:51:18.394000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:51:18.394000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:51:18.395000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:51:18.395000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 08:51:18.395000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:51:18.417000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:51:18.417000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:51:18.418000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:51:18.418000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:51:18.418000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:51:18.674000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:51:18.674000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:51:18.675000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:51:18.675000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:51:18.675000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:51:19.140000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 08:51:26.727123 465058 finetune.py:45] layer 2_q initial loss 2.928189530848613e-07
W0316 08:51:26.727466 465058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 08:51:27.040084 462198 finetune.py:68] layer 0_q @ epoch 3 new loss 2.419075428861106e-08 old loss 2.440019031269003e-08 BETTER
I0316 08:51:28.790529 466629 finetune.py:68] layer 3_v @ epoch 3 new loss 4.6180898038983287e-07 old loss 5.312134021551174e-07 BETTER
I0316 08:51:51.927270 463610 finetune.py:68] layer 1_q @ epoch 2 new loss 8.760880376712521e-08 old loss 9.053124472302443e-08 BETTER
I0316 08:52:02.261240 465058 finetune.py:68] layer 2_q @ epoch 0 new loss 2.719831684316887e-07 old loss 2.928189530848613e-07 BETTER
I0316 08:52:04.728395 462198 finetune.py:68] layer 0_q @ epoch 4 new loss 2.4015758270934384e-08 old loss 2.419075428861106e-08 BETTER
I0316 08:52:04.945739 466629 finetune.py:68] layer 3_v @ epoch 4 new loss 4.230749652833765e-07 old loss 4.6180898038983287e-07 BETTER
W0316 08:52:06.433171 462198 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0316 08:52:06.495224 466629 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

0_q proxy err 6.513336120406166e-05 tr(WHW.T) 6234.06884765625
  0%|          | 0/32 [00:00<?, ?it/s]3_v proxy err 0.0025086288806051016 tr(WHW.T) 40.46714782714844
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:43,  1.39s/it]  6%|▋         | 2/32 [00:01<00:23,  1.25it/s]  9%|▉         | 3/32 [00:02<00:17,  1.65it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.94it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.15it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.28it/s]  3%|▎         | 1/32 [00:03<01:44,  3.36s/it] 22%|██▏       | 7/32 [00:03<00:10,  2.38it/s]  6%|▋         | 2/32 [00:03<00:48,  1.61s/it] 25%|██▌       | 8/32 [00:04<00:09,  2.47it/s]  9%|▉         | 3/32 [00:04<00:30,  1.05s/it] 28%|██▊       | 9/32 [00:04<00:09,  2.54it/s] 12%|█▎        | 4/32 [00:04<00:22,  1.27it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 16%|█▌        | 5/32 [00:04<00:17,  1.55it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.58it/s] 19%|█▉        | 6/32 [00:05<00:14,  1.80it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 22%|██▏       | 7/32 [00:05<00:12,  1.99it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 25%|██▌       | 8/32 [00:06<00:11,  2.15it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.63it/s] 28%|██▊       | 9/32 [00:06<00:10,  2.28it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 31%|███▏      | 10/32 [00:06<00:09,  2.36it/s] 50%|█████     | 16/32 [00:07<00:06,  2.64it/s] 34%|███▍      | 11/32 [00:07<00:08,  2.42it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.51it/s] 38%|███▊      | 12/32 [00:07<00:08,  2.46it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.41it/s] 41%|████      | 13/32 [00:08<00:07,  2.48it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.32it/s] 44%|████▍     | 14/32 [00:08<00:07,  2.51it/s] 47%|████▋     | 15/32 [00:08<00:06,  2.53it/s] 62%|██████▎   | 20/32 [00:08<00:05,  2.34it/s] 50%|█████     | 16/32 [00:09<00:06,  2.54it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.34it/s] 53%|█████▎    | 17/32 [00:09<00:05,  2.56it/s] 69%|██████▉   | 22/32 [00:09<00:04,  2.34it/s] 56%|█████▋    | 18/32 [00:09<00:05,  2.56it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.30it/s] 59%|█████▉    | 19/32 [00:10<00:05,  2.57it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.25it/s] 62%|██████▎   | 20/32 [00:10<00:04,  2.57it/s] 78%|███████▊  | 25/32 [00:11<00:03,  2.22it/s] 66%|██████▌   | 21/32 [00:11<00:04,  2.57it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.22it/s] 69%|██████▉   | 22/32 [00:11<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:11<00:03,  2.58it/s] 84%|████████▍ | 27/32 [00:11<00:02,  2.24it/s] 75%|███████▌  | 24/32 [00:12<00:03,  2.55it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.25it/s] 78%|███████▊  | 25/32 [00:12<00:02,  2.58it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.22it/s] 81%|████████▏ | 26/32 [00:13<00:02,  2.56it/s] 94%|█████████▍| 30/32 [00:13<00:00,  2.20it/s] 84%|████████▍ | 27/32 [00:13<00:01,  2.56it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.19it/s] 88%|████████▊ | 28/32 [00:13<00:01,  2.55it/s]100%|██████████| 32/32 [00:14<00:00,  2.26it/s]100%|██████████| 32/32 [00:14<00:00,  2.25it/s]
 91%|█████████ | 29/32 [00:14<00:01,  2.52it/s] 94%|█████████▍| 30/32 [00:14<00:00,  2.50it/s] 97%|█████████▋| 31/32 [00:15<00:00,  2.48it/s]100%|██████████| 32/32 [00:15<00:00,  2.48it/s]100%|██████████| 32/32 [00:15<00:00,  2.07it/s]
I0316 08:52:28.416982 463610 finetune.py:68] layer 1_q @ epoch 3 new loss 8.693915987123546e-08 old loss 8.760880376712521e-08 BETTER
I0316 08:52:29.215548 462198 finetune.py:45] layer 0_k initial loss 2.5203028997111687e-08
W0316 08:52:29.215977 462198 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0316 08:52:29.465000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.465000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.466000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.466000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.466000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.466000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.466000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.497000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.497000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.497000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.498000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.498000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.662000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.662000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.663000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.663000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.663000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.890000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.891000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.891000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.891000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.891000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.891000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.891000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.915000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.915000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.915000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.915000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.915000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.980000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.980000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.980000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.981000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 08:52:29.981000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 08:52:31.039000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 08:52:31.362000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:52:31.362000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:52:31.362000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 08:52:31.362000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:52:31.362000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:52:31.362000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 08:52:31.362000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:52:31.386000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:52:31.386000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:52:31.386000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:52:31.387000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:52:31.387000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:52:31.651000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 08:52:31.651000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 08:52:31.651000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 08:52:31.651000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 08:52:31.651000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 08:52:32.140000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 08:52:38.928138 465058 finetune.py:68] layer 2_q @ epoch 1 new loss 2.599942661163368e-07 old loss 2.719831684316887e-07 BETTER
I0316 08:52:39.614030 466629 finetune.py:45] layer 3_q initial loss 6.862670716145658e-07
W0316 08:52:39.614341 466629 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 08:53:04.720672 463610 finetune.py:68] layer 1_q @ epoch 4 new loss 8.503328530196086e-08 old loss 8.693915987123546e-08 BETTER
I0316 08:53:05.727398 462198 finetune.py:68] layer 0_k @ epoch 0 new loss 2.477544747137017e-08 old loss 2.5203028997111687e-08 BETTER
W0316 08:53:06.495879 463610 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

1_q proxy err 2.3874334146967158e-05 tr(WHW.T) 7567.0185546875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:43,  1.39s/it]  6%|▋         | 2/32 [00:01<00:24,  1.24it/s]  9%|▉         | 3/32 [00:02<00:17,  1.62it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.88it/s] 16%|█▌        | 5/32 [00:02<00:13,  2.07it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.19it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.29it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.36it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.41it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.45it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.46it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.45it/s] 41%|████      | 13/32 [00:06<00:07,  2.47it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.48it/s]I0316 08:53:14.313272 466629 finetune.py:68] layer 3_q @ epoch 0 new loss 6.477883403022133e-07 old loss 6.862670716145658e-07 BETTER
 47%|████▋     | 15/32 [00:06<00:06,  2.49it/s] 50%|█████     | 16/32 [00:07<00:06,  2.50it/s]I0316 08:53:15.383465 465058 finetune.py:68] layer 2_q @ epoch 2 new loss 2.51839225029471e-07 old loss 2.599942661163368e-07 BETTER
 53%|█████▎    | 17/32 [00:07<00:05,  2.51it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.53it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.53it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.53it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.53it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.53it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.51it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.50it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.49it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.50it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.50it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.50it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.51it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.51it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.52it/s]100%|██████████| 32/32 [00:13<00:00,  2.53it/s]100%|██████████| 32/32 [00:13<00:00,  2.33it/s]
I0316 08:53:28.114878 463610 finetune.py:45] layer 1_k initial loss 9.597828665164343e-08
W0316 08:53:28.115239 463610 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 08:53:43.050203 462198 finetune.py:68] layer 0_k @ epoch 1 new loss 2.4616333860194572e-08 old loss 2.477544747137017e-08 BETTER
I0316 08:53:49.808881 466629 finetune.py:68] layer 3_q @ epoch 1 new loss 6.252670345929801e-07 old loss 6.477883403022133e-07 BETTER
I0316 08:53:51.905900 465058 finetune.py:68] layer 2_q @ epoch 3 new loss 2.456597201216937e-07 old loss 2.51839225029471e-07 BETTER
I0316 08:54:02.919523 463610 finetune.py:68] layer 1_k @ epoch 0 new loss 9.228843822484123e-08 old loss 9.597828665164343e-08 BETTER
I0316 08:54:20.562246 462198 finetune.py:68] layer 0_k @ epoch 2 new loss 2.4468620907214245e-08 old loss 2.4616333860194572e-08 BETTER
I0316 08:54:25.589676 466629 finetune.py:68] layer 3_q @ epoch 2 new loss 6.101090548327193e-07 old loss 6.252670345929801e-07 BETTER
I0316 08:54:28.366974 465058 finetune.py:68] layer 2_q @ epoch 4 new loss 2.410304205113789e-07 old loss 2.456597201216937e-07 BETTER
W0316 08:54:30.383552 465058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_q proxy err 0.00021799634851049632 tr(WHW.T) 7137.451171875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:46,  1.51s/it]  6%|▋         | 2/32 [00:01<00:26,  1.13it/s]  9%|▉         | 3/32 [00:02<00:19,  1.45it/s] 12%|█▎        | 4/32 [00:02<00:16,  1.68it/s] 16%|█▌        | 5/32 [00:03<00:14,  1.83it/s] 19%|█▉        | 6/32 [00:03<00:13,  1.94it/s] 22%|██▏       | 7/32 [00:04<00:11,  2.09it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.23it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.33it/s] 31%|███▏      | 10/32 [00:05<00:09,  2.40it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.45it/s] 38%|███▊      | 12/32 [00:06<00:08,  2.49it/s] 41%|████      | 13/32 [00:06<00:07,  2.51it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.51it/s] 47%|████▋     | 15/32 [00:07<00:06,  2.54it/s]I0316 08:54:38.984457 463610 finetune.py:68] layer 1_k @ epoch 1 new loss 8.920596883399412e-08 old loss 9.228843822484123e-08 BETTER
 50%|█████     | 16/32 [00:07<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:08<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.60it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.61it/s] 62%|██████▎   | 20/32 [00:09<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.63it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.63it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.64it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.63it/s] 78%|███████▊  | 25/32 [00:11<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.62it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.63it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.64it/s]100%|██████████| 32/32 [00:13<00:00,  2.64it/s]100%|██████████| 32/32 [00:13<00:00,  2.33it/s]
I0316 08:54:52.134838 465058 finetune.py:45] layer 2_k initial loss 3.374809978140547e-07
W0316 08:54:52.135121 465058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 08:54:58.296982 462198 finetune.py:68] layer 0_k @ epoch 3 new loss 2.434201995527019e-08 old loss 2.4468620907214245e-08 BETTER
I0316 08:55:01.286503 466629 finetune.py:68] layer 3_q @ epoch 3 new loss 5.987976123833505e-07 old loss 6.101090548327193e-07 BETTER
I0316 08:55:15.167651 463610 finetune.py:68] layer 1_k @ epoch 2 new loss 8.793256967010166e-08 old loss 8.920596883399412e-08 BETTER
I0316 08:55:27.481196 465058 finetune.py:68] layer 2_k @ epoch 0 new loss 3.151585019622871e-07 old loss 3.374809978140547e-07 BETTER
I0316 08:55:35.817136 462198 finetune.py:68] layer 0_k @ epoch 4 new loss 2.4225077055461952e-08 old loss 2.434201995527019e-08 BETTER
I0316 08:55:37.211532 466629 finetune.py:68] layer 3_q @ epoch 4 new loss 5.8964883464796e-07 old loss 5.987976123833505e-07 BETTER
W0316 08:55:37.828848 462198 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0316 08:55:38.840749 466629 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

0_k proxy err 2.4965313059510663e-05 tr(WHW.T) 2167.85595703125
  0%|          | 0/32 [00:00<?, ?it/s]3_q proxy err 0.00031438819132745266 tr(WHW.T) 6657.2578125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.19it/s]  6%|▋         | 2/32 [00:01<00:17,  1.75it/s]  9%|▉         | 3/32 [00:01<00:14,  2.05it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.24it/s]  3%|▎         | 1/32 [00:01<00:42,  1.39s/it] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s]  6%|▋         | 2/32 [00:01<00:23,  1.25it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s]  9%|▉         | 3/32 [00:02<00:17,  1.64it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.91it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.11it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.56it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.26it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.36it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.43it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.48it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.52it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.60it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.53it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 41%|████      | 13/32 [00:06<00:07,  2.55it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.58it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.56it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.57it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.58it/s] 50%|█████     | 16/32 [00:07<00:06,  2.58it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.58it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.47it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 69%|██████▉   | 22/32 [00:09<00:04,  2.39it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.38it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.60it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.61it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.38it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.61it/s] 78%|███████▊  | 25/32 [00:10<00:03,  2.33it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.27it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.60it/s] 84%|████████▍ | 27/32 [00:11<00:02,  2.25it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.24it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.57it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.21it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.58it/s]I0316 08:55:51.422259 463610 finetune.py:68] layer 1_k @ epoch 3 new loss 8.680759577828212e-08 old loss 8.793256967010166e-08 BETTER
 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.20it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.60it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.19it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:13<00:00,  2.19it/s]100%|██████████| 32/32 [00:13<00:00,  2.36it/s]
 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:13<00:00,  2.60it/s]100%|██████████| 32/32 [00:13<00:00,  2.40it/s]
I0316 08:56:00.255013 466629 finetune.py:45] layer 3_k initial loss 9.265623930332367e-07
W0316 08:56:00.255380 466629 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 08:56:00.569900 462198 finetune.py:45] layer 0_o initial loss 4.74061181421348e-08
W0316 08:56:00.570226 462198 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 08:56:03.967415 465058 finetune.py:68] layer 2_k @ epoch 1 new loss 3.112962474460801e-07 old loss 3.151585019622871e-07 BETTER
I0316 08:56:27.596689 463610 finetune.py:76] layer 1_k @ epoch 4 new loss 8.750087232556325e-08 old loss 8.680759577828212e-08 WORSE
W0316 08:56:28.938231 463610 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

1_k proxy err 2.3559781766380183e-05 tr(WHW.T) 3947.257568359375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:31,  1.00s/it]  6%|▋         | 2/32 [00:01<00:20,  1.44it/s]  9%|▉         | 3/32 [00:01<00:17,  1.68it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.90it/s] 16%|█▌        | 5/32 [00:02<00:13,  2.07it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.17it/s] 22%|██▏       | 7/32 [00:03<00:11,  2.23it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.22it/s]I0316 08:56:34.740580 466629 finetune.py:68] layer 3_k @ epoch 0 new loss 8.130492119562405e-07 old loss 9.265623930332367e-07 BETTER
 28%|██▊       | 9/32 [00:04<00:10,  2.19it/s] 31%|███▏      | 10/32 [00:04<00:09,  2.23it/s] 34%|███▍      | 11/32 [00:05<00:09,  2.31it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.34it/s] 41%|████      | 13/32 [00:06<00:08,  2.26it/s] 44%|████▍     | 14/32 [00:06<00:08,  2.23it/s]I0316 08:56:37.132128 462198 finetune.py:68] layer 0_o @ epoch 0 new loss 4.704747880168725e-08 old loss 4.74061181421348e-08 BETTER
 47%|████▋     | 15/32 [00:07<00:07,  2.31it/s] 50%|█████     | 16/32 [00:07<00:06,  2.36it/s] 53%|█████▎    | 17/32 [00:07<00:06,  2.39it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.42it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.36it/s] 62%|██████▎   | 20/32 [00:09<00:05,  2.29it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.24it/s]I0316 08:56:40.435464 465058 finetune.py:68] layer 2_k @ epoch 2 new loss 3.0818492291473376e-07 old loss 3.112962474460801e-07 BETTER
 69%|██████▉   | 22/32 [00:10<00:04,  2.28it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.35it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.41it/s] 78%|███████▊  | 25/32 [00:11<00:02,  2.43it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.46it/s] 84%|████████▍ | 27/32 [00:12<00:02,  2.48it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.50it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.49it/s] 94%|█████████▍| 30/32 [00:13<00:00,  2.50it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.51it/s]100%|██████████| 32/32 [00:14<00:00,  2.52it/s]100%|██████████| 32/32 [00:14<00:00,  2.27it/s]
I0316 08:56:51.418565 463610 finetune.py:45] layer 1_o initial loss 2.1902130242779094e-07
W0316 08:56:51.418985 463610 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 08:57:10.576459 466629 finetune.py:68] layer 3_k @ epoch 1 new loss 8.041567980399122e-07 old loss 8.130492119562405e-07 BETTER
I0316 08:57:15.008118 462198 finetune.py:68] layer 0_o @ epoch 1 new loss 4.6910177076142645e-08 old loss 4.704747880168725e-08 BETTER
I0316 08:57:16.825306 465058 finetune.py:68] layer 2_k @ epoch 3 new loss 3.056915431898233e-07 old loss 3.0818492291473376e-07 BETTER
I0316 08:57:26.706491 463610 finetune.py:68] layer 1_o @ epoch 0 new loss 2.1655762338923523e-07 old loss 2.1902130242779094e-07 BETTER
I0316 08:57:46.396801 466629 finetune.py:68] layer 3_k @ epoch 2 new loss 7.965335839799081e-07 old loss 8.041567980399122e-07 BETTER
I0316 08:57:52.421734 462198 finetune.py:68] layer 0_o @ epoch 2 new loss 4.676834564065757e-08 old loss 4.6910177076142645e-08 BETTER
I0316 08:57:53.209092 465058 finetune.py:68] layer 2_k @ epoch 4 new loss 3.034369910892565e-07 old loss 3.056915431898233e-07 BETTER
W0316 08:57:54.879656 465058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_k proxy err 0.00023493365733884275 tr(WHW.T) 3892.17724609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.20it/s]  6%|▋         | 2/32 [00:01<00:17,  1.74it/s]  9%|▉         | 3/32 [00:01<00:14,  2.03it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.40it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.50it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.51it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.52it/s] 41%|████      | 13/32 [00:05<00:07,  2.52it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.54it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.55it/s] 50%|█████     | 16/32 [00:06<00:06,  2.57it/s]I0316 08:58:02.936614 463610 finetune.py:76] layer 1_o @ epoch 1 new loss 2.1674013339634257e-07 old loss 2.1655762338923523e-07 WORSE
 53%|█████▎    | 17/32 [00:07<00:05,  2.58it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.58it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.56it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.56it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.56it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.57it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.58it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.59it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
I0316 08:58:16.216267 465058 finetune.py:45] layer 2_o initial loss 5.300616408021597e-07
W0316 08:58:16.216661 465058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 08:58:22.171043 466629 finetune.py:68] layer 3_k @ epoch 3 new loss 7.90655803939444e-07 old loss 7.965335839799081e-07 BETTER
I0316 08:58:30.096984 462198 finetune.py:68] layer 0_o @ epoch 3 new loss 4.666062025648898e-08 old loss 4.676834564065757e-08 BETTER
I0316 08:58:38.210110 463610 finetune.py:68] layer 1_o @ epoch 2 new loss 2.143958681699587e-07 old loss 2.1655762338923523e-07 BETTER
I0316 08:58:52.104998 465058 finetune.py:68] layer 2_o @ epoch 0 new loss 5.21281322107825e-07 old loss 5.300616408021597e-07 BETTER
I0316 08:58:58.094406 466629 finetune.py:68] layer 3_k @ epoch 4 new loss 7.855823582758603e-07 old loss 7.90655803939444e-07 BETTER
W0316 08:58:59.734408 466629 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

3_k proxy err 0.0003348943719174713 tr(WHW.T) 3661.170166015625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.20it/s]  6%|▋         | 2/32 [00:01<00:17,  1.76it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.24it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.56it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.56it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.58it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.59it/s] 50%|█████     | 16/32 [00:06<00:06,  2.59it/s]I0316 08:59:07.865847 462198 finetune.py:68] layer 0_o @ epoch 4 new loss 4.6556088761917636e-08 old loss 4.666062025648898e-08 BETTER
 53%|█████▎    | 17/32 [00:07<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.61it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.58it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.58it/s]W0316 08:59:09.798993 462198 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:08<00:03,  2.59it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.59it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s]0_o proxy err 0.00030150762177072465 tr(WHW.T) 0.2301577776670456
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.59it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.60it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.57it/s]  3%|▎         | 1/32 [00:02<01:07,  2.19s/it]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
I0316 08:59:14.772453 463610 finetune.py:68] layer 1_o @ epoch 3 new loss 2.1359466018111561e-07 old loss 2.143958681699587e-07 BETTER
  6%|▋         | 2/32 [00:03<00:58,  1.95s/it]  9%|▉         | 3/32 [00:05<00:54,  1.87s/it] 12%|█▎        | 4/32 [00:07<00:51,  1.85s/it] 16%|█▌        | 5/32 [00:09<00:49,  1.85s/it]I0316 08:59:20.987763 466629 finetune.py:45] layer 3_o initial loss 1.3190116305850097e-06
W0316 08:59:20.988180 466629 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:11<00:47,  1.84s/it] 22%|██▏       | 7/32 [00:13<00:45,  1.83s/it] 25%|██▌       | 8/32 [00:14<00:42,  1.79s/it] 28%|██▊       | 9/32 [00:16<00:40,  1.77s/it]I0316 08:59:28.824160 465058 finetune.py:68] layer 2_o @ epoch 1 new loss 5.1804568101943e-07 old loss 5.21281322107825e-07 BETTER
 31%|███▏      | 10/32 [00:18<00:38,  1.75s/it] 34%|███▍      | 11/32 [00:19<00:36,  1.74s/it] 38%|███▊      | 12/32 [00:21<00:34,  1.71s/it] 41%|████      | 13/32 [00:23<00:31,  1.68s/it] 44%|████▍     | 14/32 [00:24<00:29,  1.65s/it] 47%|████▋     | 15/32 [00:26<00:27,  1.60s/it] 50%|█████     | 16/32 [00:27<00:25,  1.57s/it] 53%|█████▎    | 17/32 [00:29<00:23,  1.56s/it] 56%|█████▋    | 18/32 [00:30<00:21,  1.54s/it] 59%|█████▉    | 19/32 [00:32<00:19,  1.52s/it] 62%|██████▎   | 20/32 [00:33<00:18,  1.52s/it] 66%|██████▌   | 21/32 [00:35<00:16,  1.52s/it] 69%|██████▉   | 22/32 [00:36<00:15,  1.52s/it] 72%|███████▏  | 23/32 [00:38<00:13,  1.52s/it]I0316 08:59:51.151943 463610 finetune.py:68] layer 1_o @ epoch 4 new loss 2.1283737794419721e-07 old loss 2.1359466018111561e-07 BETTER
 75%|███████▌  | 24/32 [00:39<00:12,  1.52s/it] 78%|███████▊  | 25/32 [00:41<00:10,  1.51s/it]W0316 08:59:52.824984 463610 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

1_o proxy err 0.0011715376749634743 tr(WHW.T) 0.3143611550331116
  0%|          | 0/32 [00:00<?, ?it/s] 81%|████████▏ | 26/32 [00:42<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:44<00:07,  1.51s/it]I0316 08:59:56.085123 466629 finetune.py:68] layer 3_o @ epoch 0 new loss 1.2943239653395722e-06 old loss 1.3190116305850097e-06 BETTER
  3%|▎         | 1/32 [00:02<01:05,  2.10s/it] 88%|████████▊ | 28/32 [00:46<00:06,  1.57s/it]  6%|▋         | 2/32 [00:03<00:54,  1.80s/it] 91%|█████████ | 29/32 [00:47<00:04,  1.63s/it]  9%|▉         | 3/32 [00:05<00:49,  1.71s/it] 94%|█████████▍| 30/32 [00:49<00:03,  1.67s/it] 12%|█▎        | 4/32 [00:06<00:46,  1.67s/it] 16%|█▌        | 5/32 [00:08<00:44,  1.64s/it] 97%|█████████▋| 31/32 [00:51<00:01,  1.72s/it] 19%|█▉        | 6/32 [00:10<00:42,  1.63s/it]100%|██████████| 32/32 [00:53<00:00,  1.73s/it]100%|██████████| 32/32 [00:53<00:00,  1.66s/it]
I0316 09:00:04.973022 465058 finetune.py:68] layer 2_o @ epoch 2 new loss 5.156351221557998e-07 old loss 5.1804568101943e-07 BETTER
 22%|██▏       | 7/32 [00:11<00:40,  1.63s/it] 25%|██▌       | 8/32 [00:13<00:38,  1.62s/it] 28%|██▊       | 9/32 [00:15<00:38,  1.65s/it] 31%|███▏      | 10/32 [00:16<00:35,  1.63s/it] 34%|███▍      | 11/32 [00:18<00:35,  1.68s/it]I0316 09:00:13.725073 462198 finetune.py:45] layer 0_up initial loss 1.477179694120423e-07
W0316 09:00:13.725343 462198 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:20<00:33,  1.70s/it] 41%|████      | 13/32 [00:21<00:32,  1.72s/it] 44%|████▍     | 14/32 [00:23<00:31,  1.73s/it] 47%|████▋     | 15/32 [00:25<00:29,  1.72s/it] 50%|█████     | 16/32 [00:27<00:27,  1.73s/it] 53%|█████▎    | 17/32 [00:28<00:26,  1.74s/it] 56%|█████▋    | 18/32 [00:30<00:24,  1.72s/it] 59%|█████▉    | 19/32 [00:32<00:21,  1.68s/it] 62%|██████▎   | 20/32 [00:33<00:19,  1.65s/it] 66%|██████▌   | 21/32 [00:35<00:17,  1.63s/it] 69%|██████▉   | 22/32 [00:36<00:16,  1.62s/it]I0316 09:00:31.765019 466629 finetune.py:68] layer 3_o @ epoch 1 new loss 1.2862697076343466e-06 old loss 1.2943239653395722e-06 BETTER
 72%|███████▏  | 23/32 [00:38<00:14,  1.60s/it] 75%|███████▌  | 24/32 [00:40<00:12,  1.58s/it] 78%|███████▊  | 25/32 [00:41<00:11,  1.58s/it] 81%|████████▏ | 26/32 [00:43<00:09,  1.57s/it] 84%|████████▍ | 27/32 [00:44<00:07,  1.56s/it] 88%|████████▊ | 28/32 [00:46<00:06,  1.57s/it]I0316 09:00:41.105783 465058 finetune.py:68] layer 2_o @ epoch 3 new loss 5.141092742633191e-07 old loss 5.156351221557998e-07 BETTER
 91%|█████████ | 29/32 [00:47<00:04,  1.56s/it] 94%|█████████▍| 30/32 [00:49<00:03,  1.56s/it] 97%|█████████▋| 31/32 [00:50<00:01,  1.56s/it]100%|██████████| 32/32 [00:52<00:00,  1.56s/it]100%|██████████| 32/32 [00:52<00:00,  1.64s/it]
I0316 09:00:49.375555 462198 finetune.py:68] layer 0_up @ epoch 0 new loss 1.4614091981002275e-07 old loss 1.477179694120423e-07 BETTER
I0316 09:00:55.087971 463610 finetune.py:45] layer 1_up initial loss 5.208072479945258e-07
W0316 09:00:55.088385 463610 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:01:07.523563 466629 finetune.py:68] layer 3_o @ epoch 2 new loss 1.281399590880028e-06 old loss 1.2862697076343466e-06 BETTER
I0316 09:01:17.480051 465058 finetune.py:68] layer 2_o @ epoch 4 new loss 5.123558253217197e-07 old loss 5.141092742633191e-07 BETTER
W0316 09:01:19.328017 465058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_o proxy err 0.0011438456131145358 tr(WHW.T) 0.5577508211135864
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:04,  2.09s/it]  6%|▋         | 2/32 [00:03<00:53,  1.77s/it]I0316 09:01:25.723066 462198 finetune.py:68] layer 0_up @ epoch 1 new loss 1.4592490060749697e-07 old loss 1.4614091981002275e-07 BETTER
  9%|▉         | 3/32 [00:05<00:48,  1.67s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.63s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.60s/it]I0316 09:01:29.106076 463610 finetune.py:68] layer 1_up @ epoch 0 new loss 4.50868384405112e-07 old loss 5.208072479945258e-07 BETTER
 19%|█▉        | 6/32 [00:09<00:41,  1.58s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.57s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.55s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.55s/it] 38%|███▊      | 12/32 [00:19<00:30,  1.54s/it] 41%|████      | 13/32 [00:20<00:29,  1.55s/it] 44%|████▍     | 14/32 [00:22<00:27,  1.55s/it]I0316 09:01:43.056742 466629 finetune.py:68] layer 3_o @ epoch 3 new loss 1.2755162970279343e-06 old loss 1.281399590880028e-06 BETTER
 47%|████▋     | 15/32 [00:23<00:26,  1.54s/it] 50%|█████     | 16/32 [00:25<00:24,  1.54s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.54s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.54s/it] 59%|█████▉    | 19/32 [00:29<00:20,  1.54s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.54s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.54s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it] 72%|███████▏  | 23/32 [00:36<00:13,  1.54s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.54s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.54s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.54s/it]I0316 09:02:01.853179 462198 finetune.py:68] layer 0_up @ epoch 2 new loss 1.457744360777724e-07 old loss 1.4592490060749697e-07 BETTER
 84%|████████▍ | 27/32 [00:42<00:07,  1.54s/it]I0316 09:02:03.926959 463610 finetune.py:68] layer 1_up @ epoch 1 new loss 4.4855337932858674e-07 old loss 4.50868384405112e-07 BETTER
 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.54s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.56s/it]
I0316 09:02:18.545969 466629 finetune.py:68] layer 3_o @ epoch 4 new loss 1.271110818379384e-06 old loss 1.2755162970279343e-06 BETTER
I0316 09:02:18.742000 465058 finetune.py:45] layer 2_up initial loss 1.0917200370386126e-06
W0316 09:02:18.742322 465058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0316 09:02:20.208019 466629 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

3_o proxy err 0.0018216324970126152 tr(WHW.T) 0.938117265701294
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:03,  2.05s/it]  6%|▋         | 2/32 [00:03<00:52,  1.75s/it]  9%|▉         | 3/32 [00:05<00:47,  1.65s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it]I0316 09:02:38.403796 462198 finetune.py:68] layer 0_up @ epoch 3 new loss 1.4566276718142035e-07 old loss 1.457744360777724e-07 BETTER
I0316 09:02:38.507731 463610 finetune.py:68] layer 1_up @ epoch 2 new loss 4.479721837924444e-07 old loss 4.4855337932858674e-07 BETTER
 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.54s/it]I0316 09:02:52.885042 465058 finetune.py:68] layer 2_up @ epoch 0 new loss 1.0849479394892114e-06 old loss 1.0917200370386126e-06 BETTER
 66%|██████▌   | 21/32 [00:32<00:16,  1.54s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.56s/it] 72%|███████▏  | 23/32 [00:35<00:14,  1.57s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.55s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.54s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.54s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.53s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.53s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.52s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
I0316 09:03:13.243247 463610 finetune.py:76] layer 1_up @ epoch 3 new loss 4.484827513806522e-07 old loss 4.479721837924444e-07 WORSE
I0316 09:03:14.991712 462198 finetune.py:68] layer 0_up @ epoch 4 new loss 1.4557197403064492e-07 old loss 1.4566276718142035e-07 BETTER
W0316 09:03:16.844778 462198 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

0_up proxy err 0.002578668063506484 tr(WHW.T) 101.63921356201172
  0%|          | 0/32 [00:00<?, ?it/s]I0316 09:03:19.502895 466629 finetune.py:45] layer 3_up initial loss 2.554026650614105e-06
W0316 09:03:19.503281 466629 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:02<01:10,  2.29s/it]  6%|▋         | 2/32 [00:04<01:00,  2.03s/it]  9%|▉         | 3/32 [00:05<00:54,  1.86s/it] 12%|█▎        | 4/32 [00:07<00:48,  1.74s/it] 16%|█▌        | 5/32 [00:08<00:44,  1.66s/it]I0316 09:03:28.103550 465058 finetune.py:68] layer 2_up @ epoch 1 new loss 1.082926246454008e-06 old loss 1.0849479394892114e-06 BETTER
 19%|█▉        | 6/32 [00:10<00:41,  1.60s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.56s/it] 25%|██▌       | 8/32 [00:13<00:37,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.56s/it] 34%|███▍      | 11/32 [00:18<00:33,  1.61s/it] 38%|███▊      | 12/32 [00:19<00:33,  1.66s/it] 41%|████      | 13/32 [00:21<00:32,  1.70s/it] 44%|████▍     | 14/32 [00:23<00:31,  1.72s/it] 47%|████▋     | 15/32 [00:25<00:29,  1.73s/it] 50%|█████     | 16/32 [00:27<00:28,  1.76s/it] 53%|█████▎    | 17/32 [00:28<00:25,  1.72s/it]I0316 09:03:47.375431 463610 finetune.py:68] layer 1_up @ epoch 4 new loss 4.469436021281581e-07 old loss 4.479721837924444e-07 BETTER
 56%|█████▋    | 18/32 [00:30<00:23,  1.65s/it]W0316 09:03:48.868537 463610 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 59%|█████▉    | 19/32 [00:31<00:20,  1.60s/it]1_up proxy err 0.0030461172573268414 tr(WHW.T) 159.80999755859375
  0%|          | 0/32 [00:00<?, ?it/s] 62%|██████▎   | 20/32 [00:33<00:18,  1.57s/it]  3%|▎         | 1/32 [00:02<01:02,  2.00s/it] 66%|██████▌   | 21/32 [00:34<00:16,  1.54s/it]I0316 09:03:53.210992 466629 finetune.py:68] layer 3_up @ epoch 0 new loss 2.5415079107915517e-06 old loss 2.554026650614105e-06 BETTER
  6%|▋         | 2/32 [00:03<00:52,  1.75s/it] 69%|██████▉   | 22/32 [00:36<00:15,  1.53s/it]  9%|▉         | 3/32 [00:05<00:48,  1.67s/it] 72%|███████▏  | 23/32 [00:37<00:13,  1.54s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.63s/it] 75%|███████▌  | 24/32 [00:39<00:12,  1.59s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.61s/it] 78%|███████▊  | 25/32 [00:41<00:11,  1.65s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.60s/it] 81%|████████▏ | 26/32 [00:43<00:10,  1.70s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.60s/it]I0316 09:04:03.048619 465058 finetune.py:68] layer 2_up @ epoch 2 new loss 1.0810416597450967e-06 old loss 1.082926246454008e-06 BETTER
 84%|████████▍ | 27/32 [00:44<00:08,  1.72s/it] 25%|██▌       | 8/32 [00:13<00:38,  1.59s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.59s/it] 88%|████████▊ | 28/32 [00:46<00:06,  1.73s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.59s/it] 91%|█████████ | 29/32 [00:48<00:05,  1.76s/it] 34%|███▍      | 11/32 [00:17<00:33,  1.59s/it] 94%|█████████▍| 30/32 [00:50<00:03,  1.74s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.58s/it] 97%|█████████▋| 31/32 [00:51<00:01,  1.73s/it] 41%|████      | 13/32 [00:20<00:29,  1.57s/it]100%|██████████| 32/32 [00:53<00:00,  1.65s/it]100%|██████████| 32/32 [00:53<00:00,  1.67s/it]
 44%|████▍     | 14/32 [00:22<00:28,  1.57s/it] 47%|████▋     | 15/32 [00:24<00:26,  1.56s/it] 50%|█████     | 16/32 [00:25<00:24,  1.56s/it] 53%|█████▎    | 17/32 [00:27<00:23,  1.56s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.56s/it]I0316 09:04:20.185010 462198 finetune.py:45] layer 0_gate initial loss 2.2401513888326008e-07
W0316 09:04:20.185779 462198 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 59%|█████▉    | 19/32 [00:30<00:20,  1.56s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.57s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.57s/it] 69%|██████▉   | 22/32 [00:35<00:15,  1.57s/it] 72%|███████▏  | 23/32 [00:36<00:14,  1.58s/it]I0316 09:04:27.906766 466629 finetune.py:68] layer 3_up @ epoch 1 new loss 2.53447387876804e-06 old loss 2.5415079107915517e-06 BETTER
 75%|███████▌  | 24/32 [00:38<00:12,  1.57s/it] 78%|███████▊  | 25/32 [00:39<00:11,  1.58s/it] 81%|████████▏ | 26/32 [00:41<00:09,  1.58s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.58s/it] 88%|████████▊ | 28/32 [00:44<00:06,  1.58s/it] 91%|█████████ | 29/32 [00:46<00:04,  1.57s/it]I0316 09:04:37.904495 465058 finetune.py:68] layer 2_up @ epoch 3 new loss 1.0791402473842027e-06 old loss 1.0810416597450967e-06 BETTER
 94%|█████████▍| 30/32 [00:47<00:03,  1.57s/it] 97%|█████████▋| 31/32 [00:49<00:01,  1.57s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]100%|██████████| 32/32 [00:50<00:00,  1.59s/it]
I0316 09:04:49.253027 463610 finetune.py:45] layer 1_gate initial loss 6.844991276011569e-07
W0316 09:04:49.253346 463610 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:04:54.087249 462198 finetune.py:68] layer 0_gate @ epoch 0 new loss 2.2143947830954858e-07 old loss 2.2401513888326008e-07 BETTER
I0316 09:05:02.134602 466629 finetune.py:68] layer 3_up @ epoch 2 new loss 2.528950517444173e-06 old loss 2.53447387876804e-06 BETTER
I0316 09:05:12.820072 465058 finetune.py:68] layer 2_up @ epoch 4 new loss 1.0776034287118819e-06 old loss 1.0791402473842027e-06 BETTER
W0316 09:05:14.355021 465058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_up proxy err 0.003558606840670109 tr(WHW.T) 225.97817993164062
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:04,  2.10s/it]  6%|▋         | 2/32 [00:03<00:52,  1.77s/it]  9%|▉         | 3/32 [00:05<00:48,  1.66s/it]I0316 09:05:22.031486 463610 finetune.py:68] layer 1_gate @ epoch 0 new loss 6.393203761945188e-07 old loss 6.844991276011569e-07 BETTER
 12%|█▎        | 4/32 [00:06<00:45,  1.62s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.56s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it]I0316 09:05:28.808893 462198 finetune.py:68] layer 0_gate @ epoch 1 new loss 2.2119003517673264e-07 old loss 2.2143947830954858e-07 BETTER
 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.54s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it]I0316 09:05:36.447738 466629 finetune.py:68] layer 3_up @ epoch 3 new loss 2.5237854970328044e-06 old loss 2.528950517444173e-06 BETTER
 44%|████▍     | 14/32 [00:22<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it] 50%|█████     | 16/32 [00:25<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.58s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.57s/it]I0316 09:05:55.049032 463610 finetune.py:68] layer 1_gate @ epoch 1 new loss 6.387816711139749e-07 old loss 6.393203761945188e-07 BETTER
 81%|████████▏ | 26/32 [00:40<00:09,  1.56s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.55s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.54s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it]I0316 09:06:03.559529 462198 finetune.py:68] layer 0_gate @ epoch 2 new loss 2.2104234176367754e-07 old loss 2.2119003517673264e-07 BETTER
 97%|█████████▋| 31/32 [00:48<00:01,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
I0316 09:06:10.560428 466629 finetune.py:68] layer 3_up @ epoch 4 new loss 2.517779648769647e-06 old loss 2.5237854970328044e-06 BETTER
W0316 09:06:12.020328 466629 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

3_up proxy err 0.003429675940424204 tr(WHW.T) 315.96026611328125
  0%|          | 0/32 [00:00<?, ?it/s]I0316 09:06:14.036301 465058 finetune.py:45] layer 2_gate initial loss 1.5399019730466534e-06
W0316 09:06:14.036780 465058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<01:01,  1.98s/it]  6%|▋         | 2/32 [00:03<00:54,  1.81s/it]  9%|▉         | 3/32 [00:05<00:48,  1.68s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.63s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.59s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.56s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it]I0316 09:06:28.251290 463610 finetune.py:68] layer 1_gate @ epoch 2 new loss 6.384050266206032e-07 old loss 6.387816711139749e-07 BETTER
 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.54s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it] 44%|████▍     | 14/32 [00:22<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it]I0316 09:06:38.388903 462198 finetune.py:68] layer 0_gate @ epoch 3 new loss 2.2088902085215523e-07 old loss 2.2104234176367754e-07 BETTER
 50%|█████     | 16/32 [00:25<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it]I0316 09:06:47.142510 465058 finetune.py:68] layer 2_gate @ epoch 0 new loss 1.5326204447774217e-06 old loss 1.5399019730466534e-06 BETTER
 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.52s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.52s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.52s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.52s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.52s/it]I0316 09:07:01.752829 463610 finetune.py:68] layer 1_gate @ epoch 3 new loss 6.38052256363153e-07 old loss 6.384050266206032e-07 BETTER
100%|██████████| 32/32 [00:49<00:00,  1.51s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
I0316 09:07:10.752200 466629 finetune.py:45] layer 3_gate initial loss 3.411748139114934e-06
W0316 09:07:10.752828 466629 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:07:13.379571 462198 finetune.py:68] layer 0_gate @ epoch 4 new loss 2.2076977757023997e-07 old loss 2.2088902085215523e-07 BETTER
W0316 09:07:14.609000 462198 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

0_gate proxy err 0.001780770835466683 tr(WHW.T) 179.70040893554688
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:01<01:57,  1.06s/it]  2%|▏         | 2/112 [00:01<01:17,  1.41it/s]  3%|▎         | 3/112 [00:01<01:04,  1.68it/s]I0316 09:07:20.410398 465058 finetune.py:68] layer 2_gate @ epoch 1 new loss 1.5312484720197972e-06 old loss 1.5326204447774217e-06 BETTER
  4%|▎         | 4/112 [00:02<00:58,  1.86it/s]  4%|▍         | 5/112 [00:02<00:53,  2.01it/s]  5%|▌         | 6/112 [00:03<00:51,  2.07it/s]  6%|▋         | 7/112 [00:03<00:49,  2.13it/s]  7%|▋         | 8/112 [00:04<00:47,  2.20it/s]  8%|▊         | 9/112 [00:04<00:45,  2.24it/s]  9%|▉         | 10/112 [00:05<00:45,  2.25it/s] 10%|▉         | 11/112 [00:05<00:44,  2.28it/s] 11%|█         | 12/112 [00:05<00:43,  2.29it/s] 12%|█▏        | 13/112 [00:06<00:44,  2.22it/s] 12%|█▎        | 14/112 [00:06<00:44,  2.22it/s] 13%|█▎        | 15/112 [00:07<00:43,  2.22it/s] 14%|█▍        | 16/112 [00:07<00:42,  2.23it/s] 15%|█▌        | 17/112 [00:08<00:42,  2.25it/s] 16%|█▌        | 18/112 [00:08<00:41,  2.24it/s] 17%|█▋        | 19/112 [00:09<00:40,  2.30it/s] 18%|█▊        | 20/112 [00:09<00:39,  2.31it/s] 19%|█▉        | 21/112 [00:09<00:37,  2.41it/s] 20%|█▉        | 22/112 [00:10<00:36,  2.49it/s] 21%|██        | 23/112 [00:10<00:34,  2.54it/s] 21%|██▏       | 24/112 [00:10<00:34,  2.55it/s] 22%|██▏       | 25/112 [00:11<00:34,  2.52it/s] 23%|██▎       | 26/112 [00:11<00:34,  2.48it/s] 24%|██▍       | 27/112 [00:12<00:33,  2.50it/s] 25%|██▌       | 28/112 [00:12<00:33,  2.51it/s] 26%|██▌       | 29/112 [00:12<00:33,  2.51it/s] 27%|██▋       | 30/112 [00:13<00:33,  2.46it/s] 28%|██▊       | 31/112 [00:13<00:32,  2.48it/s] 29%|██▊       | 32/112 [00:14<00:32,  2.48it/s] 29%|██▉       | 33/112 [00:14<00:31,  2.49it/s] 30%|███       | 34/112 [00:15<00:31,  2.48it/s] 31%|███▏      | 35/112 [00:15<00:31,  2.45it/s] 32%|███▏      | 36/112 [00:15<00:30,  2.47it/s] 33%|███▎      | 37/112 [00:16<00:30,  2.48it/s] 34%|███▍      | 38/112 [00:16<00:29,  2.48it/s]I0316 09:07:35.105470 463610 finetune.py:76] layer 1_gate @ epoch 4 new loss 6.382934429893794e-07 old loss 6.38052256363153e-07 WORSE
 35%|███▍      | 39/112 [00:17<00:29,  2.49it/s] 36%|███▌      | 40/112 [00:17<00:28,  2.49it/s]W0316 09:07:36.143599 463610 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 37%|███▋      | 41/112 [00:17<00:28,  2.51it/s] 38%|███▊      | 42/112 [00:18<00:27,  2.50it/s] 38%|███▊      | 43/112 [00:18<00:27,  2.50it/s] 39%|███▉      | 44/112 [00:19<00:27,  2.44it/s] 40%|████      | 45/112 [00:19<00:27,  2.47it/s] 41%|████      | 46/112 [00:19<00:27,  2.43it/s] 42%|████▏     | 47/112 [00:20<00:27,  2.33it/s] 43%|████▎     | 48/112 [00:20<00:27,  2.35it/s]1_gate proxy err 0.002176495036110282 tr(WHW.T) 270.91827392578125
  0%|          | 0/112 [00:00<?, ?it/s] 44%|████▍     | 49/112 [00:21<00:26,  2.35it/s] 45%|████▍     | 50/112 [00:21<00:26,  2.38it/s] 46%|████▌     | 51/112 [00:22<00:25,  2.40it/s]  1%|          | 1/112 [00:01<02:00,  1.09s/it] 46%|████▋     | 52/112 [00:22<00:24,  2.43it/s]  2%|▏         | 2/112 [00:01<01:19,  1.38it/s] 47%|████▋     | 53/112 [00:22<00:23,  2.46it/s]  3%|▎         | 3/112 [00:01<01:02,  1.73it/s] 48%|████▊     | 54/112 [00:23<00:23,  2.42it/s]  4%|▎         | 4/112 [00:02<00:54,  1.97it/s] 49%|████▉     | 55/112 [00:23<00:23,  2.38it/s]  4%|▍         | 5/112 [00:02<00:50,  2.12it/s] 50%|█████     | 56/112 [00:24<00:23,  2.35it/s]  5%|▌         | 6/112 [00:03<00:48,  2.20it/s] 51%|█████     | 57/112 [00:24<00:23,  2.37it/s]I0316 09:07:42.933085 466629 finetune.py:68] layer 3_gate @ epoch 0 new loss 3.3987569167948095e-06 old loss 3.411748139114934e-06 BETTER
  6%|▋         | 7/112 [00:03<00:48,  2.16it/s] 52%|█████▏    | 58/112 [00:24<00:22,  2.37it/s]  7%|▋         | 8/112 [00:04<00:48,  2.14it/s] 53%|█████▎    | 59/112 [00:25<00:21,  2.41it/s]  8%|▊         | 9/112 [00:04<00:45,  2.25it/s] 54%|█████▎    | 60/112 [00:25<00:21,  2.45it/s]  9%|▉         | 10/112 [00:04<00:44,  2.31it/s] 54%|█████▍    | 61/112 [00:26<00:20,  2.47it/s] 10%|▉         | 11/112 [00:05<00:42,  2.35it/s] 55%|█████▌    | 62/112 [00:26<00:20,  2.46it/s] 11%|█         | 12/112 [00:05<00:42,  2.34it/s] 56%|█████▋    | 63/112 [00:26<00:20,  2.42it/s] 12%|█▏        | 13/112 [00:06<00:41,  2.37it/s] 57%|█████▋    | 64/112 [00:27<00:19,  2.43it/s] 58%|█████▊    | 65/112 [00:27<00:19,  2.43it/s] 12%|█▎        | 14/112 [00:06<00:42,  2.33it/s] 59%|█████▉    | 66/112 [00:28<00:19,  2.41it/s] 13%|█▎        | 15/112 [00:07<00:42,  2.29it/s] 60%|█████▉    | 67/112 [00:28<00:18,  2.39it/s] 14%|█▍        | 16/112 [00:07<00:42,  2.25it/s] 61%|██████    | 68/112 [00:29<00:18,  2.40it/s] 15%|█▌        | 17/112 [00:08<00:42,  2.24it/s] 62%|██████▏   | 69/112 [00:29<00:17,  2.44it/s] 16%|█▌        | 18/112 [00:08<00:40,  2.33it/s] 62%|██████▎   | 70/112 [00:29<00:16,  2.51it/s] 17%|█▋        | 19/112 [00:08<00:38,  2.40it/s] 63%|██████▎   | 71/112 [00:30<00:16,  2.52it/s] 18%|█▊        | 20/112 [00:09<00:37,  2.43it/s] 64%|██████▍   | 72/112 [00:30<00:16,  2.44it/s] 19%|█▉        | 21/112 [00:09<00:37,  2.45it/s] 65%|██████▌   | 73/112 [00:31<00:16,  2.40it/s] 20%|█▉        | 22/112 [00:09<00:36,  2.46it/s] 66%|██████▌   | 74/112 [00:31<00:16,  2.37it/s] 21%|██        | 23/112 [00:10<00:36,  2.45it/s] 67%|██████▋   | 75/112 [00:31<00:15,  2.43it/s] 21%|██▏       | 24/112 [00:10<00:35,  2.45it/s] 68%|██████▊   | 76/112 [00:32<00:14,  2.50it/s] 22%|██▏       | 25/112 [00:11<00:35,  2.46it/s] 69%|██████▉   | 77/112 [00:32<00:14,  2.48it/s] 23%|██▎       | 26/112 [00:11<00:34,  2.46it/s] 70%|██████▉   | 78/112 [00:33<00:14,  2.42it/s] 24%|██▍       | 27/112 [00:12<00:34,  2.47it/s] 71%|███████   | 79/112 [00:33<00:13,  2.38it/s] 25%|██▌       | 28/112 [00:12<00:33,  2.48it/s] 71%|███████▏  | 80/112 [00:33<00:13,  2.35it/s] 26%|██▌       | 29/112 [00:12<00:33,  2.45it/s] 72%|███████▏  | 81/112 [00:34<00:13,  2.36it/s] 27%|██▋       | 30/112 [00:13<00:34,  2.35it/s] 73%|███████▎  | 82/112 [00:34<00:12,  2.40it/s] 28%|██▊       | 31/112 [00:13<00:35,  2.26it/s] 74%|███████▍  | 83/112 [00:35<00:11,  2.44it/s] 29%|██▊       | 32/112 [00:14<00:36,  2.21it/s]I0316 09:07:53.856574 465058 finetune.py:68] layer 2_gate @ epoch 2 new loss 1.5296074025172857e-06 old loss 1.5312484720197972e-06 BETTER
 75%|███████▌  | 84/112 [00:35<00:11,  2.48it/s] 29%|██▉       | 33/112 [00:14<00:35,  2.22it/s] 76%|███████▌  | 85/112 [00:35<00:10,  2.51it/s] 30%|███       | 34/112 [00:15<00:34,  2.29it/s] 77%|███████▋  | 86/112 [00:36<00:10,  2.43it/s] 31%|███▏      | 35/112 [00:15<00:32,  2.34it/s] 78%|███████▊  | 87/112 [00:36<00:10,  2.40it/s] 32%|███▏      | 36/112 [00:15<00:31,  2.39it/s] 79%|███████▊  | 88/112 [00:37<00:10,  2.36it/s] 33%|███▎      | 37/112 [00:16<00:31,  2.39it/s] 79%|███████▉  | 89/112 [00:37<00:09,  2.43it/s] 34%|███▍      | 38/112 [00:16<00:30,  2.42it/s] 80%|████████  | 90/112 [00:38<00:08,  2.49it/s] 35%|███▍      | 39/112 [00:17<00:29,  2.44it/s] 81%|████████▏ | 91/112 [00:38<00:08,  2.54it/s] 36%|███▌      | 40/112 [00:17<00:29,  2.46it/s] 82%|████████▏ | 92/112 [00:38<00:08,  2.48it/s] 37%|███▋      | 41/112 [00:17<00:28,  2.48it/s] 83%|████████▎ | 93/112 [00:39<00:07,  2.47it/s] 38%|███▊      | 42/112 [00:18<00:28,  2.49it/s] 84%|████████▍ | 94/112 [00:39<00:07,  2.42it/s] 38%|███▊      | 43/112 [00:18<00:27,  2.47it/s] 85%|████████▍ | 95/112 [00:40<00:07,  2.41it/s] 39%|███▉      | 44/112 [00:19<00:27,  2.49it/s] 86%|████████▌ | 96/112 [00:40<00:06,  2.48it/s] 40%|████      | 45/112 [00:19<00:26,  2.48it/s] 87%|████████▋ | 97/112 [00:40<00:05,  2.55it/s] 41%|████      | 46/112 [00:19<00:26,  2.50it/s] 88%|████████▊ | 98/112 [00:41<00:05,  2.59it/s] 42%|████▏     | 47/112 [00:20<00:26,  2.48it/s] 88%|████████▊ | 99/112 [00:41<00:05,  2.55it/s] 43%|████▎     | 48/112 [00:20<00:25,  2.47it/s] 89%|████████▉ | 100/112 [00:42<00:04,  2.57it/s] 44%|████▍     | 49/112 [00:21<00:25,  2.49it/s] 90%|█████████ | 101/112 [00:42<00:04,  2.60it/s] 45%|████▍     | 50/112 [00:21<00:24,  2.51it/s] 91%|█████████ | 102/112 [00:42<00:03,  2.62it/s] 46%|████▌     | 51/112 [00:21<00:24,  2.50it/s] 92%|█████████▏| 103/112 [00:43<00:03,  2.54it/s] 46%|████▋     | 52/112 [00:22<00:23,  2.51it/s] 93%|█████████▎| 104/112 [00:43<00:03,  2.42it/s] 47%|████▋     | 53/112 [00:22<00:23,  2.50it/s] 94%|█████████▍| 105/112 [00:44<00:02,  2.36it/s] 48%|████▊     | 54/112 [00:23<00:23,  2.50it/s] 95%|█████████▍| 106/112 [00:44<00:02,  2.35it/s] 49%|████▉     | 55/112 [00:23<00:22,  2.50it/s] 96%|█████████▌| 107/112 [00:44<00:02,  2.38it/s] 50%|█████     | 56/112 [00:23<00:22,  2.49it/s] 96%|█████████▋| 108/112 [00:45<00:01,  2.47it/s] 51%|█████     | 57/112 [00:24<00:21,  2.50it/s] 97%|█████████▋| 109/112 [00:45<00:01,  2.53it/s] 52%|█████▏    | 58/112 [00:24<00:21,  2.51it/s] 98%|█████████▊| 110/112 [00:46<00:00,  2.58it/s] 53%|█████▎    | 59/112 [00:25<00:21,  2.52it/s] 99%|█████████▉| 111/112 [00:46<00:00,  2.61it/s] 54%|█████▎    | 60/112 [00:25<00:20,  2.53it/s]100%|██████████| 112/112 [00:46<00:00,  2.63it/s]100%|██████████| 112/112 [00:46<00:00,  2.39it/s]
 54%|█████▍    | 61/112 [00:25<00:20,  2.54it/s] 55%|█████▌    | 62/112 [00:26<00:19,  2.53it/s] 56%|█████▋    | 63/112 [00:26<00:19,  2.53it/s] 57%|█████▋    | 64/112 [00:27<00:19,  2.47it/s] 58%|█████▊    | 65/112 [00:27<00:18,  2.48it/s] 59%|█████▉    | 66/112 [00:27<00:18,  2.48it/s] 60%|█████▉    | 67/112 [00:28<00:18,  2.39it/s] 61%|██████    | 68/112 [00:28<00:18,  2.35it/s] 62%|██████▏   | 69/112 [00:29<00:18,  2.28it/s] 62%|██████▎   | 70/112 [00:29<00:17,  2.35it/s] 63%|██████▎   | 71/112 [00:30<00:17,  2.41it/s] 64%|██████▍   | 72/112 [00:30<00:16,  2.44it/s] 65%|██████▌   | 73/112 [00:30<00:16,  2.39it/s] 66%|██████▌   | 74/112 [00:31<00:15,  2.43it/s] 67%|██████▋   | 75/112 [00:31<00:14,  2.47it/s] 68%|██████▊   | 76/112 [00:32<00:14,  2.48it/s] 69%|██████▉   | 77/112 [00:32<00:14,  2.48it/s] 70%|██████▉   | 78/112 [00:32<00:13,  2.48it/s]W0316 09:08:12.601000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:08:12.602000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:08:12.602000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:08:12.602000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:08:12.602000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:08:12.602000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:08:12.602000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:08:12.647000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:08:12.647000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:08:12.647000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:08:12.647000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:08:12.647000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:08:12.836000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:08:12.836000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:08:12.836000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:08:12.836000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:08:12.836000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 71%|███████   | 79/112 [00:33<00:13,  2.39it/s]W0316 09:08:13.159000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:08:13.159000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:08:13.160000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:08:13.160000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:08:13.160000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:08:13.160000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:08:13.160000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:08:13.195000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:08:13.195000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:08:13.195000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:08:13.195000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:08:13.195000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:08:13.266000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:08:13.266000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:08:13.266000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:08:13.266000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:08:13.267000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 71%|███████▏  | 80/112 [00:33<00:13,  2.43it/s] 72%|███████▏  | 81/112 [00:34<00:12,  2.47it/s] 73%|███████▎  | 82/112 [00:34<00:12,  2.49it/s] 74%|███████▍  | 83/112 [00:34<00:11,  2.51it/s]W0316 09:08:14.622000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 84/112 [00:35<00:11,  2.52it/s]W0316 09:08:15.103000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:08:15.104000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:08:15.104000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:08:15.104000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:08:15.104000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:08:15.104000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:08:15.104000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:08:15.137000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:08:15.137000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:08:15.137000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:08:15.137000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:08:15.137000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 76%|███████▌  | 85/112 [00:35<00:10,  2.52it/s]W0316 09:08:15.529000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:08:15.530000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:08:15.530000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:08:15.530000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:08:15.530000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 77%|███████▋  | 86/112 [00:36<00:10,  2.53it/s]I0316 09:08:15.731119 466629 finetune.py:68] layer 3_gate @ epoch 1 new loss 3.3933324630197603e-06 old loss 3.3987569167948095e-06 BETTER
 78%|███████▊  | 87/112 [00:36<00:09,  2.54it/s]W0316 09:08:16.084000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:08:16.090000 139715611125568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 79%|███████▊  | 88/112 [00:36<00:09,  2.53it/s] 79%|███████▉  | 89/112 [00:37<00:09,  2.53it/s] 80%|████████  | 90/112 [00:37<00:08,  2.51it/s] 81%|████████▏ | 91/112 [00:38<00:08,  2.52it/s] 82%|████████▏ | 92/112 [00:38<00:07,  2.52it/s] 83%|████████▎ | 93/112 [00:38<00:07,  2.53it/s] 84%|████████▍ | 94/112 [00:39<00:07,  2.55it/s] 85%|████████▍ | 95/112 [00:39<00:06,  2.55it/s] 86%|████████▌ | 96/112 [00:40<00:06,  2.55it/s] 87%|████████▋ | 97/112 [00:40<00:05,  2.54it/s] 88%|████████▊ | 98/112 [00:40<00:05,  2.54it/s] 88%|████████▊ | 99/112 [00:41<00:05,  2.54it/s] 89%|████████▉ | 100/112 [00:41<00:04,  2.54it/s] 90%|█████████ | 101/112 [00:42<00:04,  2.51it/s] 91%|█████████ | 102/112 [00:42<00:03,  2.52it/s] 92%|█████████▏| 103/112 [00:42<00:03,  2.53it/s] 93%|█████████▎| 104/112 [00:43<00:03,  2.53it/s] 94%|█████████▍| 105/112 [00:43<00:02,  2.53it/s] 95%|█████████▍| 106/112 [00:44<00:02,  2.53it/s] 96%|█████████▌| 107/112 [00:44<00:01,  2.54it/s]I0316 09:08:24.258721 462198 finetune.py:45] layer 0_down initial loss 4.513701981068152e-07
W0316 09:08:24.259345 462198 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 96%|█████████▋| 108/112 [00:44<00:01,  2.53it/s] 97%|█████████▋| 109/112 [00:45<00:01,  2.54it/s] 98%|█████████▊| 110/112 [00:45<00:00,  2.54it/s] 99%|█████████▉| 111/112 [00:45<00:00,  2.53it/s]100%|██████████| 112/112 [00:46<00:00,  2.53it/s]100%|██████████| 112/112 [00:46<00:00,  2.41it/s]
I0316 09:08:27.355635 465058 finetune.py:68] layer 2_gate @ epoch 3 new loss 1.52788470586529e-06 old loss 1.5296074025172857e-06 BETTER
W0316 09:08:33.266000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.267000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.267000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.267000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.267000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.267000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.267000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.311000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.311000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.311000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.311000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.311000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.494000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.494000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.494000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.494000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.494000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.821000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.821000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.821000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.821000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.821000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.821000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.822000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.855000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.855000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.855000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.855000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.855000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.928000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.928000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.928000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.929000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:08:33.929000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:08:35.298000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:08:35.797000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:08:35.798000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:08:35.798000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:08:35.798000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:08:35.798000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:08:35.798000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:08:35.798000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:08:35.832000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:08:35.832000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:08:35.832000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:08:35.832000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:08:35.832000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:08:36.225000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:08:36.225000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:08:36.225000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:08:36.225000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:08:36.225000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:08:36.789000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:08:36.794000 140135239014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0316 09:08:44.756875 463610 finetune.py:45] layer 1_down initial loss 1.0818378086696612e-06
W0316 09:08:44.757382 463610 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:08:48.444376 466629 finetune.py:68] layer 3_gate @ epoch 2 new loss 3.3885535231092945e-06 old loss 3.3933324630197603e-06 BETTER
I0316 09:08:56.691237 462198 finetune.py:68] layer 0_down @ epoch 0 new loss 4.510794724410516e-07 old loss 4.513701981068152e-07 BETTER
I0316 09:09:00.602699 465058 finetune.py:68] layer 2_gate @ epoch 4 new loss 1.5264453168128966e-06 old loss 1.52788470586529e-06 BETTER
W0316 09:09:01.949220 465058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_gate proxy err 0.0023111249320209026 tr(WHW.T) 449.24481201171875
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:41,  1.10it/s]  2%|▏         | 2/112 [00:01<01:07,  1.64it/s]  3%|▎         | 3/112 [00:01<00:56,  1.95it/s]  4%|▎         | 4/112 [00:02<00:50,  2.15it/s]  4%|▍         | 5/112 [00:02<00:46,  2.28it/s]  5%|▌         | 6/112 [00:02<00:44,  2.37it/s]  6%|▋         | 7/112 [00:03<00:43,  2.44it/s]  7%|▋         | 8/112 [00:03<00:41,  2.48it/s]  8%|▊         | 9/112 [00:04<00:40,  2.52it/s]  9%|▉         | 10/112 [00:04<00:40,  2.54it/s] 10%|▉         | 11/112 [00:04<00:39,  2.55it/s] 11%|█         | 12/112 [00:05<00:39,  2.56it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.56it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.56it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.57it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.55it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.55it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.56it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.57it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.58it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.59it/s] 20%|█▉        | 22/112 [00:09<00:34,  2.59it/s] 21%|██        | 23/112 [00:09<00:34,  2.59it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.59it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.58it/s]I0316 09:09:15.800324 463610 finetune.py:68] layer 1_down @ epoch 0 new loss 1.081130562852195e-06 old loss 1.0818378086696612e-06 BETTER
 23%|██▎       | 26/112 [00:10<00:33,  2.57it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.53it/s] 25%|██▌       | 28/112 [00:11<00:33,  2.54it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.55it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.55it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.55it/s] 29%|██▊       | 32/112 [00:12<00:31,  2.57it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.57it/s] 30%|███       | 34/112 [00:13<00:30,  2.57it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.57it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.55it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.54it/s] 34%|███▍      | 38/112 [00:15<00:29,  2.53it/s] 35%|███▍      | 39/112 [00:15<00:29,  2.51it/s]I0316 09:09:21.315436 466629 finetune.py:68] layer 3_gate @ epoch 3 new loss 3.3841706681414507e-06 old loss 3.3885535231092945e-06 BETTER
 36%|███▌      | 40/112 [00:16<00:28,  2.52it/s] 37%|███▋      | 41/112 [00:16<00:28,  2.53it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.54it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.56it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.57it/s] 40%|████      | 45/112 [00:18<00:26,  2.57it/s] 41%|████      | 46/112 [00:18<00:25,  2.57it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.57it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.57it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.57it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.54it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.56it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.56it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.56it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.57it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.57it/s] 50%|█████     | 56/112 [00:22<00:21,  2.58it/s] 51%|█████     | 57/112 [00:22<00:21,  2.58it/s] 52%|█████▏    | 58/112 [00:23<00:20,  2.58it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.58it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.58it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.57it/s]I0316 09:09:29.581296 462198 finetune.py:68] layer 0_down @ epoch 1 new loss 4.510432916049467e-07 old loss 4.510794724410516e-07 BETTER
 55%|█████▌    | 62/112 [00:24<00:19,  2.55it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.56it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.57it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.57it/s] 59%|█████▉    | 66/112 [00:26<00:17,  2.58it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.58it/s] 61%|██████    | 68/112 [00:27<00:17,  2.59it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.59it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.59it/s] 63%|██████▎   | 71/112 [00:28<00:15,  2.59it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.58it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.59it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.56it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.57it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.57it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.58it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.58it/s] 71%|███████   | 79/112 [00:31<00:12,  2.58it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.59it/s] 72%|███████▏  | 81/112 [00:32<00:11,  2.59it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.59it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.58it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.58it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.55it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.56it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.57it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.57it/s] 79%|███████▉  | 89/112 [00:35<00:08,  2.57it/s] 80%|████████  | 90/112 [00:35<00:08,  2.58it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.59it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.61it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.61it/s] 84%|████████▍ | 94/112 [00:37<00:06,  2.60it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.60it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.59it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.57it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.57it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.58it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.57it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.57it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.57it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.57it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.57it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.57it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.57it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.56it/s]I0316 09:09:47.459677 463610 finetune.py:68] layer 1_down @ epoch 1 new loss 1.0809031891767518e-06 old loss 1.081130562852195e-06 BETTER
 96%|█████████▋| 108/112 [00:42<00:01,  2.57it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.53it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.55it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.55it/s]100%|██████████| 112/112 [00:44<00:00,  2.55it/s]100%|██████████| 112/112 [00:44<00:00,  2.54it/s]
I0316 09:09:54.028434 466629 finetune.py:68] layer 3_gate @ epoch 4 new loss 3.3797618925746065e-06 old loss 3.3841706681414507e-06 BETTER
W0316 09:09:55.230796 466629 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0316 09:09:56.450000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:09:56.450000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:09:56.451000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:09:56.451000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:09:56.451000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:09:56.451000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:09:56.451000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:09:56.497000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:09:56.497000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:09:56.497000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:09:56.497000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:09:56.497000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:09:56.684000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:09:56.685000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:09:56.685000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:09:56.685000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:09:56.685000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:09:57.010000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:09:57.011000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:09:57.011000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:09:57.011000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:09:57.011000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:09:57.011000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:09:57.011000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:09:57.044000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:09:57.044000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:09:57.044000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:09:57.044000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:09:57.044000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:09:57.115000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:09:57.116000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:09:57.116000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:09:57.116000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:09:57.116000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
3_gate proxy err 0.0018731580348685384 tr(WHW.T) 874.9923095703125
  0%|          | 0/112 [00:00<?, ?it/s]W0316 09:09:58.469000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:09:58.935000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:09:58.935000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:09:58.935000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:09:58.935000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:09:58.936000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:09:58.936000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:09:58.936000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:09:58.968000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:09:58.968000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:09:58.969000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:09:58.969000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:09:58.969000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
  1%|          | 1/112 [00:00<01:39,  1.12it/s]W0316 09:09:59.357000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:09:59.358000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:09:59.358000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:09:59.358000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:09:59.358000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
  2%|▏         | 2/112 [00:01<01:05,  1.67it/s]  3%|▎         | 3/112 [00:01<00:54,  1.99it/s]W0316 09:09:59.908000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:09:59.913000 140611049146176 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
  4%|▎         | 4/112 [00:02<00:49,  2.18it/s]  4%|▍         | 5/112 [00:02<00:46,  2.31it/s]  5%|▌         | 6/112 [00:02<00:44,  2.39it/s]  6%|▋         | 7/112 [00:03<00:42,  2.45it/s]  7%|▋         | 8/112 [00:03<00:41,  2.50it/s]  8%|▊         | 9/112 [00:04<00:40,  2.53it/s]I0316 09:10:02.326376 462198 finetune.py:76] layer 0_down @ epoch 2 new loss 4.510471853791387e-07 old loss 4.510432916049467e-07 WORSE
  9%|▉         | 10/112 [00:04<00:39,  2.55it/s] 10%|▉         | 11/112 [00:04<00:39,  2.57it/s] 11%|█         | 12/112 [00:05<00:38,  2.58it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.58it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.57it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.54it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.56it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.57it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.57it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.57it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.58it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.59it/s] 20%|█▉        | 22/112 [00:09<00:34,  2.59it/s] 21%|██        | 23/112 [00:09<00:34,  2.59it/s]I0316 09:10:07.866127 465058 finetune.py:45] layer 2_down initial loss 2.4673315692780307e-06
W0316 09:10:07.866562 465058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 21%|██▏       | 24/112 [00:09<00:33,  2.59it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.59it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.58it/s] 24%|██▍       | 27/112 [00:10<00:33,  2.55it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.56it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.57it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.57it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.58it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.58it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.59it/s] 30%|███       | 34/112 [00:13<00:30,  2.59it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.59it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.59it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.58it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.58it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.55it/s] 36%|███▌      | 40/112 [00:16<00:28,  2.56it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.57it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.57it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.58it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.59it/s] 40%|████      | 45/112 [00:17<00:25,  2.59it/s] 41%|████      | 46/112 [00:18<00:25,  2.59it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.60it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.60it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.59it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.59it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.57it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.58it/s]I0316 09:10:19.182669 463610 finetune.py:68] layer 1_down @ epoch 2 new loss 1.0808487331814831e-06 old loss 1.0809031891767518e-06 BETTER
 47%|████▋     | 53/112 [00:21<00:22,  2.59it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.59it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.60it/s] 50%|█████     | 56/112 [00:22<00:21,  2.60it/s] 51%|█████     | 57/112 [00:22<00:21,  2.60it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.60it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.60it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.60it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.59it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.59it/s] 56%|█████▋    | 63/112 [00:24<00:19,  2.56it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.58it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.59it/s] 59%|█████▉    | 66/112 [00:26<00:17,  2.60it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.60it/s] 61%|██████    | 68/112 [00:26<00:16,  2.61it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.61it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.61it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.61it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.61it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.60it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.59it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.57it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.58it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.59it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.60it/s] 71%|███████   | 79/112 [00:31<00:12,  2.60it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.60it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.60it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.61it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.61it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.60it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.60it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.60it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.57it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.58it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.59it/s] 80%|████████  | 90/112 [00:35<00:08,  2.60it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.60it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.60it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.61it/s]I0316 09:10:34.945690 462198 finetune.py:68] layer 0_down @ epoch 3 new loss 4.5102547119313385e-07 old loss 4.510432916049467e-07 BETTER
 84%|████████▍ | 94/112 [00:36<00:06,  2.61it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.62it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.61it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.61it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.60it/s] 88%|████████▊ | 99/112 [00:38<00:05,  2.57it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.58it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.59it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.60it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.60it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.61it/s]I0316 09:10:39.086854 465058 finetune.py:68] layer 2_down @ epoch 0 new loss 2.4660776034579612e-06 old loss 2.4673315692780307e-06 BETTER
 94%|█████████▍| 105/112 [00:41<00:02,  2.62it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.62it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.61it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.60it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.60it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.57it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.58it/s]100%|██████████| 112/112 [00:43<00:00,  2.59it/s]100%|██████████| 112/112 [00:43<00:00,  2.56it/s]
W0316 09:10:49.126000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.126000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.127000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.127000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.127000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.127000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.127000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.171000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.171000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.171000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.171000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.171000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.350000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.350000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.350000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.351000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.351000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.677000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.677000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.677000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.677000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.678000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.678000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.678000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.713000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.713000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.713000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.713000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.713000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.789000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.789000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.789000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.789000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:10:49.789000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
I0316 09:10:50.904249 463610 finetune.py:76] layer 1_down @ epoch 3 new loss 1.084144400920195e-06 old loss 1.0808487331814831e-06 WORSE
W0316 09:10:51.176000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:10:51.642000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:10:51.642000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:10:51.642000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:10:51.642000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:10:51.642000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:10:51.643000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:10:51.643000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:10:51.673000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:10:51.673000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:10:51.674000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:10:51.674000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:10:51.674000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:10:52.062000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:10:52.062000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:10:52.062000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:10:52.062000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:10:52.062000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:10:52.610000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:10:52.616000 140279900317504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0316 09:11:00.556902 466629 finetune.py:45] layer 3_down initial loss 5.248292836768087e-06
W0316 09:11:00.557349 466629 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:11:07.908620 462198 finetune.py:68] layer 0_down @ epoch 4 new loss 4.5098556711309357e-07 old loss 4.5102547119313385e-07 BETTER
W0316 09:11:08.781900 462198 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

0_down proxy err 0.0019571352750062943 tr(WHW.T) 0.48143965005874634
I0316 09:11:11.009654 465058 finetune.py:76] layer 2_down @ epoch 1 new loss 2.4660885173943825e-06 old loss 2.4660776034579612e-06 WORSE
I0316 09:11:22.446844 463610 finetune.py:76] layer 1_down @ epoch 4 new loss 1.0840914228538168e-06 old loss 1.0808487331814831e-06 WORSE
W0316 09:11:23.002225 463610 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

1_down proxy err 5.7878940424416214e-05 tr(WHW.T) 67.13075256347656
I0316 09:11:31.437427 466629 finetune.py:68] layer 3_down @ epoch 0 new loss 5.246762611932354e-06 old loss 5.248292836768087e-06 BETTER
I0316 09:11:42.613413 465058 finetune.py:68] layer 2_down @ epoch 2 new loss 2.465663555994979e-06 old loss 2.4660776034579612e-06 BETTER
I0316 09:12:02.765785 466629 finetune.py:68] layer 3_down @ epoch 1 new loss 5.246205546427518e-06 old loss 5.246762611932354e-06 BETTER
I0316 09:12:14.297250 465058 finetune.py:68] layer 2_down @ epoch 3 new loss 2.4655323613842484e-06 old loss 2.465663555994979e-06 BETTER
I0316 09:12:33.983188 460347 quantize_finetune_llama.py:186] computed original embedding for layer 4 in 65.37155103683472s
I0316 09:12:34.412543 460347 quantize_finetune_llama.py:159] layer 5 gpu 1
I0316 09:12:34.428769 466629 finetune.py:76] layer 3_down @ epoch 2 new loss 5.24644747201819e-06 old loss 5.246205546427518e-06 WORSE
I0316 09:12:36.474169 483572 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 09:12:36.474280 483572 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 09:12:36.474340 483572 utils.py:162] NumExpr defaulting to 16 threads.
I0316 09:12:36.680004 483572 config.py:58] PyTorch version 2.4.0 available.
I0316 09:12:38.859918 483572 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 09:12:39.264408 483572 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:54,  1.75s/it]  6%|▋         | 2/32 [00:02<00:27,  1.09it/s]  9%|▉         | 3/32 [00:02<00:18,  1.53it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.89it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.16it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.65it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.78it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.81it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.84it/s] 41%|████      | 13/32 [00:05<00:06,  2.87it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.88it/s]I0316 09:12:46.566869 465058 finetune.py:76] layer 2_down @ epoch 4 new loss 2.4656592358951457e-06 old loss 2.4655323613842484e-06 WORSE
 47%|████▋     | 15/32 [00:06<00:05,  2.90it/s]W0316 09:12:47.115092 465058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 50%|█████     | 16/32 [00:06<00:05,  2.90it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.91it/s]2_down proxy err 0.0033008328173309565 tr(WHW.T) 1.2011618614196777
 56%|█████▋    | 18/32 [00:07<00:04,  2.93it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.93it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.94it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.95it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.94it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.94it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.95it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.95it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.97it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.96it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.95it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.95it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.96it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.96it/s]100%|██████████| 32/32 [00:12<00:00,  2.95it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
W0316 09:12:55.459000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:12:55.459000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:12:55.459000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:12:55.460000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:12:55.460000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:12:55.460000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:12:55.460000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:12:55.487000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:12:55.487000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:12:55.488000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:12:55.488000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:12:55.488000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:12:55.780000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:12:55.780000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:12:55.781000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:12:55.781000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:12:55.781000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:12:56.405000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:12:56.405000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:12:56.405000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:12:56.405000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:12:56.405000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:12:56.405000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:12:56.405000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:12:56.422000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:12:56.422000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:12:56.422000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:12:56.423000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:12:56.423000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:12:56.626000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:12:56.626000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:12:56.626000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:12:56.626000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:12:56.626000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:12:57.804000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:12:57.804000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:12:57.804000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:12:57.804000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:12:57.804000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:12:57.804000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:12:57.804000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:12:57.823000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:12:57.823000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:12:57.823000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:12:57.823000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:12:57.823000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:12:58.456000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:12:58.456000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:12:58.456000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:12:58.456000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:12:58.456000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 09:13:04.810658 483572 finetune.py:45] layer 4_v initial loss 3.7929687550786184e-06
W0316 09:13:04.810848 483572 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:13:05.780184 466629 finetune.py:68] layer 3_down @ epoch 3 new loss 5.246058663033182e-06 old loss 5.246205546427518e-06 BETTER
I0316 09:13:37.256730 466629 finetune.py:68] layer 3_down @ epoch 4 new loss 5.245640295470366e-06 old loss 5.246058663033182e-06 BETTER
W0316 09:13:38.096844 466629 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

3_down proxy err 0.003667144337669015 tr(WHW.T) 2.1171681880950928
I0316 09:13:40.891560 483572 finetune.py:68] layer 4_v @ epoch 0 new loss 9.927346127369674e-07 old loss 3.7929687550786184e-06 BETTER
I0316 09:13:55.312745 460347 quantize_finetune_llama.py:186] computed original embedding for layer 5 in 62.49453377723694s
I0316 09:13:55.730983 460347 quantize_finetune_llama.py:159] layer 6 gpu 2
I0316 09:13:57.792957 484904 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 09:13:57.793122 484904 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 09:13:57.793185 484904 utils.py:162] NumExpr defaulting to 16 threads.
I0316 09:13:58.124858 484904 config.py:58] PyTorch version 2.4.0 available.
I0316 09:14:00.451237 484904 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 09:14:00.914939 484904 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it]  6%|▋         | 2/32 [00:02<00:30,  1.01s/it]  9%|▉         | 3/32 [00:02<00:20,  1.42it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.76it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.04it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.26it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.53it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.62it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.68it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.71it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.75it/s] 41%|████      | 13/32 [00:06<00:06,  2.77it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.80it/s] 50%|█████     | 16/32 [00:07<00:05,  2.80it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.81it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.83it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.82it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.82it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.85it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.82it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.84it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.87it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.86it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.86it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.85it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.83it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.82it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
W0316 09:14:17.870000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:14:17.870000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:14:17.870000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:14:17.870000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:14:17.870000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:14:17.870000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:14:17.870000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:14:17.898000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:14:17.898000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:14:17.898000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:14:17.898000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:14:17.898000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:14:18.201000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:14:18.201000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:14:18.201000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:14:18.201000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:14:18.201000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
I0316 09:14:18.379376 483572 finetune.py:68] layer 4_v @ epoch 1 new loss 7.206651275737386e-07 old loss 9.927346127369674e-07 BETTER
W0316 09:14:18.831000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:14:18.831000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:14:18.831000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:14:18.831000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:14:18.831000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:14:18.831000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:14:18.831000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:14:18.849000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:14:18.849000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:14:18.849000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:14:18.849000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:14:18.849000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:14:19.049000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:14:19.049000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:14:19.049000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:14:19.049000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:14:19.049000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:14:20.211000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:14:20.211000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:14:20.211000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:14:20.211000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:14:20.211000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:14:20.211000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:14:20.211000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:14:20.229000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:14:20.230000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:14:20.230000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:14:20.230000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:14:20.230000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:14:20.858000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:14:20.858000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:14:20.858000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:14:20.858000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:14:20.858000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 09:14:27.080554 484904 finetune.py:45] layer 5_v initial loss 2.9857424124202225e-06
W0316 09:14:27.080823 484904 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:14:56.228395 483572 finetune.py:68] layer 4_v @ epoch 2 new loss 6.25202517312573e-07 old loss 7.206651275737386e-07 BETTER
I0316 09:14:57.312755 460347 quantize_finetune_llama.py:186] computed original embedding for layer 6 in 61.139272689819336s
I0316 09:14:57.719801 460347 quantize_finetune_llama.py:159] layer 7 gpu 3
I0316 09:14:59.766618 486049 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 09:14:59.766735 486049 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 09:14:59.766828 486049 utils.py:162] NumExpr defaulting to 16 threads.
I0316 09:14:59.967604 486049 config.py:58] PyTorch version 2.4.0 available.
I0316 09:15:01.476984 484904 finetune.py:68] layer 5_v @ epoch 0 new loss 8.524697818756977e-07 old loss 2.9857424124202225e-06 BETTER
I0316 09:15:02.168171 486049 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 09:15:02.557332 486049 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.95s/it]  6%|▋         | 2/32 [00:02<00:30,  1.02s/it]  9%|▉         | 3/32 [00:02<00:20,  1.40it/s] 12%|█▎        | 4/32 [00:03<00:16,  1.74it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.01it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.22it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.37it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.48it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.56it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.62it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.65it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.67it/s] 41%|████      | 13/32 [00:06<00:07,  2.71it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.75it/s] 47%|████▋     | 15/32 [00:07<00:06,  2.74it/s] 50%|█████     | 16/32 [00:07<00:05,  2.74it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.75it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.74it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.75it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.75it/s] 66%|██████▌   | 21/32 [00:09<00:03,  2.76it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.75it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.75it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.75it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.75it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.75it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.75it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.76it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.75it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.75it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.76it/s]100%|██████████| 32/32 [00:13<00:00,  2.76it/s]100%|██████████| 32/32 [00:13<00:00,  2.43it/s]
W0316 09:15:19.604000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:15:19.604000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:15:19.604000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:15:19.604000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:15:19.604000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:15:19.604000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:15:19.605000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:15:19.632000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:15:19.632000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:15:19.632000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:15:19.633000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:15:19.633000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:15:19.934000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:15:19.934000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:15:19.935000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:15:19.935000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:15:19.935000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:15:20.574000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:15:20.574000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:15:20.574000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:15:20.574000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:15:20.574000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:15:20.574000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:15:20.574000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:15:20.593000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:15:20.593000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:15:20.593000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:15:20.593000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:15:20.593000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:15:20.801000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:15:20.802000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:15:20.802000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:15:20.802000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:15:20.802000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:15:21.982000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:15:21.983000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:15:21.983000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:15:21.983000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:15:21.983000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:15:21.983000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:15:21.983000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:15:22.002000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:15:22.002000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:15:22.002000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:15:22.002000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:15:22.003000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:15:22.640000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:15:22.640000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:15:22.640000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:15:22.640000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:15:22.640000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 09:15:28.860410 486049 finetune.py:45] layer 6_v initial loss 2.669183004400111e-06
W0316 09:15:28.860688 486049 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:15:34.234213 483572 finetune.py:68] layer 4_v @ epoch 3 new loss 5.772864710706926e-07 old loss 6.25202517312573e-07 BETTER
I0316 09:15:36.880299 484904 finetune.py:68] layer 5_v @ epoch 1 new loss 6.638450713580824e-07 old loss 8.524697818756977e-07 BETTER
I0316 09:15:59.190865 460347 quantize_finetune_llama.py:186] computed original embedding for layer 7 in 60.96876645088196s
I0316 09:15:59.597398 460347 quantize_finetune_llama.py:159] layer 8 gpu 0
I0316 09:16:01.742689 487200 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 09:16:01.742807 487200 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 09:16:01.742870 487200 utils.py:162] NumExpr defaulting to 16 threads.
I0316 09:16:01.939341 487200 config.py:58] PyTorch version 2.4.0 available.
I0316 09:16:03.576185 486049 finetune.py:68] layer 6_v @ epoch 0 new loss 1.0529446399232256e-06 old loss 2.669183004400111e-06 BETTER
I0316 09:16:04.177079 487200 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 09:16:04.549864 487200 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.95s/it]  6%|▋         | 2/32 [00:02<00:30,  1.02s/it]  9%|▉         | 3/32 [00:02<00:20,  1.40it/s] 12%|█▎        | 4/32 [00:03<00:16,  1.74it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.01it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.22it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.37it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.57it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.63it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.67it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.71it/s] 41%|████      | 13/32 [00:06<00:06,  2.74it/s]I0316 09:16:12.062171 483572 finetune.py:68] layer 4_v @ epoch 4 new loss 5.486108989316563e-07 old loss 5.772864710706926e-07 BETTER
 44%|████▍     | 14/32 [00:06<00:06,  2.75it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.76it/s]I0316 09:16:12.807934 484904 finetune.py:68] layer 5_v @ epoch 2 new loss 5.966094249743037e-07 old loss 6.638450713580824e-07 BETTER
 50%|█████     | 16/32 [00:07<00:05,  2.76it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.78it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.78it/s]W0316 09:16:13.658663 483572 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 59%|█████▉    | 19/32 [00:08<00:04,  2.78it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.79it/s] 66%|██████▌   | 21/32 [00:09<00:03,  2.79it/s]4_v proxy err 0.002547520911321044 tr(WHW.T) 38.379119873046875
  0%|          | 0/32 [00:00<?, ?it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.81it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.80it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.79it/s]  3%|▎         | 1/32 [00:01<00:40,  1.32s/it] 78%|███████▊  | 25/32 [00:10<00:02,  2.80it/s]  6%|▋         | 2/32 [00:01<00:22,  1.31it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.80it/s]  9%|▉         | 3/32 [00:02<00:17,  1.70it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.80it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.80it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.98it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.80it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.19it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.80it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.33it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.80it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s]100%|██████████| 32/32 [00:13<00:00,  2.81it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.57it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.63it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.67it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.66it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s]W0316 09:16:21.470000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:16:21.471000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:16:21.471000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:16:21.471000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:16:21.471000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:16:21.471000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:16:21.471000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:16:21.498000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:16:21.498000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:16:21.498000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:16:21.499000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:16:21.499000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:06,  2.67it/s]W0316 09:16:21.794000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:16:21.794000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:16:21.794000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:16:21.794000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:16:21.794000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:07<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s]W0316 09:16:22.416000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:16:22.417000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:16:22.417000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:16:22.417000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:16:22.417000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:16:22.417000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:16:22.417000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:16:22.435000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:16:22.435000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:16:22.435000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:16:22.435000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:16:22.435000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:16:22.639000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:16:22.639000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:16:22.639000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:16:22.639000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:16:22.639000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 19/32 [00:08<00:04,  2.67it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s]W0316 09:16:23.807000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:16:23.807000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:16:23.807000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:16:23.807000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:16:23.807000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:16:23.807000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:16:23.807000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:16:23.826000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:16:23.826000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:16:23.826000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:16:23.826000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:16:23.826000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:09<00:03,  2.68it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s]W0316 09:16:24.463000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:16:24.463000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:16:24.463000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:16:24.463000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:16:24.463000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.69it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.66it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.66it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.68it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
I0316 09:16:30.836455 487200 finetune.py:45] layer 7_v initial loss 2.7163466711499495e-06
W0316 09:16:30.836669 487200 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0316 09:16:33.633000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:16:33.633000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:16:33.634000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:16:33.634000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:16:33.634000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:16:33.634000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:16:33.634000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:16:33.663000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:16:33.663000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:16:33.663000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:16:33.663000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:16:33.664000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:16:33.830000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:16:33.830000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:16:33.830000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:16:33.831000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:16:33.831000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:16:34.063000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:16:34.063000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:16:34.063000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:16:34.063000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:16:34.063000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:16:34.064000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:16:34.064000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:16:34.084000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:16:34.085000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:16:34.085000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:16:34.085000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:16:34.085000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:16:34.151000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:16:34.151000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:16:34.151000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:16:34.151000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:16:34.151000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:16:35.232000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:16:35.558000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:16:35.559000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:16:35.559000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:16:35.559000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:16:35.559000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:16:35.559000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:16:35.559000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:16:35.582000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:16:35.582000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:16:35.582000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:16:35.582000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:16:35.582000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:16:35.838000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:16:35.839000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:16:35.839000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:16:35.839000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:16:35.839000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:16:36.305000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 09:16:39.440493 486049 finetune.py:68] layer 6_v @ epoch 1 new loss 9.105082767746353e-07 old loss 1.0529446399232256e-06 BETTER
I0316 09:16:43.104783 483572 finetune.py:45] layer 4_q initial loss 9.297305609834439e-07
W0316 09:16:43.105181 483572 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:16:49.096625 484904 finetune.py:68] layer 5_v @ epoch 3 new loss 5.674540943800821e-07 old loss 5.966094249743037e-07 BETTER
I0316 09:17:04.968620 487200 finetune.py:68] layer 7_v @ epoch 0 new loss 1.2265144277989748e-06 old loss 2.7163466711499495e-06 BETTER
I0316 09:17:16.131769 486049 finetune.py:68] layer 6_v @ epoch 2 new loss 8.549520202905114e-07 old loss 9.105082767746353e-07 BETTER
I0316 09:17:19.676599 483572 finetune.py:68] layer 4_q @ epoch 0 new loss 8.898179544303275e-07 old loss 9.297305609834439e-07 BETTER
I0316 09:17:25.426958 484904 finetune.py:76] layer 5_v @ epoch 4 new loss 5.732310341954872e-07 old loss 5.674540943800821e-07 WORSE
W0316 09:17:26.594028 484904 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

5_v proxy err 0.002414981834590435 tr(WHW.T) 37.700050354003906
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.39s/it]  6%|▋         | 2/32 [00:01<00:24,  1.23it/s]  9%|▉         | 3/32 [00:02<00:18,  1.59it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.84it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.02it/s] 19%|█▉        | 6/32 [00:03<00:12,  2.13it/s] 22%|██▏       | 7/32 [00:03<00:11,  2.24it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.33it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.39it/s] 31%|███▏      | 10/32 [00:05<00:09,  2.44it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.47it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.49it/s] 41%|████      | 13/32 [00:06<00:07,  2.49it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.51it/s] 47%|████▋     | 15/32 [00:07<00:06,  2.49it/s] 50%|█████     | 16/32 [00:07<00:06,  2.50it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.51it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.52it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.54it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.55it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.52it/s] 69%|██████▉   | 22/32 [00:09<00:04,  2.49it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.51it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.51it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.52it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.50it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.51it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.50it/s]I0316 09:17:39.988698 487200 finetune.py:68] layer 7_v @ epoch 1 new loss 1.11722454221308e-06 old loss 1.2265144277989748e-06 BETTER
 91%|█████████ | 29/32 [00:12<00:01,  2.52it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.53it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.52it/s]100%|██████████| 32/32 [00:13<00:00,  2.53it/s]100%|██████████| 32/32 [00:13<00:00,  2.33it/s]
W0316 09:17:47.452000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.453000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.453000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.453000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.453000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.453000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.453000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.484000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.484000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.484000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.484000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.485000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.654000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.654000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.654000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.654000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.654000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.889000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.889000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.889000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.889000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.889000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.889000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.889000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.910000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.910000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.910000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.910000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.910000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.977000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.977000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.977000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.977000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:17:47.978000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:17:49.097000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:17:49.421000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:17:49.421000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:17:49.422000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:17:49.422000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:17:49.422000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:17:49.422000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:17:49.422000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:17:49.445000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:17:49.446000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:17:49.446000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:17:49.446000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:17:49.446000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:17:49.698000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:17:49.698000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:17:49.698000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:17:49.698000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:17:49.698000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:17:50.160000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 09:17:52.506280 486049 finetune.py:68] layer 6_v @ epoch 3 new loss 8.263750146397797e-07 old loss 8.549520202905114e-07 BETTER
I0316 09:17:56.839043 484904 finetune.py:45] layer 5_q initial loss 1.1281983915978344e-06
W0316 09:17:56.839376 484904 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:17:56.973694 483572 finetune.py:68] layer 4_q @ epoch 1 new loss 8.663587891533098e-07 old loss 8.898179544303275e-07 BETTER
I0316 09:18:15.617506 487200 finetune.py:68] layer 7_v @ epoch 2 new loss 1.1061348459406872e-06 old loss 1.11722454221308e-06 BETTER
I0316 09:18:29.021602 486049 finetune.py:68] layer 6_v @ epoch 4 new loss 8.153837711688539e-07 old loss 8.263750146397797e-07 BETTER
W0316 09:18:30.898312 486049 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 09:18:32.028649 484904 finetune.py:68] layer 5_q @ epoch 0 new loss 1.0789912039399496e-06 old loss 1.1281983915978344e-06 BETTER
6_v proxy err 0.002533421153202653 tr(WHW.T) 42.837894439697266
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.36s/it]  6%|▋         | 2/32 [00:01<00:23,  1.26it/s]  9%|▉         | 3/32 [00:02<00:17,  1.63it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.89it/s]I0316 09:18:34.841723 483572 finetune.py:68] layer 4_q @ epoch 2 new loss 8.485581020067912e-07 old loss 8.663587891533098e-07 BETTER
 16%|█▌        | 5/32 [00:02<00:13,  2.07it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.21it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.30it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.37it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.41it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.45it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.47it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.48it/s] 41%|████      | 13/32 [00:06<00:07,  2.49it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.48it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.49it/s] 50%|█████     | 16/32 [00:07<00:06,  2.49it/s] 53%|█████▎    | 17/32 [00:07<00:06,  2.49it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.50it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.50it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.50it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.51it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.51it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.51it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.51it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.50it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.48it/s] 84%|████████▍ | 27/32 [00:11<00:02,  2.49it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.49it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.50it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.50it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.51it/s]100%|██████████| 32/32 [00:13<00:00,  2.51it/s]100%|██████████| 32/32 [00:13<00:00,  2.33it/s]
I0316 09:18:51.177981 487200 finetune.py:68] layer 7_v @ epoch 3 new loss 1.0858111636480317e-06 old loss 1.1061348459406872e-06 BETTER
W0316 09:18:51.839000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:18:51.840000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:18:51.840000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:18:51.840000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:18:51.840000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:18:51.840000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:18:51.840000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:18:51.870000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:18:51.870000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:18:51.871000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:18:51.871000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:18:51.871000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.041000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.041000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.041000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.041000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.041000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.269000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.269000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.269000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.269000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.269000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.269000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.269000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.290000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.290000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.290000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.290000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.290000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.357000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.357000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.357000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.358000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:18:52.358000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:18:53.432000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:18:53.753000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:18:53.753000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:18:53.753000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:18:53.754000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:18:53.754000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:18:53.754000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:18:53.754000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:18:53.777000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:18:53.777000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:18:53.777000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:18:53.777000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:18:53.777000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:18:54.029000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:18:54.029000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:18:54.029000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:18:54.029000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:18:54.029000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:18:54.497000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 09:19:01.213193 486049 finetune.py:45] layer 6_q initial loss 1.5911678019620012e-06
W0316 09:19:01.213533 486049 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:19:08.203243 484904 finetune.py:68] layer 5_q @ epoch 1 new loss 1.0467898619026528e-06 old loss 1.0789912039399496e-06 BETTER
I0316 09:19:12.762351 483572 finetune.py:68] layer 4_q @ epoch 3 new loss 8.340100521309068e-07 old loss 8.485581020067912e-07 BETTER
I0316 09:19:26.922540 487200 finetune.py:76] layer 7_v @ epoch 4 new loss 1.2278104577490012e-06 old loss 1.0858111636480317e-06 WORSE
W0316 09:19:28.137586 487200 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

7_v proxy err 0.0021123685874044895 tr(WHW.T) 53.14668655395508
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.36s/it]  6%|▋         | 2/32 [00:01<00:23,  1.26it/s]  9%|▉         | 3/32 [00:02<00:17,  1.64it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.90it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.08it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.22it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.30it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.38it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.43it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.47it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.50it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.51it/s] 41%|████      | 13/32 [00:06<00:07,  2.53it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.54it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.54it/s] 50%|█████     | 16/32 [00:07<00:06,  2.54it/s]I0316 09:19:36.671519 486049 finetune.py:68] layer 6_q @ epoch 0 new loss 1.5228039274006733e-06 old loss 1.5911678019620012e-06 BETTER
 53%|█████▎    | 17/32 [00:07<00:05,  2.54it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.55it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.54it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.55it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.55it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.57it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.56it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.56it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.56it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.56it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.56it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.54it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.55it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.54it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.55it/s]100%|██████████| 32/32 [00:13<00:00,  2.55it/s]100%|██████████| 32/32 [00:13<00:00,  2.37it/s]
I0316 09:19:44.267433 484904 finetune.py:68] layer 5_q @ epoch 2 new loss 1.0306348485755734e-06 old loss 1.0467898619026528e-06 BETTER
W0316 09:19:48.931000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:19:48.932000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:19:48.932000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:19:48.932000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:19:48.932000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:19:48.932000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:19:48.932000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:19:48.963000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:19:48.963000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:19:48.963000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:19:48.963000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:19:48.963000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.132000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.133000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.133000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.133000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.133000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.364000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.364000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.365000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.365000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.365000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.365000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.365000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.387000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.387000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.387000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.387000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.387000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.453000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.453000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.453000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.453000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:19:49.453000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
I0316 09:19:50.472117 483572 finetune.py:68] layer 4_q @ epoch 4 new loss 8.221119855988945e-07 old loss 8.340100521309068e-07 BETTER
W0316 09:19:50.543000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:19:50.865000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:19:50.866000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:19:50.866000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:19:50.866000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:19:50.866000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:19:50.866000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:19:50.866000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:19:50.890000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:19:50.890000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:19:50.890000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:19:50.890000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:19:50.890000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:19:51.145000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:19:51.145000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:19:51.145000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:19:51.145000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:19:51.145000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:19:51.612000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:19:52.045493 483572 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_q proxy err 0.0002518074179533869 tr(WHW.T) 6740.84375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:38,  1.26s/it]  6%|▋         | 2/32 [00:01<00:22,  1.36it/s]  9%|▉         | 3/32 [00:01<00:16,  1.77it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.05it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.25it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.39it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.59it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s]I0316 09:19:58.151264 487200 finetune.py:45] layer 7_q initial loss 2.138293666575919e-06
W0316 09:19:58.151684 487200 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:05<00:07,  2.67it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.70it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.71it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.72it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.72it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.71it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.70it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.70it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.72it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.72it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.73it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
I0316 09:20:12.861478 483572 finetune.py:45] layer 4_k initial loss 1.2214353546369239e-06
W0316 09:20:12.861856 483572 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:20:12.922303 486049 finetune.py:68] layer 6_q @ epoch 1 new loss 1.497022481089516e-06 old loss 1.5228039274006733e-06 BETTER
I0316 09:20:20.729862 484904 finetune.py:68] layer 5_q @ epoch 3 new loss 1.0128864005309879e-06 old loss 1.0306348485755734e-06 BETTER
I0316 09:20:32.859939 487200 finetune.py:68] layer 7_q @ epoch 0 new loss 2.0833028884226223e-06 old loss 2.138293666575919e-06 BETTER
I0316 09:20:49.327983 486049 finetune.py:68] layer 6_q @ epoch 2 new loss 1.4740146525582531e-06 old loss 1.497022481089516e-06 BETTER
I0316 09:20:49.693660 483572 finetune.py:68] layer 4_k @ epoch 0 new loss 1.0948855333481333e-06 old loss 1.2214353546369239e-06 BETTER
I0316 09:20:57.286421 484904 finetune.py:68] layer 5_q @ epoch 4 new loss 9.984146345232148e-07 old loss 1.0128864005309879e-06 BETTER
W0316 09:20:58.877627 484904 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

5_q proxy err 0.00035007987753488123 tr(WHW.T) 6497.6005859375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.26s/it]  6%|▋         | 2/32 [00:01<00:22,  1.34it/s]  9%|▉         | 3/32 [00:02<00:16,  1.72it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.99it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.18it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.31it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.42it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.48it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.53it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.58it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s] 41%|████      | 13/32 [00:05<00:07,  2.58it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.61it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.63it/s]I0316 09:21:08.344265 487200 finetune.py:68] layer 7_q @ epoch 1 new loss 2.0570707874867367e-06 old loss 2.0833028884226223e-06 BETTER
 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.63it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.62it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:13<00:00,  2.63it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
I0316 09:21:19.865080 484904 finetune.py:45] layer 5_k initial loss 1.4584837799702655e-06
W0316 09:21:19.865462 484904 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:21:26.035722 486049 finetune.py:68] layer 6_q @ epoch 3 new loss 1.47138268857816e-06 old loss 1.4740146525582531e-06 BETTER
I0316 09:21:27.362075 483572 finetune.py:68] layer 4_k @ epoch 1 new loss 1.0802322094605188e-06 old loss 1.0948855333481333e-06 BETTER
I0316 09:21:44.031989 487200 finetune.py:68] layer 7_q @ epoch 2 new loss 1.989190650419914e-06 old loss 2.0570707874867367e-06 BETTER
I0316 09:21:55.698557 484904 finetune.py:68] layer 5_k @ epoch 0 new loss 1.3174126252124552e-06 old loss 1.4584837799702655e-06 BETTER
I0316 09:22:02.761747 486049 finetune.py:68] layer 6_q @ epoch 4 new loss 1.4639784922110266e-06 old loss 1.47138268857816e-06 BETTER
W0316 09:22:04.658818 486049 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 09:22:05.807864 483572 finetune.py:68] layer 4_k @ epoch 2 new loss 1.0693114518289804e-06 old loss 1.0802322094605188e-06 BETTER
6_q proxy err 0.0003952838305849582 tr(WHW.T) 6020.662109375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:40,  1.29s/it]  6%|▋         | 2/32 [00:01<00:22,  1.31it/s]  9%|▉         | 3/32 [00:02<00:17,  1.69it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.96it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.14it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.27it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.35it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.41it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.43it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.47it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.50it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.52it/s] 41%|████      | 13/32 [00:05<00:07,  2.53it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.54it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.53it/s] 50%|█████     | 16/32 [00:07<00:06,  2.52it/s] 53%|█████▎    | 17/32 [00:07<00:06,  2.50it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.49it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.48it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.46it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.47it/s] 69%|██████▉   | 22/32 [00:09<00:04,  2.49it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.50it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.51it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.53it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.53it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.54it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.54it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.54it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.53it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.51it/s]100%|██████████| 32/32 [00:13<00:00,  2.52it/s]100%|██████████| 32/32 [00:13<00:00,  2.36it/s]
I0316 09:22:19.979909 487200 finetune.py:76] layer 7_q @ epoch 3 new loss 2.0232209863024764e-06 old loss 1.989190650419914e-06 WORSE
I0316 09:22:26.409784 486049 finetune.py:45] layer 6_k initial loss 2.086472477458301e-06
W0316 09:22:26.410208 486049 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:22:32.117469 484904 finetune.py:68] layer 5_k @ epoch 1 new loss 1.3006244898861041e-06 old loss 1.3174126252124552e-06 BETTER
I0316 09:22:44.097969 483572 finetune.py:68] layer 4_k @ epoch 3 new loss 1.0608358707031584e-06 old loss 1.0693114518289804e-06 BETTER
I0316 09:22:55.373570 487200 finetune.py:76] layer 7_q @ epoch 4 new loss 2.0619766019081e-06 old loss 1.989190650419914e-06 WORSE
W0316 09:22:56.651845 487200 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

7_q proxy err 0.00038183783181011677 tr(WHW.T) 6037.65966796875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:40,  1.32s/it]  6%|▋         | 2/32 [00:01<00:23,  1.29it/s]  9%|▉         | 3/32 [00:02<00:17,  1.67it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.93it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.11it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.22it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.31it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.37it/s]I0316 09:23:02.076625 486049 finetune.py:68] layer 6_k @ epoch 0 new loss 1.8918332216344425e-06 old loss 2.086472477458301e-06 BETTER
 28%|██▊       | 9/32 [00:04<00:09,  2.44it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.47it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.50it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.52it/s] 41%|████      | 13/32 [00:06<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.56it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.56it/s] 50%|█████     | 16/32 [00:07<00:06,  2.56it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.56it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.57it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.56it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.57it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.57it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.59it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.60it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.61it/s]I0316 09:23:08.771804 484904 finetune.py:68] layer 5_k @ epoch 2 new loss 1.2899103012387059e-06 old loss 1.3006244898861041e-06 BETTER
 81%|████████▏ | 26/32 [00:11<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.60it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.58it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:13<00:00,  2.58it/s]100%|██████████| 32/32 [00:13<00:00,  2.39it/s]
I0316 09:23:18.722222 487200 finetune.py:45] layer 7_k initial loss 2.728387244133046e-06
W0316 09:23:18.722680 487200 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:23:21.960560 483572 finetune.py:68] layer 4_k @ epoch 4 new loss 1.0532486385272932e-06 old loss 1.0608358707031584e-06 BETTER
W0316 09:23:23.632541 483572 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_k proxy err 0.0002504689327906817 tr(WHW.T) 3940.978759765625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.21it/s]  6%|▋         | 2/32 [00:01<00:16,  1.78it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.27it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.47it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.64it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.65it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.66it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.67it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.67it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.67it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.66it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.64it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.65it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.66it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
I0316 09:23:38.744353 486049 finetune.py:68] layer 6_k @ epoch 1 new loss 1.8673197246243944e-06 old loss 1.8918332216344425e-06 BETTER
I0316 09:23:44.925174 483572 finetune.py:45] layer 4_o initial loss 1.7527784166304627e-06
W0316 09:23:44.925511 483572 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:23:45.232207 484904 finetune.py:68] layer 5_k @ epoch 3 new loss 1.2826195643356186e-06 old loss 1.2899103012387059e-06 BETTER
I0316 09:23:53.812664 487200 finetune.py:68] layer 7_k @ epoch 0 new loss 2.5928679860953707e-06 old loss 2.728387244133046e-06 BETTER
I0316 09:24:15.635420 486049 finetune.py:68] layer 6_k @ epoch 2 new loss 1.849704403866781e-06 old loss 1.8673197246243944e-06 BETTER
I0316 09:24:21.685719 483572 finetune.py:68] layer 4_o @ epoch 0 new loss 1.7201278978973278e-06 old loss 1.7527784166304627e-06 BETTER
I0316 09:24:22.005419 484904 finetune.py:68] layer 5_k @ epoch 4 new loss 1.2734731171804015e-06 old loss 1.2826195643356186e-06 BETTER
W0316 09:24:23.802735 484904 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

5_k proxy err 0.00032390185515396297 tr(WHW.T) 4151.1123046875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.18it/s]  6%|▋         | 2/32 [00:01<00:17,  1.72it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.20it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.40it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.45it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.49it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.50it/s]I0316 09:24:29.523867 487200 finetune.py:76] layer 7_k @ epoch 1 new loss 2.612419166325708e-06 old loss 2.5928679860953707e-06 WORSE
 34%|███▍      | 11/32 [00:04<00:08,  2.52it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.50it/s] 41%|████      | 13/32 [00:05<00:07,  2.53it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.54it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.56it/s] 50%|█████     | 16/32 [00:06<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.57it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.56it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.56it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.55it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.54it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.53it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.52it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.53it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.54it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.55it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.56it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.56it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.57it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.56it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.56it/s]100%|██████████| 32/32 [00:13<00:00,  2.51it/s]100%|██████████| 32/32 [00:13<00:00,  2.46it/s]
I0316 09:24:45.523221 484904 finetune.py:45] layer 5_o initial loss 2.0699574179161573e-06
W0316 09:24:45.523690 484904 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:24:52.047377 486049 finetune.py:68] layer 6_k @ epoch 3 new loss 1.84265286407026e-06 old loss 1.849704403866781e-06 BETTER
I0316 09:24:59.854734 483572 finetune.py:68] layer 4_o @ epoch 1 new loss 1.7061945527530042e-06 old loss 1.7201278978973278e-06 BETTER
I0316 09:25:04.555801 487200 finetune.py:76] layer 7_k @ epoch 2 new loss 2.5967831334128277e-06 old loss 2.5928679860953707e-06 WORSE
I0316 09:25:21.028106 484904 finetune.py:68] layer 5_o @ epoch 0 new loss 2.0361237602628535e-06 old loss 2.0699574179161573e-06 BETTER
I0316 09:25:28.933494 486049 finetune.py:68] layer 6_k @ epoch 4 new loss 1.8351021253693034e-06 old loss 1.84265286407026e-06 BETTER
W0316 09:25:30.563274 486049 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_k proxy err 0.00030534632969647646 tr(WHW.T) 4417.54150390625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.19it/s]  6%|▋         | 2/32 [00:01<00:17,  1.72it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.31it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.37it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.42it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.45it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.47it/s] 31%|███▏      | 10/32 [00:04<00:09,  2.43it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.43it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.46it/s] 41%|████      | 13/32 [00:05<00:07,  2.48it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.50it/s]I0316 09:25:37.898617 483572 finetune.py:68] layer 4_o @ epoch 2 new loss 1.6951752286331612e-06 old loss 1.7061945527530042e-06 BETTER
 47%|████▋     | 15/32 [00:06<00:06,  2.52it/s] 50%|█████     | 16/32 [00:06<00:06,  2.52it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.53it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.52it/s]I0316 09:25:39.773468 487200 finetune.py:68] layer 7_k @ epoch 3 new loss 2.5730039396876236e-06 old loss 2.5928679860953707e-06 BETTER
 59%|█████▉    | 19/32 [00:08<00:05,  2.53it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.53it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.53it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.52it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.51it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.52it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.52it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.53it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.53it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.50it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.51it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.51it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.51it/s]100%|██████████| 32/32 [00:13<00:00,  2.51it/s]100%|██████████| 32/32 [00:13<00:00,  2.43it/s]
I0316 09:25:52.439589 486049 finetune.py:45] layer 6_o initial loss 3.210092700101086e-06
W0316 09:25:52.440042 486049 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:25:57.759774 484904 finetune.py:68] layer 5_o @ epoch 1 new loss 2.021204636548646e-06 old loss 2.0361237602628535e-06 BETTER
I0316 09:26:16.194295 483572 finetune.py:68] layer 4_o @ epoch 3 new loss 1.6867235217432608e-06 old loss 1.6951752286331612e-06 BETTER
I0316 09:26:16.315921 487200 finetune.py:76] layer 7_k @ epoch 4 new loss 2.7463827336760005e-06 old loss 2.5730039396876236e-06 WORSE
W0316 09:26:17.552172 487200 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

7_k proxy err 0.0003150350821670145 tr(WHW.T) 4611.390625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.21it/s]  6%|▋         | 2/32 [00:01<00:17,  1.76it/s]  9%|▉         | 3/32 [00:01<00:14,  2.05it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.39it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.47it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.50it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.52it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.53it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.54it/s] 41%|████      | 13/32 [00:05<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.55it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.55it/s] 50%|█████     | 16/32 [00:06<00:06,  2.54it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.54it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.52it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.53it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.53it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.54it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.55it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.55it/s]I0316 09:26:28.386644 486049 finetune.py:68] layer 6_o @ epoch 0 new loss 3.155862032144796e-06 old loss 3.210092700101086e-06 BETTER
 75%|███████▌  | 24/32 [00:09<00:03,  2.56it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.57it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.56it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.55it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.55it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.52it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.53it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.54it/s]100%|██████████| 32/32 [00:13<00:00,  2.54it/s]100%|██████████| 32/32 [00:13<00:00,  2.46it/s]
I0316 09:26:34.506754 484904 finetune.py:68] layer 5_o @ epoch 2 new loss 2.0086222320969682e-06 old loss 2.021204636548646e-06 BETTER
I0316 09:26:39.215058 487200 finetune.py:45] layer 7_o initial loss 4.441230430529686e-06
W0316 09:26:39.215391 487200 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:26:54.216408 483572 finetune.py:68] layer 4_o @ epoch 4 new loss 1.6787589629529975e-06 old loss 1.6867235217432608e-06 BETTER
W0316 09:26:55.827994 483572 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_o proxy err 0.0021115655545145273 tr(WHW.T) 1.3682918548583984
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:02,  2.02s/it]  6%|▋         | 2/32 [00:03<00:50,  1.70s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.55s/it]I0316 09:27:05.043015 486049 finetune.py:68] layer 6_o @ epoch 1 new loss 3.1259364732250106e-06 old loss 3.155862032144796e-06 BETTER
 16%|█▌        | 5/32 [00:07<00:41,  1.52s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.50s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it]I0316 09:27:11.387827 484904 finetune.py:68] layer 5_o @ epoch 3 new loss 1.9979818262072513e-06 old loss 2.0086222320969682e-06 BETTER
 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it]I0316 09:27:14.812304 487200 finetune.py:68] layer 7_o @ epoch 0 new loss 4.355402779765427e-06 old loss 4.441230430529686e-06 BETTER
 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.48s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.48s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.52s/it]I0316 09:27:41.535053 486049 finetune.py:68] layer 6_o @ epoch 2 new loss 3.105592440988403e-06 old loss 3.1259364732250106e-06 BETTER
 94%|█████████▍| 30/32 [00:44<00:03,  1.51s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it]100%|██████████| 32/32 [00:47<00:00,  1.50s/it]100%|██████████| 32/32 [00:47<00:00,  1.50s/it]
I0316 09:27:48.270590 484904 finetune.py:68] layer 5_o @ epoch 4 new loss 1.9894109755114187e-06 old loss 1.9979818262072513e-06 BETTER
W0316 09:27:50.088959 484904 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 09:27:50.862547 487200 finetune.py:68] layer 7_o @ epoch 1 new loss 4.2806777855730616e-06 old loss 4.355402779765427e-06 BETTER
5_o proxy err 0.002030034549534321 tr(WHW.T) 1.8142595291137695
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:03,  2.05s/it]I0316 09:27:54.208586 483572 finetune.py:45] layer 4_up initial loss 3.966286385548301e-06
W0316 09:27:54.208956 483572 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:03<00:52,  1.76s/it]  9%|▉         | 3/32 [00:05<00:48,  1.66s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.61s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.54s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:22<00:27,  1.54s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.55s/it] 50%|█████     | 16/32 [00:25<00:24,  1.55s/it]I0316 09:28:17.902116 486049 finetune.py:68] layer 6_o @ epoch 3 new loss 3.0854769192956155e-06 old loss 3.105592440988403e-06 BETTER
 53%|█████▎    | 17/32 [00:26<00:23,  1.55s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.55s/it] 59%|█████▉    | 19/32 [00:29<00:20,  1.55s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.55s/it] 66%|██████▌   | 21/32 [00:32<00:17,  1.55s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.55s/it]I0316 09:28:26.512790 487200 finetune.py:68] layer 7_o @ epoch 2 new loss 4.241954229655676e-06 old loss 4.2806777855730616e-06 BETTER
 72%|███████▏  | 23/32 [00:35<00:13,  1.54s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.54s/it]I0316 09:28:30.028364 483572 finetune.py:68] layer 4_up @ epoch 0 new loss 3.933017069357447e-06 old loss 3.966286385548301e-06 BETTER
 78%|███████▊  | 25/32 [00:39<00:10,  1.54s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.54s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.54s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.54s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.56s/it]
I0316 09:28:49.956978 484904 finetune.py:45] layer 5_up initial loss 5.361438525142148e-06
W0316 09:28:49.957723 484904 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:28:54.329172 486049 finetune.py:68] layer 6_o @ epoch 4 new loss 3.068532578254235e-06 old loss 3.0854769192956155e-06 BETTER
W0316 09:28:55.963598 486049 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_o proxy err 0.002658930839970708 tr(WHW.T) 2.602741241455078
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:05,  2.11s/it]  6%|▋         | 2/32 [00:03<00:53,  1.79s/it]I0316 09:29:02.242867 487200 finetune.py:68] layer 7_o @ epoch 3 new loss 4.2109372770937625e-06 old loss 4.241954229655676e-06 BETTER
  9%|▉         | 3/32 [00:05<00:48,  1.69s/it] 12%|█▎        | 4/32 [00:06<00:46,  1.65s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.62s/it]I0316 09:29:06.477459 483572 finetune.py:68] layer 4_up @ epoch 1 new loss 3.913100954378024e-06 old loss 3.933017069357447e-06 BETTER
 19%|█▉        | 6/32 [00:09<00:41,  1.60s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.59s/it] 25%|██▌       | 8/32 [00:13<00:38,  1.58s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.58s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.58s/it] 34%|███▍      | 11/32 [00:17<00:33,  1.58s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.58s/it] 41%|████      | 13/32 [00:20<00:29,  1.57s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.57s/it] 47%|████▋     | 15/32 [00:24<00:26,  1.57s/it] 50%|█████     | 16/32 [00:25<00:25,  1.57s/it]I0316 09:29:24.130849 484904 finetune.py:68] layer 5_up @ epoch 0 new loss 5.300551038089907e-06 old loss 5.361438525142148e-06 BETTER
 53%|█████▎    | 17/32 [00:27<00:23,  1.56s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.57s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.57s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.56s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.57s/it] 69%|██████▉   | 22/32 [00:35<00:15,  1.57s/it] 72%|███████▏  | 23/32 [00:36<00:14,  1.57s/it] 75%|███████▌  | 24/32 [00:38<00:12,  1.58s/it] 78%|███████▊  | 25/32 [00:39<00:11,  1.57s/it]I0316 09:29:37.912523 487200 finetune.py:68] layer 7_o @ epoch 4 new loss 4.1872181100188754e-06 old loss 4.2109372770937625e-06 BETTER
 81%|████████▏ | 26/32 [00:41<00:09,  1.57s/it]W0316 09:29:39.449141 487200 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:42<00:07,  1.58s/it]7_o proxy err 0.0024997435975819826 tr(WHW.T) 3.79241943359375
  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:44<00:06,  1.57s/it]  3%|▎         | 1/32 [00:02<01:04,  2.07s/it]I0316 09:29:42.940517 483572 finetune.py:68] layer 4_up @ epoch 2 new loss 3.895140253007412e-06 old loss 3.913100954378024e-06 BETTER
 91%|█████████ | 29/32 [00:46<00:04,  1.57s/it]  6%|▋         | 2/32 [00:03<00:52,  1.76s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.57s/it]  9%|▉         | 3/32 [00:05<00:48,  1.66s/it] 97%|█████████▋| 31/32 [00:49<00:01,  1.56s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.62s/it]100%|██████████| 32/32 [00:50<00:00,  1.56s/it]100%|██████████| 32/32 [00:50<00:00,  1.59s/it]
 16%|█▌        | 5/32 [00:08<00:42,  1.59s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.57s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.56s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.56s/it]I0316 09:29:56.515213 486049 finetune.py:45] layer 6_up initial loss 7.361224106716691e-06
W0316 09:29:56.515537 486049 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:16<00:34,  1.56s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.56s/it]I0316 09:29:59.466705 484904 finetune.py:68] layer 5_up @ epoch 1 new loss 5.262994818622246e-06 old loss 5.300551038089907e-06 BETTER
 38%|███▊      | 12/32 [00:19<00:31,  1.55s/it] 41%|████      | 13/32 [00:20<00:29,  1.55s/it] 44%|████▍     | 14/32 [00:22<00:27,  1.55s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.55s/it] 50%|█████     | 16/32 [00:25<00:24,  1.56s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.55s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.55s/it] 59%|█████▉    | 19/32 [00:29<00:20,  1.55s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.54s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.55s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.55s/it] 72%|███████▏  | 23/32 [00:36<00:13,  1.55s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.56s/it]I0316 09:30:19.735548 483572 finetune.py:68] layer 4_up @ epoch 3 new loss 3.878901679854607e-06 old loss 3.895140253007412e-06 BETTER
 78%|███████▊  | 25/32 [00:39<00:10,  1.56s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.55s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.55s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.55s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.54s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.55s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.55s/it]100%|██████████| 32/32 [00:50<00:00,  1.55s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]
I0316 09:30:31.295702 486049 finetune.py:68] layer 6_up @ epoch 0 new loss 7.269543857546523e-06 old loss 7.361224106716691e-06 BETTER
I0316 09:30:34.901915 484904 finetune.py:68] layer 5_up @ epoch 2 new loss 5.230340320849791e-06 old loss 5.262994818622246e-06 BETTER
I0316 09:30:39.877412 487200 finetune.py:45] layer 7_up initial loss 8.983090083347633e-06
W0316 09:30:39.877781 487200 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:30:56.294449 483572 finetune.py:68] layer 4_up @ epoch 4 new loss 3.862424819089938e-06 old loss 3.878901679854607e-06 BETTER
W0316 09:30:57.657839 483572 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_up proxy err 0.00334274978376925 tr(WHW.T) 400.40167236328125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.55s/it]I0316 09:31:06.267441 486049 finetune.py:68] layer 6_up @ epoch 1 new loss 7.207671842479613e-06 old loss 7.269543857546523e-06 BETTER
 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it]I0316 09:31:09.871065 484904 finetune.py:68] layer 5_up @ epoch 3 new loss 5.199110546527663e-06 old loss 5.230340320849791e-06 BETTER
 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it]I0316 09:31:13.175783 487200 finetune.py:68] layer 7_up @ epoch 0 new loss 8.86936322785914e-06 old loss 8.983090083347633e-06 BETTER
 31%|███▏      | 10/32 [00:15<00:32,  1.48s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.48s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.48s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.48s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.48s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.48s/it]I0316 09:31:41.401027 486049 finetune.py:68] layer 6_up @ epoch 2 new loss 7.14997440809384e-06 old loss 7.207671842479613e-06 BETTER
 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it]I0316 09:31:44.963848 484904 finetune.py:68] layer 5_up @ epoch 4 new loss 5.1698184506676625e-06 old loss 5.199110546527663e-06 BETTER
 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it]W0316 09:31:46.355292 484904 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
I0316 09:31:47.357676 487200 finetune.py:68] layer 7_up @ epoch 1 new loss 8.788266313786153e-06 old loss 8.86936322785914e-06 BETTER
5_up proxy err 0.003268145490437746 tr(WHW.T) 497.5318298339844
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:02,  2.01s/it]  6%|▋         | 2/32 [00:03<00:52,  1.75s/it]  9%|▉         | 3/32 [00:05<00:48,  1.66s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.62s/it]I0316 09:31:55.288912 483572 finetune.py:45] layer 4_gate initial loss 5.279843662719941e-06
W0316 09:31:55.289401 483572 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:08<00:43,  1.60s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.59s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.58s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.57s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.57s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.57s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.56s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.56s/it] 41%|████      | 13/32 [00:20<00:29,  1.56s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.56s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.56s/it] 50%|█████     | 16/32 [00:25<00:24,  1.56s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.56s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.56s/it]I0316 09:32:16.433403 486049 finetune.py:68] layer 6_up @ epoch 3 new loss 7.098567039065529e-06 old loss 7.14997440809384e-06 BETTER
 59%|█████▉    | 19/32 [00:30<00:20,  1.56s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.56s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.56s/it]I0316 09:32:21.356287 487200 finetune.py:68] layer 7_up @ epoch 2 new loss 8.717240234545898e-06 old loss 8.788266313786153e-06 BETTER
 69%|██████▉   | 22/32 [00:34<00:15,  1.56s/it] 72%|███████▏  | 23/32 [00:36<00:13,  1.55s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.56s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.56s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.55s/it]I0316 09:32:30.038104 483572 finetune.py:68] layer 4_gate @ epoch 0 new loss 5.250225967756705e-06 old loss 5.279843662719941e-06 BETTER
 84%|████████▍ | 27/32 [00:42<00:08,  1.61s/it] 88%|████████▊ | 28/32 [00:44<00:06,  1.59s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.58s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.58s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.57s/it]100%|██████████| 32/32 [00:50<00:00,  1.56s/it]100%|██████████| 32/32 [00:50<00:00,  1.58s/it]
I0316 09:32:46.575090 484904 finetune.py:45] layer 5_gate initial loss 7.210937383206328e-06
W0316 09:32:46.575534 484904 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:32:51.585093 486049 finetune.py:68] layer 6_up @ epoch 4 new loss 7.0511287049157545e-06 old loss 7.098567039065529e-06 BETTER
W0316 09:32:53.113204 486049 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_up proxy err 0.003088749013841152 tr(WHW.T) 571.84619140625
  0%|          | 0/32 [00:00<?, ?it/s]I0316 09:32:55.493922 487200 finetune.py:68] layer 7_up @ epoch 3 new loss 8.653034456074238e-06 old loss 8.717240234545898e-06 BETTER
  3%|▎         | 1/32 [00:02<01:02,  2.03s/it]  6%|▋         | 2/32 [00:03<00:52,  1.76s/it]  9%|▉         | 3/32 [00:05<00:48,  1.68s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.64s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.61s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.61s/it]I0316 09:33:05.244496 483572 finetune.py:68] layer 4_gate @ epoch 1 new loss 5.234653599472949e-06 old loss 5.250225967756705e-06 BETTER
 22%|██▏       | 7/32 [00:11<00:39,  1.59s/it] 25%|██▌       | 8/32 [00:13<00:38,  1.59s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.58s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.58s/it] 34%|███▍      | 11/32 [00:17<00:33,  1.58s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.59s/it] 41%|████      | 13/32 [00:20<00:30,  1.58s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.58s/it] 47%|████▋     | 15/32 [00:24<00:26,  1.59s/it]I0316 09:33:19.115203 484904 finetune.py:68] layer 5_gate @ epoch 0 new loss 7.1578624556423165e-06 old loss 7.210937383206328e-06 BETTER
 50%|█████     | 16/32 [00:25<00:25,  1.58s/it] 53%|█████▎    | 17/32 [00:27<00:23,  1.58s/it] 56%|█████▋    | 18/32 [00:28<00:22,  1.58s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.58s/it] 62%|██████▎   | 20/32 [00:32<00:19,  1.58s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.58s/it] 69%|██████▉   | 22/32 [00:35<00:15,  1.58s/it]I0316 09:33:29.957637 487200 finetune.py:68] layer 7_up @ epoch 4 new loss 8.593341590312775e-06 old loss 8.653034456074238e-06 BETTER
W0316 09:33:31.322570 487200 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:36<00:14,  1.58s/it]7_up proxy err 0.0028787474147975445 tr(WHW.T) 651.4107666015625
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:38<00:12,  1.58s/it] 78%|███████▊  | 25/32 [00:39<00:11,  1.57s/it]  3%|▎         | 1/32 [00:01<01:01,  1.99s/it] 81%|████████▏ | 26/32 [00:41<00:09,  1.58s/it]  6%|▋         | 2/32 [00:03<00:52,  1.74s/it] 84%|████████▍ | 27/32 [00:43<00:07,  1.57s/it]  9%|▉         | 3/32 [00:05<00:47,  1.66s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.62s/it] 88%|████████▊ | 28/32 [00:44<00:06,  1.57s/it]I0316 09:33:40.573944 483572 finetune.py:68] layer 4_gate @ epoch 2 new loss 5.220005732553545e-06 old loss 5.234653599472949e-06 BETTER
 16%|█▌        | 5/32 [00:08<00:43,  1.60s/it] 91%|█████████ | 29/32 [00:46<00:04,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.58s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.58s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.58s/it] 97%|█████████▋| 31/32 [00:49<00:01,  1.58s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.57s/it]100%|██████████| 32/32 [00:50<00:00,  1.58s/it]100%|██████████| 32/32 [00:50<00:00,  1.59s/it]
 28%|██▊       | 9/32 [00:14<00:36,  1.57s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.57s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.56s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.56s/it]I0316 09:33:52.659659 484904 finetune.py:68] layer 5_gate @ epoch 1 new loss 7.127923709049355e-06 old loss 7.1578624556423165e-06 BETTER
 41%|████      | 13/32 [00:20<00:29,  1.56s/it]I0316 09:33:53.997583 486049 finetune.py:45] layer 6_gate initial loss 9.522294021735433e-06
W0316 09:33:53.997801 486049 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 44%|████▍     | 14/32 [00:22<00:28,  1.56s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.56s/it] 50%|█████     | 16/32 [00:25<00:24,  1.56s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.56s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.56s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.56s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.56s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.56s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.56s/it] 72%|███████▏  | 23/32 [00:36<00:14,  1.56s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.56s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.56s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.56s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.56s/it]I0316 09:34:15.826946 483572 finetune.py:68] layer 4_gate @ epoch 3 new loss 5.205825800658204e-06 old loss 5.220005732553545e-06 BETTER
 88%|████████▊ | 28/32 [00:44<00:06,  1.57s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.56s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.56s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.56s/it]100%|██████████| 32/32 [00:50<00:00,  1.56s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]
I0316 09:34:26.180282 484904 finetune.py:68] layer 5_gate @ epoch 2 new loss 7.100290986272739e-06 old loss 7.127923709049355e-06 BETTER
I0316 09:34:27.018992 486049 finetune.py:68] layer 6_gate @ epoch 0 new loss 9.44981184147764e-06 old loss 9.522294021735433e-06 BETTER
I0316 09:34:31.933810 487200 finetune.py:45] layer 7_gate initial loss 1.151584365288727e-05
W0316 09:34:31.934297 487200 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:34:50.967930 483572 finetune.py:68] layer 4_gate @ epoch 4 new loss 5.192544449528214e-06 old loss 5.205825800658204e-06 BETTER
W0316 09:34:52.259962 483572 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_gate proxy err 0.0015062559396028519 tr(WHW.T) 1579.2774658203125
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:40,  1.10it/s]  2%|▏         | 2/112 [00:01<01:05,  1.68it/s]  3%|▎         | 3/112 [00:01<00:53,  2.02it/s]  4%|▎         | 4/112 [00:02<00:48,  2.23it/s]  4%|▍         | 5/112 [00:02<00:45,  2.36it/s]  5%|▌         | 6/112 [00:02<00:43,  2.45it/s]  6%|▋         | 7/112 [00:03<00:41,  2.51it/s]  7%|▋         | 8/112 [00:03<00:40,  2.56it/s]  8%|▊         | 9/112 [00:03<00:39,  2.58it/s]I0316 09:35:00.067631 484904 finetune.py:68] layer 5_gate @ epoch 3 new loss 7.075305802572984e-06 old loss 7.100290986272739e-06 BETTER
  9%|▉         | 10/112 [00:04<00:39,  2.61it/s] 10%|▉         | 11/112 [00:04<00:38,  2.62it/s] 11%|█         | 12/112 [00:05<00:37,  2.63it/s]I0316 09:35:01.093062 486049 finetune.py:68] layer 6_gate @ epoch 1 new loss 9.402489013154991e-06 old loss 9.44981184147764e-06 BETTER
 12%|█▏        | 13/112 [00:05<00:37,  2.65it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.66it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.66it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.65it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.66it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.66it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.65it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.66it/s]I0316 09:35:04.336652 487200 finetune.py:68] layer 7_gate @ epoch 0 new loss 1.1430822269176133e-05 old loss 1.151584365288727e-05 BETTER
 19%|█▉        | 21/112 [00:08<00:34,  2.66it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.65it/s] 21%|██        | 23/112 [00:09<00:33,  2.66it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.67it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.67it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.68it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.68it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.68it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.68it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.68it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.68it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.68it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.67it/s] 30%|███       | 34/112 [00:13<00:29,  2.66it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.67it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.65it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.66it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.67it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.68it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.68it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.68it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.69it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.69it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.68it/s] 40%|████      | 45/112 [00:17<00:25,  2.68it/s] 41%|████      | 46/112 [00:17<00:24,  2.67it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.66it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.65it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.66it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.65it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.66it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.67it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.69it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.69it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.70it/s] 50%|█████     | 56/112 [00:21<00:20,  2.70it/s] 51%|█████     | 57/112 [00:21<00:20,  2.71it/s] 52%|█████▏    | 58/112 [00:22<00:19,  2.71it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.70it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.70it/s] 54%|█████▍    | 61/112 [00:23<00:18,  2.69it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.69it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.68it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.66it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.67it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.67it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.67it/s] 61%|██████    | 68/112 [00:25<00:16,  2.68it/s] 62%|██████▏   | 69/112 [00:26<00:15,  2.69it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.70it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.69it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.69it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.69it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.69it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.69it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.68it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.66it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.67it/s] 71%|███████   | 79/112 [00:30<00:12,  2.67it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.68it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.69it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.69it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.70it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.69it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.69it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.65it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.62it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.62it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.60it/s] 80%|████████  | 90/112 [00:34<00:08,  2.59it/s] 81%|████████▏ | 91/112 [00:34<00:08,  2.57it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.58it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.58it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.58it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.59it/s] 86%|████████▌ | 96/112 [00:36<00:06,  2.60it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.60it/s]I0316 09:35:32.999242 484904 finetune.py:68] layer 5_gate @ epoch 4 new loss 7.050045951473294e-06 old loss 7.075305802572984e-06 BETTER
 88%|████████▊ | 98/112 [00:37<00:05,  2.60it/s] 88%|████████▊ | 99/112 [00:37<00:05,  2.59it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.59it/s]I0316 09:35:34.314052 486049 finetune.py:68] layer 6_gate @ epoch 2 new loss 9.358667739434168e-06 old loss 9.402489013154991e-06 BETTER
 90%|█████████ | 101/112 [00:38<00:04,  2.58it/s]W0316 09:35:34.549695 484904 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 102/112 [00:38<00:03,  2.59it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.62it/s] 93%|█████████▎| 104/112 [00:39<00:03,  2.65it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.65it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.67it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.68it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.68it/s]I0316 09:35:37.110250 487200 finetune.py:68] layer 7_gate @ epoch 1 new loss 1.1370789252396207e-05 old loss 1.1430822269176133e-05 BETTER
 97%|█████████▋| 109/112 [00:41<00:01,  2.69it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.69it/s]5_gate proxy err 0.0014433433534577489 tr(WHW.T) 1974.525634765625
  0%|          | 0/112 [00:00<?, ?it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.69it/s]100%|██████████| 112/112 [00:42<00:00,  2.69it/s]100%|██████████| 112/112 [00:42<00:00,  2.63it/s]
  1%|          | 1/112 [00:00<01:37,  1.14it/s]  2%|▏         | 2/112 [00:01<01:05,  1.69it/s]  3%|▎         | 3/112 [00:01<00:54,  1.99it/s]  4%|▎         | 4/112 [00:02<00:49,  2.18it/s]  4%|▍         | 5/112 [00:02<00:46,  2.29it/s]  5%|▌         | 6/112 [00:02<00:44,  2.37it/s]  6%|▋         | 7/112 [00:03<00:43,  2.42it/s]  7%|▋         | 8/112 [00:03<00:42,  2.47it/s]  8%|▊         | 9/112 [00:04<00:41,  2.50it/s]  9%|▉         | 10/112 [00:04<00:40,  2.53it/s] 10%|▉         | 11/112 [00:04<00:39,  2.56it/s] 11%|█         | 12/112 [00:05<00:38,  2.56it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.57it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.57it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.56it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.56it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.56it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.56it/s]W0316 09:35:45.721000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:35:45.722000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:35:45.722000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:35:45.722000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:35:45.722000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:35:45.722000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:35:45.722000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:35:45.766000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:35:45.766000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:35:45.766000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:35:45.766000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:35:45.766000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:35:45.944000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:35:45.944000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:35:45.944000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:35:45.944000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:35:45.944000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 17%|█▋        | 19/112 [00:07<00:36,  2.56it/s]W0316 09:35:46.265000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:35:46.265000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:35:46.266000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:35:46.266000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:35:46.266000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:35:46.266000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:35:46.266000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:35:46.296000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:35:46.296000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:35:46.296000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:35:46.297000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:35:46.297000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:35:46.368000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:35:46.368000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:35:46.368000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:35:46.368000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:35:46.368000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 18%|█▊        | 20/112 [00:08<00:36,  2.55it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.55it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.55it/s] 21%|██        | 23/112 [00:09<00:34,  2.56it/s]W0316 09:35:47.725000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 21%|██▏       | 24/112 [00:09<00:34,  2.56it/s]W0316 09:35:48.187000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:35:48.187000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:35:48.187000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:35:48.187000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:35:48.187000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:35:48.188000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:35:48.188000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:35:48.220000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:35:48.220000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:35:48.220000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:35:48.220000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:35:48.220000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 25/112 [00:10<00:33,  2.56it/s]W0316 09:35:48.603000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:35:48.604000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:35:48.604000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:35:48.604000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:35:48.604000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 23%|██▎       | 26/112 [00:10<00:33,  2.57it/s]W0316 09:35:49.147000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:35:49.153000 140220953036608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 24%|██▍       | 27/112 [00:11<00:33,  2.57it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.56it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.56it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.56it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.55it/s] 29%|██▊       | 32/112 [00:12<00:31,  2.54it/s] 29%|██▉       | 33/112 [00:13<00:31,  2.54it/s] 30%|███       | 34/112 [00:13<00:30,  2.54it/s] 31%|███▏      | 35/112 [00:14<00:30,  2.54it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.56it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.57it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.57it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.58it/s] 36%|███▌      | 40/112 [00:16<00:27,  2.57it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.57it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.57it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.56it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.56it/s] 40%|████      | 45/112 [00:18<00:26,  2.56it/s]I0316 09:35:56.394494 483572 finetune.py:45] layer 4_down initial loss 8.290322512038983e-06
W0316 09:35:56.394726 483572 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 46/112 [00:18<00:25,  2.54it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.55it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.55it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.56it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.57it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.57it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.57it/s] 47%|████▋     | 53/112 [00:21<00:22,  2.57it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.57it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.56it/s] 50%|█████     | 56/112 [00:22<00:21,  2.56it/s] 51%|█████     | 57/112 [00:22<00:21,  2.56it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.55it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.56it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.54it/s] 54%|█████▍    | 61/112 [00:24<00:20,  2.55it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.55it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.56it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.56it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.57it/s] 59%|█████▉    | 66/112 [00:26<00:17,  2.58it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.58it/s] 61%|██████    | 68/112 [00:27<00:17,  2.58it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.57it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.56it/s] 63%|██████▎   | 71/112 [00:28<00:15,  2.57it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.56it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.55it/s]I0316 09:36:07.447352 486049 finetune.py:68] layer 6_gate @ epoch 3 new loss 9.31871727516409e-06 old loss 9.358667739434168e-06 BETTER
 66%|██████▌   | 74/112 [00:29<00:14,  2.56it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.56it/s] 68%|██████▊   | 76/112 [00:30<00:13,  2.57it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.58it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.58it/s] 71%|███████   | 79/112 [00:31<00:12,  2.58it/s]I0316 09:36:09.710566 487200 finetune.py:68] layer 7_gate @ epoch 2 new loss 1.1321459169266745e-05 old loss 1.1370789252396207e-05 BETTER
 71%|███████▏  | 80/112 [00:31<00:12,  2.58it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.57it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.57it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.57it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.58it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.58it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.56it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.57it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.56it/s] 79%|███████▉  | 89/112 [00:35<00:08,  2.58it/s] 80%|████████  | 90/112 [00:35<00:08,  2.58it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.58it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.59it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.58it/s] 84%|████████▍ | 94/112 [00:37<00:06,  2.58it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.58it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.57it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.58it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.58it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.58it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.57it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.57it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.57it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.57it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.57it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.58it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.59it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.58it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.58it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.57it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.58it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.58it/s]100%|██████████| 112/112 [00:44<00:00,  2.57it/s]100%|██████████| 112/112 [00:44<00:00,  2.54it/s]
I0316 09:36:28.939405 483572 finetune.py:68] layer 4_down @ epoch 0 new loss 8.287064702017233e-06 old loss 8.290322512038983e-06 BETTER
W0316 09:36:29.795000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:36:29.795000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:36:29.795000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:36:29.796000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:36:29.796000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:36:29.796000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:36:29.796000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:36:29.841000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:36:29.841000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:36:29.841000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:36:29.841000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:36:29.841000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.023000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.023000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.023000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.023000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.023000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.350000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.350000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.350000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.350000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.350000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.350000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.350000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.388000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.388000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.388000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.388000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.388000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.458000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.458000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.458000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.459000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:36:30.459000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:36:31.827000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:36:32.315000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:36:32.315000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:36:32.315000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:36:32.315000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:36:32.315000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:36:32.315000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:36:32.316000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:36:32.350000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:36:32.350000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:36:32.350000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:36:32.350000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:36:32.350000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:36:32.745000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:36:32.745000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:36:32.745000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:36:32.745000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:36:32.745000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:36:33.308000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:36:33.314000 139766737102656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0316 09:36:40.778649 484904 finetune.py:45] layer 5_down initial loss 1.1396985428291373e-05
W0316 09:36:40.779180 484904 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:36:40.779443 486049 finetune.py:68] layer 6_gate @ epoch 4 new loss 9.279296136810444e-06 old loss 9.31871727516409e-06 BETTER
W0316 09:36:41.965545 486049 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 09:36:42.823079 487200 finetune.py:68] layer 7_gate @ epoch 3 new loss 1.126856204791693e-05 old loss 1.1321459169266745e-05 BETTER
6_gate proxy err 0.0012141388142481446 tr(WHW.T) 2579.375
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:39,  1.11it/s]  2%|▏         | 2/112 [00:01<01:07,  1.64it/s]  3%|▎         | 3/112 [00:01<00:56,  1.94it/s]  4%|▎         | 4/112 [00:02<00:51,  2.12it/s]  4%|▍         | 5/112 [00:02<00:47,  2.23it/s]  5%|▌         | 6/112 [00:02<00:45,  2.33it/s]  6%|▋         | 7/112 [00:03<00:44,  2.38it/s]  7%|▋         | 8/112 [00:03<00:42,  2.42it/s]  8%|▊         | 9/112 [00:04<00:42,  2.44it/s]  9%|▉         | 10/112 [00:04<00:41,  2.47it/s] 10%|▉         | 11/112 [00:04<00:40,  2.48it/s] 11%|█         | 12/112 [00:05<00:39,  2.51it/s] 12%|█▏        | 13/112 [00:05<00:39,  2.52it/s] 12%|█▎        | 14/112 [00:06<00:38,  2.53it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.55it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.55it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.55it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.54it/s] 17%|█▋        | 19/112 [00:08<00:36,  2.54it/s] 18%|█▊        | 20/112 [00:08<00:36,  2.54it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.53it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.53it/s] 21%|██        | 23/112 [00:09<00:35,  2.53it/s] 21%|██▏       | 24/112 [00:10<00:34,  2.54it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.54it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.55it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.55it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.56it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.56it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.56it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.56it/s] 29%|██▊       | 32/112 [00:13<00:31,  2.55it/s] 29%|██▉       | 33/112 [00:13<00:31,  2.55it/s] 30%|███       | 34/112 [00:13<00:30,  2.55it/s] 31%|███▏      | 35/112 [00:14<00:30,  2.54it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.54it/s] 33%|███▎      | 37/112 [00:15<00:29,  2.53it/s] 34%|███▍      | 38/112 [00:15<00:29,  2.53it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.54it/s]I0316 09:37:02.101529 483572 finetune.py:68] layer 4_down @ epoch 1 new loss 8.28695556265302e-06 old loss 8.287064702017233e-06 BETTER
 36%|███▌      | 40/112 [00:16<00:28,  2.54it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.56it/s] 38%|███▊      | 42/112 [00:17<00:27,  2.56it/s] 38%|███▊      | 43/112 [00:17<00:27,  2.55it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.55it/s] 40%|████      | 45/112 [00:18<00:26,  2.55it/s] 41%|████      | 46/112 [00:18<00:25,  2.55it/s] 42%|████▏     | 47/112 [00:19<00:25,  2.55it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.54it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.53it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.51it/s] 46%|████▌     | 51/112 [00:20<00:24,  2.51it/s] 46%|████▋     | 52/112 [00:21<00:23,  2.51it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.51it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.52it/s] 49%|████▉     | 55/112 [00:22<00:22,  2.53it/s] 50%|█████     | 56/112 [00:22<00:22,  2.53it/s] 51%|█████     | 57/112 [00:23<00:21,  2.54it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.55it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.55it/s] 54%|█████▎    | 60/112 [00:24<00:20,  2.55it/s] 54%|█████▍    | 61/112 [00:24<00:20,  2.55it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.54it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.54it/s] 57%|█████▋    | 64/112 [00:25<00:19,  2.52it/s] 58%|█████▊    | 65/112 [00:26<00:18,  2.52it/s]I0316 09:37:12.276207 484904 finetune.py:68] layer 5_down @ epoch 0 new loss 1.1392450687708333e-05 old loss 1.1396985428291373e-05 BETTER
 59%|█████▉    | 66/112 [00:26<00:18,  2.50it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.51it/s] 61%|██████    | 68/112 [00:27<00:17,  2.51it/s] 62%|██████▏   | 69/112 [00:27<00:17,  2.51it/s] 62%|██████▎   | 70/112 [00:28<00:16,  2.52it/s] 63%|██████▎   | 71/112 [00:28<00:16,  2.52it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.53it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.54it/s]I0316 09:37:15.602742 487200 finetune.py:68] layer 7_gate @ epoch 4 new loss 1.1223212823097128e-05 old loss 1.126856204791693e-05 BETTER
 66%|██████▌   | 74/112 [00:29<00:14,  2.54it/s] 67%|██████▋   | 75/112 [00:30<00:14,  2.53it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.53it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.53it/s]W0316 09:37:16.840123 487200 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 70%|██████▉   | 78/112 [00:31<00:13,  2.53it/s] 71%|███████   | 79/112 [00:31<00:13,  2.53it/s] 71%|███████▏  | 80/112 [00:32<00:12,  2.53it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.52it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.52it/s] 74%|███████▍  | 83/112 [00:33<00:11,  2.53it/s] 75%|███████▌  | 84/112 [00:33<00:11,  2.53it/s] 76%|███████▌  | 85/112 [00:34<00:10,  2.53it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.54it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.54it/s] 79%|███████▊  | 88/112 [00:35<00:09,  2.55it/s] 79%|███████▉  | 89/112 [00:35<00:09,  2.54it/s]7_gate proxy err 0.0011832956224679947 tr(WHW.T) 2633.853515625
  0%|          | 0/112 [00:00<?, ?it/s] 80%|████████  | 90/112 [00:36<00:08,  2.53it/s] 81%|████████▏ | 91/112 [00:36<00:08,  2.53it/s]  1%|          | 1/112 [00:00<01:45,  1.06it/s] 82%|████████▏ | 92/112 [00:36<00:08,  2.49it/s]  2%|▏         | 2/112 [00:01<01:08,  1.60it/s] 83%|████████▎ | 93/112 [00:37<00:07,  2.51it/s]  3%|▎         | 3/112 [00:01<00:56,  1.93it/s] 84%|████████▍ | 94/112 [00:37<00:07,  2.52it/s]  4%|▎         | 4/112 [00:02<00:50,  2.14it/s] 85%|████████▍ | 95/112 [00:38<00:06,  2.53it/s]  4%|▍         | 5/112 [00:02<00:47,  2.27it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.54it/s]  5%|▌         | 6/112 [00:02<00:44,  2.37it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.54it/s]  6%|▋         | 7/112 [00:03<00:43,  2.44it/s] 88%|████████▊ | 98/112 [00:39<00:05,  2.55it/s]  7%|▋         | 8/112 [00:03<00:41,  2.49it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.55it/s]  8%|▊         | 9/112 [00:04<00:40,  2.52it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.55it/s]  9%|▉         | 10/112 [00:04<00:40,  2.53it/s] 90%|█████████ | 101/112 [00:40<00:04,  2.55it/s] 10%|▉         | 11/112 [00:04<00:39,  2.54it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.55it/s] 11%|█         | 12/112 [00:05<00:39,  2.54it/s] 92%|█████████▏| 103/112 [00:41<00:03,  2.55it/s] 12%|█▏        | 13/112 [00:05<00:39,  2.54it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.50it/s] 12%|█▎        | 14/112 [00:06<00:38,  2.53it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.51it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.53it/s] 95%|█████████▍| 106/112 [00:42<00:02,  2.52it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.53it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.53it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.54it/s] 96%|█████████▋| 108/112 [00:43<00:01,  2.53it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.55it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.54it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.55it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.54it/s] 18%|█▊        | 20/112 [00:08<00:36,  2.55it/s] 99%|█████████▉| 111/112 [00:44<00:00,  2.55it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.56it/s]100%|██████████| 112/112 [00:44<00:00,  2.55it/s]100%|██████████| 112/112 [00:44<00:00,  2.50it/s]
 20%|█▉        | 22/112 [00:09<00:35,  2.56it/s] 21%|██        | 23/112 [00:09<00:34,  2.56it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.56it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.55it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.56it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.56it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.56it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.56it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.56it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.56it/s] 29%|██▊       | 32/112 [00:13<00:31,  2.56it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.56it/s]I0316 09:37:35.244746 483572 finetune.py:76] layer 4_down @ epoch 2 new loss 8.287360287795309e-06 old loss 8.28695556265302e-06 WORSE
 30%|███       | 34/112 [00:13<00:30,  2.56it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.57it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.58it/s] 33%|███▎      | 37/112 [00:15<00:29,  2.56it/s] 34%|███▍      | 38/112 [00:15<00:29,  2.52it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.53it/s] 36%|███▌      | 40/112 [00:16<00:28,  2.54it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.55it/s]W0316 09:37:38.395000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.395000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.395000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.396000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.396000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.396000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.396000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.441000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.441000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.441000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.441000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.441000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.626000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.626000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.626000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.626000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.626000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 42/112 [00:16<00:27,  2.55it/s]W0316 09:37:38.944000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.944000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.944000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.944000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.944000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.944000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.944000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.981000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.981000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.981000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.981000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:37:38.981000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:37:39.052000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:37:39.052000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:37:39.053000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:37:39.053000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:37:39.053000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 43/112 [00:17<00:27,  2.55it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.54it/s] 40%|████      | 45/112 [00:18<00:26,  2.53it/s] 41%|████      | 46/112 [00:18<00:26,  2.53it/s]W0316 09:37:40.422000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 42%|████▏     | 47/112 [00:18<00:25,  2.53it/s]W0316 09:37:40.887000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:37:40.887000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:37:40.887000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:37:40.887000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:37:40.888000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:37:40.888000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:37:40.888000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:37:40.918000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:37:40.919000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:37:40.919000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:37:40.919000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:37:40.919000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 43%|████▎     | 48/112 [00:19<00:25,  2.53it/s]W0316 09:37:41.303000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:37:41.303000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:37:41.303000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:37:41.303000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:37:41.303000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 49/112 [00:19<00:25,  2.52it/s]W0316 09:37:41.856000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:37:41.861000 139985063348032 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 45%|████▍     | 50/112 [00:20<00:24,  2.53it/s] 46%|████▌     | 51/112 [00:20<00:24,  2.53it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.54it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.54it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.55it/s] 49%|████▉     | 55/112 [00:22<00:22,  2.55it/s]I0316 09:37:44.089362 484904 finetune.py:68] layer 5_down @ epoch 1 new loss 1.1391580301278736e-05 old loss 1.1392450687708333e-05 BETTER
 50%|█████     | 56/112 [00:22<00:21,  2.55it/s] 51%|█████     | 57/112 [00:22<00:21,  2.55it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.55it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.54it/s] 54%|█████▎    | 60/112 [00:24<00:20,  2.54it/s] 54%|█████▍    | 61/112 [00:24<00:20,  2.53it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.52it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.53it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.54it/s] 58%|█████▊    | 65/112 [00:26<00:21,  2.22it/s] 59%|█████▉    | 66/112 [00:26<00:19,  2.34it/s] 60%|█████▉    | 67/112 [00:26<00:18,  2.45it/s] 61%|██████    | 68/112 [00:27<00:17,  2.53it/s]I0316 09:37:49.208434 486049 finetune.py:45] layer 6_down initial loss 1.4545170415658504e-05
W0316 09:37:49.208850 486049 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▏   | 69/112 [00:27<00:16,  2.59it/s] 62%|██████▎   | 70/112 [00:28<00:16,  2.62it/s] 63%|██████▎   | 71/112 [00:28<00:15,  2.65it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.67it/s] 65%|██████▌   | 73/112 [00:29<00:14,  2.66it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.67it/s] 67%|██████▋   | 75/112 [00:29<00:13,  2.70it/s] 68%|██████▊   | 76/112 [00:30<00:13,  2.72it/s] 69%|██████▉   | 77/112 [00:30<00:12,  2.73it/s] 70%|██████▉   | 78/112 [00:31<00:12,  2.75it/s] 71%|███████   | 79/112 [00:31<00:11,  2.76it/s] 71%|███████▏  | 80/112 [00:31<00:11,  2.77it/s] 72%|███████▏  | 81/112 [00:32<00:11,  2.76it/s] 73%|███████▎  | 82/112 [00:32<00:10,  2.75it/s] 74%|███████▍  | 83/112 [00:32<00:10,  2.74it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.73it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.67it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.70it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.72it/s] 79%|███████▊  | 88/112 [00:34<00:08,  2.73it/s] 79%|███████▉  | 89/112 [00:35<00:08,  2.74it/s] 80%|████████  | 90/112 [00:35<00:07,  2.75it/s] 81%|████████▏ | 91/112 [00:35<00:07,  2.76it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.76it/s] 83%|████████▎ | 93/112 [00:36<00:06,  2.76it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.76it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.76it/s] 86%|████████▌ | 96/112 [00:37<00:05,  2.76it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.76it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.75it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.73it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.72it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.71it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.72it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.72it/s] 93%|█████████▎| 104/112 [00:40<00:02,  2.73it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.74it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.75it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.75it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.76it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.77it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.77it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.77it/s]100%|██████████| 112/112 [00:43<00:00,  2.77it/s]100%|██████████| 112/112 [00:43<00:00,  2.58it/s]
I0316 09:38:08.392665 483572 finetune.py:68] layer 4_down @ epoch 3 new loss 8.285978765343316e-06 old loss 8.28695556265302e-06 BETTER
W0316 09:38:13.395000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.396000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.396000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.396000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.396000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.396000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.396000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.437000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.437000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.437000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.437000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.438000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.619000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.619000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.620000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.620000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.620000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.948000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.948000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.948000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.949000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.949000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.949000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.949000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.986000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.986000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.986000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.986000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:38:13.986000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:38:14.058000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:38:14.059000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:38:14.059000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:38:14.059000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:38:14.059000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:38:15.435000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
I0316 09:38:15.864292 484904 finetune.py:68] layer 5_down @ epoch 2 new loss 1.1391046427888796e-05 old loss 1.1391580301278736e-05 BETTER
W0316 09:38:15.907000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:38:15.908000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:38:15.908000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:38:15.908000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:38:15.908000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:38:15.908000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:38:15.908000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:38:15.941000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:38:15.941000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:38:15.941000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:38:15.941000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:38:15.941000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:38:16.332000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:38:16.333000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:38:16.333000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:38:16.333000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:38:16.333000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:38:16.905000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:38:16.911000 140356595238720 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0316 09:38:20.964817 486049 finetune.py:68] layer 6_down @ epoch 0 new loss 1.4539617041009478e-05 old loss 1.4545170415658504e-05 BETTER
I0316 09:38:24.900435 487200 finetune.py:45] layer 7_down initial loss 1.7145444871857762e-05
W0316 09:38:24.900836 487200 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:38:41.784143 483572 finetune.py:68] layer 4_down @ epoch 4 new loss 8.285561307275202e-06 old loss 8.285978765343316e-06 BETTER
W0316 09:38:42.668429 483572 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

4_down proxy err 0.0037914153654128313 tr(WHW.T) 3.4217827320098877
I0316 09:38:47.404138 484904 finetune.py:68] layer 5_down @ epoch 3 new loss 1.139040705311345e-05 old loss 1.1391046427888796e-05 BETTER
I0316 09:38:53.261814 486049 finetune.py:68] layer 6_down @ epoch 1 new loss 1.453923869121354e-05 old loss 1.4539617041009478e-05 BETTER
I0316 09:38:55.646439 487200 finetune.py:68] layer 7_down @ epoch 0 new loss 1.7139322153525427e-05 old loss 1.7145444871857762e-05 BETTER
I0316 09:39:19.692566 484904 finetune.py:68] layer 5_down @ epoch 4 new loss 1.139036157837836e-05 old loss 1.139040705311345e-05 BETTER
W0316 09:39:20.610959 484904 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

5_down proxy err 0.003721156856045127 tr(WHW.T) 4.912694931030273
I0316 09:39:24.961163 486049 finetune.py:76] layer 6_down @ epoch 2 new loss 1.4539431504090317e-05 old loss 1.453923869121354e-05 WORSE
I0316 09:39:27.208682 487200 finetune.py:68] layer 7_down @ epoch 1 new loss 1.7137417671619914e-05 old loss 1.7139322153525427e-05 BETTER
I0316 09:39:56.560989 486049 finetune.py:68] layer 6_down @ epoch 3 new loss 1.453683216823265e-05 old loss 1.453923869121354e-05 BETTER
I0316 09:39:58.846077 487200 finetune.py:68] layer 7_down @ epoch 2 new loss 1.713629717414733e-05 old loss 1.7137417671619914e-05 BETTER
I0316 09:40:28.404000 486049 finetune.py:76] layer 6_down @ epoch 4 new loss 1.4537936294800602e-05 old loss 1.453683216823265e-05 WORSE
W0316 09:40:29.009836 486049 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

6_down proxy err 0.0035639614798128605 tr(WHW.T) 6.232579231262207
I0316 09:40:29.747591 460347 quantize_finetune_llama.py:186] computed original embedding for layer 8 in 64.5092556476593s
I0316 09:40:30.161617 460347 quantize_finetune_llama.py:159] layer 9 gpu 1
I0316 09:40:30.195654 487200 finetune.py:68] layer 7_down @ epoch 3 new loss 1.7135002053692006e-05 old loss 1.713629717414733e-05 BETTER
I0316 09:40:32.206022 497511 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 09:40:32.206130 497511 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 09:40:32.206190 497511 utils.py:162] NumExpr defaulting to 16 threads.
I0316 09:40:32.417814 497511 config.py:58] PyTorch version 2.4.0 available.
I0316 09:40:34.657214 497511 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 09:40:35.098689 497511 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:54,  1.77s/it]  6%|▋         | 2/32 [00:02<00:27,  1.07it/s]  9%|▉         | 3/32 [00:02<00:19,  1.51it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.87it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.16it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.40it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.66it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.75it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.82it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.83it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.87it/s] 41%|████      | 13/32 [00:05<00:06,  2.89it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.90it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.93it/s] 50%|█████     | 16/32 [00:06<00:05,  2.94it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.96it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.95it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.94it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.94it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.97it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.97it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.98it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.98it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.99it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.97it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.98it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.98it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.97it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.95it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.96it/s]100%|██████████| 32/32 [00:12<00:00,  2.97it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
W0316 09:40:51.275000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:40:51.275000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:40:51.275000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:40:51.276000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:40:51.276000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:40:51.276000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:40:51.276000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:40:51.306000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:40:51.306000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:40:51.306000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:40:51.306000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:40:51.306000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:40:51.610000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:40:51.610000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:40:51.610000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:40:51.610000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:40:51.610000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:40:52.253000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:40:52.253000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:40:52.254000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:40:52.254000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:40:52.254000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:40:52.254000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:40:52.254000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:40:52.271000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:40:52.271000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:40:52.271000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:40:52.272000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:40:52.272000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:40:52.477000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:40:52.478000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:40:52.478000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:40:52.478000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:40:52.478000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:40:53.656000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:40:53.656000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:40:53.656000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:40:53.656000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:40:53.657000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:40:53.657000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:40:53.657000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:40:53.677000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:40:53.677000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:40:53.677000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:40:53.677000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:40:53.677000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:40:54.333000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:40:54.333000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:40:54.333000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:40:54.333000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:40:54.333000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 09:41:01.010677 497511 finetune.py:45] layer 8_v initial loss 2.567137471487513e-06
W0316 09:41:01.010990 497511 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:41:01.396588 487200 finetune.py:76] layer 7_down @ epoch 4 new loss 1.7136757378466427e-05 old loss 1.7135002053692006e-05 WORSE
W0316 09:41:01.960018 487200 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

7_down proxy err 0.0036712680011987686 tr(WHW.T) 6.808709144592285
I0316 09:41:35.591304 460347 quantize_finetune_llama.py:186] computed original embedding for layer 9 in 61.93757152557373s
I0316 09:41:35.959136 460347 quantize_finetune_llama.py:159] layer 10 gpu 2
I0316 09:41:37.021256 497511 finetune.py:68] layer 8_v @ epoch 0 new loss 1.4964591628086055e-06 old loss 2.567137471487513e-06 BETTER
I0316 09:41:37.989310 498015 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 09:41:37.989426 498015 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 09:41:37.989491 498015 utils.py:162] NumExpr defaulting to 16 threads.
I0316 09:41:38.182689 498015 config.py:58] PyTorch version 2.4.0 available.
I0316 09:41:40.469166 498015 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 09:41:40.939882 498015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.99s/it]  6%|▋         | 2/32 [00:02<00:30,  1.03s/it]  9%|▉         | 3/32 [00:02<00:20,  1.40it/s] 12%|█▎        | 4/32 [00:03<00:15,  1.75it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.04it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.27it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.66it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.73it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.78it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.82it/s] 41%|████      | 13/32 [00:06<00:06,  2.86it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.87it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.87it/s] 50%|█████     | 16/32 [00:07<00:05,  2.87it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.86it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.87it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.90it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.91it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.90it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.88it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.86it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.87it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.88it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.90it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.91it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.91it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.88it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
W0316 09:41:57.455000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:41:57.455000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:41:57.456000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:41:57.456000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:41:57.456000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:41:57.456000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:41:57.456000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:41:57.484000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:41:57.484000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:41:57.484000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:41:57.484000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:41:57.484000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:41:57.780000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:41:57.780000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:41:57.780000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:41:57.780000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:41:57.780000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:41:58.393000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:41:58.393000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:41:58.393000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:41:58.393000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:41:58.393000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:41:58.393000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:41:58.393000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:41:58.412000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:41:58.412000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:41:58.412000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:41:58.412000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:41:58.412000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:41:58.620000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:41:58.621000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:41:58.621000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:41:58.621000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:41:58.621000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:41:59.800000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:41:59.800000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:41:59.800000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:41:59.800000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:41:59.800000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:41:59.800000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:41:59.800000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:41:59.818000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:41:59.818000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:41:59.819000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:41:59.819000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:41:59.819000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:42:00.448000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:42:00.448000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:42:00.448000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:42:00.449000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:42:00.449000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 09:42:07.128977 498015 finetune.py:45] layer 9_v initial loss 4.274263119441457e-06
W0316 09:42:07.129350 498015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:42:14.219254 497511 finetune.py:68] layer 8_v @ epoch 1 new loss 1.3814366184305982e-06 old loss 1.4964591628086055e-06 BETTER
I0316 09:42:37.765616 460347 quantize_finetune_llama.py:186] computed original embedding for layer 10 in 61.36832618713379s
I0316 09:42:38.180601 460347 quantize_finetune_llama.py:159] layer 11 gpu 3
I0316 09:42:40.239100 498521 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 09:42:40.239233 498521 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 09:42:40.239301 498521 utils.py:162] NumExpr defaulting to 16 threads.
I0316 09:42:40.462822 498521 config.py:58] PyTorch version 2.4.0 available.
I0316 09:42:41.851203 498015 finetune.py:68] layer 9_v @ epoch 0 new loss 2.148994099115953e-06 old loss 4.274263119441457e-06 BETTER
I0316 09:42:42.741274 498521 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 09:42:43.127180 498521 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it]  6%|▋         | 2/32 [00:02<00:30,  1.01s/it]  9%|▉         | 3/32 [00:02<00:20,  1.42it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.79it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.09it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.28it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.39it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.51it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.58it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.64it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.67it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.70it/s] 41%|████      | 13/32 [00:06<00:07,  2.71it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.72it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.74it/s]I0316 09:42:51.872480 497511 finetune.py:76] layer 8_v @ epoch 2 new loss 1.4109300536802039e-06 old loss 1.3814366184305982e-06 WORSE
 50%|█████     | 16/32 [00:07<00:05,  2.74it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.76it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.76it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.76it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.76it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.75it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.75it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.75it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.76it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.75it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.75it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.73it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.77it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.77it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.78it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.77it/s]100%|██████████| 32/32 [00:13<00:00,  2.76it/s]100%|██████████| 32/32 [00:13<00:00,  2.44it/s]
W0316 09:43:00.754000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:43:00.754000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:43:00.754000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:43:00.754000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:43:00.754000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:43:00.754000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:43:00.754000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:43:00.781000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:43:00.781000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:43:00.782000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:43:00.782000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:43:00.782000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.081000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.081000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.081000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.081000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.081000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.710000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.711000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.711000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.711000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.711000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.711000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.711000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.729000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.729000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.729000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.729000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.729000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.931000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.931000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.931000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.931000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:43:01.931000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:43:03.118000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:43:03.118000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:43:03.118000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:43:03.119000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:43:03.119000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:43:03.119000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:43:03.119000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:43:03.137000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:43:03.137000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:43:03.137000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:43:03.138000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:43:03.138000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:43:03.778000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:43:03.779000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:43:03.779000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:43:03.779000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:43:03.779000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 09:43:10.510427 498521 finetune.py:45] layer 10_v initial loss 3.569184855223284e-06
W0316 09:43:10.510685 498521 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:43:17.595767 498015 finetune.py:68] layer 9_v @ epoch 1 new loss 1.949304078152636e-06 old loss 2.148994099115953e-06 BETTER
I0316 09:43:28.916258 497511 finetune.py:76] layer 8_v @ epoch 3 new loss 1.4546363900080905e-06 old loss 1.3814366184305982e-06 WORSE
I0316 09:43:39.653629 460347 quantize_finetune_llama.py:186] computed original embedding for layer 11 in 60.99296498298645s
I0316 09:43:40.051425 460347 quantize_finetune_llama.py:159] layer 12 gpu 0
I0316 09:43:42.150059 499027 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 09:43:42.150195 499027 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 09:43:42.150267 499027 utils.py:162] NumExpr defaulting to 16 threads.
I0316 09:43:42.381529 499027 config.py:58] PyTorch version 2.4.0 available.
I0316 09:43:44.630538 499027 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 09:43:45.014834 499027 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 09:43:45.315186 498521 finetune.py:68] layer 10_v @ epoch 0 new loss 1.9999313281005016e-06 old loss 3.569184855223284e-06 BETTER
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.95s/it]  6%|▋         | 2/32 [00:02<00:30,  1.01s/it]  9%|▉         | 3/32 [00:02<00:20,  1.41it/s] 12%|█▎        | 4/32 [00:03<00:15,  1.76it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.04it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.25it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.41it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.53it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.63it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.70it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.75it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.81it/s] 41%|████      | 13/32 [00:06<00:06,  2.88it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.94it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.99it/s] 50%|█████     | 16/32 [00:07<00:05,  3.03it/s] 53%|█████▎    | 17/32 [00:07<00:04,  3.06it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.00it/s]I0316 09:43:53.794320 498015 finetune.py:68] layer 9_v @ epoch 2 new loss 1.9053338746743975e-06 old loss 1.949304078152636e-06 BETTER
 59%|█████▉    | 19/32 [00:08<00:04,  2.93it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.88it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.83it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.80it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.81it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.81it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.83it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.84it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.85it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.84it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
W0316 09:44:01.573000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:44:01.573000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:44:01.573000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:44:01.574000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:44:01.574000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:44:01.574000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:44:01.574000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:44:01.601000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:44:01.601000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:44:01.601000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:44:01.601000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:44:01.601000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:44:01.903000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:44:01.904000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:44:01.904000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:44:01.904000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:44:01.904000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:44:02.544000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:44:02.544000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:44:02.544000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:44:02.544000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:44:02.544000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:44:02.544000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:44:02.544000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:44:02.562000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:44:02.563000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:44:02.563000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:44:02.563000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:44:02.563000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:44:02.770000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:44:02.771000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:44:02.771000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:44:02.771000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:44:02.771000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:44:03.960000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:44:03.960000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:44:03.960000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:44:03.960000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:44:03.961000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:44:03.961000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:44:03.961000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:44:03.980000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:44:03.980000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:44:03.980000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:44:03.980000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:44:03.981000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:44:04.627000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:44:04.627000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:44:04.627000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:44:04.627000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:44:04.627000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 09:44:06.651086 497511 finetune.py:76] layer 8_v @ epoch 4 new loss 1.4240548580346513e-06 old loss 1.3814366184305982e-06 WORSE
W0316 09:44:08.046988 497511 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_v proxy err 0.00274472963064909 tr(WHW.T) 53.42280197143555
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.34s/it]  6%|▋         | 2/32 [00:01<00:23,  1.29it/s]I0316 09:44:11.283876 499027 finetune.py:45] layer 11_v initial loss 2.8058166208211333e-06
W0316 09:44:11.284276 499027 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:02<00:17,  1.69it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.98it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.19it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.34it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.56it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.63it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.64it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.65it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.67it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.70it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.70it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.70it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.69it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.69it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.69it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.69it/s]I0316 09:44:21.635974 498521 finetune.py:68] layer 10_v @ epoch 1 new loss 1.9035492186958436e-06 old loss 1.9999313281005016e-06 BETTER
 97%|█████████▋| 31/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
W0316 09:44:28.439000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.439000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.439000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.439000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.440000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.440000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.440000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.471000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.471000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.471000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.471000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.471000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.640000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.640000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.640000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.640000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.640000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.871000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.871000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.871000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.871000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.871000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.871000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.871000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.893000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.894000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.894000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.894000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.894000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.960000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.960000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.960000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.960000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:44:28.960000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:44:30.038000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 09:44:30.185705 498015 finetune.py:76] layer 9_v @ epoch 3 new loss 1.937572505994467e-06 old loss 1.9053338746743975e-06 WORSE
W0316 09:44:30.366000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:44:30.366000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:44:30.366000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:44:30.366000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:44:30.366000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:44:30.366000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:44:30.366000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:44:30.390000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:44:30.390000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:44:30.390000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:44:30.390000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:44:30.391000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:44:30.650000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:44:30.650000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:44:30.650000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:44:30.651000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:44:30.651000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:44:31.126000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 09:44:37.900765 497511 finetune.py:45] layer 8_q initial loss 2.5793460736167617e-06
W0316 09:44:37.901151 497511 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:44:45.615012 499027 finetune.py:68] layer 11_v @ epoch 0 new loss 1.7293548353336519e-06 old loss 2.8058166208211333e-06 BETTER
I0316 09:44:58.230519 498521 finetune.py:68] layer 10_v @ epoch 2 new loss 1.8701041426538723e-06 old loss 1.9035492186958436e-06 BETTER
I0316 09:45:05.754403 498015 finetune.py:76] layer 9_v @ epoch 4 new loss 2.149079818991595e-06 old loss 1.9053338746743975e-06 WORSE
W0316 09:45:06.928067 498015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

9_v proxy err 0.0027735563926398754 tr(WHW.T) 72.69827270507812
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.33s/it]  6%|▋         | 2/32 [00:01<00:23,  1.29it/s]  9%|▉         | 3/32 [00:02<00:17,  1.68it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.95it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.15it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.29it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.38it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.46it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.50it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.53it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.55it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.56it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s]I0316 09:45:14.772944 497511 finetune.py:68] layer 8_q @ epoch 0 new loss 2.4709809167688945e-06 old loss 2.5793460736167617e-06 BETTER
 50%|█████     | 16/32 [00:07<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.63it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.61it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.62it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.63it/s]I0316 09:45:20.549171 499027 finetune.py:68] layer 11_v @ epoch 1 new loss 1.672591793067113e-06 old loss 1.7293548353336519e-06 BETTER
 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:13<00:00,  2.64it/s]100%|██████████| 32/32 [00:13<00:00,  2.43it/s]
W0316 09:45:27.214000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.214000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.215000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.215000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.215000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.215000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.215000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.246000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.246000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.246000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.246000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.246000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.414000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.414000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.414000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.414000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.414000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.645000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.645000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.645000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.645000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.645000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.646000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.646000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.668000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.669000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.669000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.669000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.669000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.737000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.737000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.737000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.737000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:45:27.737000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:45:28.808000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:45:29.127000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:45:29.127000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:45:29.127000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:45:29.127000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:45:29.127000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:45:29.127000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:45:29.127000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:45:29.149000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:45:29.150000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:45:29.150000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:45:29.150000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:45:29.150000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:45:29.405000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:45:29.405000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:45:29.405000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:45:29.406000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:45:29.406000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:45:29.873000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 09:45:34.641264 498521 finetune.py:76] layer 10_v @ epoch 3 new loss 1.972317704712623e-06 old loss 1.8701041426538723e-06 WORSE
I0316 09:45:36.912412 498015 finetune.py:45] layer 9_q initial loss 3.2411489883088507e-06
W0316 09:45:36.912890 498015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:45:52.419107 497511 finetune.py:68] layer 8_q @ epoch 1 new loss 2.4131300051521976e-06 old loss 2.4709809167688945e-06 BETTER
I0316 09:45:56.652262 499027 finetune.py:76] layer 11_v @ epoch 2 new loss 1.6792066617199453e-06 old loss 1.672591793067113e-06 WORSE
I0316 09:46:10.283687 498521 finetune.py:76] layer 10_v @ epoch 4 new loss 1.8851103504857747e-06 old loss 1.8701041426538723e-06 WORSE
W0316 09:46:11.547481 498521 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 09:46:12.124154 498015 finetune.py:68] layer 9_q @ epoch 0 new loss 3.123138412775006e-06 old loss 3.2411489883088507e-06 BETTER
10_v proxy err 0.00260100606828928 tr(WHW.T) 60.08286666870117
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.37s/it]  6%|▋         | 2/32 [00:01<00:24,  1.24it/s]  9%|▉         | 3/32 [00:02<00:17,  1.62it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.88it/s] 16%|█▌        | 5/32 [00:02<00:13,  2.07it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.21it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.30it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.37it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.41it/s] 31%|███▏      | 10/32 [00:04<00:09,  2.44it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.46it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.47it/s] 41%|████      | 13/32 [00:06<00:07,  2.48it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.48it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.48it/s] 50%|█████     | 16/32 [00:07<00:06,  2.49it/s] 53%|█████▎    | 17/32 [00:07<00:06,  2.49it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.51it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.52it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.53it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.53it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.53it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.53it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.52it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.52it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.51it/s] 84%|████████▍ | 27/32 [00:11<00:02,  2.50it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.50it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.50it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.49it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.50it/s]100%|██████████| 32/32 [00:13<00:00,  2.50it/s]100%|██████████| 32/32 [00:13<00:00,  2.33it/s]
I0316 09:46:30.275887 497511 finetune.py:68] layer 8_q @ epoch 2 new loss 2.373727056692587e-06 old loss 2.4131300051521976e-06 BETTER
I0316 09:46:31.803714 499027 finetune.py:76] layer 11_v @ epoch 3 new loss 2.183589231208316e-06 old loss 1.672591793067113e-06 WORSE
W0316 09:46:32.627000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:46:32.627000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:46:32.627000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:46:32.627000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:46:32.627000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:46:32.627000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:46:32.627000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:46:32.657000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:46:32.658000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:46:32.658000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:46:32.658000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:46:32.658000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:46:32.823000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:46:32.823000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:46:32.823000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:46:32.824000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:46:32.824000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:46:33.051000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:46:33.051000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:46:33.052000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:46:33.052000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:46:33.052000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:46:33.052000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:46:33.052000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:46:33.074000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:46:33.074000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:46:33.074000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:46:33.074000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:46:33.074000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:46:33.138000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:46:33.138000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:46:33.138000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:46:33.138000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:46:33.139000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:46:34.216000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:46:34.543000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:46:34.544000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:46:34.544000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:46:34.544000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:46:34.544000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:46:34.544000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:46:34.544000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:46:34.565000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:46:34.565000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:46:34.566000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:46:34.566000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:46:34.566000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:46:34.824000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:46:34.824000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:46:34.824000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:46:34.825000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:46:34.825000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:46:35.292000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 09:46:42.774789 498521 finetune.py:45] layer 10_q initial loss 3.390587380636134e-06
W0316 09:46:42.775365 498521 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:46:48.739099 498015 finetune.py:68] layer 9_q @ epoch 1 new loss 3.036971975234337e-06 old loss 3.123138412775006e-06 BETTER
I0316 09:47:07.219035 499027 finetune.py:76] layer 11_v @ epoch 4 new loss 1.9387166503292974e-06 old loss 1.672591793067113e-06 WORSE
I0316 09:47:08.209606 497511 finetune.py:68] layer 8_q @ epoch 3 new loss 2.360905682508019e-06 old loss 2.373727056692587e-06 BETTER
W0316 09:47:08.449738 499027 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

11_v proxy err 0.002135798567906022 tr(WHW.T) 74.2698974609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.34s/it]  6%|▋         | 2/32 [00:01<00:23,  1.28it/s]  9%|▉         | 3/32 [00:02<00:17,  1.66it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.92it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.12it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.26it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.33it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.42it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.47it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.54it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.56it/s] 41%|████      | 13/32 [00:05<00:07,  2.58it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.59it/s] 50%|█████     | 16/32 [00:07<00:06,  2.60it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s]I0316 09:47:17.873312 498521 finetune.py:68] layer 10_q @ epoch 0 new loss 3.2400778309238376e-06 old loss 3.390587380636134e-06 BETTER
 59%|█████▉    | 19/32 [00:08<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.60it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.61it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.62it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.58it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:13<00:00,  2.58it/s]100%|██████████| 32/32 [00:13<00:00,  2.41it/s]
I0316 09:47:25.227152 498015 finetune.py:68] layer 9_q @ epoch 2 new loss 3.026107378900633e-06 old loss 3.036971975234337e-06 BETTER
W0316 09:47:29.109000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.110000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.110000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.110000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.110000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.110000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.110000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.143000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.144000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.144000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.144000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.144000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.314000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.314000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.314000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.314000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.315000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.549000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.549000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.549000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.550000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.550000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.550000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.550000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.572000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.573000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.573000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.573000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.573000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.638000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.639000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.639000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.639000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 09:47:29.639000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 09:47:30.717000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 09:47:31.046000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:47:31.046000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:47:31.046000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 09:47:31.046000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:47:31.046000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:47:31.047000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:47:31.047000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 09:47:31.069000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:47:31.070000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:47:31.070000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:47:31.070000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:47:31.070000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:47:31.329000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 09:47:31.330000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 09:47:31.330000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 09:47:31.330000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 09:47:31.330000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 09:47:31.803000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 09:47:39.180185 499027 finetune.py:45] layer 11_q initial loss 3.338667966090725e-06
W0316 09:47:39.180497 499027 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:47:46.487652 497511 finetune.py:68] layer 8_q @ epoch 4 new loss 2.3470208816434024e-06 old loss 2.360905682508019e-06 BETTER
W0316 09:47:48.329099 497511 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_q proxy err 0.0004965277039445937 tr(WHW.T) 5514.61962890625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:38,  1.24s/it]  6%|▋         | 2/32 [00:01<00:21,  1.38it/s]  9%|▉         | 3/32 [00:01<00:16,  1.78it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.07it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.41it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.61it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.68it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.70it/s]I0316 09:47:54.755430 498521 finetune.py:68] layer 10_q @ epoch 1 new loss 3.139207137792255e-06 old loss 3.2400778309238376e-06 BETTER
 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.77it/s] 50%|█████     | 16/32 [00:06<00:05,  2.77it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.77it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.77it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.73it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.69it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.70it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.71it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.71it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.71it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.72it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s]I0316 09:48:01.134618 498015 finetune.py:68] layer 9_q @ epoch 3 new loss 3.002591256517917e-06 old loss 3.026107378900633e-06 BETTER
 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
I0316 09:48:09.152039 497511 finetune.py:45] layer 8_k initial loss 3.138649617540068e-06
W0316 09:48:09.152474 497511 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:48:14.311969 499027 finetune.py:68] layer 11_q @ epoch 0 new loss 3.211986268070177e-06 old loss 3.338667966090725e-06 BETTER
I0316 09:48:31.696727 498521 finetune.py:68] layer 10_q @ epoch 2 new loss 3.0907556265447056e-06 old loss 3.139207137792255e-06 BETTER
I0316 09:48:38.372353 498015 finetune.py:76] layer 9_q @ epoch 4 new loss 3.1142890293267556e-06 old loss 3.002591256517917e-06 WORSE
W0316 09:48:39.650374 498015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

9_q proxy err 0.0005250293761491776 tr(WHW.T) 5307.5107421875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it]  6%|▋         | 2/32 [00:01<00:22,  1.33it/s]  9%|▉         | 3/32 [00:02<00:16,  1.72it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.98it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.17it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.31it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.41it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.48it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.59it/s]I0316 09:48:46.097104 497511 finetune.py:68] layer 8_k @ epoch 0 new loss 2.949453119072132e-06 old loss 3.138649617540068e-06 BETTER
 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.62it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.62it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.61it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.61it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.61it/s]I0316 09:48:49.431032 499027 finetune.py:76] layer 11_q @ epoch 1 new loss 3.230871470805141e-06 old loss 3.211986268070177e-06 WORSE
 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.64it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.64it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.66it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.66it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:13<00:00,  2.62it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
I0316 09:49:01.525040 498015 finetune.py:45] layer 9_k initial loss 3.9549813664052635e-06
W0316 09:49:01.525432 498015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:49:08.493647 498521 finetune.py:76] layer 10_q @ epoch 3 new loss 3.098667775702779e-06 old loss 3.0907556265447056e-06 WORSE
I0316 09:49:24.609308 497511 finetune.py:68] layer 8_k @ epoch 1 new loss 2.8953027140232734e-06 old loss 2.949453119072132e-06 BETTER
I0316 09:49:25.274736 499027 finetune.py:76] layer 11_q @ epoch 2 new loss 3.3189387522725156e-06 old loss 3.211986268070177e-06 WORSE
I0316 09:49:36.629309 498015 finetune.py:68] layer 9_k @ epoch 0 new loss 3.6491699120233534e-06 old loss 3.9549813664052635e-06 BETTER
I0316 09:49:43.909463 498521 finetune.py:68] layer 10_q @ epoch 4 new loss 3.042310481760069e-06 old loss 3.0907556265447056e-06 BETTER
W0316 09:49:45.595790 498521 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

10_q proxy err 0.000558947678655386 tr(WHW.T) 5568.54296875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it]  6%|▋         | 2/32 [00:01<00:22,  1.32it/s]  9%|▉         | 3/32 [00:02<00:16,  1.71it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.96it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.15it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.28it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.37it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.43it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.48it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.54it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.56it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.59it/s] 50%|█████     | 16/32 [00:07<00:06,  2.59it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.60it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.61it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.59it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.59it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.59it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.60it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:13<00:00,  2.60it/s]100%|██████████| 32/32 [00:13<00:00,  2.42it/s]
I0316 09:50:00.509426 499027 finetune.py:76] layer 11_q @ epoch 3 new loss 3.281724730186397e-06 old loss 3.211986268070177e-06 WORSE
I0316 09:50:02.459155 497511 finetune.py:68] layer 8_k @ epoch 2 new loss 2.8892059162899386e-06 old loss 2.8953027140232734e-06 BETTER
I0316 09:50:07.126163 498521 finetune.py:45] layer 10_k initial loss 3.9404785638907924e-06
W0316 09:50:07.126572 498521 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:50:12.689829 498015 finetune.py:68] layer 9_k @ epoch 1 new loss 3.599672936616116e-06 old loss 3.6491699120233534e-06 BETTER
I0316 09:50:36.047887 499027 finetune.py:76] layer 11_q @ epoch 4 new loss 3.2308732897945447e-06 old loss 3.211986268070177e-06 WORSE
W0316 09:50:37.642047 499027 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

11_q proxy err 0.0005627699429169297 tr(WHW.T) 5158.1845703125
  0%|          | 0/32 [00:00<?, ?it/s]I0316 09:50:40.229068 497511 finetune.py:76] layer 8_k @ epoch 3 new loss 2.910397142841248e-06 old loss 2.8892059162899386e-06 WORSE
  3%|▎         | 1/32 [00:01<00:40,  1.32s/it]  6%|▋         | 2/32 [00:01<00:23,  1.30it/s]  9%|▉         | 3/32 [00:02<00:17,  1.68it/s]I0316 09:50:42.220268 498521 finetune.py:68] layer 10_k @ epoch 0 new loss 3.724562930074171e-06 old loss 3.9404785638907924e-06 BETTER
 12%|█▎        | 4/32 [00:02<00:14,  1.93it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.13it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.23it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.32it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.41it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.46it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.49it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.52it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.53it/s] 41%|████      | 13/32 [00:05<00:07,  2.56it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.55it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.54it/s] 50%|█████     | 16/32 [00:07<00:06,  2.53it/s] 53%|█████▎    | 17/32 [00:07<00:06,  2.50it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.46it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.50it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.53it/s]I0316 09:50:48.814064 498015 finetune.py:68] layer 9_k @ epoch 2 new loss 3.577258212317247e-06 old loss 3.599672936616116e-06 BETTER
 66%|██████▌   | 21/32 [00:09<00:04,  2.56it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.60it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.62it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.59it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.59it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.55it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.51it/s]100%|██████████| 32/32 [00:13<00:00,  2.51it/s]100%|██████████| 32/32 [00:13<00:00,  2.38it/s]
I0316 09:51:00.402945 499027 finetune.py:45] layer 11_k initial loss 4.042626642331015e-06
W0316 09:51:00.403416 499027 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:51:17.616762 497511 finetune.py:76] layer 8_k @ epoch 4 new loss 2.892332759074634e-06 old loss 2.8892059162899386e-06 WORSE
I0316 09:51:18.619580 498521 finetune.py:76] layer 10_k @ epoch 1 new loss 3.763358336072997e-06 old loss 3.724562930074171e-06 WORSE
W0316 09:51:18.766937 497511 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_k proxy err 0.00036105976323597133 tr(WHW.T) 4667.9208984375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.25it/s]  6%|▋         | 2/32 [00:01<00:16,  1.82it/s]  9%|▉         | 3/32 [00:01<00:13,  2.13it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.32it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.66it/s] 41%|████      | 13/32 [00:05<00:07,  2.67it/s]I0316 09:51:25.352552 498015 finetune.py:68] layer 9_k @ epoch 3 new loss 3.554404429451097e-06 old loss 3.577258212317247e-06 BETTER
 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.68it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.67it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.66it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.66it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.67it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.69it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.69it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.69it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.69it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
I0316 09:51:34.826549 499027 finetune.py:68] layer 11_k @ epoch 0 new loss 3.9176279642560985e-06 old loss 4.042626642331015e-06 BETTER
I0316 09:51:39.898593 497511 finetune.py:45] layer 8_o initial loss 5.245600277703488e-06
W0316 09:51:39.899016 497511 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:51:54.240201 498521 finetune.py:68] layer 10_k @ epoch 2 new loss 3.715221737365937e-06 old loss 3.724562930074171e-06 BETTER
I0316 09:52:01.873770 498015 finetune.py:76] layer 9_k @ epoch 4 new loss 3.569621867427486e-06 old loss 3.554404429451097e-06 WORSE
W0316 09:52:03.117520 498015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

9_k proxy err 0.0003952203260269016 tr(WHW.T) 4331.41796875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.23it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.53it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.55it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.54it/s] 41%|████      | 13/32 [00:05<00:07,  2.54it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.55it/s]I0316 09:52:10.535222 499027 finetune.py:76] layer 11_k @ epoch 1 new loss 3.92797574022552e-06 old loss 3.9176279642560985e-06 WORSE
 47%|████▋     | 15/32 [00:06<00:06,  2.56it/s] 50%|█████     | 16/32 [00:06<00:06,  2.56it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.57it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.57it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.57it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.57it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.57it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.56it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.55it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.54it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.55it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.55it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.56it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.57it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.57it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.57it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.57it/s]I0316 09:52:17.118370 497511 finetune.py:68] layer 8_o @ epoch 0 new loss 5.137974767421838e-06 old loss 5.245600277703488e-06 BETTER
100%|██████████| 32/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
I0316 09:52:24.840346 498015 finetune.py:45] layer 9_o initial loss 6.369910806824919e-06
W0316 09:52:24.840781 498015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:52:31.140824 498521 finetune.py:76] layer 10_k @ epoch 3 new loss 3.735824520845199e-06 old loss 3.715221737365937e-06 WORSE
I0316 09:52:45.253510 499027 finetune.py:68] layer 11_k @ epoch 2 new loss 3.868002750095911e-06 old loss 3.9176279642560985e-06 BETTER
I0316 09:52:54.772545 497511 finetune.py:68] layer 8_o @ epoch 1 new loss 5.081724339106586e-06 old loss 5.137974767421838e-06 BETTER
I0316 09:53:00.296872 498015 finetune.py:68] layer 9_o @ epoch 0 new loss 6.247607871046057e-06 old loss 6.369910806824919e-06 BETTER
I0316 09:53:07.341747 498521 finetune.py:76] layer 10_k @ epoch 4 new loss 3.7324489312595688e-06 old loss 3.715221737365937e-06 WORSE
W0316 09:53:08.511262 498521 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

10_k proxy err 0.0003979139728471637 tr(WHW.T) 4709.005859375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.22it/s]  6%|▋         | 2/32 [00:01<00:17,  1.75it/s]  9%|▉         | 3/32 [00:01<00:14,  2.05it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.22it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.44it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.47it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.48it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.49it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.50it/s] 41%|████      | 13/32 [00:05<00:07,  2.51it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.50it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.51it/s] 50%|█████     | 16/32 [00:06<00:06,  2.52it/s] 53%|█████▎    | 17/32 [00:07<00:06,  2.25it/s] 56%|█████▋    | 18/32 [00:07<00:06,  2.33it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.39it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.43it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.46it/s] 69%|██████▉   | 22/32 [00:09<00:04,  2.47it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.49it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.50it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.49it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.49it/s]I0316 09:53:20.897052 499027 finetune.py:68] layer 11_k @ epoch 3 new loss 3.7834836348338285e-06 old loss 3.868002750095911e-06 BETTER
 84%|████████▍ | 27/32 [00:11<00:01,  2.50it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.51it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.52it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.53it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.53it/s]100%|██████████| 32/32 [00:13<00:00,  2.53it/s]100%|██████████| 32/32 [00:13<00:00,  2.41it/s]
I0316 09:53:30.583448 498521 finetune.py:45] layer 10_o initial loss 6.468582796514966e-06
W0316 09:53:30.583805 498521 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:53:32.526208 497511 finetune.py:68] layer 8_o @ epoch 2 new loss 5.037878054281464e-06 old loss 5.081724339106586e-06 BETTER
I0316 09:53:37.036114 498015 finetune.py:68] layer 9_o @ epoch 1 new loss 6.177507202664856e-06 old loss 6.247607871046057e-06 BETTER
I0316 09:53:56.489824 499027 finetune.py:76] layer 11_k @ epoch 4 new loss 3.8787884477642365e-06 old loss 3.7834836348338285e-06 WORSE
W0316 09:53:57.593491 499027 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

11_k proxy err 0.000452715961728245 tr(WHW.T) 4189.666015625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.21it/s]  6%|▋         | 2/32 [00:01<00:17,  1.75it/s]  9%|▉         | 3/32 [00:01<00:14,  2.04it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.54it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.54it/s] 41%|████      | 13/32 [00:05<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.55it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.54it/s] 50%|█████     | 16/32 [00:06<00:06,  2.53it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.53it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.54it/s]I0316 09:54:06.599008 498521 finetune.py:68] layer 10_o @ epoch 0 new loss 6.297587333392585e-06 old loss 6.468582796514966e-06 BETTER
 59%|█████▉    | 19/32 [00:07<00:05,  2.55it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.56it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.56it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.56it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.56it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.56it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.55it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.55it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.56it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.56it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.56it/s]I0316 09:54:10.871329 497511 finetune.py:68] layer 8_o @ epoch 3 new loss 5.000741111871321e-06 old loss 5.037878054281464e-06 BETTER
 94%|█████████▍| 30/32 [00:12<00:00,  2.57it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.56it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
I0316 09:54:13.779559 498015 finetune.py:68] layer 9_o @ epoch 2 new loss 6.1219589042593725e-06 old loss 6.177507202664856e-06 BETTER
I0316 09:54:19.070815 499027 finetune.py:45] layer 11_o initial loss 6.674873475276399e-06
W0316 09:54:19.071385 499027 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:54:43.332124 498521 finetune.py:68] layer 10_o @ epoch 1 new loss 6.227036919881357e-06 old loss 6.297587333392585e-06 BETTER
I0316 09:54:49.439452 497511 finetune.py:68] layer 8_o @ epoch 4 new loss 4.968044777342584e-06 old loss 5.000741111871321e-06 BETTER
I0316 09:54:50.883678 498015 finetune.py:68] layer 9_o @ epoch 3 new loss 6.074099019315327e-06 old loss 6.1219589042593725e-06 BETTER
W0316 09:54:51.265589 497511 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_o proxy err 0.0032780084293335676 tr(WHW.T) 3.738487482070923
  0%|          | 0/32 [00:00<?, ?it/s]I0316 09:54:53.810703 499027 finetune.py:68] layer 11_o @ epoch 0 new loss 6.5132344388985075e-06 old loss 6.674873475276399e-06 BETTER
  3%|▎         | 1/32 [00:02<01:02,  2.01s/it]  6%|▋         | 2/32 [00:03<00:50,  1.70s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.50s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.51s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.50s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.50s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.50s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.50s/it]I0316 09:55:19.702289 498521 finetune.py:68] layer 10_o @ epoch 2 new loss 6.157736606837716e-06 old loss 6.227036919881357e-06 BETTER
 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.49s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.50s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.50s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it]I0316 09:55:27.158862 498015 finetune.py:68] layer 9_o @ epoch 4 new loss 6.0343022596498486e-06 old loss 6.074099019315327e-06 BETTER
 72%|███████▏  | 23/32 [00:34<00:13,  1.50s/it]W0316 09:55:28.779523 498015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:36<00:12,  1.52s/it]I0316 09:55:29.261985 499027 finetune.py:68] layer 11_o @ epoch 1 new loss 6.426582331187092e-06 old loss 6.5132344388985075e-06 BETTER
9_o proxy err 0.003330328268930316 tr(WHW.T) 4.44639778137207
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:37<00:10,  1.52s/it]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.53s/it]  6%|▋         | 2/32 [00:03<00:51,  1.70s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.54s/it]  9%|▉         | 3/32 [00:05<00:46,  1.62s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.54s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.57s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.54s/it] 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.55s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.54s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.56s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.57s/it]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]
 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.52s/it] 41%|████      | 13/32 [00:20<00:28,  1.52s/it]I0316 09:55:50.227575 497511 finetune.py:45] layer 8_up initial loss 1.0231318810838275e-05
W0316 09:55:50.228019 497511 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it]I0316 09:55:55.974973 498521 finetune.py:68] layer 10_o @ epoch 3 new loss 6.110228241595905e-06 old loss 6.157736606837716e-06 BETTER
 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.53s/it]I0316 09:56:04.622285 499027 finetune.py:68] layer 11_o @ epoch 2 new loss 6.365360150084598e-06 old loss 6.426582331187092e-06 BETTER
 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.53s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.53s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.53s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.53s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
I0316 09:56:26.267524 497511 finetune.py:68] layer 8_up @ epoch 0 new loss 1.009672814689111e-05 old loss 1.0231318810838275e-05 BETTER
I0316 09:56:28.429310 498015 finetune.py:45] layer 9_up initial loss 1.190478269563755e-05
W0316 09:56:28.429692 498015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:56:32.677032 498521 finetune.py:68] layer 10_o @ epoch 4 new loss 6.075003057048889e-06 old loss 6.110228241595905e-06 BETTER
W0316 09:56:34.270121 498521 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

10_o proxy err 0.003285106038674712 tr(WHW.T) 4.194187641143799
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:05,  2.11s/it]  6%|▋         | 2/32 [00:03<00:53,  1.80s/it]I0316 09:56:40.248094 499027 finetune.py:68] layer 11_o @ epoch 3 new loss 6.3147545006359e-06 old loss 6.365360150084598e-06 BETTER
  9%|▉         | 3/32 [00:05<00:48,  1.68s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.63s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.61s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.59s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.58s/it] 25%|██▌       | 8/32 [00:13<00:37,  1.57s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.57s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.56s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.56s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.56s/it] 41%|████      | 13/32 [00:20<00:29,  1.56s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.56s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.56s/it] 50%|█████     | 16/32 [00:25<00:24,  1.56s/it]I0316 09:57:02.596571 498015 finetune.py:68] layer 9_up @ epoch 0 new loss 1.1746327800210565e-05 old loss 1.190478269563755e-05 BETTER
 53%|█████▎    | 17/32 [00:27<00:23,  1.56s/it]I0316 09:57:03.159784 497511 finetune.py:68] layer 8_up @ epoch 1 new loss 1.0005312105931807e-05 old loss 1.009672814689111e-05 BETTER
 56%|█████▋    | 18/32 [00:28<00:21,  1.56s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.56s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.56s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.56s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.57s/it] 72%|███████▏  | 23/32 [00:36<00:14,  1.57s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.56s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.56s/it]I0316 09:57:15.547487 499027 finetune.py:68] layer 11_o @ epoch 4 new loss 6.26892415311886e-06 old loss 6.3147545006359e-06 BETTER
 81%|████████▏ | 26/32 [00:41<00:09,  1.57s/it]W0316 09:57:17.085934 499027 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:42<00:07,  1.57s/it]11_o proxy err 0.003399620996788144 tr(WHW.T) 4.461423873901367
  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:44<00:06,  1.57s/it]  3%|▎         | 1/32 [00:02<01:03,  2.05s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.57s/it]  6%|▋         | 2/32 [00:03<00:52,  1.74s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.56s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.56s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.61s/it]100%|██████████| 32/32 [00:50<00:00,  1.56s/it]100%|██████████| 32/32 [00:50<00:00,  1.58s/it]
 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.58s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.58s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.58s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.57s/it]I0316 09:57:34.721259 498521 finetune.py:45] layer 10_up initial loss 1.2252152373548597e-05
W0316 09:57:34.721660 498521 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:17<00:32,  1.57s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.56s/it]I0316 09:57:37.761710 498015 finetune.py:68] layer 9_up @ epoch 1 new loss 1.1630422704911325e-05 old loss 1.1746327800210565e-05 BETTER
 41%|████      | 13/32 [00:20<00:29,  1.56s/it]I0316 09:57:39.938367 497511 finetune.py:68] layer 8_up @ epoch 2 new loss 9.923206562234554e-06 old loss 1.0005312105931807e-05 BETTER
 44%|████▍     | 14/32 [00:22<00:28,  1.56s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.56s/it] 50%|█████     | 16/32 [00:25<00:24,  1.56s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.56s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.56s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.55s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.55s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.56s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.56s/it] 72%|███████▏  | 23/32 [00:36<00:14,  1.57s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.56s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.55s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.54s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.54s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.54s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.55s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.55s/it]100%|██████████| 32/32 [00:50<00:00,  1.55s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]
I0316 09:58:09.696711 498521 finetune.py:68] layer 10_up @ epoch 0 new loss 1.2086233255104162e-05 old loss 1.2252152373548597e-05 BETTER
I0316 09:58:13.250844 498015 finetune.py:68] layer 9_up @ epoch 2 new loss 1.153187440650072e-05 old loss 1.1630422704911325e-05 BETTER
I0316 09:58:16.586066 497511 finetune.py:68] layer 8_up @ epoch 3 new loss 9.846962711890228e-06 old loss 9.923206562234554e-06 BETTER
I0316 09:58:17.094227 499027 finetune.py:45] layer 11_up initial loss 1.2919444998260587e-05
W0316 09:58:17.094578 499027 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 09:58:44.609302 498521 finetune.py:68] layer 10_up @ epoch 1 new loss 1.1966993952228222e-05 old loss 1.2086233255104162e-05 BETTER
I0316 09:58:47.847853 498015 finetune.py:68] layer 9_up @ epoch 3 new loss 1.1441326023486909e-05 old loss 1.153187440650072e-05 BETTER
I0316 09:58:50.475904 499027 finetune.py:68] layer 11_up @ epoch 0 new loss 1.2728628462355118e-05 old loss 1.2919444998260587e-05 BETTER
I0316 09:58:53.073025 497511 finetune.py:68] layer 8_up @ epoch 4 new loss 9.779261745279655e-06 old loss 9.846962711890228e-06 BETTER
W0316 09:58:54.416477 497511 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_up proxy err 0.002929523354396224 tr(WHW.T) 668.7066040039062
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:59,  1.91s/it]  6%|▋         | 2/32 [00:03<00:49,  1.66s/it]  9%|▉         | 3/32 [00:04<00:45,  1.58s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.53s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.52s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.50s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.48s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.48s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.48s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.47s/it] 41%|████      | 13/32 [00:19<00:28,  1.47s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.47s/it]I0316 09:59:19.353074 498521 finetune.py:68] layer 10_up @ epoch 2 new loss 1.1858320249302778e-05 old loss 1.1966993952228222e-05 BETTER
 50%|█████     | 16/32 [00:24<00:23,  1.47s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.47s/it]I0316 09:59:22.664841 498015 finetune.py:68] layer 9_up @ epoch 4 new loss 1.1360167263774201e-05 old loss 1.1441326023486909e-05 BETTER
W0316 09:59:24.061131 498015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it]I0316 09:59:24.555957 499027 finetune.py:68] layer 11_up @ epoch 1 new loss 1.2591791346494574e-05 old loss 1.2728628462355118e-05 BETTER
9_up proxy err 0.0028215311467647552 tr(WHW.T) 723.5986328125
  0%|          | 0/32 [00:00<?, ?it/s] 62%|██████▎   | 20/32 [00:29<00:17,  1.47s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it]  6%|▋         | 2/32 [00:03<00:50,  1.69s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.46s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.54s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.48s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.48s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.54s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.54s/it]I0316 09:59:51.762362 497511 finetune.py:45] layer 8_gate initial loss 1.297223298024619e-05
W0316 09:59:51.762814 497511 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▋    | 18/32 [00:27<00:21,  1.54s/it]I0316 09:59:54.459186 498521 finetune.py:68] layer 10_up @ epoch 3 new loss 1.176400746771833e-05 old loss 1.1858320249302778e-05 BETTER
 59%|█████▉    | 19/32 [00:29<00:20,  1.54s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.54s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it]I0316 09:59:58.352164 499027 finetune.py:68] layer 11_up @ epoch 2 new loss 1.247575619345298e-05 old loss 1.2591791346494574e-05 BETTER
 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.52s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.52s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.52s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.52s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
I0316 10:00:23.318839 498015 finetune.py:45] layer 9_gate initial loss 1.4891547834849916e-05
W0316 10:00:23.319244 498015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:00:26.430768 497511 finetune.py:68] layer 8_gate @ epoch 0 new loss 1.2876458640675992e-05 old loss 1.297223298024619e-05 BETTER
I0316 10:00:29.704436 498521 finetune.py:68] layer 10_up @ epoch 4 new loss 1.1679068848025054e-05 old loss 1.176400746771833e-05 BETTER
W0316 10:00:31.300489 498521 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 10:00:32.350683 499027 finetune.py:68] layer 11_up @ epoch 3 new loss 1.2366257578833029e-05 old loss 1.247575619345298e-05 BETTER
10_up proxy err 0.0028577283956110477 tr(WHW.T) 748.9395751953125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:02,  2.00s/it]  6%|▋         | 2/32 [00:03<00:52,  1.75s/it]  9%|▉         | 3/32 [00:05<00:48,  1.68s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.63s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.60s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.59s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.57s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.56s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.56s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.56s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.55s/it] 38%|███▊      | 12/32 [00:19<00:30,  1.54s/it] 41%|████      | 13/32 [00:20<00:29,  1.55s/it] 44%|████▍     | 14/32 [00:22<00:27,  1.55s/it]I0316 10:00:56.303266 498015 finetune.py:68] layer 9_gate @ epoch 0 new loss 1.4771610949537717e-05 old loss 1.4891547834849916e-05 BETTER
 47%|████▋     | 15/32 [00:23<00:26,  1.54s/it] 50%|█████     | 16/32 [00:25<00:24,  1.54s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.55s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.55s/it]I0316 10:01:01.509070 497511 finetune.py:68] layer 8_gate @ epoch 1 new loss 1.2808042811229825e-05 old loss 1.2876458640675992e-05 BETTER
 59%|█████▉    | 19/32 [00:29<00:20,  1.55s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.55s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.55s/it]I0316 10:01:06.221884 499027 finetune.py:68] layer 11_up @ epoch 4 new loss 1.2272793355805334e-05 old loss 1.2366257578833029e-05 BETTER
 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it]W0316 10:01:07.588968 499027 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

11_up proxy err 0.002842459362000227 tr(WHW.T) 789.1629638671875
  0%|          | 0/32 [00:00<?, ?it/s] 72%|███████▏  | 23/32 [00:36<00:13,  1.54s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.55s/it]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.55s/it]  6%|▋         | 2/32 [00:03<00:51,  1.73s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.55s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.55s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.54s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.55s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it]100%|██████████| 32/32 [00:50<00:00,  1.54s/it]100%|██████████| 32/32 [00:50<00:00,  1.56s/it]
 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:34,  1.55s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.57s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.57s/it] 41%|████      | 13/32 [00:20<00:29,  1.58s/it]I0316 10:01:29.924216 498015 finetune.py:68] layer 9_gate @ epoch 1 new loss 1.4693555385747459e-05 old loss 1.4771610949537717e-05 BETTER
 44%|████▍     | 14/32 [00:22<00:28,  1.58s/it]I0316 10:01:31.430741 498521 finetune.py:45] layer 10_gate initial loss 1.5369299944723025e-05
W0316 10:01:31.431095 498521 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 47%|████▋     | 15/32 [00:23<00:26,  1.58s/it] 50%|█████     | 16/32 [00:25<00:25,  1.58s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.59s/it]I0316 10:01:37.033844 497511 finetune.py:68] layer 8_gate @ epoch 2 new loss 1.2746631909976713e-05 old loss 1.2808042811229825e-05 BETTER
 56%|█████▋    | 18/32 [00:28<00:22,  1.59s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.58s/it] 62%|██████▎   | 20/32 [00:31<00:19,  1.59s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.59s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.59s/it] 72%|███████▏  | 23/32 [00:36<00:14,  1.59s/it] 75%|███████▌  | 24/32 [00:38<00:12,  1.60s/it] 78%|███████▊  | 25/32 [00:39<00:11,  1.60s/it] 81%|████████▏ | 26/32 [00:41<00:09,  1.59s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.60s/it] 88%|████████▊ | 28/32 [00:44<00:06,  1.61s/it] 91%|█████████ | 29/32 [00:46<00:04,  1.62s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.60s/it] 97%|█████████▋| 31/32 [00:49<00:01,  1.59s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]100%|██████████| 32/32 [00:50<00:00,  1.59s/it]
I0316 10:02:03.773076 498015 finetune.py:68] layer 9_gate @ epoch 2 new loss 1.4621302398154512e-05 old loss 1.4693555385747459e-05 BETTER
I0316 10:02:05.124071 498521 finetune.py:68] layer 10_gate @ epoch 0 new loss 1.5251383956638165e-05 old loss 1.5369299944723025e-05 BETTER
I0316 10:02:08.454835 499027 finetune.py:45] layer 11_gate initial loss 1.61525204021018e-05
W0316 10:02:08.455426 499027 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:02:12.325018 497511 finetune.py:68] layer 8_gate @ epoch 3 new loss 1.2689421964751091e-05 old loss 1.2746631909976713e-05 BETTER
I0316 10:02:37.128468 498015 finetune.py:68] layer 9_gate @ epoch 3 new loss 1.4553056644217577e-05 old loss 1.4621302398154512e-05 BETTER
I0316 10:02:38.617395 498521 finetune.py:68] layer 10_gate @ epoch 1 new loss 1.5167100173130166e-05 old loss 1.5251383956638165e-05 BETTER
I0316 10:02:40.681268 499027 finetune.py:68] layer 11_gate @ epoch 0 new loss 1.6022326235543005e-05 old loss 1.61525204021018e-05 BETTER
I0316 10:02:47.332521 497511 finetune.py:68] layer 8_gate @ epoch 4 new loss 1.263547073904192e-05 old loss 1.2689421964751091e-05 BETTER
W0316 10:02:48.597525 497511 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_gate proxy err 0.0011331222485750914 tr(WHW.T) 2930.460693359375
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:38,  1.12it/s]  2%|▏         | 2/112 [00:01<01:04,  1.70it/s]  3%|▎         | 3/112 [00:01<00:53,  2.03it/s]  4%|▎         | 4/112 [00:02<00:48,  2.25it/s]  4%|▍         | 5/112 [00:02<00:45,  2.37it/s]  5%|▌         | 6/112 [00:02<00:43,  2.46it/s]  6%|▋         | 7/112 [00:03<00:41,  2.52it/s]  7%|▋         | 8/112 [00:03<00:40,  2.56it/s]  8%|▊         | 9/112 [00:03<00:39,  2.59it/s]  9%|▉         | 10/112 [00:04<00:39,  2.61it/s] 10%|▉         | 11/112 [00:04<00:38,  2.63it/s] 11%|█         | 12/112 [00:05<00:37,  2.63it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.64it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.65it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.66it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.66it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.62it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.63it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.64it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.66it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.66it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.66it/s] 21%|██        | 23/112 [00:09<00:33,  2.66it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.66it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.67it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.68it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.67it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.67it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.63it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.64it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.64it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.64it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.66it/s] 30%|███       | 34/112 [00:13<00:29,  2.67it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.67it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.67it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.66it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.67it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.66it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.67it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.64it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.65it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.65it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.66it/s] 40%|████      | 45/112 [00:17<00:25,  2.66it/s] 41%|████      | 46/112 [00:17<00:24,  2.66it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.66it/s]I0316 10:03:10.398369 498015 finetune.py:68] layer 9_gate @ epoch 4 new loss 1.4492163245449774e-05 old loss 1.4553056644217577e-05 BETTER
 43%|████▎     | 48/112 [00:18<00:24,  2.66it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.67it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.66it/s]W0316 10:03:11.655157 498015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 10:03:11.677217 498521 finetune.py:68] layer 10_gate @ epoch 2 new loss 1.509334470028989e-05 old loss 1.5167100173130166e-05 BETTER
 46%|████▌     | 51/112 [00:19<00:22,  2.65it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.66it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.67it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.64it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.65it/s]I0316 10:03:13.435042 499027 finetune.py:68] layer 11_gate @ epoch 1 new loss 1.593220076756552e-05 old loss 1.6022326235543005e-05 BETTER
 50%|█████     | 56/112 [00:21<00:21,  2.65it/s] 51%|█████     | 57/112 [00:21<00:20,  2.66it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.67it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.68it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.69it/s]9_gate proxy err 0.001094273873604834 tr(WHW.T) 3176.39697265625
  0%|          | 0/112 [00:00<?, ?it/s] 54%|█████▍    | 61/112 [00:23<00:18,  2.69it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.71it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.70it/s]  1%|          | 1/112 [00:00<01:38,  1.13it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.69it/s]  2%|▏         | 2/112 [00:01<01:05,  1.69it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.68it/s]  3%|▎         | 3/112 [00:01<00:54,  2.01it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.65it/s]  4%|▎         | 4/112 [00:02<00:48,  2.21it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.66it/s]  4%|▍         | 5/112 [00:02<00:45,  2.34it/s] 61%|██████    | 68/112 [00:26<00:16,  2.67it/s]  5%|▌         | 6/112 [00:02<00:43,  2.43it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.68it/s]  6%|▋         | 7/112 [00:03<00:42,  2.49it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.68it/s]  7%|▋         | 8/112 [00:03<00:41,  2.52it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.67it/s]  8%|▊         | 9/112 [00:03<00:40,  2.55it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.68it/s]  9%|▉         | 10/112 [00:04<00:39,  2.57it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.68it/s] 10%|▉         | 11/112 [00:04<00:39,  2.58it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.68it/s] 11%|█         | 12/112 [00:05<00:38,  2.59it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.68it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.56it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.67it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.53it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.66it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.55it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.63it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.55it/s] 71%|███████   | 79/112 [00:30<00:12,  2.65it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.56it/s] 71%|███████▏  | 80/112 [00:30<00:12,  2.66it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.57it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.66it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.58it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.64it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.59it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.65it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.61it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.66it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.61it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.66it/s] 21%|██        | 23/112 [00:09<00:34,  2.61it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.67it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.61it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.67it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.60it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.67it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.60it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.68it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.60it/s] 80%|████████  | 90/112 [00:34<00:08,  2.64it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.59it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.65it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.58it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.65it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.58it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.66it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.67it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.58it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.68it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.59it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.68it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.60it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.68it/s] 30%|███       | 34/112 [00:13<00:29,  2.61it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.69it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.61it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.69it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.61it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.67it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.61it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.67it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.61it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.64it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.60it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.66it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.60it/s] 93%|█████████▎| 104/112 [00:39<00:03,  2.66it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.60it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.66it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.60it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.66it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.60it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.66it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.59it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.68it/s] 40%|████      | 45/112 [00:17<00:25,  2.59it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.69it/s] 41%|████      | 46/112 [00:18<00:25,  2.59it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.69it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.60it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.69it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.60it/s]100%|██████████| 112/112 [00:42<00:00,  2.68it/s]100%|██████████| 112/112 [00:42<00:00,  2.63it/s]
 44%|████▍     | 49/112 [00:19<00:24,  2.60it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.60it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.60it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.61it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.61it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.60it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.60it/s] 50%|█████     | 56/112 [00:22<00:21,  2.58it/s] 51%|█████     | 57/112 [00:22<00:21,  2.59it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.60it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.59it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.60it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.59it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.60it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.60it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.61it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.61it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.61it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.62it/s]W0316 10:03:41.990000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:03:41.990000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:03:41.991000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:03:41.991000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:03:41.991000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:03:41.991000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:03:41.991000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.036000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.037000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.037000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.037000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.037000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 61%|██████    | 68/112 [00:26<00:16,  2.62it/s]W0316 10:03:42.222000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.222000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.222000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.223000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.223000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 62%|██████▏   | 69/112 [00:27<00:16,  2.62it/s]W0316 10:03:42.554000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.554000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.554000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.554000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.555000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.555000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.555000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.591000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.591000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.592000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.592000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.592000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.663000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.664000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.664000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.664000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:03:42.664000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 70/112 [00:27<00:16,  2.62it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.60it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.60it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.59it/s]W0316 10:03:44.050000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 74/112 [00:28<00:14,  2.59it/s]W0316 10:03:44.535000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:03:44.535000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:03:44.535000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:03:44.535000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:03:44.536000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:03:44.536000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:03:44.536000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:03:44.567000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:03:44.567000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:03:44.567000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:03:44.567000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:03:44.567000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 67%|██████▋   | 75/112 [00:29<00:14,  2.60it/s]W0316 10:03:44.966000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:03:44.966000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:03:44.966000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:03:44.967000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:03:44.967000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 68%|██████▊   | 76/112 [00:29<00:13,  2.60it/s]I0316 10:03:45.451866 498521 finetune.py:68] layer 10_gate @ epoch 3 new loss 1.5023892956378404e-05 old loss 1.509334470028989e-05 BETTER
 69%|██████▉   | 77/112 [00:30<00:13,  2.60it/s]W0316 10:03:45.534000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:03:45.540000 140451116963648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 70%|██████▉   | 78/112 [00:30<00:13,  2.61it/s]I0316 10:03:45.984134 499027 finetune.py:68] layer 11_gate @ epoch 2 new loss 1.5848223483772017e-05 old loss 1.593220076756552e-05 BETTER
 71%|███████   | 79/112 [00:30<00:12,  2.62it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.62it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.61it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.60it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.59it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.59it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.59it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.58it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.58it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.58it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.59it/s] 80%|████████  | 90/112 [00:35<00:08,  2.59it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.59it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.60it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.60it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.60it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.61it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.61it/s]I0316 10:03:53.077570 497511 finetune.py:45] layer 8_down initial loss 1.9001487089553848e-05
W0316 10:03:53.077941 497511 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 87%|████████▋ | 97/112 [00:37<00:05,  2.61it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.60it/s] 88%|████████▊ | 99/112 [00:38<00:05,  2.59it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.59it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.58it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.58it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.58it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.58it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.59it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.60it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.60it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.60it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.60it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.61it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.60it/s]100%|██████████| 112/112 [00:43<00:00,  2.60it/s]100%|██████████| 112/112 [00:43<00:00,  2.57it/s]
W0316 10:04:06.689000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:04:06.689000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:04:06.689000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:04:06.689000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:04:06.690000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:04:06.690000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:04:06.690000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:04:06.737000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:04:06.737000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:04:06.737000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:04:06.738000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:04:06.738000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:04:06.922000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:04:06.923000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:04:06.923000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:04:06.923000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:04:06.923000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:04:07.251000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:04:07.251000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:04:07.252000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:04:07.252000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:04:07.252000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:04:07.252000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:04:07.252000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:04:07.287000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:04:07.287000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:04:07.287000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:04:07.287000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:04:07.287000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:04:07.358000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:04:07.358000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:04:07.358000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:04:07.359000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:04:07.359000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:04:08.728000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:04:09.202000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:04:09.203000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:04:09.203000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:04:09.203000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:04:09.203000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:04:09.203000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:04:09.203000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:04:09.236000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:04:09.236000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:04:09.236000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:04:09.236000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:04:09.236000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:04:09.623000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:04:09.623000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:04:09.623000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:04:09.623000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:04:09.623000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:04:10.190000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:04:10.196000 140244453328704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0316 10:04:17.965422 498015 finetune.py:45] layer 9_down initial loss 2.1461928554344922e-05
W0316 10:04:17.965792 498015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:04:18.712651 499027 finetune.py:68] layer 11_gate @ epoch 3 new loss 1.5770950994919986e-05 old loss 1.5848223483772017e-05 BETTER
I0316 10:04:19.067143 498521 finetune.py:68] layer 10_gate @ epoch 4 new loss 1.4959317923057824e-05 old loss 1.5023892956378404e-05 BETTER
W0316 10:04:20.382231 498521 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

10_gate proxy err 0.00110600923653692 tr(WHW.T) 3042.759033203125
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:38,  1.12it/s]  2%|▏         | 2/112 [00:01<01:05,  1.68it/s]I0316 10:04:25.901673 497511 finetune.py:68] layer 8_down @ epoch 0 new loss 1.899639391922392e-05 old loss 1.9001487089553848e-05 BETTER
  3%|▎         | 3/112 [00:01<00:54,  1.99it/s]  4%|▎         | 4/112 [00:02<00:49,  2.18it/s]  4%|▍         | 5/112 [00:02<00:46,  2.31it/s]  5%|▌         | 6/112 [00:02<00:44,  2.38it/s]  6%|▋         | 7/112 [00:03<00:43,  2.44it/s]  7%|▋         | 8/112 [00:03<00:42,  2.47it/s]  8%|▊         | 9/112 [00:04<00:41,  2.50it/s]  9%|▉         | 10/112 [00:04<00:40,  2.51it/s] 10%|▉         | 11/112 [00:04<00:40,  2.52it/s] 11%|█         | 12/112 [00:05<00:39,  2.52it/s] 12%|█▏        | 13/112 [00:05<00:39,  2.53it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.53it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.53it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.54it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.54it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.55it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.55it/s] 18%|█▊        | 20/112 [00:08<00:36,  2.55it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.55it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.55it/s] 21%|██        | 23/112 [00:09<00:34,  2.56it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.56it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.55it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.55it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.55it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.55it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.55it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.56it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.55it/s] 29%|██▊       | 32/112 [00:13<00:31,  2.56it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.56it/s] 30%|███       | 34/112 [00:13<00:30,  2.57it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.57it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.58it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.57it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.57it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.57it/s] 36%|███▌      | 40/112 [00:16<00:28,  2.56it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.55it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.55it/s] 38%|███▊      | 43/112 [00:17<00:27,  2.55it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.55it/s] 40%|████      | 45/112 [00:18<00:26,  2.55it/s] 41%|████      | 46/112 [00:18<00:26,  2.53it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.55it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.55it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.55it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.55it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.55it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.55it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.55it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.55it/s] 49%|████▉     | 55/112 [00:22<00:22,  2.55it/s] 50%|█████     | 56/112 [00:22<00:22,  2.54it/s] 51%|█████     | 57/112 [00:22<00:21,  2.53it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.54it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.54it/s] 54%|█████▎    | 60/112 [00:24<00:20,  2.52it/s] 54%|█████▍    | 61/112 [00:24<00:20,  2.53it/s]I0316 10:04:48.959622 498015 finetune.py:68] layer 9_down @ epoch 0 new loss 2.1455522073665634e-05 old loss 2.1461928554344922e-05 BETTER
 55%|█████▌    | 62/112 [00:24<00:19,  2.53it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.54it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.54it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.55it/s] 59%|█████▉    | 66/112 [00:26<00:18,  2.55it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.56it/s]I0316 10:04:51.301418 499027 finetune.py:68] layer 11_gate @ epoch 4 new loss 1.5699479263275862e-05 old loss 1.5770950994919986e-05 BETTER
 61%|██████    | 68/112 [00:27<00:17,  2.56it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.56it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.55it/s]W0316 10:04:52.524538 499027 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 63%|██████▎   | 71/112 [00:28<00:16,  2.55it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.54it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.55it/s] 66%|██████▌   | 74/112 [00:29<00:15,  2.53it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.52it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.53it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.53it/s] 70%|██████▉   | 78/112 [00:31<00:13,  2.54it/s] 71%|███████   | 79/112 [00:31<00:12,  2.55it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.55it/s]11_gate proxy err 0.0010721874423325062 tr(WHW.T) 3163.92529296875
  0%|          | 0/112 [00:00<?, ?it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.55it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.50it/s]  1%|          | 1/112 [00:00<01:43,  1.08it/s] 74%|███████▍  | 83/112 [00:33<00:11,  2.51it/s]  2%|▏         | 2/112 [00:01<01:07,  1.63it/s] 75%|███████▌  | 84/112 [00:33<00:11,  2.52it/s]  3%|▎         | 3/112 [00:01<00:55,  1.96it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.54it/s]  4%|▎         | 4/112 [00:02<00:49,  2.16it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.55it/s]  4%|▍         | 5/112 [00:02<00:46,  2.29it/s]I0316 10:04:58.959237 497511 finetune.py:68] layer 8_down @ epoch 1 new loss 1.8995482605532743e-05 old loss 1.899639391922392e-05 BETTER
 78%|███████▊  | 87/112 [00:34<00:09,  2.55it/s]  5%|▌         | 6/112 [00:02<00:44,  2.38it/s] 79%|███████▊  | 88/112 [00:35<00:09,  2.55it/s]  6%|▋         | 7/112 [00:03<00:43,  2.44it/s] 79%|███████▉  | 89/112 [00:35<00:09,  2.55it/s]  7%|▋         | 8/112 [00:03<00:42,  2.47it/s] 80%|████████  | 90/112 [00:35<00:08,  2.54it/s]  8%|▊         | 9/112 [00:04<00:41,  2.50it/s] 81%|████████▏ | 91/112 [00:36<00:08,  2.54it/s]  9%|▉         | 10/112 [00:04<00:40,  2.51it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.56it/s] 10%|▉         | 11/112 [00:04<00:40,  2.52it/s] 83%|████████▎ | 93/112 [00:37<00:07,  2.54it/s] 11%|█         | 12/112 [00:05<00:39,  2.53it/s] 84%|████████▍ | 94/112 [00:37<00:07,  2.53it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.54it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.53it/s] 12%|█▎        | 14/112 [00:06<00:38,  2.55it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.53it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.56it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.53it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.57it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.52it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.57it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.52it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.58it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.51it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.59it/s] 90%|█████████ | 101/112 [00:40<00:04,  2.51it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.58it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.53it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.59it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.53it/s] 20%|█▉        | 22/112 [00:09<00:34,  2.59it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.54it/s] 21%|██        | 23/112 [00:09<00:34,  2.58it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.53it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.58it/s] 95%|█████████▍| 106/112 [00:42<00:02,  2.53it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.57it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.53it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.57it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.53it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.56it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.53it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.56it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.52it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.53it/s] 99%|█████████▉| 111/112 [00:44<00:00,  2.52it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.54it/s]100%|██████████| 112/112 [00:44<00:00,  2.51it/s]100%|██████████| 112/112 [00:44<00:00,  2.51it/s]
 28%|██▊       | 31/112 [00:12<00:31,  2.55it/s] 29%|██▊       | 32/112 [00:13<00:31,  2.55it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.56it/s] 30%|███       | 34/112 [00:13<00:30,  2.56it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.57it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.57it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.57it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.57it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.57it/s] 36%|███▌      | 40/112 [00:16<00:27,  2.58it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.57it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.57it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.57it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.56it/s] 40%|████      | 45/112 [00:18<00:26,  2.55it/s] 41%|████      | 46/112 [00:18<00:25,  2.54it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.55it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.55it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.56it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.56it/s]W0316 10:05:16.504000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:05:16.504000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:05:16.504000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:05:16.504000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:05:16.504000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:05:16.504000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:05:16.504000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:05:16.549000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:05:16.549000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:05:16.549000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:05:16.549000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:05:16.549000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:05:16.732000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:05:16.732000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:05:16.732000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:05:16.732000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:05:16.733000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 46%|████▌     | 51/112 [00:20<00:23,  2.56it/s]W0316 10:05:17.059000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:05:17.060000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:05:17.060000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:05:17.060000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:05:17.060000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:05:17.060000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:05:17.060000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:05:17.097000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:05:17.097000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:05:17.097000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:05:17.097000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:05:17.097000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 46%|████▋     | 52/112 [00:20<00:23,  2.57it/s]W0316 10:05:17.169000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:05:17.169000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:05:17.169000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:05:17.169000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:05:17.169000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 53/112 [00:21<00:22,  2.57it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.57it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.57it/s]W0316 10:05:18.535000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 56/112 [00:22<00:21,  2.57it/s]W0316 10:05:19.002000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:05:19.002000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:05:19.002000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:05:19.002000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:05:19.002000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:05:19.002000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:05:19.002000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:05:19.033000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:05:19.034000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:05:19.034000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:05:19.034000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:05:19.034000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 51%|█████     | 57/112 [00:22<00:21,  2.57it/s]W0316 10:05:19.425000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:05:19.426000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:05:19.426000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:05:19.426000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:05:19.426000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 52%|█████▏    | 58/112 [00:23<00:21,  2.56it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.56it/s]W0316 10:05:19.982000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:05:19.988000 140506021074752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 54%|█████▎    | 60/112 [00:23<00:20,  2.56it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.56it/s]I0316 10:05:20.819561 498015 finetune.py:68] layer 9_down @ epoch 1 new loss 2.145321741409134e-05 old loss 2.1455522073665634e-05 BETTER
 55%|█████▌    | 62/112 [00:24<00:19,  2.57it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.58it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.58it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.58it/s] 59%|█████▉    | 66/112 [00:26<00:17,  2.58it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.58it/s] 61%|██████    | 68/112 [00:27<00:17,  2.57it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.57it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.58it/s] 63%|██████▎   | 71/112 [00:28<00:15,  2.57it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.57it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.57it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.56it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.55it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.55it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.55it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.56it/s]I0316 10:05:27.368151 498521 finetune.py:45] layer 10_down initial loss 2.236110594822094e-05
W0316 10:05:27.368736 498521 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 71%|███████   | 79/112 [00:31<00:12,  2.56it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.56it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.56it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.56it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.56it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.55it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.55it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.55it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.55it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.53it/s] 79%|███████▉  | 89/112 [00:35<00:09,  2.54it/s] 80%|████████  | 90/112 [00:35<00:08,  2.54it/s]I0316 10:05:32.341155 497511 finetune.py:68] layer 8_down @ epoch 2 new loss 1.8993590856553055e-05 old loss 1.8995482605532743e-05 BETTER
 81%|████████▏ | 91/112 [00:36<00:08,  2.54it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.55it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.55it/s] 84%|████████▍ | 94/112 [00:37<00:07,  2.55it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.56it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.55it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.56it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.55it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.55it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.55it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.54it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.54it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.53it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.51it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.52it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.52it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.50it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.50it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.49it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.50it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.52it/s]100%|██████████| 112/112 [00:44<00:00,  2.51it/s]100%|██████████| 112/112 [00:44<00:00,  2.53it/s]
W0316 10:05:47.973000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:05:47.973000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:05:47.974000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:05:47.974000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:05:47.974000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:05:47.974000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:05:47.974000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.020000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.020000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.020000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.020000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.021000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.206000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.206000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.206000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.206000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.206000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.534000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.534000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.534000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.534000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.534000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.535000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.535000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.568000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.569000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.569000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.569000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.569000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.641000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.641000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.641000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.641000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:05:48.642000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:05:50.035000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:05:50.526000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:05:50.526000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:05:50.526000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:05:50.526000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:05:50.526000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:05:50.526000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:05:50.527000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:05:50.560000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:05:50.560000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:05:50.560000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:05:50.560000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:05:50.560000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:05:50.956000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:05:50.956000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:05:50.956000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:05:50.956000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:05:50.956000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:05:51.513000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:05:51.518000 140242125420352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0316 10:05:52.920446 498015 finetune.py:68] layer 9_down @ epoch 2 new loss 2.1453090084833093e-05 old loss 2.145321741409134e-05 BETTER
I0316 10:05:58.576849 499027 finetune.py:45] layer 11_down initial loss 2.3455901100533083e-05
W0316 10:05:58.577423 499027 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:05:58.909342 498521 finetune.py:68] layer 10_down @ epoch 0 new loss 2.2354290194925852e-05 old loss 2.236110594822094e-05 BETTER
I0316 10:06:05.681777 497511 finetune.py:68] layer 8_down @ epoch 3 new loss 1.8992492186953314e-05 old loss 1.8993590856553055e-05 BETTER
I0316 10:06:24.566914 498015 finetune.py:68] layer 9_down @ epoch 3 new loss 2.1453060981002636e-05 old loss 2.1453090084833093e-05 BETTER
I0316 10:06:29.109944 499027 finetune.py:68] layer 11_down @ epoch 0 new loss 2.345236498513259e-05 old loss 2.3455901100533083e-05 BETTER
I0316 10:06:30.624014 498521 finetune.py:68] layer 10_down @ epoch 1 new loss 2.2353167878463864e-05 old loss 2.2354290194925852e-05 BETTER
I0316 10:06:39.013368 497511 finetune.py:68] layer 8_down @ epoch 4 new loss 1.8992113837157376e-05 old loss 1.8992492186953314e-05 BETTER
W0316 10:06:39.958815 497511 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

8_down proxy err 0.003702545538544655 tr(WHW.T) 7.264981269836426
I0316 10:06:56.401045 498015 finetune.py:76] layer 9_down @ epoch 4 new loss 2.1454552552313544e-05 old loss 2.1453060981002636e-05 WORSE
W0316 10:06:56.961210 498015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

9_down proxy err 0.003689059754833579 tr(WHW.T) 8.011770248413086
I0316 10:07:00.285634 499027 finetune.py:68] layer 11_down @ epoch 1 new loss 2.3451228116755374e-05 old loss 2.345236498513259e-05 BETTER
I0316 10:07:02.627226 498521 finetune.py:68] layer 10_down @ epoch 2 new loss 2.2352418454829603e-05 old loss 2.2353167878463864e-05 BETTER
I0316 10:07:31.442109 499027 finetune.py:68] layer 11_down @ epoch 2 new loss 2.345022039662581e-05 old loss 2.3451228116755374e-05 BETTER
I0316 10:07:34.378852 498521 finetune.py:76] layer 10_down @ epoch 3 new loss 2.2352916857926175e-05 old loss 2.2352418454829603e-05 WORSE
I0316 10:08:02.382554 499027 finetune.py:68] layer 11_down @ epoch 3 new loss 2.3447339117410593e-05 old loss 2.345022039662581e-05 BETTER
I0316 10:08:05.802124 498521 finetune.py:68] layer 10_down @ epoch 4 new loss 2.2351774532580748e-05 old loss 2.2352418454829603e-05 BETTER
W0316 10:08:06.509091 498521 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

I0316 10:08:06.575767 460347 quantize_finetune_llama.py:186] computed original embedding for layer 12 in 65.25567388534546s
I0316 10:08:06.956519 460347 quantize_finetune_llama.py:159] layer 13 gpu 1
10_down proxy err 0.00368892471306026 tr(WHW.T) 8.426718711853027
I0316 10:08:08.974716 499725 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 10:08:08.974827 499725 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 10:08:08.974890 499725 utils.py:162] NumExpr defaulting to 16 threads.
I0316 10:08:09.160202 499725 config.py:58] PyTorch version 2.4.0 available.
I0316 10:08:11.658576 499725 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 10:08:12.326669 499725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:55,  1.80s/it]  6%|▋         | 2/32 [00:02<00:28,  1.06it/s]  9%|▉         | 3/32 [00:02<00:19,  1.46it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.80it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.07it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.28it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.66it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.72it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.76it/s] 41%|████      | 13/32 [00:06<00:06,  2.81it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.82it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.84it/s] 50%|█████     | 16/32 [00:07<00:05,  2.85it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.81it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.83it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.85it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.84it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.84it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.85it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.85it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.86it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.87it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
W0316 10:08:29.773000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:08:29.773000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:08:29.773000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:08:29.773000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:08:29.773000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:08:29.774000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:08:29.774000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:08:29.801000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:08:29.802000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:08:29.802000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:08:29.802000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:08:29.802000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.109000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.109000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.109000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.109000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.109000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.754000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.754000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.754000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.754000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.754000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.754000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.754000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.773000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.773000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.773000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.773000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.773000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.984000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.985000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.985000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.985000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:08:30.985000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:08:32.192000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:08:32.193000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:08:32.193000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:08:32.193000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:08:32.193000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:08:32.193000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:08:32.193000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:08:32.212000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:08:32.212000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:08:32.212000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:08:32.212000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:08:32.212000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:08:32.869000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:08:32.869000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:08:32.869000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:08:32.869000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:08:32.869000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
I0316 10:08:33.543467 499027 finetune.py:76] layer 11_down @ epoch 4 new loss 2.3447952116839588e-05 old loss 2.3447339117410593e-05 WORSE
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0316 10:08:34.108493 499027 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

11_down proxy err 0.0035749676171690226 tr(WHW.T) 9.077404975891113
I0316 10:08:39.570403 499725 finetune.py:45] layer 12_v initial loss 4.2031829252664465e-06
W0316 10:08:39.570601 499725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:09:13.365824 460347 quantize_finetune_llama.py:186] computed original embedding for layer 13 in 61.77156352996826s
I0316 10:09:13.771433 460347 quantize_finetune_llama.py:159] layer 14 gpu 2
I0316 10:09:15.836821 500229 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 10:09:15.836947 500229 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 10:09:15.837023 500229 utils.py:162] NumExpr defaulting to 16 threads.
I0316 10:09:16.026006 500229 config.py:58] PyTorch version 2.4.0 available.
I0316 10:09:16.093488 499725 finetune.py:68] layer 12_v @ epoch 0 new loss 2.684391802176833e-06 old loss 4.2031829252664465e-06 BETTER
I0316 10:09:18.216189 500229 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 10:09:18.560982 500229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:03,  2.03s/it]  6%|▋         | 2/32 [00:02<00:31,  1.04s/it]  9%|▉         | 3/32 [00:02<00:21,  1.36it/s] 12%|█▎        | 4/32 [00:03<00:16,  1.72it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.02it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.26it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.56it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.66it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.71it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.78it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.82it/s] 41%|████      | 13/32 [00:06<00:06,  2.86it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.87it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.88it/s] 50%|█████     | 16/32 [00:07<00:05,  2.91it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.88it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.88it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.91it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.90it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.90it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.92it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.91it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.88it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.90it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.89it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.89it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.91it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.92it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.91it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.93it/s]100%|██████████| 32/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
W0316 10:09:35.182000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:09:35.182000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:09:35.182000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:09:35.182000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:09:35.182000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:09:35.182000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:09:35.182000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:09:35.207000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:09:35.207000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:09:35.208000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:09:35.208000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:09:35.208000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:09:35.507000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:09:35.507000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:09:35.507000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:09:35.507000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:09:35.507000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:09:36.118000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:09:36.118000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:09:36.118000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:09:36.118000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:09:36.118000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:09:36.118000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:09:36.118000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:09:36.136000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:09:36.136000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:09:36.136000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:09:36.136000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:09:36.136000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:09:36.340000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:09:36.340000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:09:36.340000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:09:36.340000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:09:36.340000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:09:37.511000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:09:37.511000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:09:37.511000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:09:37.511000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:09:37.511000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:09:37.512000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:09:37.512000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:09:37.530000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:09:37.530000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:09:37.530000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:09:37.530000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:09:37.530000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:09:38.156000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:09:38.156000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:09:38.156000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:09:38.156000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:09:38.156000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 10:09:44.436158 500229 finetune.py:45] layer 13_v initial loss 3.821811787929619e-06
W0316 10:09:44.437052 500229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:09:53.565325 499725 finetune.py:68] layer 12_v @ epoch 1 new loss 2.52028985414654e-06 old loss 2.684391802176833e-06 BETTER
I0316 10:10:16.293337 460347 quantize_finetune_llama.py:186] computed original embedding for layer 14 in 62.07438278198242s
I0316 10:10:16.681823 460347 quantize_finetune_llama.py:159] layer 15 gpu 3
I0316 10:10:18.729077 500735 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 10:10:18.729202 500735 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 10:10:18.729265 500735 utils.py:162] NumExpr defaulting to 16 threads.
I0316 10:10:18.763611 500229 finetune.py:68] layer 13_v @ epoch 0 new loss 2.4433938961010426e-06 old loss 3.821811787929619e-06 BETTER
I0316 10:10:18.919780 500735 config.py:58] PyTorch version 2.4.0 available.
I0316 10:10:21.150693 500735 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 10:10:21.507954 500735 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it]  6%|▋         | 2/32 [00:02<00:30,  1.03s/it]  9%|▉         | 3/32 [00:02<00:21,  1.38it/s] 12%|█▎        | 4/32 [00:03<00:16,  1.73it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.01it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.22it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.38it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.45it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.56it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.62it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.67it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.71it/s] 41%|████      | 13/32 [00:06<00:06,  2.73it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.75it/s] 47%|████▋     | 15/32 [00:07<00:06,  2.74it/s] 50%|█████     | 16/32 [00:07<00:05,  2.80it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.85it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.76it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.75it/s]I0316 10:10:31.605644 499725 finetune.py:76] layer 12_v @ epoch 2 new loss 2.6961902221955825e-06 old loss 2.52028985414654e-06 WORSE
 66%|██████▌   | 21/32 [00:09<00:03,  2.75it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.72it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.72it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.74it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.74it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.75it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.75it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.76it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.73it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.74it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:13<00:00,  2.75it/s]100%|██████████| 32/32 [00:13<00:00,  2.42it/s]
W0316 10:10:38.610000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:10:38.610000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:10:38.610000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:10:38.611000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:10:38.611000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:10:38.611000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:10:38.611000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:10:38.636000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:10:38.636000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:10:38.636000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:10:38.637000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:10:38.637000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:10:38.932000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:10:38.933000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:10:38.933000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:10:38.933000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:10:38.933000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:10:39.581000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:10:39.581000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:10:39.581000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:10:39.581000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:10:39.582000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:10:39.582000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:10:39.582000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:10:39.599000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:10:39.599000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:10:39.600000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:10:39.600000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:10:39.600000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:10:39.806000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:10:39.806000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:10:39.807000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:10:39.807000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:10:39.807000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:10:40.999000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:10:41.000000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:10:41.000000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:10:41.000000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:10:41.000000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:10:41.000000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:10:41.000000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:10:41.018000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:10:41.018000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:10:41.019000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:10:41.019000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:10:41.019000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:10:41.668000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:10:41.668000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:10:41.668000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:10:41.668000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:10:41.668000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 10:10:47.916388 500735 finetune.py:45] layer 14_v initial loss 3.689717004817794e-06
W0316 10:10:47.916783 500735 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:10:54.242245 500229 finetune.py:68] layer 13_v @ epoch 1 new loss 2.3061268166202353e-06 old loss 2.4433938961010426e-06 BETTER
I0316 10:11:08.875054 499725 finetune.py:76] layer 12_v @ epoch 3 new loss 2.6664297365641687e-06 old loss 2.52028985414654e-06 WORSE
I0316 10:11:18.074712 460347 quantize_finetune_llama.py:186] computed original embedding for layer 15 in 60.88624572753906s
I0316 10:11:18.453301 460347 quantize_finetune_llama.py:159] layer 16 gpu 0
I0316 10:11:20.534611 501241 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 10:11:20.534770 501241 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 10:11:20.534836 501241 utils.py:162] NumExpr defaulting to 16 threads.
I0316 10:11:20.752144 501241 config.py:58] PyTorch version 2.4.0 available.
I0316 10:11:22.590678 500735 finetune.py:68] layer 14_v @ epoch 0 new loss 2.5536467092024395e-06 old loss 3.689717004817794e-06 BETTER
I0316 10:11:23.037038 501241 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 10:11:23.439897 501241 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:02,  2.00s/it]  6%|▋         | 2/32 [00:02<00:30,  1.03s/it]  9%|▉         | 3/32 [00:02<00:20,  1.38it/s] 12%|█▎        | 4/32 [00:03<00:16,  1.74it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.02it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.24it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.40it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.61it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.68it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.73it/s]I0316 10:11:30.211109 500229 finetune.py:76] layer 13_v @ epoch 2 new loss 2.391695943515515e-06 old loss 2.3061268166202353e-06 WORSE
 38%|███▊      | 12/32 [00:05<00:07,  2.75it/s] 41%|████      | 13/32 [00:06<00:06,  2.78it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.81it/s] 50%|█████     | 16/32 [00:07<00:05,  2.83it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.83it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.83it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.83it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.83it/s] 66%|██████▌   | 21/32 [00:09<00:03,  2.84it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.83it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.84it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.83it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.84it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.84it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.84it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.84it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.84it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
W0316 10:11:40.272000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:11:40.272000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:11:40.272000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:11:40.272000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:11:40.272000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:11:40.272000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:11:40.272000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:11:40.299000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:11:40.299000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:11:40.299000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:11:40.300000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:11:40.300000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:11:40.599000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:11:40.599000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:11:40.600000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:11:40.600000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:11:40.600000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:11:41.236000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:11:41.237000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:11:41.237000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:11:41.237000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:11:41.237000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:11:41.237000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:11:41.237000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:11:41.255000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:11:41.255000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:11:41.255000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:11:41.255000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:11:41.256000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:11:41.466000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:11:41.466000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:11:41.466000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:11:41.466000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:11:41.466000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:11:42.654000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:11:42.654000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:11:42.654000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:11:42.654000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:11:42.655000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:11:42.655000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:11:42.655000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:11:42.673000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:11:42.673000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:11:42.673000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:11:42.673000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:11:42.673000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:11:43.320000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:11:43.320000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:11:43.320000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:11:43.320000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:11:43.320000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 10:11:46.332278 499725 finetune.py:76] layer 12_v @ epoch 4 new loss 3.7868016988795716e-06 old loss 2.52028985414654e-06 WORSE
W0316 10:11:47.575381 499725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_v proxy err 0.0027501918375492096 tr(WHW.T) 68.45219421386719
  0%|          | 0/32 [00:00<?, ?it/s]I0316 10:11:49.797617 501241 finetune.py:45] layer 15_v initial loss 6.8534704951161984e-06
W0316 10:11:49.797868 501241 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:41,  1.35s/it]  6%|▋         | 2/32 [00:01<00:23,  1.28it/s]  9%|▉         | 3/32 [00:02<00:17,  1.66it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.94it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.14it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.27it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.38it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.45it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.51it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.56it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.61it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.62it/s] 50%|█████     | 16/32 [00:07<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.61it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.61it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.63it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s]I0316 10:11:58.697863 500735 finetune.py:68] layer 14_v @ epoch 1 new loss 2.4226135337812593e-06 old loss 2.5536467092024395e-06 BETTER
 75%|███████▌  | 24/32 [00:10<00:03,  2.64it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.64it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.64it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.63it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.64it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.62it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:13<00:00,  2.63it/s]100%|██████████| 32/32 [00:13<00:00,  2.43it/s]
I0316 10:12:05.698024 500229 finetune.py:68] layer 13_v @ epoch 3 new loss 2.2896779228176456e-06 old loss 2.3061268166202353e-06 BETTER
W0316 10:12:08.183000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.184000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.184000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.184000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.184000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.184000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.184000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.214000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.214000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.214000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.214000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.214000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.380000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.381000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.381000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.381000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.381000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.609000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.609000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.609000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.609000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.609000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.609000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.609000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.632000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.632000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.632000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.632000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.632000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.696000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.696000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.697000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.697000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:12:08.697000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:12:09.753000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:12:10.074000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:12:10.075000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:12:10.075000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:12:10.075000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:12:10.075000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:12:10.075000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:12:10.075000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:12:10.098000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:12:10.098000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:12:10.098000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:12:10.098000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:12:10.098000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:12:10.353000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:12:10.353000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:12:10.353000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:12:10.353000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:12:10.353000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:12:10.820000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 10:12:17.960416 499725 finetune.py:45] layer 12_q initial loss 3.884062152792467e-06
W0316 10:12:17.960874 499725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:12:23.655967 501241 finetune.py:68] layer 15_v @ epoch 0 new loss 3.514864829412545e-06 old loss 6.8534704951161984e-06 BETTER
I0316 10:12:35.051675 500735 finetune.py:68] layer 14_v @ epoch 2 new loss 2.3724687707726844e-06 old loss 2.4226135337812593e-06 BETTER
I0316 10:12:42.181621 500229 finetune.py:76] layer 13_v @ epoch 4 new loss 3.078858071603463e-06 old loss 2.2896779228176456e-06 WORSE
W0316 10:12:43.314795 500229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

13_v proxy err 0.00287695974111557 tr(WHW.T) 71.04966735839844
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.33s/it]  6%|▋         | 2/32 [00:01<00:23,  1.29it/s]  9%|▉         | 3/32 [00:02<00:17,  1.69it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.97it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.17it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.31it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.40it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.47it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s] 41%|████      | 13/32 [00:05<00:07,  2.63it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.64it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.65it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.64it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.63it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.64it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.66it/s]I0316 10:12:54.787950 499725 finetune.py:68] layer 12_q @ epoch 0 new loss 3.747114305951982e-06 old loss 3.884062152792467e-06 BETTER
 78%|███████▊  | 25/32 [00:10<00:02,  2.67it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.68it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.68it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.66it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:13<00:00,  2.64it/s]100%|██████████| 32/32 [00:13<00:00,  2.46it/s]
I0316 10:12:59.024537 501241 finetune.py:68] layer 15_v @ epoch 1 new loss 3.2260224998026388e-06 old loss 3.514864829412545e-06 BETTER
W0316 10:13:03.536000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.536000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.536000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.536000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.536000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.536000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.536000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.567000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.567000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.567000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.567000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.567000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.737000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.737000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.737000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.737000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.737000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.962000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.962000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.962000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.962000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.963000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.963000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.963000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.985000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.985000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.985000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.985000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:13:03.985000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:13:04.050000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:13:04.050000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:13:04.050000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:13:04.050000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:13:04.050000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:13:05.107000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:13:05.427000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:13:05.427000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:13:05.427000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:13:05.427000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:13:05.427000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:13:05.428000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:13:05.428000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:13:05.452000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:13:05.452000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:13:05.452000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:13:05.452000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:13:05.452000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:13:05.705000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:13:05.705000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:13:05.706000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:13:05.706000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:13:05.706000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:13:06.174000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 10:13:11.428393 500735 finetune.py:76] layer 14_v @ epoch 3 new loss 2.728050276346039e-06 old loss 2.3724687707726844e-06 WORSE
I0316 10:13:13.081158 500229 finetune.py:45] layer 13_q initial loss 3.919435584975872e-06
W0316 10:13:13.081603 500229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:13:32.524372 499725 finetune.py:76] layer 12_q @ epoch 1 new loss 3.7709362459281692e-06 old loss 3.747114305951982e-06 WORSE
I0316 10:13:34.714354 501241 finetune.py:68] layer 15_v @ epoch 2 new loss 3.114707851636922e-06 old loss 3.2260224998026388e-06 BETTER
I0316 10:13:47.560406 500735 finetune.py:76] layer 14_v @ epoch 4 new loss 2.432470409985399e-06 old loss 2.3724687707726844e-06 WORSE
I0316 10:13:48.804648 500229 finetune.py:68] layer 13_q @ epoch 0 new loss 3.7201621125859674e-06 old loss 3.919435584975872e-06 BETTER
W0316 10:13:48.820107 500735 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_v proxy err 0.0026320805773139 tr(WHW.T) 74.802978515625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:45,  1.48s/it]  6%|▋         | 2/32 [00:01<00:25,  1.18it/s]  9%|▉         | 3/32 [00:02<00:18,  1.56it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.84it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.03it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.17it/s] 22%|██▏       | 7/32 [00:03<00:11,  2.26it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.32it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.37it/s] 31%|███▏      | 10/32 [00:05<00:09,  2.40it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.43it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.45it/s] 41%|████      | 13/32 [00:06<00:07,  2.46it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.48it/s] 47%|████▋     | 15/32 [00:07<00:06,  2.49it/s] 50%|█████     | 16/32 [00:07<00:06,  2.49it/s] 53%|█████▎    | 17/32 [00:07<00:06,  2.50it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.49it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.49it/s] 62%|██████▎   | 20/32 [00:09<00:04,  2.48it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.48it/s] 69%|██████▉   | 22/32 [00:09<00:04,  2.49it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.49it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.51it/s] 78%|███████▊  | 25/32 [00:11<00:02,  2.51it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.51it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.52it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.52it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.53it/s] 94%|█████████▍| 30/32 [00:13<00:00,  2.54it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.52it/s]100%|██████████| 32/32 [00:13<00:00,  2.52it/s]100%|██████████| 32/32 [00:13<00:00,  2.31it/s]
I0316 10:14:09.728186 499725 finetune.py:68] layer 12_q @ epoch 2 new loss 3.6099179396842374e-06 old loss 3.747114305951982e-06 BETTER
W0316 10:14:10.028000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.028000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.029000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.029000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.029000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.029000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.029000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.060000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.060000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.060000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.060000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.060000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
I0316 10:14:10.224534 501241 finetune.py:68] layer 15_v @ epoch 3 new loss 3.0511928343912587e-06 old loss 3.114707851636922e-06 BETTER
W0316 10:14:10.226000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.226000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.226000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.227000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.227000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.453000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.453000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.453000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.454000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.454000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.454000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.454000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.478000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.478000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.478000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.478000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.478000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.545000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.545000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.545000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.545000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:14:10.545000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:14:11.614000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:14:11.940000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:14:11.940000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:14:11.940000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:14:11.941000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:14:11.941000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:14:11.941000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:14:11.941000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:14:11.962000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:14:11.964000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:14:11.964000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:14:11.964000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:14:11.964000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:14:12.218000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:14:12.218000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:14:12.218000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:14:12.218000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:14:12.219000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:14:12.690000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 10:14:19.392446 500735 finetune.py:45] layer 14_q initial loss 4.498711859923787e-06
W0316 10:14:19.392867 500735 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:14:24.944221 500229 finetune.py:68] layer 13_q @ epoch 1 new loss 3.638701173258596e-06 old loss 3.7201621125859674e-06 BETTER
I0316 10:14:46.333311 501241 finetune.py:76] layer 15_v @ epoch 4 new loss 3.1867432426224696e-06 old loss 3.0511928343912587e-06 WORSE
W0316 10:14:47.420315 501241 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 10:14:47.852727 499725 finetune.py:76] layer 12_q @ epoch 3 new loss 3.978939275839366e-06 old loss 3.6099179396842374e-06 WORSE
15_v proxy err 0.0034280496183782816 tr(WHW.T) 67.86465454101562
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.35s/it]  6%|▋         | 2/32 [00:01<00:23,  1.27it/s]  9%|▉         | 3/32 [00:02<00:17,  1.65it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.92it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.11it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.25it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.36it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.43it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.48it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.54it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:06<00:07,  2.56it/s]I0316 10:14:54.835669 500735 finetune.py:68] layer 14_q @ epoch 0 new loss 4.343547971075168e-06 old loss 4.498711859923787e-06 BETTER
 44%|████▍     | 14/32 [00:06<00:07,  2.57it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:07<00:06,  2.58it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.58it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.60it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.60it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.60it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.59it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.57it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.58it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.59it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]I0316 10:15:01.822273 500229 finetune.py:76] layer 13_q @ epoch 2 new loss 3.6708411244035233e-06 old loss 3.638701173258596e-06 WORSE
100%|██████████| 32/32 [00:13<00:00,  2.60it/s]100%|██████████| 32/32 [00:13<00:00,  2.40it/s]
W0316 10:15:07.895000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:15:07.895000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:15:07.896000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:15:07.896000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:15:07.896000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:15:07.896000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:15:07.896000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:15:07.926000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:15:07.926000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:15:07.926000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:15:07.926000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:15:07.926000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.093000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.094000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.094000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.094000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.094000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.335000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.335000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.335000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.335000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.335000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.335000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.335000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.356000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.356000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.356000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.356000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.356000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.423000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.423000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.423000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.423000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:15:08.423000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:15:09.492000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:15:09.820000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:15:09.820000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:15:09.820000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:15:09.820000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:15:09.820000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:15:09.820000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:15:09.820000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:15:09.844000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:15:09.844000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:15:09.844000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:15:09.844000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:15:09.844000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:15:10.100000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:15:10.100000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:15:10.100000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:15:10.100000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:15:10.100000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:15:10.570000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 10:15:17.152370 501241 finetune.py:45] layer 15_q initial loss 4.614606041286606e-06
W0316 10:15:17.152781 501241 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:15:25.400750 499725 finetune.py:76] layer 12_q @ epoch 4 new loss 3.6748369893757626e-06 old loss 3.6099179396842374e-06 WORSE
W0316 10:15:26.765400 499725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_q proxy err 0.0003944222698919475 tr(WHW.T) 6420.396484375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it]  6%|▋         | 2/32 [00:01<00:22,  1.33it/s]  9%|▉         | 3/32 [00:02<00:16,  1.72it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.99it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.16it/s]I0316 10:15:31.168027 500735 finetune.py:68] layer 14_q @ epoch 1 new loss 4.245309810357867e-06 old loss 4.343547971075168e-06 BETTER
 19%|█▉        | 6/32 [00:03<00:11,  2.31it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.42it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.48it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.53it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.60it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.63it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.65it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.65it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.65it/s]I0316 10:15:37.274812 500229 finetune.py:76] layer 13_q @ epoch 3 new loss 3.648009396783891e-06 old loss 3.638701173258596e-06 WORSE
 72%|███████▏  | 23/32 [00:09<00:03,  2.66it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.67it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.66it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.67it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.66it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
I0316 10:15:48.410730 499725 finetune.py:45] layer 12_k initial loss 4.87970055473852e-06
W0316 10:15:48.411290 499725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:15:52.049289 501241 finetune.py:68] layer 15_q @ epoch 0 new loss 4.524596079136245e-06 old loss 4.614606041286606e-06 BETTER
I0316 10:16:07.679813 500735 finetune.py:68] layer 14_q @ epoch 2 new loss 4.153584086452611e-06 old loss 4.245309810357867e-06 BETTER
I0316 10:16:13.087988 500229 finetune.py:76] layer 13_q @ epoch 4 new loss 3.889893832820235e-06 old loss 3.638701173258596e-06 WORSE
W0316 10:16:14.215753 500229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

13_q proxy err 0.000621593149844557 tr(WHW.T) 5300.38525390625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it]  6%|▋         | 2/32 [00:01<00:22,  1.34it/s]  9%|▉         | 3/32 [00:02<00:16,  1.73it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.01it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.21it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.36it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.59it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.66it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.65it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.69it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.71it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.71it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.72it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.71it/s]I0316 10:16:25.640257 499725 finetune.py:68] layer 12_k @ epoch 0 new loss 4.781255938723916e-06 old loss 4.87970055473852e-06 BETTER
 81%|████████▏ | 26/32 [00:10<00:02,  2.72it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.71it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.71it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.71it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.71it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.71it/s]I0316 10:16:27.768677 501241 finetune.py:68] layer 15_q @ epoch 1 new loss 4.426386112754699e-06 old loss 4.524596079136245e-06 BETTER
100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
I0316 10:16:35.000229 500229 finetune.py:45] layer 13_k initial loss 4.575524144456722e-06
W0316 10:16:35.000584 500229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:16:44.292319 500735 finetune.py:68] layer 14_q @ epoch 3 new loss 4.127487045479938e-06 old loss 4.153584086452611e-06 BETTER
I0316 10:17:03.851290 501241 finetune.py:76] layer 15_q @ epoch 2 new loss 4.4696807890431955e-06 old loss 4.426386112754699e-06 WORSE
I0316 10:17:04.021806 499725 finetune.py:68] layer 12_k @ epoch 1 new loss 4.708185315394076e-06 old loss 4.781255938723916e-06 BETTER
I0316 10:17:10.412518 500229 finetune.py:68] layer 13_k @ epoch 0 new loss 4.44906618213281e-06 old loss 4.575524144456722e-06 BETTER
I0316 10:17:20.976939 500735 finetune.py:68] layer 14_q @ epoch 4 new loss 4.122639438719489e-06 old loss 4.127487045479938e-06 BETTER
W0316 10:17:22.601081 500735 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_q proxy err 0.0005587742780335248 tr(WHW.T) 5552.7578125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.32s/it]  6%|▋         | 2/32 [00:01<00:23,  1.28it/s]  9%|▉         | 3/32 [00:02<00:17,  1.64it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.89it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.08it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.21it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.32it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.37it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.42it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.46it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.49it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.49it/s] 41%|████      | 13/32 [00:06<00:07,  2.50it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.51it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.49it/s] 50%|█████     | 16/32 [00:07<00:06,  2.50it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.51it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.52it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.52it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.52it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.52it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.53it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.53it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.52it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.52it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.52it/s] 84%|████████▍ | 27/32 [00:11<00:02,  2.50it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.50it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.51it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.52it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.52it/s]100%|██████████| 32/32 [00:13<00:00,  2.51it/s]100%|██████████| 32/32 [00:13<00:00,  2.34it/s]
I0316 10:17:39.536011 501241 finetune.py:68] layer 15_q @ epoch 3 new loss 4.326885573391337e-06 old loss 4.426386112754699e-06 BETTER
I0316 10:17:42.180018 499725 finetune.py:68] layer 12_k @ epoch 2 new loss 4.627159341907827e-06 old loss 4.708185315394076e-06 BETTER
I0316 10:17:44.828590 500735 finetune.py:45] layer 14_k initial loss 5.794297067041043e-06
W0316 10:17:44.829021 500735 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:17:46.836316 500229 finetune.py:68] layer 13_k @ epoch 1 new loss 4.396863005240448e-06 old loss 4.44906618213281e-06 BETTER
I0316 10:18:16.024586 501241 finetune.py:68] layer 15_q @ epoch 4 new loss 4.31701209890889e-06 old loss 4.326885573391337e-06 BETTER
W0316 10:18:17.890963 501241 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

15_q proxy err 0.0005635324632748961 tr(WHW.T) 6709.3623046875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:40,  1.31s/it]  6%|▋         | 2/32 [00:01<00:22,  1.31it/s]I0316 10:18:20.931211 499725 finetune.py:68] layer 12_k @ epoch 3 new loss 4.475329205888556e-06 old loss 4.627159341907827e-06 BETTER
I0316 10:18:21.072158 500735 finetune.py:68] layer 14_k @ epoch 0 new loss 5.480750132846879e-06 old loss 5.794297067041043e-06 BETTER
  9%|▉         | 3/32 [00:02<00:17,  1.70it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.98it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.18it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.41it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.58it/s]I0316 10:18:23.993007 500229 finetune.py:68] layer 13_k @ epoch 2 new loss 4.38878441855195e-06 old loss 4.396863005240448e-06 BETTER
 34%|███▍      | 11/32 [00:05<00:08,  2.62it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.64it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.66it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.64it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.65it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.67it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.68it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.68it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
I0316 10:18:38.898479 501241 finetune.py:45] layer 15_k initial loss 5.447752300824504e-06
W0316 10:18:38.898852 501241 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:18:58.069493 500735 finetune.py:68] layer 14_k @ epoch 1 new loss 5.4346864999388345e-06 old loss 5.480750132846879e-06 BETTER
I0316 10:18:59.384119 499725 finetune.py:76] layer 12_k @ epoch 4 new loss 4.533436367637478e-06 old loss 4.475329205888556e-06 WORSE
W0316 10:19:00.528226 499725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 10:19:00.740552 500229 finetune.py:76] layer 13_k @ epoch 3 new loss 4.596321559802163e-06 old loss 4.38878441855195e-06 WORSE
12_k proxy err 0.00035401599598117173 tr(WHW.T) 4338.8623046875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.00it/s]  6%|▋         | 2/32 [00:01<00:19,  1.57it/s]  9%|▉         | 3/32 [00:01<00:15,  1.92it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.13it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.27it/s] 19%|█▉        | 6/32 [00:02<00:11,  2.36it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.48it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.57it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.61it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.57it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.58it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.58it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.60it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.61it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.59it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.60it/s]I0316 10:19:14.080757 501241 finetune.py:68] layer 15_k @ epoch 0 new loss 5.33750335307559e-06 old loss 5.447752300824504e-06 BETTER
 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
I0316 10:19:22.396285 499725 finetune.py:45] layer 12_o initial loss 7.838780220481567e-06
W0316 10:19:22.396689 499725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:19:35.053372 500735 finetune.py:68] layer 14_k @ epoch 2 new loss 5.416219664766686e-06 old loss 5.4346864999388345e-06 BETTER
I0316 10:19:36.946090 500229 finetune.py:76] layer 13_k @ epoch 4 new loss 4.576117135002278e-06 old loss 4.38878441855195e-06 WORSE
W0316 10:19:38.039697 500229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

13_k proxy err 0.0004645470471587032 tr(WHW.T) 4513.53173828125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.23it/s]  6%|▋         | 2/32 [00:01<00:16,  1.80it/s]  9%|▉         | 3/32 [00:01<00:13,  2.12it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.31it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.62it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.64it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.67it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.67it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.68it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.67it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.66it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.67it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.66it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s]I0316 10:19:49.594900 501241 finetune.py:68] layer 15_k @ epoch 1 new loss 5.2411651267902926e-06 old loss 5.33750335307559e-06 BETTER
 84%|████████▍ | 27/32 [00:10<00:01,  2.67it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.68it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.68it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
I0316 10:19:59.050451 500229 finetune.py:45] layer 13_o initial loss 8.486612387059722e-06
W0316 10:19:59.050921 500229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:19:59.325557 499725 finetune.py:68] layer 12_o @ epoch 0 new loss 7.652630301890895e-06 old loss 7.838780220481567e-06 BETTER
I0316 10:20:11.853919 500735 finetune.py:68] layer 14_k @ epoch 3 new loss 5.388281806517625e-06 old loss 5.416219664766686e-06 BETTER
I0316 10:20:25.990546 501241 finetune.py:68] layer 15_k @ epoch 2 new loss 5.2131363190710545e-06 old loss 5.2411651267902926e-06 BETTER
I0316 10:20:35.100677 500229 finetune.py:68] layer 13_o @ epoch 0 new loss 8.265370524895843e-06 old loss 8.486612387059722e-06 BETTER
I0316 10:20:37.207557 499725 finetune.py:68] layer 12_o @ epoch 1 new loss 7.549420843133703e-06 old loss 7.652630301890895e-06 BETTER
I0316 10:20:48.627580 500735 finetune.py:76] layer 14_k @ epoch 4 new loss 5.570592293224763e-06 old loss 5.388281806517625e-06 WORSE
W0316 10:20:49.722853 500735 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_k proxy err 0.0004098455247003585 tr(WHW.T) 4947.99267578125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.22it/s]  6%|▋         | 2/32 [00:01<00:17,  1.75it/s]  9%|▉         | 3/32 [00:01<00:14,  2.03it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.20it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.31it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.47it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.49it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.52it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.52it/s] 41%|████      | 13/32 [00:05<00:07,  2.52it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.52it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.52it/s] 50%|█████     | 16/32 [00:06<00:06,  2.51it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.51it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.52it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.53it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.53it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.54it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.54it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.54it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.54it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.54it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.53it/s]I0316 10:21:01.727158 501241 finetune.py:68] layer 15_k @ epoch 3 new loss 5.179322215553839e-06 old loss 5.2131363190710545e-06 BETTER
 84%|████████▍ | 27/32 [00:11<00:01,  2.53it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.53it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.53it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.54it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.54it/s]100%|██████████| 32/32 [00:13<00:00,  2.55it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
I0316 10:21:11.375092 500735 finetune.py:45] layer 14_o initial loss 9.647228580433875e-06
W0316 10:21:11.375572 500735 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:21:11.554831 500229 finetune.py:68] layer 13_o @ epoch 1 new loss 8.152273949235678e-06 old loss 8.265370524895843e-06 BETTER
I0316 10:21:15.205382 499725 finetune.py:68] layer 12_o @ epoch 2 new loss 7.464524060196709e-06 old loss 7.549420843133703e-06 BETTER
I0316 10:21:37.724906 501241 finetune.py:68] layer 15_k @ epoch 4 new loss 5.1441325013001915e-06 old loss 5.179322215553839e-06 BETTER
W0316 10:21:39.311030 501241 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

15_k proxy err 0.00043446136987768114 tr(WHW.T) 4506.8935546875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.24it/s]  6%|▋         | 2/32 [00:01<00:16,  1.79it/s]  9%|▉         | 3/32 [00:01<00:13,  2.09it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.28it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.50it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.55it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.57it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.58it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.59it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s]I0316 10:21:47.438423 500735 finetune.py:68] layer 14_o @ epoch 0 new loss 9.421711183676962e-06 old loss 9.647228580433875e-06 BETTER
 53%|█████▎    | 17/32 [00:06<00:05,  2.61it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.61it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.61it/s]I0316 10:21:48.331370 500229 finetune.py:68] layer 13_o @ epoch 2 new loss 8.064448593358975e-06 old loss 8.152273949235678e-06 BETTER
 62%|██████▎   | 20/32 [00:08<00:04,  2.61it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.61it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.61it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.58it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.59it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.59it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]I0316 10:21:53.171020 499725 finetune.py:68] layer 12_o @ epoch 3 new loss 7.397859462798806e-06 old loss 7.464524060196709e-06 BETTER
100%|██████████| 32/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
I0316 10:22:00.600665 501241 finetune.py:45] layer 15_o initial loss 9.65792150964262e-06
W0316 10:22:00.601237 501241 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:22:24.542404 500735 finetune.py:68] layer 14_o @ epoch 1 new loss 9.246357876691036e-06 old loss 9.421711183676962e-06 BETTER
I0316 10:22:25.482263 500229 finetune.py:68] layer 13_o @ epoch 3 new loss 7.989283403730951e-06 old loss 8.064448593358975e-06 BETTER
I0316 10:22:31.491654 499725 finetune.py:68] layer 12_o @ epoch 4 new loss 7.3405690272920765e-06 old loss 7.397859462798806e-06 BETTER
W0316 10:22:33.564096 499725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_o proxy err 0.0031941758934408426 tr(WHW.T) 5.630328178405762
  0%|          | 0/32 [00:00<?, ?it/s]I0316 10:22:35.448407 501241 finetune.py:68] layer 15_o @ epoch 0 new loss 9.414311534783337e-06 old loss 9.65792150964262e-06 BETTER
  3%|▎         | 1/32 [00:02<01:03,  2.05s/it]  6%|▋         | 2/32 [00:03<00:52,  1.75s/it]  9%|▉         | 3/32 [00:05<00:48,  1.66s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.61s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.56s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it] 31%|███▏      | 10/32 [00:15<00:34,  1.55s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:19<00:30,  1.54s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it] 44%|████▍     | 14/32 [00:22<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.53s/it] 50%|█████     | 16/32 [00:25<00:24,  1.53s/it]I0316 10:23:00.983180 500735 finetune.py:68] layer 14_o @ epoch 2 new loss 9.146227966994047e-06 old loss 9.246357876691036e-06 BETTER
I0316 10:23:01.604596 500229 finetune.py:68] layer 13_o @ epoch 4 new loss 7.930364517960697e-06 old loss 7.989283403730951e-06 BETTER
 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it]W0316 10:23:03.129773 500229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 56%|█████▋    | 18/32 [00:28<00:21,  1.52s/it]13_o proxy err 0.003195239929482341 tr(WHW.T) 6.751659393310547
  0%|          | 0/32 [00:00<?, ?it/s] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.52s/it]  3%|▎         | 1/32 [00:02<01:02,  2.01s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.52s/it]  9%|▉         | 3/32 [00:05<00:46,  1.61s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.57s/it]I0316 10:23:11.016688 501241 finetune.py:68] layer 15_o @ epoch 1 new loss 9.25792755879229e-06 old loss 9.414311534783337e-06 BETTER
 75%|███████▌  | 24/32 [00:37<00:12,  1.51s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.52s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.50s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.52s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.50s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.53s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.52s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.53s/it] 41%|████      | 13/32 [00:19<00:28,  1.49s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.50s/it]I0316 10:23:33.390502 499725 finetune.py:45] layer 12_up initial loss 1.454326047678478e-05
W0316 10:23:33.390792 499725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it]I0316 10:23:37.426972 500735 finetune.py:68] layer 14_o @ epoch 3 new loss 9.059155672730412e-06 old loss 9.146227966994047e-06 BETTER
 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.48s/it]I0316 10:23:46.747672 501241 finetune.py:68] layer 15_o @ epoch 2 new loss 9.15713553695241e-06 old loss 9.25792755879229e-06 BETTER
 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.48s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.48s/it]100%|██████████| 32/32 [00:48<00:00,  1.49s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]
I0316 10:24:00.832785 500229 finetune.py:45] layer 13_up initial loss 1.6168769434443675e-05
W0316 10:24:00.833178 500229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:24:09.527776 499725 finetune.py:68] layer 12_up @ epoch 0 new loss 1.430445809091907e-05 old loss 1.454326047678478e-05 BETTER
I0316 10:24:14.039030 500735 finetune.py:68] layer 14_o @ epoch 4 new loss 8.987347428046633e-06 old loss 9.059155672730412e-06 BETTER
W0316 10:24:15.814852 500735 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_o proxy err 0.0033376484643667936 tr(WHW.T) 6.8611578941345215
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:03,  2.06s/it]  6%|▋         | 2/32 [00:03<00:53,  1.77s/it]  9%|▉         | 3/32 [00:05<00:48,  1.68s/it]I0316 10:24:22.481055 501241 finetune.py:68] layer 15_o @ epoch 3 new loss 9.074433364730794e-06 old loss 9.15713553695241e-06 BETTER
 12%|█▎        | 4/32 [00:06<00:45,  1.63s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.61s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.60s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.59s/it] 25%|██▌       | 8/32 [00:13<00:37,  1.58s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.58s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.57s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.57s/it]I0316 10:24:35.205626 500229 finetune.py:68] layer 13_up @ epoch 0 new loss 1.5889221685938537e-05 old loss 1.6168769434443675e-05 BETTER
 38%|███▊      | 12/32 [00:19<00:31,  1.57s/it] 41%|████      | 13/32 [00:20<00:29,  1.57s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.57s/it] 47%|████▋     | 15/32 [00:24<00:26,  1.57s/it] 50%|█████     | 16/32 [00:25<00:25,  1.57s/it] 53%|█████▎    | 17/32 [00:27<00:23,  1.57s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.57s/it]I0316 10:24:46.324572 499725 finetune.py:68] layer 12_up @ epoch 1 new loss 1.4132110663922504e-05 old loss 1.430445809091907e-05 BETTER
 59%|█████▉    | 19/32 [00:30<00:20,  1.56s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.57s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.57s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.57s/it] 72%|███████▏  | 23/32 [00:36<00:14,  1.57s/it] 75%|███████▌  | 24/32 [00:38<00:12,  1.58s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.57s/it]I0316 10:24:58.026844 501241 finetune.py:68] layer 15_o @ epoch 4 new loss 9.006786967802327e-06 old loss 9.074433364730794e-06 BETTER
 81%|████████▏ | 26/32 [00:41<00:09,  1.57s/it]W0316 10:24:59.517099 501241 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:42<00:07,  1.57s/it]15_o proxy err 0.0035643046721816063 tr(WHW.T) 6.771666526794434
  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:44<00:06,  1.56s/it]  3%|▎         | 1/32 [00:02<01:02,  2.02s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.57s/it]  6%|▋         | 2/32 [00:03<00:51,  1.73s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.56s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it] 97%|█████████▋| 31/32 [00:49<00:01,  1.56s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]100%|██████████| 32/32 [00:50<00:00,  1.58s/it]
 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it]I0316 10:25:10.331136 500229 finetune.py:68] layer 13_up @ epoch 1 new loss 1.5692454326199368e-05 old loss 1.5889221685938537e-05 BETTER
 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it]I0316 10:25:16.266664 500735 finetune.py:45] layer 14_up initial loss 1.8847507817554288e-05
W0316 10:25:16.267008 500735 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:28,  1.52s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it]I0316 10:25:23.356443 499725 finetune.py:68] layer 12_up @ epoch 2 new loss 1.3985800251248293e-05 old loss 1.4132110663922504e-05 BETTER
 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.52s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.52s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.52s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.52s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.52s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.52s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it]I0316 10:25:45.401461 500229 finetune.py:68] layer 13_up @ epoch 2 new loss 1.5518871805397794e-05 old loss 1.5692454326199368e-05 BETTER
 94%|█████████▍| 30/32 [00:46<00:03,  1.52s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
I0316 10:25:50.923054 500735 finetune.py:68] layer 14_up @ epoch 0 new loss 1.8479393474990502e-05 old loss 1.8847507817554288e-05 BETTER
I0316 10:25:58.139050 501241 finetune.py:45] layer 15_up initial loss 2.0601131836883724e-05
W0316 10:25:58.139474 501241 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:26:00.491503 499725 finetune.py:68] layer 12_up @ epoch 3 new loss 1.3853328709956259e-05 old loss 1.3985800251248293e-05 BETTER
I0316 10:26:20.030868 500229 finetune.py:68] layer 13_up @ epoch 3 new loss 1.5369447282864712e-05 old loss 1.5518871805397794e-05 BETTER
I0316 10:26:26.015684 500735 finetune.py:68] layer 14_up @ epoch 1 new loss 1.822269769036211e-05 old loss 1.8479393474990502e-05 BETTER
I0316 10:26:31.619429 501241 finetune.py:68] layer 15_up @ epoch 0 new loss 2.0116814994253218e-05 old loss 2.0601131836883724e-05 BETTER
I0316 10:26:37.250909 499725 finetune.py:68] layer 12_up @ epoch 4 new loss 1.3732999832427595e-05 old loss 1.3853328709956259e-05 BETTER
W0316 10:26:38.807199 499725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_up proxy err 0.002655650023370981 tr(WHW.T) 855.1527709960938
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.96s/it]  6%|▋         | 2/32 [00:03<00:50,  1.70s/it]  9%|▉         | 3/32 [00:05<00:47,  1.62s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it]I0316 10:26:54.985331 500229 finetune.py:68] layer 13_up @ epoch 4 new loss 1.5230943063215818e-05 old loss 1.5369447282864712e-05 BETTER
 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it]W0316 10:26:56.377815 500229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it]13_up proxy err 0.002616952173411846 tr(WHW.T) 916.1627197265625
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:18<00:30,  1.52s/it]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it] 41%|████      | 13/32 [00:20<00:28,  1.52s/it]I0316 10:27:01.107017 500735 finetune.py:68] layer 14_up @ epoch 2 new loss 1.8003162040258758e-05 old loss 1.822269769036211e-05 BETTER
  6%|▋         | 2/32 [00:03<00:50,  1.67s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it]I0316 10:27:05.997141 501241 finetune.py:68] layer 15_up @ epoch 1 new loss 1.9782712115556933e-05 old loss 2.0116814994253218e-05 BETTER
 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.50s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.52s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.52s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.52s/it] 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.52s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.52s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.52s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.49s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.53s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it]100%|██████████| 32/32 [00:49<00:00,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]
 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.50s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.50s/it]I0316 10:27:35.985352 500735 finetune.py:68] layer 14_up @ epoch 3 new loss 1.780859201971907e-05 old loss 1.8003162040258758e-05 BETTER
 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it]I0316 10:27:37.872761 499725 finetune.py:45] layer 12_gate initial loss 1.8051734514301643e-05
W0316 10:27:37.873122 499725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 84%|████████▍ | 27/32 [00:40<00:07,  1.49s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.49s/it]I0316 10:27:40.209135 501241 finetune.py:68] layer 15_up @ epoch 2 new loss 1.9506702301441692e-05 old loss 1.9782712115556933e-05 BETTER
 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.49s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.49s/it]100%|██████████| 32/32 [00:48<00:00,  1.49s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
I0316 10:27:54.154676 500229 finetune.py:45] layer 13_gate initial loss 2.0002273231511936e-05
W0316 10:27:54.155121 500229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:28:10.994114 500735 finetune.py:68] layer 14_up @ epoch 4 new loss 1.763346699590329e-05 old loss 1.780859201971907e-05 BETTER
I0316 10:28:12.356655 499725 finetune.py:68] layer 12_gate @ epoch 0 new loss 1.7900150851346552e-05 old loss 1.8051734514301643e-05 BETTER
W0316 10:28:12.605893 500735 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_up proxy err 0.002814600709825754 tr(WHW.T) 931.8086547851562
  0%|          | 0/32 [00:00<?, ?it/s]I0316 10:28:14.371991 501241 finetune.py:68] layer 15_up @ epoch 3 new loss 1.9261988200014457e-05 old loss 1.9506702301441692e-05 BETTER
  3%|▎         | 1/32 [00:02<01:02,  2.03s/it]  6%|▋         | 2/32 [00:03<00:52,  1.76s/it]  9%|▉         | 3/32 [00:05<00:48,  1.66s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.63s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.61s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.59s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.59s/it] 25%|██▌       | 8/32 [00:13<00:37,  1.58s/it]I0316 10:28:27.205335 500229 finetune.py:68] layer 13_gate @ epoch 0 new loss 1.9813473045360297e-05 old loss 2.0002273231511936e-05 BETTER
 28%|██▊       | 9/32 [00:14<00:36,  1.58s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.58s/it] 34%|███▍      | 11/32 [00:17<00:33,  1.57s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.57s/it] 41%|████      | 13/32 [00:20<00:29,  1.57s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.56s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.57s/it] 50%|█████     | 16/32 [00:25<00:25,  1.57s/it] 53%|█████▎    | 17/32 [00:27<00:23,  1.56s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.56s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.56s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.56s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.56s/it]I0316 10:28:47.456154 499725 finetune.py:68] layer 12_gate @ epoch 1 new loss 1.778558907972183e-05 old loss 1.7900150851346552e-05 BETTER
I0316 10:28:48.859034 501241 finetune.py:68] layer 15_up @ epoch 4 new loss 1.9044351574848406e-05 old loss 1.9261988200014457e-05 BETTER
 69%|██████▉   | 22/32 [00:34<00:15,  1.55s/it]W0316 10:28:50.211531 501241 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:36<00:13,  1.55s/it]15_up proxy err 0.0028269756585359573 tr(WHW.T) 989.11474609375
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:37<00:12,  1.56s/it]  3%|▎         | 1/32 [00:01<01:01,  1.97s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.55s/it]  6%|▋         | 2/32 [00:03<00:51,  1.72s/it] 81%|████████▏ | 26/32 [00:41<00:09,  1.55s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.55s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 88%|████████▊ | 28/32 [00:44<00:06,  1.55s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.55s/it]I0316 10:29:00.902899 500229 finetune.py:68] layer 13_gate @ epoch 1 new loss 1.9679780962178484e-05 old loss 1.9813473045360297e-05 BETTER
 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.55s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.56s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it]100%|██████████| 32/32 [00:50<00:00,  1.55s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]
 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it] 31%|███▏      | 10/32 [00:15<00:34,  1.55s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.55s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it]I0316 10:29:12.720227 500735 finetune.py:45] layer 14_gate initial loss 2.3024709662422538e-05
W0316 10:29:12.720682 500735 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 44%|████▍     | 14/32 [00:22<00:27,  1.54s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.54s/it] 50%|█████     | 16/32 [00:25<00:24,  1.54s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.53s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:20,  1.59s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.56s/it]I0316 10:29:22.878184 499725 finetune.py:68] layer 12_gate @ epoch 2 new loss 1.768013316905126e-05 old loss 1.778558907972183e-05 BETTER
 66%|██████▌   | 21/32 [00:32<00:17,  1.55s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.52s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it]I0316 10:29:34.439293 500229 finetune.py:68] layer 13_gate @ epoch 2 new loss 1.9559845895855688e-05 old loss 1.9679780962178484e-05 BETTER
 88%|████████▊ | 28/32 [00:43<00:06,  1.52s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.52s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.52s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.51s/it]100%|██████████| 32/32 [00:49<00:00,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
I0316 10:29:45.575540 500735 finetune.py:68] layer 14_gate @ epoch 0 new loss 2.279003820149228e-05 old loss 2.3024709662422538e-05 BETTER
I0316 10:29:49.515528 501241 finetune.py:45] layer 15_gate initial loss 2.5207313228747807e-05
W0316 10:29:49.515956 501241 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:29:58.013460 499725 finetune.py:68] layer 12_gate @ epoch 3 new loss 1.7586980902706273e-05 old loss 1.768013316905126e-05 BETTER
I0316 10:30:08.022501 500229 finetune.py:68] layer 13_gate @ epoch 3 new loss 1.9451828848104924e-05 old loss 1.9559845895855688e-05 BETTER
I0316 10:30:19.024527 500735 finetune.py:68] layer 14_gate @ epoch 1 new loss 2.2621306925429963e-05 old loss 2.279003820149228e-05 BETTER
I0316 10:30:21.859148 501241 finetune.py:68] layer 15_gate @ epoch 0 new loss 2.490075712557882e-05 old loss 2.5207313228747807e-05 BETTER
I0316 10:30:33.347915 499725 finetune.py:68] layer 12_gate @ epoch 4 new loss 1.749832881614566e-05 old loss 1.7586980902706273e-05 BETTER
W0316 10:30:34.542463 499725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_gate proxy err 0.0010145511478185654 tr(WHW.T) 3183.501953125
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:39,  1.12it/s]  2%|▏         | 2/112 [00:01<01:05,  1.68it/s]  3%|▎         | 3/112 [00:01<00:54,  2.00it/s]  4%|▎         | 4/112 [00:02<00:49,  2.20it/s]  4%|▍         | 5/112 [00:02<00:45,  2.34it/s]  5%|▌         | 6/112 [00:02<00:43,  2.43it/s]I0316 10:30:41.200645 500229 finetune.py:68] layer 13_gate @ epoch 4 new loss 1.9353396055521443e-05 old loss 1.9451828848104924e-05 BETTER
  6%|▋         | 7/112 [00:03<00:42,  2.49it/s]  7%|▋         | 8/112 [00:03<00:41,  2.53it/s]  8%|▊         | 9/112 [00:03<00:40,  2.55it/s]W0316 10:30:42.419135 500229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 10/112 [00:04<00:40,  2.54it/s] 10%|▉         | 11/112 [00:04<00:39,  2.56it/s] 11%|█         | 12/112 [00:05<00:38,  2.57it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.58it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.57it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.57it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.58it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.58it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.59it/s]13_gate proxy err 0.0009554001735523343 tr(WHW.T) 3560.2294921875
  0%|          | 0/112 [00:00<?, ?it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.60it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.60it/s]  1%|          | 1/112 [00:00<01:36,  1.15it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.60it/s]  2%|▏         | 2/112 [00:01<01:03,  1.73it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.60it/s]  3%|▎         | 3/112 [00:01<00:53,  2.06it/s] 21%|██        | 23/112 [00:09<00:34,  2.60it/s]  4%|▎         | 4/112 [00:01<00:47,  2.26it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.60it/s]  4%|▍         | 5/112 [00:02<00:44,  2.39it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.60it/s]  5%|▌         | 6/112 [00:02<00:42,  2.47it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.59it/s]  6%|▋         | 7/112 [00:03<00:41,  2.52it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.59it/s]  7%|▋         | 8/112 [00:03<00:40,  2.56it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.60it/s]  8%|▊         | 9/112 [00:03<00:39,  2.59it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.59it/s]  9%|▉         | 10/112 [00:04<00:39,  2.60it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.58it/s] 10%|▉         | 11/112 [00:04<00:38,  2.62it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.59it/s] 11%|█         | 12/112 [00:05<00:37,  2.64it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.60it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.63it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.61it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.62it/s] 30%|███       | 34/112 [00:13<00:29,  2.61it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.63it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.61it/s]I0316 10:30:52.359981 500735 finetune.py:68] layer 14_gate @ epoch 2 new loss 2.2473552235169336e-05 old loss 2.2621306925429963e-05 BETTER
 14%|█▍        | 16/112 [00:06<00:36,  2.63it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.61it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.64it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.62it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.65it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.61it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.65it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.61it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.65it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.60it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.66it/s]I0316 10:30:54.593482 501241 finetune.py:68] layer 15_gate @ epoch 1 new loss 2.4690602003829554e-05 old loss 2.490075712557882e-05 BETTER
 37%|███▋      | 41/112 [00:16<00:27,  2.59it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.66it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.59it/s] 21%|██        | 23/112 [00:09<00:33,  2.65it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.58it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.65it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.57it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.65it/s] 40%|████      | 45/112 [00:17<00:25,  2.58it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.64it/s] 41%|████      | 46/112 [00:18<00:25,  2.59it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.64it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.59it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.63it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.59it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.64it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.60it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.64it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.59it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.64it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.59it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.65it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.59it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.65it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.59it/s] 30%|███       | 34/112 [00:13<00:29,  2.65it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.59it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.65it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.59it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.66it/s] 50%|█████     | 56/112 [00:22<00:21,  2.59it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.66it/s] 51%|█████     | 57/112 [00:22<00:21,  2.58it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.66it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.58it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.67it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.58it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.66it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.65it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.58it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.66it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.60it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.67it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.61it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.67it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.61it/s] 40%|████      | 45/112 [00:17<00:25,  2.67it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.61it/s] 41%|████      | 46/112 [00:17<00:24,  2.68it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.62it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.67it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.61it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.68it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.60it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.68it/s] 61%|██████    | 68/112 [00:26<00:16,  2.59it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.68it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.60it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.68it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.59it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.67it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.59it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.66it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.57it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.67it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.57it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.65it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.58it/s] 50%|█████     | 56/112 [00:21<00:21,  2.65it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.59it/s] 51%|█████     | 57/112 [00:21<00:20,  2.66it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.60it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.66it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.60it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.67it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.61it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.68it/s] 71%|███████   | 79/112 [00:30<00:12,  2.60it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.68it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.61it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.68it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.61it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.69it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.60it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.69it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.60it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.69it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.59it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.69it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.60it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.69it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.59it/s] 61%|██████    | 68/112 [00:26<00:16,  2.68it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.58it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.66it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.59it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.65it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.60it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.67it/s] 80%|████████  | 90/112 [00:35<00:08,  2.60it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.66it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.61it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.67it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.61it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.67it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.62it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.68it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.62it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.69it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.62it/s] 69%|██████▉   | 77/112 [00:29<00:12,  2.69it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.69it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.62it/s] 71%|███████   | 79/112 [00:30<00:12,  2.69it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.61it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.68it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.61it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.68it/s] 88%|████████▊ | 99/112 [00:38<00:05,  2.60it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.67it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.60it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.67it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.60it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.65it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.59it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.66it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.60it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.67it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.60it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.67it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.60it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.67it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.61it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.68it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.60it/s] 80%|████████  | 90/112 [00:34<00:08,  2.67it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.60it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.67it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.60it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.67it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.60it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.66it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.60it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.65it/s]100%|██████████| 112/112 [00:43<00:00,  2.60it/s]100%|██████████| 112/112 [00:43<00:00,  2.57it/s]
 85%|████████▍ | 95/112 [00:36<00:06,  2.65it/s] 86%|████████▌ | 96/112 [00:36<00:06,  2.57it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.60it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.63it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.64it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.65it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.65it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.66it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.66it/s] 93%|█████████▎| 104/112 [00:39<00:03,  2.67it/s]I0316 10:31:25.838867 500735 finetune.py:68] layer 14_gate @ epoch 3 new loss 2.233995655842591e-05 old loss 2.2473552235169336e-05 BETTER
 94%|█████████▍| 105/112 [00:39<00:02,  2.67it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.67it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.67it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.67it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.64it/s]I0316 10:31:27.816116 501241 finetune.py:68] layer 15_gate @ epoch 2 new loss 2.4506114641553722e-05 old loss 2.4690602003829554e-05 BETTER
 98%|█████████▊| 110/112 [00:41<00:00,  2.66it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.68it/s]100%|██████████| 112/112 [00:42<00:00,  2.68it/s]100%|██████████| 112/112 [00:42<00:00,  2.63it/s]
W0316 10:31:29.814000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:31:29.815000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:31:29.815000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:31:29.815000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:29.815000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:31:29.815000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:31:29.815000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:31:29.860000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:29.860000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:31:29.860000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:31:29.860000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:31:29.860000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.044000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.044000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.044000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.045000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.045000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.375000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.375000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.375000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.375000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.375000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.375000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.375000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.410000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.411000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.411000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.411000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.411000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.482000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.482000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.482000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.482000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:31:30.482000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:31:31.862000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:31:32.329000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:31:32.329000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:31:32.329000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:31:32.329000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:31:32.329000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:31:32.329000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:31:32.330000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:32.363000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:31:32.364000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:31:32.364000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:32.364000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:31:32.364000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:31:32.754000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:31:32.754000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:31:32.754000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:32.754000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:31:32.754000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:31:33.320000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:31:33.326000 140319111481152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:35.822000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:31:35.823000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:31:35.823000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:31:35.823000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:31:35.823000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:35.823000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:31:35.824000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:31:35.870000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:31:35.870000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:35.870000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:31:35.870000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:31:35.870000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.049000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.049000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.049000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.049000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.050000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.377000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.377000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.377000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.377000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.377000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.377000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.377000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.411000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.411000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.411000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.411000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.411000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.483000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.483000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.483000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.483000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:31:36.483000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:31:37.855000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:31:38.340000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:31:38.340000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:31:38.340000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:31:38.340000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:38.340000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:31:38.341000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:31:38.341000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:31:38.376000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:31:38.376000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:31:38.377000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:31:38.377000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:31:38.377000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:38.772000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:31:38.772000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:31:38.772000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:31:38.772000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:31:38.772000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:31:39.352000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:31:39.358000 139941932033856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0316 10:31:40.721310 499725 finetune.py:45] layer 12_down initial loss 2.5783851015148684e-05
W0316 10:31:40.721729 499725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:31:46.580558 500229 finetune.py:45] layer 13_down initial loss 2.9224454920040444e-05
W0316 10:31:46.580986 500229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:31:59.224238 500735 finetune.py:68] layer 14_gate @ epoch 4 new loss 2.2219714082893915e-05 old loss 2.233995655842591e-05 BETTER
W0316 10:32:00.477897 500735 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 10:32:00.864994 501241 finetune.py:68] layer 15_gate @ epoch 3 new loss 2.4340779418707825e-05 old loss 2.4506114641553722e-05 BETTER
14_gate proxy err 0.0009288883884437382 tr(WHW.T) 4245.71630859375
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:40,  1.10it/s]  2%|▏         | 2/112 [00:01<01:06,  1.65it/s]  3%|▎         | 3/112 [00:01<00:55,  1.95it/s]  4%|▎         | 4/112 [00:02<00:50,  2.14it/s]  4%|▍         | 5/112 [00:02<00:47,  2.26it/s]  5%|▌         | 6/112 [00:02<00:45,  2.34it/s]  6%|▋         | 7/112 [00:03<00:43,  2.40it/s]  7%|▋         | 8/112 [00:03<00:42,  2.44it/s]  8%|▊         | 9/112 [00:04<00:41,  2.47it/s]  9%|▉         | 10/112 [00:04<00:40,  2.49it/s] 10%|▉         | 11/112 [00:04<00:40,  2.51it/s] 11%|█         | 12/112 [00:05<00:39,  2.51it/s] 12%|█▏        | 13/112 [00:05<00:39,  2.51it/s] 12%|█▎        | 14/112 [00:06<00:38,  2.51it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.51it/s] 14%|█▍        | 16/112 [00:06<00:38,  2.51it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.51it/s] 16%|█▌        | 18/112 [00:07<00:37,  2.50it/s] 17%|█▋        | 19/112 [00:08<00:37,  2.51it/s] 18%|█▊        | 20/112 [00:08<00:36,  2.52it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.53it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.53it/s]I0316 10:32:13.285715 499725 finetune.py:68] layer 12_down @ epoch 0 new loss 2.5772720618988387e-05 old loss 2.5783851015148684e-05 BETTER
 21%|██        | 23/112 [00:09<00:35,  2.54it/s] 21%|██▏       | 24/112 [00:10<00:34,  2.54it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.53it/s] 23%|██▎       | 26/112 [00:10<00:34,  2.52it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.51it/s] 25%|██▌       | 28/112 [00:11<00:33,  2.51it/s] 26%|██▌       | 29/112 [00:12<00:33,  2.50it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.50it/s] 28%|██▊       | 31/112 [00:12<00:32,  2.50it/s] 29%|██▊       | 32/112 [00:13<00:32,  2.48it/s] 29%|██▉       | 33/112 [00:13<00:32,  2.41it/s]I0316 10:32:17.771372 500229 finetune.py:68] layer 13_down @ epoch 0 new loss 2.9211158107500523e-05 old loss 2.9224454920040444e-05 BETTER
 30%|███       | 34/112 [00:14<00:32,  2.37it/s] 31%|███▏      | 35/112 [00:14<00:31,  2.41it/s] 32%|███▏      | 36/112 [00:14<00:31,  2.45it/s] 33%|███▎      | 37/112 [00:15<00:30,  2.47it/s] 34%|███▍      | 38/112 [00:15<00:29,  2.48it/s] 35%|███▍      | 39/112 [00:16<00:29,  2.49it/s] 36%|███▌      | 40/112 [00:16<00:28,  2.49it/s] 37%|███▋      | 41/112 [00:16<00:28,  2.50it/s] 38%|███▊      | 42/112 [00:17<00:28,  2.49it/s] 38%|███▊      | 43/112 [00:17<00:27,  2.50it/s] 39%|███▉      | 44/112 [00:18<00:27,  2.50it/s] 40%|████      | 45/112 [00:18<00:26,  2.48it/s] 41%|████      | 46/112 [00:18<00:26,  2.49it/s] 42%|████▏     | 47/112 [00:19<00:25,  2.50it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.50it/s] 44%|████▍     | 49/112 [00:20<00:25,  2.51it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.51it/s] 46%|████▌     | 51/112 [00:20<00:24,  2.52it/s] 46%|████▋     | 52/112 [00:21<00:23,  2.52it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.52it/s] 48%|████▊     | 54/112 [00:22<00:23,  2.52it/s] 49%|████▉     | 55/112 [00:22<00:22,  2.51it/s] 50%|█████     | 56/112 [00:22<00:22,  2.51it/s] 51%|█████     | 57/112 [00:23<00:21,  2.51it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.51it/s] 53%|█████▎    | 59/112 [00:24<00:21,  2.50it/s] 54%|█████▎    | 60/112 [00:24<00:20,  2.50it/s] 54%|█████▍    | 61/112 [00:24<00:20,  2.50it/s] 55%|█████▌    | 62/112 [00:25<00:19,  2.51it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.51it/s] 57%|█████▋    | 64/112 [00:26<00:19,  2.50it/s] 58%|█████▊    | 65/112 [00:26<00:18,  2.51it/s] 59%|█████▉    | 66/112 [00:26<00:18,  2.52it/s] 60%|█████▉    | 67/112 [00:27<00:17,  2.51it/s] 61%|██████    | 68/112 [00:27<00:17,  2.51it/s] 62%|██████▏   | 69/112 [00:28<00:17,  2.51it/s] 62%|██████▎   | 70/112 [00:28<00:16,  2.51it/s] 63%|██████▎   | 71/112 [00:28<00:16,  2.50it/s] 64%|██████▍   | 72/112 [00:29<00:15,  2.50it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.50it/s]I0316 10:32:33.729397 501241 finetune.py:68] layer 15_gate @ epoch 4 new loss 2.4188479073927738e-05 old loss 2.4340779418707825e-05 BETTER
 66%|██████▌   | 74/112 [00:30<00:15,  2.51it/s] 67%|██████▋   | 75/112 [00:30<00:14,  2.51it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.52it/s]W0316 10:32:34.959153 501241 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 77/112 [00:31<00:14,  2.49it/s] 70%|██████▉   | 78/112 [00:31<00:13,  2.50it/s] 71%|███████   | 79/112 [00:32<00:13,  2.52it/s] 71%|███████▏  | 80/112 [00:32<00:12,  2.53it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.53it/s] 73%|███████▎  | 82/112 [00:33<00:11,  2.53it/s] 74%|███████▍  | 83/112 [00:33<00:11,  2.52it/s] 75%|███████▌  | 84/112 [00:34<00:11,  2.52it/s] 76%|███████▌  | 85/112 [00:34<00:10,  2.52it/s]15_gate proxy err 0.0008844273397698998 tr(WHW.T) 5081.2568359375
  0%|          | 0/112 [00:00<?, ?it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.52it/s] 78%|███████▊  | 87/112 [00:35<00:09,  2.50it/s]  1%|          | 1/112 [00:00<01:39,  1.11it/s] 79%|███████▊  | 88/112 [00:35<00:09,  2.51it/s]  2%|▏         | 2/112 [00:01<01:05,  1.68it/s] 79%|███████▉  | 89/112 [00:36<00:09,  2.52it/s]  3%|▎         | 3/112 [00:01<00:54,  2.01it/s] 80%|████████  | 90/112 [00:36<00:08,  2.52it/s]  4%|▎         | 4/112 [00:02<00:48,  2.21it/s] 81%|████████▏ | 91/112 [00:36<00:08,  2.52it/s]  4%|▍         | 5/112 [00:02<00:45,  2.33it/s] 82%|████████▏ | 92/112 [00:37<00:07,  2.52it/s]  5%|▌         | 6/112 [00:02<00:43,  2.42it/s] 83%|████████▎ | 93/112 [00:37<00:07,  2.52it/s]  6%|▋         | 7/112 [00:03<00:42,  2.49it/s] 84%|████████▍ | 94/112 [00:37<00:07,  2.52it/s]  7%|▋         | 8/112 [00:03<00:41,  2.52it/s] 85%|████████▍ | 95/112 [00:38<00:06,  2.52it/s]  8%|▊         | 9/112 [00:03<00:40,  2.55it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.52it/s]  9%|▉         | 10/112 [00:04<00:39,  2.57it/s] 87%|████████▋ | 97/112 [00:39<00:05,  2.53it/s] 10%|▉         | 11/112 [00:04<00:39,  2.56it/s] 88%|████████▊ | 98/112 [00:39<00:05,  2.52it/s] 11%|█         | 12/112 [00:05<00:38,  2.58it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.52it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.57it/s] 89%|████████▉ | 100/112 [00:40<00:04,  2.52it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.57it/s] 90%|█████████ | 101/112 [00:40<00:04,  2.51it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.59it/s] 91%|█████████ | 102/112 [00:41<00:04,  2.48it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.56it/s] 92%|█████████▏| 103/112 [00:41<00:03,  2.48it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.58it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.51it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.60it/s] 94%|█████████▍| 105/112 [00:42<00:02,  2.53it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.61it/s]I0316 10:32:46.522356 499725 finetune.py:68] layer 12_down @ epoch 1 new loss 2.577227132860571e-05 old loss 2.5772720618988387e-05 BETTER
 95%|█████████▍| 106/112 [00:42<00:02,  2.53it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.61it/s] 96%|█████████▌| 107/112 [00:43<00:01,  2.53it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.62it/s] 96%|█████████▋| 108/112 [00:43<00:01,  2.53it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.61it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.52it/s] 21%|██        | 23/112 [00:09<00:34,  2.61it/s] 98%|█████████▊| 110/112 [00:44<00:00,  2.52it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.61it/s] 99%|█████████▉| 111/112 [00:44<00:00,  2.52it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.61it/s]100%|██████████| 112/112 [00:45<00:00,  2.52it/s]100%|██████████| 112/112 [00:45<00:00,  2.48it/s]
 23%|██▎       | 26/112 [00:10<00:33,  2.60it/s]I0316 10:32:49.293782 500229 finetune.py:68] layer 13_down @ epoch 1 new loss 2.9209239073679782e-05 old loss 2.9211158107500523e-05 BETTER
 24%|██▍       | 27/112 [00:10<00:32,  2.61it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.59it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.60it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.60it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.60it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.61it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.62it/s] 30%|███       | 34/112 [00:13<00:29,  2.62it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.62it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.62it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.61it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.62it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.62it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.60it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.59it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.57it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.57it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.57it/s]W0316 10:32:56.210000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.210000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.211000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.211000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.211000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.211000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.211000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.255000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.255000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.255000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.255000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.255000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.432000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.432000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.433000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.433000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.433000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 40%|████      | 45/112 [00:17<00:26,  2.58it/s]W0316 10:32:56.755000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.755000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.755000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.756000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.756000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.756000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.756000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.791000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.791000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.791000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.791000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.791000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 41%|████      | 46/112 [00:18<00:25,  2.58it/s]W0316 10:32:56.861000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.862000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.862000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.862000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:32:56.862000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 42%|████▏     | 47/112 [00:18<00:25,  2.58it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.59it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.59it/s]W0316 10:32:58.208000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 45%|████▍     | 50/112 [00:19<00:23,  2.59it/s]W0316 10:32:58.678000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:32:58.678000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:32:58.679000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:32:58.679000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:32:58.679000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:32:58.679000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:32:58.679000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:32:58.710000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:32:58.710000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:32:58.710000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:32:58.710000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:32:58.710000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 46%|████▌     | 51/112 [00:20<00:23,  2.58it/s]W0316 10:32:59.104000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:32:59.104000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:32:59.104000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:32:59.105000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:32:59.105000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 46%|████▋     | 52/112 [00:20<00:23,  2.58it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.58it/s]W0316 10:32:59.651000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:32:59.657000 139705269212992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 48%|████▊     | 54/112 [00:21<00:22,  2.57it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.57it/s] 50%|█████     | 56/112 [00:22<00:21,  2.56it/s] 51%|█████     | 57/112 [00:22<00:21,  2.56it/s] 52%|█████▏    | 58/112 [00:22<00:21,  2.56it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.57it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.57it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.58it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.58it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.59it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.58it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.58it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.58it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.57it/s] 61%|██████    | 68/112 [00:26<00:17,  2.57it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.57it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.56it/s] 63%|██████▎   | 71/112 [00:27<00:16,  2.56it/s]I0316 10:33:06.851236 500735 finetune.py:45] layer 14_down initial loss 3.387815741007216e-05
W0316 10:33:06.851608 500735 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 64%|██████▍   | 72/112 [00:28<00:15,  2.57it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.57it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.57it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.58it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.58it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.59it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.59it/s] 71%|███████   | 79/112 [00:31<00:12,  2.59it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.59it/s] 72%|███████▏  | 81/112 [00:31<00:12,  2.58it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.59it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.57it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.58it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.59it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.61it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.62it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.62it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.62it/s] 80%|████████  | 90/112 [00:35<00:08,  2.62it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.61it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.60it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.61it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.61it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.61it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.60it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.59it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.60it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.60it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.61it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.62it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.62it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.62it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.62it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.62it/s]I0316 10:33:19.930297 499725 finetune.py:76] layer 12_down @ epoch 2 new loss 2.5775370886549354e-05 old loss 2.577227132860571e-05 WORSE
 95%|█████████▍| 106/112 [00:41<00:02,  2.61it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.61it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.61it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.61it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.61it/s]I0316 10:33:21.581870 500229 finetune.py:68] layer 13_down @ epoch 2 new loss 2.9207069019321352e-05 old loss 2.9209239073679782e-05 BETTER
 99%|█████████▉| 111/112 [00:43<00:00,  2.59it/s]100%|██████████| 112/112 [00:43<00:00,  2.59it/s]100%|██████████| 112/112 [00:43<00:00,  2.56it/s]
W0316 10:33:30.012000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.012000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.013000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.013000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.013000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.013000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.013000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.062000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.062000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.062000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.062000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.062000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.252000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.253000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.253000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.253000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.253000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.583000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.583000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.584000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.584000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.584000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.584000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.584000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.617000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.617000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.617000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.617000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.618000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.694000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.695000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.695000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.695000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:33:30.695000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:33:32.067000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:33:32.536000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:33:32.536000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:33:32.536000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:33:32.536000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:33:32.536000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:33:32.537000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:33:32.537000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:33:32.569000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:33:32.570000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:33:32.570000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:33:32.570000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:33:32.570000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:33:32.962000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:33:32.962000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:33:32.962000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:33:32.962000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:33:32.962000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:33:33.516000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:33:33.521000 140494359054144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0316 10:33:38.621874 500735 finetune.py:68] layer 14_down @ epoch 0 new loss 3.386345997569151e-05 old loss 3.387815741007216e-05 BETTER
I0316 10:33:40.721167 501241 finetune.py:45] layer 15_down initial loss 3.891121377819218e-05
W0316 10:33:40.721478 501241 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:33:52.887960 499725 finetune.py:68] layer 12_down @ epoch 3 new loss 2.576929546194151e-05 old loss 2.577227132860571e-05 BETTER
I0316 10:33:53.691807 500229 finetune.py:76] layer 13_down @ epoch 3 new loss 2.9208127671154216e-05 old loss 2.9207069019321352e-05 WORSE
I0316 10:34:11.029076 500735 finetune.py:68] layer 14_down @ epoch 1 new loss 3.3862943382700905e-05 old loss 3.386345997569151e-05 BETTER
I0316 10:34:11.686636 501241 finetune.py:68] layer 15_down @ epoch 0 new loss 3.889538493240252e-05 old loss 3.891121377819218e-05 BETTER
I0316 10:34:25.624921 500229 finetune.py:68] layer 13_down @ epoch 4 new loss 2.920605766121298e-05 old loss 2.9207069019321352e-05 BETTER
I0316 10:34:26.195329 499725 finetune.py:68] layer 12_down @ epoch 4 new loss 2.5769220883375965e-05 old loss 2.576929546194151e-05 BETTER
W0316 10:34:26.371719 500229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

13_down proxy err 0.0035336276050657034 tr(WHW.T) 11.711912155151367
W0316 10:34:26.950508 499725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

12_down proxy err 0.0034510279074311256 tr(WHW.T) 10.034119606018066
I0316 10:34:43.223934 501241 finetune.py:68] layer 15_down @ epoch 1 new loss 3.889478466589935e-05 old loss 3.889538493240252e-05 BETTER
I0316 10:34:43.248896 500735 finetune.py:68] layer 14_down @ epoch 2 new loss 3.386060052434914e-05 old loss 3.3862943382700905e-05 BETTER
I0316 10:35:14.507812 501241 finetune.py:68] layer 15_down @ epoch 2 new loss 3.889384970534593e-05 old loss 3.889478466589935e-05 BETTER
I0316 10:35:14.519088 500735 finetune.py:68] layer 14_down @ epoch 3 new loss 3.385956370038912e-05 old loss 3.386060052434914e-05 BETTER
I0316 10:35:38.433473 460347 quantize_finetune_llama.py:186] computed original embedding for layer 16 in 66.05182695388794s
I0316 10:35:38.905949 460347 quantize_finetune_llama.py:159] layer 17 gpu 1
I0316 10:35:40.969573 501939 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 10:35:40.969712 501939 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 10:35:40.969798 501939 utils.py:162] NumExpr defaulting to 16 threads.
I0316 10:35:41.186517 501939 config.py:58] PyTorch version 2.4.0 available.
I0316 10:35:43.368364 501939 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 10:35:43.743120 501939 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]I0316 10:35:45.654008 501241 finetune.py:68] layer 15_down @ epoch 3 new loss 3.8893365854164585e-05 old loss 3.889384970534593e-05 BETTER
I0316 10:35:46.136193 500735 finetune.py:76] layer 14_down @ epoch 4 new loss 3.386247044545598e-05 old loss 3.385956370038912e-05 WORSE
  3%|▎         | 1/32 [00:01<00:55,  1.81s/it]W0316 10:35:46.666479 500735 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

  6%|▋         | 2/32 [00:02<00:28,  1.06it/s]14_down proxy err 0.0036715336609631777 tr(WHW.T) 13.360877990722656
  9%|▉         | 3/32 [00:02<00:19,  1.49it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.82it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.10it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.29it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.56it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.63it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.74it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.79it/s] 41%|████      | 13/32 [00:06<00:06,  2.81it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.82it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.82it/s] 50%|█████     | 16/32 [00:07<00:05,  2.84it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.86it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.87it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.86it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.86it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.87it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.90it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.83it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.84it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.84it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.85it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.85it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.89it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
W0316 10:36:00.545000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:36:00.545000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:36:00.545000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:36:00.545000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:36:00.545000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:36:00.545000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:36:00.545000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:36:00.573000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:36:00.573000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:36:00.573000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:36:00.573000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:36:00.573000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:36:00.874000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:36:00.874000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:36:00.874000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:36:00.874000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:36:00.874000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:36:01.500000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:36:01.501000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:36:01.501000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:36:01.501000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:36:01.501000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:36:01.501000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:36:01.501000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:36:01.518000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:36:01.519000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:36:01.519000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:36:01.519000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:36:01.519000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:36:01.723000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:36:01.723000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:36:01.723000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:36:01.723000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:36:01.723000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:36:02.891000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:36:02.891000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:36:02.891000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:36:02.891000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:36:02.891000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:36:02.891000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:36:02.891000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:36:02.910000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:36:02.910000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:36:02.910000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:36:02.910000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:36:02.910000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:36:03.542000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:36:03.542000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:36:03.542000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:36:03.542000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:36:03.542000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 10:36:10.921334 501939 finetune.py:45] layer 16_v initial loss 6.520221631944878e-06
W0316 10:36:10.921689 501939 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:36:17.203083 501241 finetune.py:68] layer 15_down @ epoch 4 new loss 3.8890528230695054e-05 old loss 3.8893365854164585e-05 BETTER
W0316 10:36:17.914000 501241 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

15_down proxy err 0.0036781535018235445 tr(WHW.T) 16.91277313232422
I0316 10:36:48.006133 501939 finetune.py:68] layer 16_v @ epoch 0 new loss 3.2293733056576457e-06 old loss 6.520221631944878e-06 BETTER
I0316 10:36:53.963656 460347 quantize_finetune_llama.py:186] computed original embedding for layer 17 in 62.5366997718811s
I0316 10:36:54.405840 460347 quantize_finetune_llama.py:159] layer 18 gpu 2
I0316 10:36:56.446565 502443 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 10:36:56.446681 502443 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 10:36:56.446743 502443 utils.py:162] NumExpr defaulting to 16 threads.
I0316 10:36:56.640590 502443 config.py:58] PyTorch version 2.4.0 available.
I0316 10:36:58.862605 502443 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 10:36:59.235675 502443 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:58,  1.87s/it]  6%|▋         | 2/32 [00:02<00:29,  1.01it/s]  9%|▉         | 3/32 [00:02<00:20,  1.44it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.79it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.08it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.30it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.59it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.66it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.70it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.76it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.78it/s] 41%|████      | 13/32 [00:06<00:06,  2.80it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.83it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.83it/s] 50%|█████     | 16/32 [00:07<00:05,  2.85it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.82it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.83it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.85it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.86it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.88it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.87it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.89it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.87it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.87it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.89it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.88it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.89it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
W0316 10:37:15.875000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:37:15.875000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:37:15.875000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:37:15.875000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:37:15.876000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:37:15.876000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:37:15.876000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:37:15.903000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:37:15.903000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:37:15.903000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:37:15.903000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:37:15.903000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:37:16.201000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:37:16.201000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:37:16.201000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:37:16.201000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:37:16.201000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:37:16.823000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:37:16.823000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:37:16.823000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:37:16.823000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:37:16.823000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:37:16.823000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:37:16.823000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:37:16.840000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:37:16.840000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:37:16.840000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:37:16.841000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:37:16.841000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:37:17.041000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:37:17.042000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:37:17.042000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:37:17.042000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:37:17.042000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:37:18.216000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:37:18.217000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:37:18.217000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:37:18.217000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:37:18.217000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:37:18.217000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:37:18.217000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:37:18.235000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:37:18.235000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:37:18.235000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:37:18.236000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:37:18.236000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:37:18.862000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:37:18.862000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:37:18.863000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:37:18.863000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:37:18.863000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 10:37:25.325555 501939 finetune.py:68] layer 16_v @ epoch 1 new loss 2.9625746265082853e-06 old loss 3.2293733056576457e-06 BETTER
I0316 10:37:25.570396 502443 finetune.py:45] layer 17_v initial loss 8.521332347299904e-06
W0316 10:37:25.570622 502443 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:37:57.405771 460347 quantize_finetune_llama.py:186] computed original embedding for layer 18 in 62.608246088027954s
I0316 10:37:57.782313 460347 quantize_finetune_llama.py:159] layer 19 gpu 3
I0316 10:37:59.890706 502949 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 10:37:59.890863 502949 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 10:37:59.890956 502949 utils.py:162] NumExpr defaulting to 16 threads.
I0316 10:38:00.139878 502949 config.py:58] PyTorch version 2.4.0 available.
I0316 10:38:00.269272 502443 finetune.py:68] layer 17_v @ epoch 0 new loss 3.7782356230309233e-06 old loss 8.521332347299904e-06 BETTER
I0316 10:38:02.481587 502949 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 10:38:02.926644 502949 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 10:38:03.426212 501939 finetune.py:68] layer 16_v @ epoch 2 new loss 2.826442823788966e-06 old loss 2.9625746265082853e-06 BETTER
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.96s/it]  6%|▋         | 2/32 [00:02<00:30,  1.01s/it]  9%|▉         | 3/32 [00:02<00:20,  1.40it/s] 12%|█▎        | 4/32 [00:03<00:16,  1.75it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.04it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.26it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.42it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.53it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.61it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.68it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.71it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.75it/s] 41%|████      | 13/32 [00:06<00:06,  2.76it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.79it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.79it/s] 50%|█████     | 16/32 [00:07<00:05,  2.79it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.81it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.81it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.83it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.84it/s] 66%|██████▌   | 21/32 [00:09<00:03,  2.84it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.81it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.83it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.83it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.82it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.82it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.82it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.82it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.81it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.83it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
W0316 10:38:19.790000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:38:19.790000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:38:19.790000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:38:19.790000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:38:19.791000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:38:19.791000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:38:19.791000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:38:19.819000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:38:19.819000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:38:19.819000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:38:19.819000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:38:19.819000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.119000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.119000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.119000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.119000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.119000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.750000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.750000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.750000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.750000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.750000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.750000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.750000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.769000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.769000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.769000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.769000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.769000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.977000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.977000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.977000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.977000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:38:20.977000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:38:22.134000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:38:22.135000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:38:22.135000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:38:22.135000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:38:22.135000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:38:22.135000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:38:22.135000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:38:22.152000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:38:22.152000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:38:22.153000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:38:22.153000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:38:22.153000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:38:22.781000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:38:22.781000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:38:22.782000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:38:22.782000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:38:22.782000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 10:38:29.172028 502949 finetune.py:45] layer 18_v initial loss 9.193510777549818e-06
W0316 10:38:29.172255 502949 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:38:36.051441 502443 finetune.py:68] layer 17_v @ epoch 1 new loss 3.4159163533331593e-06 old loss 3.7782356230309233e-06 BETTER
I0316 10:38:41.355672 501939 finetune.py:68] layer 16_v @ epoch 3 new loss 2.7541836971067823e-06 old loss 2.826442823788966e-06 BETTER
I0316 10:38:59.696953 460347 quantize_finetune_llama.py:186] computed original embedding for layer 19 in 61.47928237915039s
I0316 10:39:00.105951 460347 quantize_finetune_llama.py:159] layer 20 gpu 0
I0316 10:39:02.234834 503455 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 10:39:02.234950 503455 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 10:39:02.235018 503455 utils.py:162] NumExpr defaulting to 16 threads.
I0316 10:39:02.431637 503455 config.py:58] PyTorch version 2.4.0 available.
I0316 10:39:03.839641 502949 finetune.py:68] layer 18_v @ epoch 0 new loss 2.6413270006742096e-06 old loss 9.193510777549818e-06 BETTER
I0316 10:39:04.650013 503455 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 10:39:05.033743 503455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:55,  1.78s/it]  6%|▋         | 2/32 [00:02<00:28,  1.07it/s]  9%|▉         | 3/32 [00:02<00:19,  1.51it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.87it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.15it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.36it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.65it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.71it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.74it/s] 41%|████      | 13/32 [00:05<00:06,  2.75it/s]I0316 10:39:12.056920 502443 finetune.py:68] layer 17_v @ epoch 2 new loss 3.2615428153803805e-06 old loss 3.4159163533331593e-06 BETTER
 44%|████▍     | 14/32 [00:06<00:06,  2.79it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.80it/s] 50%|█████     | 16/32 [00:07<00:05,  2.79it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.79it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.80it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.81it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.81it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.80it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.81it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.81it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.81it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.81it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.82it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.82it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.82it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
I0316 10:39:19.351953 501939 finetune.py:76] layer 16_v @ epoch 4 new loss 3.0964765755925328e-06 old loss 2.7541836971067823e-06 WORSE
W0316 10:39:20.529211 501939 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0316 10:39:21.870000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:39:21.870000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:39:21.870000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:39:21.870000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:39:21.871000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:39:21.871000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:39:21.871000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:39:21.898000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:39:21.898000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:39:21.898000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:39:21.898000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:39:21.898000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
16_v proxy err 0.0027892389334738255 tr(WHW.T) 68.60269165039062
  0%|          | 0/32 [00:00<?, ?it/s]W0316 10:39:22.191000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:39:22.191000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:39:22.191000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:39:22.191000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:39:22.191000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:39:22.820000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:39:22.820000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:39:22.820000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:39:22.820000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:39:22.820000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:39:22.820000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:39:22.820000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:39:22.838000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:39:22.838000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:39:22.839000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:39:22.839000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:39:22.839000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:39:23.045000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:39:23.045000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:39:23.045000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:39:23.045000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:39:23.045000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:42,  1.36s/it]  6%|▋         | 2/32 [00:01<00:23,  1.27it/s]  9%|▉         | 3/32 [00:02<00:17,  1.65it/s]W0316 10:39:24.227000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:39:24.227000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:39:24.227000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:39:24.227000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:39:24.227000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:39:24.227000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:39:24.227000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:39:24.246000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:39:24.246000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:39:24.246000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:39:24.246000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:39:24.246000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:02<00:14,  1.91it/s]W0316 10:39:24.889000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:39:24.889000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:39:24.889000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:39:24.889000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:39:24.889000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:12,  2.10it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.23it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 22%|██▏       | 7/32 [00:03<00:10,  2.33it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.41it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.44it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.48it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.51it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.53it/s] 41%|████      | 13/32 [00:06<00:07,  2.54it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.55it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.56it/s] 50%|█████     | 16/32 [00:07<00:06,  2.56it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.56it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.55it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.53it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.55it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.55it/s]I0316 10:39:31.243393 503455 finetune.py:45] layer 19_v initial loss 1.2773333764926065e-05
W0316 10:39:31.243757 503455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 69%|██████▉   | 22/32 [00:09<00:03,  2.55it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.55it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.56it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.56it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.56it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.56it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.56it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.55it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.54it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.53it/s]100%|██████████| 32/32 [00:13<00:00,  2.54it/s]100%|██████████| 32/32 [00:13<00:00,  2.37it/s]
I0316 10:39:39.767569 502949 finetune.py:68] layer 18_v @ epoch 1 new loss 2.329944209122914e-06 old loss 2.6413270006742096e-06 BETTER
W0316 10:39:41.791000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:39:41.791000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:39:41.791000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:39:41.791000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:39:41.791000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:39:41.792000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:39:41.792000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:39:41.821000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:39:41.821000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:39:41.821000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:39:41.821000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:39:41.822000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:39:41.988000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:39:41.989000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:39:41.989000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:39:41.989000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:39:41.989000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:39:42.218000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:39:42.218000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:39:42.218000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:39:42.218000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:39:42.218000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:39:42.218000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:39:42.219000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:39:42.240000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:39:42.240000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:39:42.240000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:39:42.240000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:39:42.240000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:39:42.303000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:39:42.303000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:39:42.304000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:39:42.304000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:39:42.304000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:39:43.363000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:39:43.692000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:39:43.693000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:39:43.693000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:39:43.693000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:39:43.693000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:39:43.693000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:39:43.693000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:39:43.718000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:39:43.719000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:39:43.719000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:39:43.720000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:39:43.720000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:39:43.975000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:39:43.976000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:39:43.976000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:39:43.976000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:39:43.976000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:39:44.454000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 10:39:48.146967 502443 finetune.py:68] layer 17_v @ epoch 3 new loss 3.2503078273293795e-06 old loss 3.2615428153803805e-06 BETTER
I0316 10:39:51.666306 501939 finetune.py:45] layer 16_q initial loss 4.440445081854705e-06
W0316 10:39:51.666872 501939 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:40:05.448932 503455 finetune.py:68] layer 19_v @ epoch 0 new loss 3.1703500553703634e-06 old loss 1.2773333764926065e-05 BETTER
I0316 10:40:15.960199 502949 finetune.py:68] layer 18_v @ epoch 2 new loss 2.192075044149533e-06 old loss 2.329944209122914e-06 BETTER
I0316 10:40:25.037724 502443 finetune.py:76] layer 17_v @ epoch 4 new loss 3.337607722642133e-06 old loss 3.2503078273293795e-06 WORSE
W0316 10:40:26.236807 502443 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

17_v proxy err 0.0036693287547677755 tr(WHW.T) 69.1786880493164
  0%|          | 0/32 [00:00<?, ?it/s]I0316 10:40:28.237917 501939 finetune.py:68] layer 16_q @ epoch 0 new loss 4.289526714273961e-06 old loss 4.440445081854705e-06 BETTER
  3%|▎         | 1/32 [00:01<00:41,  1.34s/it]  6%|▋         | 2/32 [00:01<00:23,  1.28it/s]  9%|▉         | 3/32 [00:02<00:17,  1.67it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.95it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.15it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.29it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.40it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.47it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.57it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.57it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 50%|█████     | 16/32 [00:07<00:06,  2.63it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.63it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.63it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.62it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.59it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.60it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:13<00:00,  2.61it/s]100%|██████████| 32/32 [00:13<00:00,  2.43it/s]
I0316 10:40:40.922836 503455 finetune.py:68] layer 19_v @ epoch 1 new loss 2.744938910836936e-06 old loss 3.1703500553703634e-06 BETTER
W0316 10:40:46.557000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.558000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.558000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.558000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.558000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.558000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.558000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.588000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.589000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.589000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.589000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.589000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.755000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.755000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.755000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.755000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.755000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.983000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.984000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.984000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.984000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.984000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.984000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:40:46.984000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:40:47.007000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:40:47.007000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:40:47.007000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:40:47.007000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:40:47.007000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:40:47.073000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:40:47.073000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:40:47.073000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:40:47.073000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:40:47.073000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:40:48.146000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:40:48.471000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:40:48.471000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:40:48.471000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:40:48.471000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:40:48.471000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:40:48.471000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:40:48.471000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:40:48.494000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:40:48.494000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:40:48.494000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:40:48.494000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:40:48.495000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:40:48.747000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:40:48.747000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:40:48.747000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:40:48.748000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:40:48.748000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:40:49.216000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 10:40:52.333310 502949 finetune.py:68] layer 18_v @ epoch 3 new loss 2.1215901142568327e-06 old loss 2.192075044149533e-06 BETTER
I0316 10:40:56.012404 502443 finetune.py:45] layer 17_q initial loss 4.7731882659718394e-06
W0316 10:40:56.012869 502443 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:41:05.981357 501939 finetune.py:68] layer 16_q @ epoch 1 new loss 4.200526745989919e-06 old loss 4.289526714273961e-06 BETTER
I0316 10:41:16.629561 503455 finetune.py:68] layer 19_v @ epoch 2 new loss 2.5752312922122655e-06 old loss 2.744938910836936e-06 BETTER
I0316 10:41:28.948019 502949 finetune.py:68] layer 18_v @ epoch 4 new loss 2.0828658762184205e-06 old loss 2.1215901142568327e-06 BETTER
W0316 10:41:30.845012 502949 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 10:41:31.284237 502443 finetune.py:68] layer 17_q @ epoch 0 new loss 4.566607003653189e-06 old loss 4.7731882659718394e-06 BETTER
18_v proxy err 0.00304727116599679 tr(WHW.T) 73.61924743652344
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.38s/it]  6%|▋         | 2/32 [00:01<00:24,  1.25it/s]  9%|▉         | 3/32 [00:02<00:17,  1.63it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.91it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.10it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.23it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.33it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.40it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.45it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.49it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.52it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.54it/s] 41%|████      | 13/32 [00:06<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.56it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.57it/s] 50%|█████     | 16/32 [00:07<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.57it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.60it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.58it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.59it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.59it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.60it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.60it/s]I0316 10:41:44.249403 501939 finetune.py:68] layer 16_q @ epoch 2 new loss 4.086012268089689e-06 old loss 4.200526745989919e-06 BETTER
 94%|█████████▍| 30/32 [00:12<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:13<00:00,  2.61it/s]100%|██████████| 32/32 [00:13<00:00,  2.39it/s]
W0316 10:41:51.346000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.346000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.346000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.346000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.346000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.346000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.346000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.375000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.375000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.375000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.376000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.376000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.543000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.544000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.544000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.544000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.544000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.772000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.772000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.772000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.772000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.773000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.773000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.773000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.793000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.793000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.793000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.793000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.793000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.858000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.858000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.859000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.859000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:41:51.859000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
I0316 10:41:52.450728 503455 finetune.py:68] layer 19_v @ epoch 3 new loss 2.4982134618767304e-06 old loss 2.5752312922122655e-06 BETTER
W0316 10:41:52.923000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:41:53.248000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:41:53.249000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:41:53.249000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:41:53.249000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:41:53.249000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:41:53.249000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:41:53.249000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:41:53.272000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:41:53.272000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:41:53.272000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:41:53.272000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:41:53.272000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:41:53.529000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:41:53.529000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:41:53.530000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:41:53.530000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:41:53.530000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:41:53.999000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 10:42:00.598782 502949 finetune.py:45] layer 18_q initial loss 3.5035618566325866e-06
W0316 10:42:00.599102 502949 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:42:07.396276 502443 finetune.py:68] layer 17_q @ epoch 1 new loss 4.448415893421043e-06 old loss 4.566607003653189e-06 BETTER
I0316 10:42:22.525003 501939 finetune.py:68] layer 16_q @ epoch 3 new loss 4.0362656363868155e-06 old loss 4.086012268089689e-06 BETTER
I0316 10:42:28.227687 503455 finetune.py:68] layer 19_v @ epoch 4 new loss 2.4369528546230868e-06 old loss 2.4982134618767304e-06 BETTER
W0316 10:42:29.867499 503455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

19_v proxy err 0.002952983370050788 tr(WHW.T) 87.24554443359375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.36s/it]  6%|▋         | 2/32 [00:01<00:23,  1.26it/s]  9%|▉         | 3/32 [00:02<00:17,  1.64it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.92it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.11it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.25it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.35it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.42it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.48it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s]I0316 10:42:36.004364 502949 finetune.py:68] layer 18_q @ epoch 0 new loss 3.3799963148339884e-06 old loss 3.5035618566325866e-06 BETTER
 34%|███▍      | 11/32 [00:05<00:08,  2.54it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:06<00:07,  2.56it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.55it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.56it/s] 50%|█████     | 16/32 [00:07<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.58it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.58it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.59it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.58it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.55it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.57it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.57it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.57it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.58it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.58it/s]I0316 10:42:43.840246 502443 finetune.py:68] layer 17_q @ epoch 2 new loss 4.36784921475919e-06 old loss 4.448415893421043e-06 BETTER
 97%|█████████▋| 31/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:13<00:00,  2.59it/s]100%|██████████| 32/32 [00:13<00:00,  2.39it/s]
W0316 10:42:50.446000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.446000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.446000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.447000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.447000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.447000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.447000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.478000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.478000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.478000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.478000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.478000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.650000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.650000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.650000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.650000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.650000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.883000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.883000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.884000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.884000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.884000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.884000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.884000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.905000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.906000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.906000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.906000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.906000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.972000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.972000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.972000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.972000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:42:50.972000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:42:52.046000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:42:52.368000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:42:52.368000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:42:52.368000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:42:52.368000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:42:52.369000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:42:52.369000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:42:52.369000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:42:52.394000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:42:52.394000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:42:52.394000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:42:52.394000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:42:52.394000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:42:52.648000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:42:52.648000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:42:52.648000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:42:52.648000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:42:52.648000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:42:53.118000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 10:42:59.916901 503455 finetune.py:45] layer 19_q initial loss 3.690598759931163e-06
W0316 10:42:59.917383 503455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:43:00.757883 501939 finetune.py:68] layer 16_q @ epoch 4 new loss 3.98383008359815e-06 old loss 4.0362656363868155e-06 BETTER
W0316 10:43:02.553426 501939 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_q proxy err 0.0005391632439568639 tr(WHW.T) 6125.4560546875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it]  6%|▋         | 2/32 [00:01<00:22,  1.33it/s]  9%|▉         | 3/32 [00:02<00:16,  1.72it/s] 12%|█▎        | 4/32 [00:02<00:14,  2.00it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.19it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.33it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.53it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.57it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.62it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.67it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.67it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.67it/s]I0316 10:43:12.645420 502949 finetune.py:68] layer 18_q @ epoch 1 new loss 3.309808334961417e-06 old loss 3.3799963148339884e-06 BETTER
 69%|██████▉   | 22/32 [00:09<00:03,  2.66it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.65it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.64it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.64it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.65it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.66it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.67it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
I0316 10:43:20.391867 502443 finetune.py:68] layer 17_q @ epoch 3 new loss 4.309088581067044e-06 old loss 4.36784921475919e-06 BETTER
I0316 10:43:24.236469 501939 finetune.py:45] layer 16_k initial loss 5.476063506648643e-06
W0316 10:43:24.236858 501939 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:43:34.482926 503455 finetune.py:68] layer 19_q @ epoch 0 new loss 3.575552000256721e-06 old loss 3.690598759931163e-06 BETTER
I0316 10:43:49.156794 502949 finetune.py:68] layer 18_q @ epoch 2 new loss 3.258879132772563e-06 old loss 3.309808334961417e-06 BETTER
I0316 10:43:57.037764 502443 finetune.py:68] layer 17_q @ epoch 4 new loss 4.281371275283163e-06 old loss 4.309088581067044e-06 BETTER
W0316 10:43:58.943078 502443 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

17_q proxy err 0.000548468844499439 tr(WHW.T) 6717.8076171875
  0%|          | 0/32 [00:00<?, ?it/s]I0316 10:44:01.109966 501939 finetune.py:68] layer 16_k @ epoch 0 new loss 4.9868158384924755e-06 old loss 5.476063506648643e-06 BETTER
  3%|▎         | 1/32 [00:01<00:40,  1.30s/it]  6%|▋         | 2/32 [00:01<00:22,  1.33it/s]  9%|▉         | 3/32 [00:02<00:16,  1.73it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.00it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.20it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.34it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.62it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.63it/s] 50%|█████     | 16/32 [00:06<00:06,  2.64it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.65it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.65it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.66it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.66it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s]I0316 10:44:09.892239 503455 finetune.py:68] layer 19_q @ epoch 1 new loss 3.4806378153007245e-06 old loss 3.575552000256721e-06 BETTER
 75%|███████▌  | 24/32 [00:09<00:03,  2.65it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.65it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.63it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.64it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.66it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
I0316 10:44:20.329981 502443 finetune.py:45] layer 17_k initial loss 5.4803072089271154e-06
W0316 10:44:20.330399 502443 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:44:25.665497 502949 finetune.py:68] layer 18_q @ epoch 3 new loss 3.21511174661282e-06 old loss 3.258879132772563e-06 BETTER
I0316 10:44:39.399137 501939 finetune.py:68] layer 16_k @ epoch 1 new loss 4.923932920064544e-06 old loss 4.9868158384924755e-06 BETTER
I0316 10:44:46.240748 503455 finetune.py:68] layer 19_q @ epoch 2 new loss 3.426181137911044e-06 old loss 3.4806378153007245e-06 BETTER
I0316 10:44:56.589386 502443 finetune.py:68] layer 17_k @ epoch 0 new loss 5.246813998383004e-06 old loss 5.4803072089271154e-06 BETTER
I0316 10:45:02.698130 502949 finetune.py:68] layer 18_q @ epoch 4 new loss 3.17379567604803e-06 old loss 3.21511174661282e-06 BETTER
W0316 10:45:04.395557 502949 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_q proxy err 0.0006732759648002684 tr(WHW.T) 5733.2939453125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.27s/it]  6%|▋         | 2/32 [00:01<00:22,  1.34it/s]  9%|▉         | 3/32 [00:02<00:16,  1.73it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.00it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.19it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.33it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.41it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.48it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.56it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.62it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 50%|█████     | 16/32 [00:06<00:06,  2.64it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.65it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.65it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.65it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.65it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.63it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.63it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.64it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.65it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.65it/s]I0316 10:45:17.863304 501939 finetune.py:76] layer 16_k @ epoch 2 new loss 4.959622401656816e-06 old loss 4.923932920064544e-06 WORSE
 97%|█████████▋| 31/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:13<00:00,  2.64it/s]100%|██████████| 32/32 [00:13<00:00,  2.46it/s]
I0316 10:45:22.326650 503455 finetune.py:68] layer 19_q @ epoch 3 new loss 3.381988562978222e-06 old loss 3.426181137911044e-06 BETTER
I0316 10:45:25.911685 502949 finetune.py:45] layer 18_k initial loss 4.2934948396577965e-06
W0316 10:45:25.912070 502949 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:45:32.986177 502443 finetune.py:68] layer 17_k @ epoch 1 new loss 5.190995580051094e-06 old loss 5.246813998383004e-06 BETTER
I0316 10:45:55.607760 501939 finetune.py:76] layer 16_k @ epoch 3 new loss 4.957551482220879e-06 old loss 4.923932920064544e-06 WORSE
I0316 10:45:58.914138 503455 finetune.py:68] layer 19_q @ epoch 4 new loss 3.3480077945569064e-06 old loss 3.381988562978222e-06 BETTER
W0316 10:46:00.765300 503455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 10:46:01.876272 502949 finetune.py:68] layer 18_k @ epoch 0 new loss 3.983569513366092e-06 old loss 4.2934948396577965e-06 BETTER
19_q proxy err 0.0006462112651206553 tr(WHW.T) 6150.62109375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it]  6%|▋         | 2/32 [00:01<00:22,  1.33it/s]  9%|▉         | 3/32 [00:02<00:16,  1.72it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.99it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.17it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.30it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.38it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.44it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.46it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.52it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.54it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.58it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:07<00:06,  2.61it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s]I0316 10:46:09.973807 502443 finetune.py:68] layer 17_k @ epoch 2 new loss 5.1548345254559536e-06 old loss 5.190995580051094e-06 BETTER
 59%|█████▉    | 19/32 [00:08<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.60it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.61it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.62it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.62it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:13<00:00,  2.58it/s]100%|██████████| 32/32 [00:13<00:00,  2.43it/s]
I0316 10:46:22.591378 503455 finetune.py:45] layer 19_k initial loss 4.3104332689836156e-06
W0316 10:46:22.591785 503455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:46:33.644317 501939 finetune.py:76] layer 16_k @ epoch 4 new loss 5.116978627484059e-06 old loss 4.923932920064544e-06 WORSE
W0316 10:46:34.868611 501939 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_k proxy err 0.00038338551530614495 tr(WHW.T) 4877.0078125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.17it/s]  6%|▋         | 2/32 [00:01<00:17,  1.73it/s]  9%|▉         | 3/32 [00:01<00:14,  2.04it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.23it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s]I0316 10:46:38.952401 502949 finetune.py:68] layer 18_k @ epoch 1 new loss 3.934199412469752e-06 old loss 3.983569513366092e-06 BETTER
 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.56it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.56it/s] 41%|████      | 13/32 [00:05<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.55it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.56it/s] 50%|█████     | 16/32 [00:06<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.57it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.57it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.58it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.58it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.58it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.57it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.55it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.57it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.58it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.59it/s]I0316 10:46:47.291664 502443 finetune.py:68] layer 17_k @ epoch 3 new loss 5.135556421009824e-06 old loss 5.1548345254559536e-06 BETTER
 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
I0316 10:46:56.929109 501939 finetune.py:45] layer 16_o initial loss 9.30325586523395e-06
W0316 10:46:56.929525 501939 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:46:57.503931 503455 finetune.py:68] layer 19_k @ epoch 0 new loss 4.182882548775524e-06 old loss 4.3104332689836156e-06 BETTER
I0316 10:47:15.257755 502949 finetune.py:68] layer 18_k @ epoch 2 new loss 3.912070496880915e-06 old loss 3.934199412469752e-06 BETTER
I0316 10:47:23.768507 502443 finetune.py:68] layer 17_k @ epoch 4 new loss 5.131437319505494e-06 old loss 5.135556421009824e-06 BETTER
W0316 10:47:25.487113 502443 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

17_k proxy err 0.0004890488344244659 tr(WHW.T) 4244.6298828125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.22it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.27it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.47it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.57it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.58it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.58it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.60it/s]I0316 10:47:32.806125 503455 finetune.py:68] layer 19_k @ epoch 1 new loss 4.154508587816963e-06 old loss 4.182882548775524e-06 BETTER
 47%|████▋     | 15/32 [00:06<00:06,  2.62it/s] 50%|█████     | 16/32 [00:06<00:06,  2.63it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.63it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s]I0316 10:47:34.032081 501939 finetune.py:68] layer 16_o @ epoch 0 new loss 9.052353561855853e-06 old loss 9.30325586523395e-06 BETTER
 59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.63it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.62it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.63it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
I0316 10:47:46.809742 502443 finetune.py:45] layer 17_o initial loss 9.206147296936251e-06
W0316 10:47:46.810139 502443 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:47:51.805038 502949 finetune.py:68] layer 18_k @ epoch 3 new loss 3.894781002600212e-06 old loss 3.912070496880915e-06 BETTER
I0316 10:48:08.326095 503455 finetune.py:68] layer 19_k @ epoch 2 new loss 4.129594344703946e-06 old loss 4.154508587816963e-06 BETTER
I0316 10:48:11.726763 501939 finetune.py:68] layer 16_o @ epoch 1 new loss 8.902493391360622e-06 old loss 9.052353561855853e-06 BETTER
I0316 10:48:22.186531 502443 finetune.py:68] layer 17_o @ epoch 0 new loss 8.942510248743929e-06 old loss 9.206147296936251e-06 BETTER
I0316 10:48:28.570803 502949 finetune.py:68] layer 18_k @ epoch 4 new loss 3.885932073899312e-06 old loss 3.894781002600212e-06 BETTER
W0316 10:48:30.292271 502949 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_k proxy err 0.0005281223566271365 tr(WHW.T) 4450.92626953125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.20it/s]  6%|▋         | 2/32 [00:01<00:17,  1.75it/s]  9%|▉         | 3/32 [00:01<00:14,  2.05it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.23it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.41it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.53it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.57it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s] 41%|████      | 13/32 [00:05<00:07,  2.58it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:06<00:06,  2.58it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.58it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.57it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.55it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.56it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.57it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.60it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.59it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.58it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.57it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.56it/s]I0316 10:48:44.240688 503455 finetune.py:68] layer 19_k @ epoch 3 new loss 4.108656867174432e-06 old loss 4.129594344703946e-06 BETTER
100%|██████████| 32/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
I0316 10:48:49.895240 501939 finetune.py:68] layer 16_o @ epoch 2 new loss 8.797314876574092e-06 old loss 8.902493391360622e-06 BETTER
I0316 10:48:52.043782 502949 finetune.py:45] layer 18_o initial loss 7.016235031187534e-06
W0316 10:48:52.044270 502949 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:48:58.589237 502443 finetune.py:68] layer 17_o @ epoch 1 new loss 8.808403435978107e-06 old loss 8.942510248743929e-06 BETTER
I0316 10:49:20.679521 503455 finetune.py:68] layer 19_k @ epoch 4 new loss 4.099002580915112e-06 old loss 4.108656867174432e-06 BETTER
W0316 10:49:22.465230 503455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

19_k proxy err 0.0005607209750451148 tr(WHW.T) 3977.8466796875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.18it/s]  6%|▋         | 2/32 [00:01<00:17,  1.73it/s]  9%|▉         | 3/32 [00:01<00:14,  2.04it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.40it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.48it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.48it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s]I0316 10:49:28.453917 502949 finetune.py:68] layer 18_o @ epoch 0 new loss 6.834503892605426e-06 old loss 7.016235031187534e-06 BETTER
I0316 10:49:28.470181 501939 finetune.py:68] layer 16_o @ epoch 3 new loss 8.701814294909127e-06 old loss 8.797314876574092e-06 BETTER
 34%|███▍      | 11/32 [00:04<00:08,  2.53it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.54it/s] 41%|████      | 13/32 [00:05<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.55it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.56it/s] 50%|█████     | 16/32 [00:06<00:06,  2.56it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.56it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.55it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.55it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.54it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.54it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.55it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.55it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.56it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.56it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.56it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.56it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.57it/s]I0316 10:49:35.551550 502443 finetune.py:68] layer 17_o @ epoch 2 new loss 8.718236131244339e-06 old loss 8.808403435978107e-06 BETTER
 91%|█████████ | 29/32 [00:11<00:01,  2.56it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.56it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.54it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
I0316 10:49:44.559868 503455 finetune.py:45] layer 19_o initial loss 7.053991339489585e-06
W0316 10:49:44.560252 503455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:50:05.065179 502949 finetune.py:68] layer 18_o @ epoch 1 new loss 6.758171366527677e-06 old loss 6.834503892605426e-06 BETTER
I0316 10:50:06.476546 501939 finetune.py:68] layer 16_o @ epoch 4 new loss 8.633406650915276e-06 old loss 8.701814294909127e-06 BETTER
W0316 10:50:08.424349 501939 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_o proxy err 0.002968769520521164 tr(WHW.T) 7.377532005310059
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:03,  2.05s/it]I0316 10:50:12.124896 502443 finetune.py:68] layer 17_o @ epoch 3 new loss 8.645629350212403e-06 old loss 8.718236131244339e-06 BETTER
  6%|▋         | 2/32 [00:03<00:51,  1.73s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it]I0316 10:50:19.469524 503455 finetune.py:68] layer 19_o @ epoch 0 new loss 6.8702647695317864e-06 old loss 7.053991339489585e-06 BETTER
 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.52s/it]I0316 10:50:41.422193 502949 finetune.py:68] layer 18_o @ epoch 2 new loss 6.695554020552663e-06 old loss 6.758171366527677e-06 BETTER
 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it]I0316 10:50:48.721531 502443 finetune.py:68] layer 17_o @ epoch 4 new loss 8.58613657328533e-06 old loss 8.645629350212403e-06 BETTER
 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it]W0316 10:50:50.181973 502443 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

17_o proxy err 0.0030867441091686487 tr(WHW.T) 6.40378475189209
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.53s/it]  3%|▎         | 1/32 [00:02<01:03,  2.04s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.53s/it]  6%|▋         | 2/32 [00:03<00:51,  1.73s/it]I0316 10:50:55.410604 503455 finetune.py:68] layer 19_o @ epoch 1 new loss 6.802523330406984e-06 old loss 6.8702647695317864e-06 BETTER
 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.54s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it]I0316 10:51:08.521425 501939 finetune.py:45] layer 16_up initial loss 2.092539580189623e-05
W0316 10:51:08.521645 501939 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it]I0316 10:51:17.850942 502949 finetune.py:68] layer 18_o @ epoch 3 new loss 6.655911420239136e-06 old loss 6.695554020552663e-06 BETTER
 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.52s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.52s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.51s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.51s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it]I0316 10:51:31.099304 503455 finetune.py:68] layer 19_o @ epoch 2 new loss 6.757886694686022e-06 old loss 6.802523330406984e-06 BETTER
 81%|████████▏ | 26/32 [00:40<00:09,  1.51s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.51s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.51s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it]100%|██████████| 32/32 [00:49<00:00,  1.51s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]
I0316 10:51:44.600093 501939 finetune.py:68] layer 16_up @ epoch 0 new loss 2.0436058548511937e-05 old loss 2.092539580189623e-05 BETTER
I0316 10:51:49.061569 502443 finetune.py:45] layer 17_up initial loss 2.2432102923630737e-05
W0316 10:51:49.061927 502443 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:51:54.183368 502949 finetune.py:68] layer 18_o @ epoch 4 new loss 6.620401109103113e-06 old loss 6.655911420239136e-06 BETTER
W0316 10:51:55.857518 502949 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_o proxy err 0.002975653624162078 tr(WHW.T) 4.733033180236816
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:04,  2.08s/it]  6%|▋         | 2/32 [00:03<00:53,  1.78s/it]  9%|▉         | 3/32 [00:05<00:48,  1.68s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.63s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.60s/it]I0316 10:52:06.426822 503455 finetune.py:68] layer 19_o @ epoch 3 new loss 6.723752903781133e-06 old loss 6.757886694686022e-06 BETTER
 19%|█▉        | 6/32 [00:09<00:41,  1.59s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.58s/it] 25%|██▌       | 8/32 [00:13<00:37,  1.57s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.57s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.56s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.57s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.57s/it] 41%|████      | 13/32 [00:20<00:29,  1.57s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.57s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.57s/it]I0316 10:52:21.509813 501939 finetune.py:68] layer 16_up @ epoch 1 new loss 2.010573007282801e-05 old loss 2.0436058548511937e-05 BETTER
 50%|█████     | 16/32 [00:25<00:25,  1.56s/it]I0316 10:52:22.940076 502443 finetune.py:68] layer 17_up @ epoch 0 new loss 2.187583413615357e-05 old loss 2.2432102923630737e-05 BETTER
 53%|█████▎    | 17/32 [00:27<00:23,  1.57s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.57s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.56s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.57s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.57s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.57s/it] 72%|███████▏  | 23/32 [00:36<00:14,  1.58s/it] 75%|███████▌  | 24/32 [00:38<00:12,  1.58s/it] 78%|███████▊  | 25/32 [00:39<00:11,  1.58s/it] 81%|████████▏ | 26/32 [00:41<00:09,  1.58s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.58s/it] 88%|████████▊ | 28/32 [00:44<00:06,  1.57s/it]I0316 10:52:41.981781 503455 finetune.py:68] layer 19_o @ epoch 4 new loss 6.693631348753115e-06 old loss 6.723752903781133e-06 BETTER
 91%|█████████ | 29/32 [00:45<00:04,  1.57s/it]W0316 10:52:43.605055 503455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:47<00:03,  1.57s/it]19_o proxy err 0.003154092701151967 tr(WHW.T) 3.9410908222198486
  0%|          | 0/32 [00:00<?, ?it/s] 97%|█████████▋| 31/32 [00:49<00:01,  1.57s/it]  3%|▎         | 1/32 [00:02<01:04,  2.09s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]100%|██████████| 32/32 [00:50<00:00,  1.58s/it]
  6%|▋         | 2/32 [00:03<00:53,  1.78s/it]  9%|▉         | 3/32 [00:05<00:48,  1.68s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.63s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.61s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.59s/it]I0316 10:52:56.262757 502949 finetune.py:45] layer 18_up initial loss 2.0873309040325694e-05
W0316 10:52:56.263040 502949 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:11<00:39,  1.58s/it] 25%|██▌       | 8/32 [00:13<00:37,  1.57s/it]I0316 10:52:58.074554 502443 finetune.py:68] layer 17_up @ epoch 1 new loss 2.1505589756998233e-05 old loss 2.187583413615357e-05 BETTER
I0316 10:52:58.141288 501939 finetune.py:68] layer 16_up @ epoch 2 new loss 1.9826080460916273e-05 old loss 2.010573007282801e-05 BETTER
 28%|██▊       | 9/32 [00:14<00:35,  1.56s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.56s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.56s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.55s/it] 41%|████      | 13/32 [00:20<00:29,  1.55s/it] 44%|████▍     | 14/32 [00:22<00:27,  1.55s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.55s/it] 50%|█████     | 16/32 [00:25<00:24,  1.55s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.55s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.55s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.55s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.55s/it] 66%|██████▌   | 21/32 [00:33<00:16,  1.54s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.55s/it] 72%|███████▏  | 23/32 [00:36<00:13,  1.55s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.54s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.55s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.55s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.55s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.55s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.55s/it]I0316 10:53:30.971556 502949 finetune.py:68] layer 18_up @ epoch 0 new loss 2.0385641619213857e-05 old loss 2.0873309040325694e-05 BETTER
 94%|█████████▍| 30/32 [00:47<00:03,  1.55s/it]I0316 10:53:33.213684 502443 finetune.py:68] layer 17_up @ epoch 2 new loss 2.1203930373303592e-05 old loss 2.1505589756998233e-05 BETTER
 97%|█████████▋| 31/32 [00:48<00:01,  1.55s/it]I0316 10:53:34.745786 501939 finetune.py:68] layer 16_up @ epoch 3 new loss 1.9580455045797862e-05 old loss 1.9826080460916273e-05 BETTER
100%|██████████| 32/32 [00:50<00:00,  1.55s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]
I0316 10:53:43.530997 503455 finetune.py:45] layer 19_up initial loss 2.2026579244993627e-05
W0316 10:53:43.531375 503455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:54:06.204276 502949 finetune.py:68] layer 18_up @ epoch 1 new loss 2.0054449123563245e-05 old loss 2.0385641619213857e-05 BETTER
I0316 10:54:08.536141 502443 finetune.py:68] layer 17_up @ epoch 3 new loss 2.0938330635544844e-05 old loss 2.1203930373303592e-05 BETTER
I0316 10:54:11.500288 501939 finetune.py:68] layer 16_up @ epoch 4 new loss 1.9365250409464352e-05 old loss 1.9580455045797862e-05 BETTER
W0316 10:54:13.113107 501939 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_up proxy err 0.0031151731964200735 tr(WHW.T) 981.8338623046875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.99s/it]I0316 10:54:17.055490 503455 finetune.py:68] layer 19_up @ epoch 0 new loss 2.152629167539999e-05 old loss 2.2026579244993627e-05 BETTER
  6%|▋         | 2/32 [00:03<00:51,  1.71s/it]  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.53s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it]I0316 10:54:41.389355 502949 finetune.py:68] layer 18_up @ epoch 2 new loss 1.9783976313192397e-05 old loss 2.0054449123563245e-05 BETTER
 56%|█████▋    | 18/32 [00:27<00:21,  1.52s/it]I0316 10:54:43.703341 502443 finetune.py:68] layer 17_up @ epoch 4 new loss 2.071483868348878e-05 old loss 2.0938330635544844e-05 BETTER
 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it]W0316 10:54:45.060346 502443 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 62%|██████▎   | 20/32 [00:30<00:18,  1.52s/it]17_up proxy err 0.0030530623625963926 tr(WHW.T) 1061.800537109375
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it]  3%|▎         | 1/32 [00:01<01:01,  1.97s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.52s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it]  9%|▉         | 3/32 [00:05<00:47,  1.62s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.52s/it]I0316 10:54:52.111739 503455 finetune.py:68] layer 19_up @ epoch 1 new loss 2.119265445799101e-05 old loss 2.152629167539999e-05 BETTER
 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.52s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.52s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.54s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.53s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.52s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it]I0316 10:55:12.612623 501939 finetune.py:45] layer 16_gate initial loss 2.634359771036543e-05
W0316 10:55:12.613068 501939 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it]I0316 10:55:16.575965 502949 finetune.py:68] layer 18_up @ epoch 3 new loss 1.954920298885554e-05 old loss 1.9783976313192397e-05 BETTER
 62%|██████▎   | 20/32 [00:31<00:18,  1.52s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.52s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it]I0316 10:55:26.427925 503455 finetune.py:68] layer 19_up @ epoch 2 new loss 2.0920531824231148e-05 old loss 2.119265445799101e-05 BETTER
 81%|████████▏ | 26/32 [00:40<00:09,  1.51s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.52s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.61s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.58s/it]100%|██████████| 32/32 [00:49<00:00,  1.56s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
I0316 10:55:44.539636 502443 finetune.py:45] layer 17_gate initial loss 2.8887478038086556e-05
W0316 10:55:44.540035 502443 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:55:46.990461 501939 finetune.py:68] layer 16_gate @ epoch 0 new loss 2.6016030460596085e-05 old loss 2.634359771036543e-05 BETTER
I0316 10:55:51.814763 502949 finetune.py:68] layer 18_up @ epoch 4 new loss 1.9344040993019007e-05 old loss 1.954920298885554e-05 BETTER
W0316 10:55:53.260147 502949 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_up proxy err 0.0033237296156585217 tr(WHW.T) 1055.842529296875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it]  6%|▋         | 2/32 [00:03<00:51,  1.72s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it]I0316 10:56:00.735110 503455 finetune.py:68] layer 19_up @ epoch 3 new loss 2.0683499315055087e-05 old loss 2.0920531824231148e-05 BETTER
 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.55s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.55s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.54s/it]I0316 10:56:17.482145 502443 finetune.py:68] layer 17_gate @ epoch 0 new loss 2.8503591238404624e-05 old loss 2.8887478038086556e-05 BETTER
 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it] 50%|█████     | 16/32 [00:25<00:24,  1.52s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it]I0316 10:56:22.031154 501939 finetune.py:68] layer 16_gate @ epoch 1 new loss 2.580926775408443e-05 old loss 2.6016030460596085e-05 BETTER
 56%|█████▋    | 18/32 [00:28<00:21,  1.52s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.52s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.52s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.52s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.52s/it]I0316 10:56:34.816542 503455 finetune.py:68] layer 19_up @ epoch 4 new loss 2.048108945018612e-05 old loss 2.0683499315055087e-05 BETTER
W0316 10:56:36.204158 503455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it]19_up proxy err 0.0034944573417305946 tr(WHW.T) 1068.244873046875
  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:43<00:06,  1.51s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it]  3%|▎         | 1/32 [00:01<01:01,  1.99s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.52s/it]  6%|▋         | 2/32 [00:03<00:51,  1.72s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it]100%|██████████| 32/32 [00:49<00:00,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.57s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.56s/it]I0316 10:56:50.845137 502443 finetune.py:68] layer 17_gate @ epoch 1 new loss 2.827701609930955e-05 old loss 2.8503591238404624e-05 BETTER
 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it]I0316 10:56:52.349281 502949 finetune.py:45] layer 18_gate initial loss 2.8249060051166452e-05
W0316 10:56:52.349642 502949 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:15<00:34,  1.56s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.55s/it] 38%|███▊      | 12/32 [00:18<00:31,  1.55s/it]I0316 10:56:57.079645 501939 finetune.py:68] layer 16_gate @ epoch 2 new loss 2.5623854526202194e-05 old loss 2.580926775408443e-05 BETTER
 41%|████      | 13/32 [00:20<00:29,  1.55s/it] 44%|████▍     | 14/32 [00:22<00:27,  1.55s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.55s/it] 50%|█████     | 16/32 [00:25<00:24,  1.55s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.54s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.54s/it] 59%|█████▉    | 19/32 [00:29<00:20,  1.55s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.54s/it] 66%|██████▌   | 21/32 [00:32<00:17,  1.55s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.55s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.55s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.55s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.55s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.55s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.55s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.55s/it]I0316 10:57:24.166281 502443 finetune.py:68] layer 17_gate @ epoch 2 new loss 2.8079302865080535e-05 old loss 2.827701609930955e-05 BETTER
 94%|█████████▍| 30/32 [00:46<00:03,  1.55s/it]I0316 10:57:25.034440 502949 finetune.py:68] layer 18_gate @ epoch 0 new loss 2.790308280964382e-05 old loss 2.8249060051166452e-05 BETTER
 97%|█████████▋| 31/32 [00:48<00:01,  1.55s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]100%|██████████| 32/32 [00:49<00:00,  1.56s/it]
I0316 10:57:32.471063 501939 finetune.py:68] layer 16_gate @ epoch 3 new loss 2.546212272136472e-05 old loss 2.5623854526202194e-05 BETTER
I0316 10:57:36.451239 503455 finetune.py:45] layer 19_gate initial loss 3.0359518859768286e-05
W0316 10:57:36.451623 503455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:57:58.034143 502443 finetune.py:68] layer 17_gate @ epoch 3 new loss 2.790573489619419e-05 old loss 2.8079302865080535e-05 BETTER
I0316 10:57:59.029236 502949 finetune.py:68] layer 18_gate @ epoch 1 new loss 2.7701951694325544e-05 old loss 2.790308280964382e-05 BETTER
I0316 10:58:07.927552 501939 finetune.py:68] layer 16_gate @ epoch 4 new loss 2.5314602680737153e-05 old loss 2.546212272136472e-05 BETTER
I0316 10:58:08.879761 503455 finetune.py:68] layer 19_gate @ epoch 0 new loss 3.0016573873581365e-05 old loss 3.0359518859768286e-05 BETTER
W0316 10:58:09.211034 501939 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_gate proxy err 0.0010908253025263548 tr(WHW.T) 4849.0322265625
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:38,  1.12it/s]  2%|▏         | 2/112 [00:01<01:05,  1.69it/s]  3%|▎         | 3/112 [00:01<00:54,  1.99it/s]  4%|▎         | 4/112 [00:02<00:49,  2.20it/s]  4%|▍         | 5/112 [00:02<00:46,  2.31it/s]  5%|▌         | 6/112 [00:02<00:44,  2.39it/s]  6%|▋         | 7/112 [00:03<00:42,  2.45it/s]  7%|▋         | 8/112 [00:03<00:41,  2.51it/s]  8%|▊         | 9/112 [00:03<00:40,  2.54it/s]  9%|▉         | 10/112 [00:04<00:39,  2.57it/s] 10%|▉         | 11/112 [00:04<00:39,  2.58it/s] 11%|█         | 12/112 [00:05<00:38,  2.59it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.60it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.59it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.60it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.59it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.57it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.58it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.59it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.59it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.60it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.61it/s] 21%|██        | 23/112 [00:09<00:34,  2.61it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.61it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.61it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.61it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.59it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.61it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.57it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.58it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.59it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.60it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.61it/s] 30%|███       | 34/112 [00:13<00:29,  2.61it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.61it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.62it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.62it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.62it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.62it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.61it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.61it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.58it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.59it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.60it/s] 40%|████      | 45/112 [00:17<00:25,  2.61it/s] 41%|████      | 46/112 [00:18<00:25,  2.62it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.63it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.63it/s]I0316 10:58:31.623166 502443 finetune.py:68] layer 17_gate @ epoch 4 new loss 2.775169377855491e-05 old loss 2.790573489619419e-05 BETTER
 44%|████▍     | 49/112 [00:19<00:23,  2.63it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.63it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.62it/s]W0316 10:58:32.891528 502443 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 10:58:32.938740 502949 finetune.py:68] layer 18_gate @ epoch 2 new loss 2.7530495572136715e-05 old loss 2.7701951694325544e-05 BETTER
 46%|████▋     | 52/112 [00:20<00:22,  2.61it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.60it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.58it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.60it/s] 50%|█████     | 56/112 [00:22<00:21,  2.60it/s] 51%|█████     | 57/112 [00:22<00:21,  2.60it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.61it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.60it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.61it/s]17_gate proxy err 0.0011013072216883302 tr(WHW.T) 5238.2060546875
  0%|          | 0/112 [00:00<?, ?it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.62it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.62it/s]  1%|          | 1/112 [00:00<01:35,  1.16it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.62it/s]  2%|▏         | 2/112 [00:01<01:03,  1.72it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.61it/s]  3%|▎         | 3/112 [00:01<00:53,  2.04it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.58it/s]  4%|▎         | 4/112 [00:02<00:48,  2.22it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.59it/s]  4%|▍         | 5/112 [00:02<00:45,  2.34it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.61it/s]  5%|▌         | 6/112 [00:02<00:43,  2.41it/s] 61%|██████    | 68/112 [00:26<00:16,  2.61it/s]  6%|▋         | 7/112 [00:03<00:42,  2.46it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.61it/s]  7%|▋         | 8/112 [00:03<00:41,  2.51it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.62it/s]  8%|▊         | 9/112 [00:03<00:40,  2.53it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.63it/s]  9%|▉         | 10/112 [00:04<00:40,  2.54it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.63it/s] 10%|▉         | 11/112 [00:04<00:39,  2.55it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.64it/s] 11%|█         | 12/112 [00:05<00:39,  2.56it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.65it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.56it/s]I0316 10:58:41.713092 503455 finetune.py:68] layer 19_gate @ epoch 1 new loss 2.9818889743182808e-05 old loss 3.0016573873581365e-05 BETTER
 67%|██████▋   | 75/112 [00:29<00:14,  2.63it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.58it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.63it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.58it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.61it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.59it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.57it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.59it/s] 71%|███████   | 79/112 [00:30<00:12,  2.58it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.60it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.59it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.60it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.59it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.61it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.60it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.61it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.61it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.61it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.61it/s] 21%|██        | 23/112 [00:09<00:34,  2.61it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.61it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.58it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.61it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.58it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.61it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.58it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.61it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.59it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.60it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.59it/s] 80%|████████  | 90/112 [00:35<00:08,  2.57it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.60it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.58it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.60it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.59it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.60it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.59it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.59it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.60it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.59it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.60it/s] 30%|███       | 34/112 [00:13<00:30,  2.59it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.60it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.60it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.61it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.58it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.61it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.58it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.61it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.59it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.59it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.59it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.59it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.60it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.56it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.59it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.57it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.59it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.58it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.60it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.58it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.59it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.59it/s] 40%|████      | 45/112 [00:17<00:25,  2.59it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.60it/s] 41%|████      | 46/112 [00:18<00:25,  2.59it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.60it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.57it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.60it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.58it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.60it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.59it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.59it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.59it/s]100%|██████████| 112/112 [00:43<00:00,  2.58it/s]100%|██████████| 112/112 [00:43<00:00,  2.57it/s]
 46%|████▌     | 51/112 [00:20<00:23,  2.59it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.59it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.59it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.60it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.60it/s] 50%|█████     | 56/112 [00:22<00:21,  2.60it/s] 51%|█████     | 57/112 [00:22<00:21,  2.58it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.58it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.56it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.59it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.60it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.60it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.61it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.61it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.61it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.61it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.60it/s] 61%|██████    | 68/112 [00:26<00:16,  2.59it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.59it/s]W0316 10:59:03.578000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:59:03.579000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:59:03.579000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:59:03.579000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:59:03.579000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:59:03.579000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:59:03.579000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:59:03.625000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:59:03.625000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:59:03.625000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:59:03.626000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:59:03.626000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 70/112 [00:27<00:16,  2.59it/s]W0316 10:59:03.808000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:59:03.808000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:59:03.808000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:59:03.808000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:59:03.809000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 63%|██████▎   | 71/112 [00:27<00:15,  2.56it/s]W0316 10:59:04.135000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:59:04.136000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:59:04.136000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:59:04.136000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:59:04.136000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:59:04.136000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:59:04.136000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:59:04.169000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:59:04.169000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:59:04.169000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:59:04.170000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:59:04.170000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:59:04.242000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:59:04.242000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:59:04.242000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:59:04.242000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:59:04.242000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 64%|██████▍   | 72/112 [00:28<00:15,  2.57it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.58it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.59it/s]W0316 10:59:05.608000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 67%|██████▋   | 75/112 [00:29<00:14,  2.59it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.60it/s]W0316 10:59:06.087000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:59:06.088000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:59:06.088000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:59:06.088000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:59:06.088000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:59:06.088000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:59:06.088000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:59:06.121000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:59:06.121000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:59:06.121000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:59:06.121000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:59:06.121000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 77/112 [00:30<00:13,  2.60it/s]I0316 10:59:06.441037 502949 finetune.py:68] layer 18_gate @ epoch 3 new loss 2.7376701837056316e-05 old loss 2.7530495572136715e-05 BETTER
W0316 10:59:06.514000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:59:06.515000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:59:06.515000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:59:06.515000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:59:06.515000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 70%|██████▉   | 78/112 [00:30<00:13,  2.60it/s]W0316 10:59:07.075000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:59:07.081000 140690016724800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 71%|███████   | 79/112 [00:30<00:12,  2.60it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.60it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.60it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.57it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.58it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.58it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.58it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.58it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.59it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.59it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.60it/s] 80%|████████  | 90/112 [00:35<00:08,  2.60it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.59it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.59it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.58it/s] 84%|████████▍ | 94/112 [00:36<00:07,  2.55it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.56it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.57it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.57it/s]I0316 10:59:14.468782 501939 finetune.py:45] layer 16_down initial loss 4.117855496588163e-05
W0316 10:59:14.469275 501939 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 98/112 [00:38<00:05,  2.58it/s]I0316 10:59:14.760917 503455 finetune.py:68] layer 19_gate @ epoch 2 new loss 2.964359191537369e-05 old loss 2.9818889743182808e-05 BETTER
 88%|████████▊ | 99/112 [00:38<00:05,  2.59it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.59it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.59it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.60it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.59it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.58it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.57it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.54it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.55it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.56it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.57it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.58it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.59it/s]100%|██████████| 112/112 [00:43<00:00,  2.60it/s]100%|██████████| 112/112 [00:43<00:00,  2.56it/s]
W0316 10:59:27.405000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.405000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.405000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.405000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.406000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.406000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.406000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.448000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.448000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.448000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.448000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.449000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.629000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.629000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.629000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.629000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.629000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.954000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.954000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.954000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.955000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.955000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.955000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.955000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.988000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.989000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.989000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.989000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:59:27.989000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:59:28.061000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 10:59:28.062000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 10:59:28.062000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 10:59:28.062000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 10:59:28.062000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 10:59:29.414000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:59:29.890000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 10:59:29.890000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 10:59:29.890000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:59:29.890000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:59:29.890000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:59:29.890000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:59:29.890000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:59:29.923000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:59:29.923000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:59:29.923000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:59:29.923000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:59:29.923000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:59:30.311000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 10:59:30.311000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 10:59:30.311000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 10:59:30.311000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 10:59:30.311000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 10:59:30.859000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 10:59:30.864000 140076968355648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0316 10:59:38.285726 502443 finetune.py:45] layer 17_down initial loss 4.7065212129382417e-05
W0316 10:59:38.286398 502443 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 10:59:39.706621 502949 finetune.py:68] layer 18_gate @ epoch 4 new loss 2.7240092094871216e-05 old loss 2.7376701837056316e-05 BETTER
W0316 10:59:40.902770 502949 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_gate proxy err 0.0013588841538876295 tr(WHW.T) 4663.220703125
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:41,  1.10it/s]  2%|▏         | 2/112 [00:01<01:06,  1.65it/s]  3%|▎         | 3/112 [00:01<00:55,  1.97it/s]  4%|▎         | 4/112 [00:02<00:49,  2.18it/s]I0316 10:59:47.262986 501939 finetune.py:68] layer 16_down @ epoch 0 new loss 4.116351919947192e-05 old loss 4.117855496588163e-05 BETTER
  4%|▍         | 5/112 [00:02<00:46,  2.31it/s]  5%|▌         | 6/112 [00:02<00:44,  2.40it/s]I0316 10:59:47.711879 503455 finetune.py:68] layer 19_gate @ epoch 3 new loss 2.9490431188605726e-05 old loss 2.964359191537369e-05 BETTER
  6%|▋         | 7/112 [00:03<00:42,  2.46it/s]  7%|▋         | 8/112 [00:03<00:41,  2.49it/s]  8%|▊         | 9/112 [00:04<00:40,  2.52it/s]  9%|▉         | 10/112 [00:04<00:40,  2.55it/s] 10%|▉         | 11/112 [00:04<00:39,  2.55it/s] 11%|█         | 12/112 [00:05<00:39,  2.55it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.56it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.56it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.56it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.56it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.57it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.58it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.58it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.59it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.60it/s] 20%|█▉        | 22/112 [00:09<00:34,  2.60it/s] 21%|██        | 23/112 [00:09<00:34,  2.59it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.59it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.58it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.57it/s] 24%|██▍       | 27/112 [00:10<00:33,  2.57it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.57it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.56it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.57it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.58it/s] 29%|██▊       | 32/112 [00:12<00:31,  2.58it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.58it/s] 30%|███       | 34/112 [00:13<00:30,  2.58it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.58it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.59it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.59it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.59it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.59it/s] 36%|███▌      | 40/112 [00:16<00:27,  2.59it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.59it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.60it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.59it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.58it/s] 40%|████      | 45/112 [00:17<00:25,  2.58it/s] 41%|████      | 46/112 [00:18<00:25,  2.57it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.57it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.58it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.58it/s] 45%|████▍     | 50/112 [00:19<00:24,  2.58it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.59it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.59it/s] 47%|████▋     | 53/112 [00:21<00:22,  2.60it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.59it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.59it/s] 50%|█████     | 56/112 [00:22<00:21,  2.58it/s] 51%|█████     | 57/112 [00:22<00:21,  2.58it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.58it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.57it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.58it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.58it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.59it/s]I0316 11:00:09.376317 502443 finetune.py:68] layer 17_down @ epoch 0 new loss 4.704236562247388e-05 old loss 4.7065212129382417e-05 BETTER
 56%|█████▋    | 63/112 [00:24<00:18,  2.59it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.60it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.60it/s] 59%|█████▉    | 66/112 [00:26<00:17,  2.60it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.60it/s] 61%|██████    | 68/112 [00:26<00:16,  2.61it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.60it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.59it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.60it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.58it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.58it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.58it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.57it/s] 68%|██████▊   | 76/112 [00:29<00:14,  2.57it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.58it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.59it/s] 71%|███████   | 79/112 [00:31<00:12,  2.60it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.60it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.59it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.59it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.59it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.58it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.57it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.57it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.56it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.56it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.56it/s] 80%|████████  | 90/112 [00:35<00:08,  2.56it/s]I0316 11:00:20.415279 501939 finetune.py:68] layer 16_down @ epoch 1 new loss 4.116115451324731e-05 old loss 4.116351919947192e-05 BETTER
 81%|████████▏ | 91/112 [00:35<00:08,  2.57it/s]I0316 11:00:20.734126 503455 finetune.py:68] layer 19_gate @ epoch 4 new loss 2.9358843676163815e-05 old loss 2.9490431188605726e-05 BETTER
 82%|████████▏ | 92/112 [00:36<00:07,  2.57it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.58it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.58it/s]W0316 11:00:21.967415 503455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 85%|████████▍ | 95/112 [00:37<00:06,  2.59it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.60it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.61it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.61it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.60it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.60it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.59it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.58it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.58it/s]19_gate proxy err 0.0015013059601187706 tr(WHW.T) 4579.662109375
  0%|          | 0/112 [00:00<?, ?it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.59it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.59it/s]  1%|          | 1/112 [00:00<01:39,  1.12it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.59it/s]  2%|▏         | 2/112 [00:01<01:06,  1.66it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.60it/s]  3%|▎         | 3/112 [00:01<00:55,  1.97it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.61it/s]  4%|▎         | 4/112 [00:02<00:49,  2.17it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.62it/s]  4%|▍         | 5/112 [00:02<00:46,  2.29it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.63it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.63it/s]  5%|▌         | 6/112 [00:02<00:44,  2.38it/s]100%|██████████| 112/112 [00:43<00:00,  2.63it/s]100%|██████████| 112/112 [00:43<00:00,  2.56it/s]
  6%|▋         | 7/112 [00:03<00:43,  2.44it/s]  7%|▋         | 8/112 [00:03<00:41,  2.48it/s]  8%|▊         | 9/112 [00:04<00:40,  2.51it/s]  9%|▉         | 10/112 [00:04<00:40,  2.54it/s] 10%|▉         | 11/112 [00:04<00:39,  2.56it/s] 11%|█         | 12/112 [00:05<00:38,  2.57it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.58it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.58it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.58it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.59it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.58it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.57it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.57it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.57it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.57it/s] 20%|█▉        | 22/112 [00:09<00:34,  2.58it/s] 21%|██        | 23/112 [00:09<00:34,  2.58it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.58it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.58it/s]W0316 11:00:35.894000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:00:35.894000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:00:35.895000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:00:35.895000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:00:35.895000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:00:35.895000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:00:35.895000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:00:35.937000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:00:35.937000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:00:35.937000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:00:35.937000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:00:35.937000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 23%|██▎       | 26/112 [00:10<00:33,  2.60it/s]W0316 11:00:36.112000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.112000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.112000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.112000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.112000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 24%|██▍       | 27/112 [00:10<00:32,  2.61it/s]W0316 11:00:36.446000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.446000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.446000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.446000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.446000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.447000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.447000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.480000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.480000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.480000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.480000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.481000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.551000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.552000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.552000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.552000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:00:36.552000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 28/112 [00:11<00:32,  2.61it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.60it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.60it/s]W0316 11:00:37.897000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 31/112 [00:12<00:31,  2.61it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.61it/s]W0316 11:00:38.358000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:00:38.359000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:00:38.359000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:00:38.359000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:00:38.359000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:00:38.359000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:00:38.359000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:00:38.392000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:00:38.392000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:00:38.392000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:00:38.392000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:00:38.392000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 29%|██▉       | 33/112 [00:13<00:30,  2.60it/s]W0316 11:00:38.776000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:00:38.776000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:00:38.776000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:00:38.776000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:00:38.776000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 30%|███       | 34/112 [00:13<00:30,  2.59it/s]W0316 11:00:39.322000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:00:39.327000 140323268310848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 35/112 [00:14<00:29,  2.58it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.57it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.59it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.59it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.59it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.59it/s]I0316 11:00:41.454520 502443 finetune.py:76] layer 17_down @ epoch 1 new loss 4.7043962695170194e-05 old loss 4.704236562247388e-05 WORSE
 37%|███▋      | 41/112 [00:16<00:27,  2.60it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.60it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.60it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.60it/s] 40%|████      | 45/112 [00:17<00:26,  2.57it/s] 41%|████      | 46/112 [00:18<00:25,  2.57it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.56it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.56it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.56it/s] 45%|████▍     | 50/112 [00:19<00:24,  2.55it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.54it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.55it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.55it/s]I0316 11:00:46.624160 502949 finetune.py:45] layer 18_down initial loss 4.679780613514595e-05
W0316 11:00:46.624499 502949 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 48%|████▊     | 54/112 [00:21<00:22,  2.56it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.56it/s] 50%|█████     | 56/112 [00:22<00:21,  2.57it/s] 51%|█████     | 57/112 [00:22<00:21,  2.58it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.58it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.58it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.58it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.58it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.57it/s] 56%|█████▋    | 63/112 [00:24<00:19,  2.54it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.55it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.56it/s] 59%|█████▉    | 66/112 [00:26<00:17,  2.57it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.58it/s] 61%|██████    | 68/112 [00:27<00:20,  2.17it/s] 62%|██████▏   | 69/112 [00:27<00:18,  2.28it/s] 62%|██████▎   | 70/112 [00:27<00:17,  2.36it/s] 63%|██████▎   | 71/112 [00:28<00:16,  2.42it/s]I0316 11:00:54.006612 501939 finetune.py:68] layer 16_down @ epoch 2 new loss 4.116070340387523e-05 old loss 4.116115451324731e-05 BETTER
 64%|██████▍   | 72/112 [00:28<00:16,  2.46it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.49it/s] 66%|██████▌   | 74/112 [00:29<00:15,  2.52it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.49it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.51it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.53it/s] 70%|██████▉   | 78/112 [00:31<00:13,  2.54it/s] 71%|███████   | 79/112 [00:31<00:12,  2.54it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.54it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.55it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.55it/s] 74%|███████▍  | 83/112 [00:33<00:11,  2.56it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.55it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.55it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.55it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.50it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.52it/s] 79%|███████▉  | 89/112 [00:35<00:09,  2.53it/s] 80%|████████  | 90/112 [00:35<00:08,  2.53it/s] 81%|████████▏ | 91/112 [00:36<00:08,  2.50it/s] 82%|████████▏ | 92/112 [00:36<00:08,  2.48it/s] 83%|████████▎ | 93/112 [00:37<00:07,  2.47it/s] 84%|████████▍ | 94/112 [00:37<00:07,  2.47it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.46it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.44it/s] 87%|████████▋ | 97/112 [00:38<00:06,  2.43it/s] 88%|████████▊ | 98/112 [00:39<00:05,  2.40it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.41it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.41it/s] 90%|█████████ | 101/112 [00:40<00:04,  2.41it/s] 91%|█████████ | 102/112 [00:40<00:04,  2.41it/s] 92%|█████████▏| 103/112 [00:41<00:03,  2.41it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.40it/s] 94%|█████████▍| 105/112 [00:42<00:02,  2.39it/s] 95%|█████████▍| 106/112 [00:42<00:02,  2.34it/s] 96%|█████████▌| 107/112 [00:42<00:02,  2.40it/s] 96%|█████████▋| 108/112 [00:43<00:01,  2.45it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.49it/s] 98%|█████████▊| 110/112 [00:44<00:00,  2.48it/s] 99%|█████████▉| 111/112 [00:44<00:00,  2.51it/s]100%|██████████| 112/112 [00:44<00:00,  2.53it/s]100%|██████████| 112/112 [00:44<00:00,  2.50it/s]
I0316 11:01:13.071296 502443 finetune.py:68] layer 17_down @ epoch 2 new loss 4.7039833589224145e-05 old loss 4.704236562247388e-05 BETTER
I0316 11:01:18.111488 502949 finetune.py:68] layer 18_down @ epoch 0 new loss 4.678283949033357e-05 old loss 4.679780613514595e-05 BETTER
W0316 11:01:18.140000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.140000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.140000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.141000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.141000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.141000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.141000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.187000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.187000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.187000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.187000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.187000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.368000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.369000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.369000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.369000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.369000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.697000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.698000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.698000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.698000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.698000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.698000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.698000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.730000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.731000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.731000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.731000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.731000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.823000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.823000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.823000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.823000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:01:18.824000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:01:20.183000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:01:20.648000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:01:20.648000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:01:20.648000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:01:20.649000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:01:20.649000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:01:20.649000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:01:20.649000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:01:20.681000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:01:20.681000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:01:20.681000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:01:20.682000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:01:20.682000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:01:21.068000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:01:21.068000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:01:21.068000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:01:21.068000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:01:21.068000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:01:21.622000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:01:21.628000 139964807571264 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0316 11:01:27.566989 501939 finetune.py:68] layer 16_down @ epoch 3 new loss 4.115700357942842e-05 old loss 4.116070340387523e-05 BETTER
I0316 11:01:29.605689 503455 finetune.py:45] layer 19_down initial loss 4.966467167832889e-05
W0316 11:01:29.606138 503455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:01:45.419097 502443 finetune.py:68] layer 17_down @ epoch 3 new loss 4.7037992771947756e-05 old loss 4.7039833589224145e-05 BETTER
I0316 11:01:49.823942 502949 finetune.py:68] layer 18_down @ epoch 1 new loss 4.677861579693854e-05 old loss 4.678283949033357e-05 BETTER
I0316 11:02:00.188645 503455 finetune.py:68] layer 19_down @ epoch 0 new loss 4.964566323906183e-05 old loss 4.966467167832889e-05 BETTER
I0316 11:02:00.769363 501939 finetune.py:76] layer 16_down @ epoch 4 new loss 4.1158349631587043e-05 old loss 4.115700357942842e-05 WORSE
W0316 11:02:01.400670 501939 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

16_down proxy err 0.0037487393710762262 tr(WHW.T) 17.941303253173828
I0316 11:02:18.022006 502443 finetune.py:68] layer 17_down @ epoch 4 new loss 4.703645390691236e-05 old loss 4.7037992771947756e-05 BETTER
W0316 11:02:19.158013 502443 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

17_down proxy err 0.0038175841327756643 tr(WHW.T) 21.333011627197266
I0316 11:02:21.989468 502949 finetune.py:76] layer 18_down @ epoch 2 new loss 4.677868855651468e-05 old loss 4.677861579693854e-05 WORSE
I0316 11:02:32.074213 503455 finetune.py:76] layer 19_down @ epoch 1 new loss 4.964624167769216e-05 old loss 4.964566323906183e-05 WORSE
I0316 11:02:53.518431 502949 finetune.py:68] layer 18_down @ epoch 3 new loss 4.677765173255466e-05 old loss 4.677861579693854e-05 BETTER
I0316 11:03:02.979218 503455 finetune.py:68] layer 19_down @ epoch 2 new loss 4.964310210198164e-05 old loss 4.964566323906183e-05 BETTER
I0316 11:03:25.208991 502949 finetune.py:76] layer 18_down @ epoch 4 new loss 4.677765173255466e-05 old loss 4.677765173255466e-05 WORSE
W0316 11:03:25.746031 502949 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

18_down proxy err 0.003908668179064989 tr(WHW.T) 21.113000869750977
I0316 11:03:29.020263 460347 quantize_finetune_llama.py:186] computed original embedding for layer 20 in 65.681795835495s
I0316 11:03:29.416117 460347 quantize_finetune_llama.py:159] layer 21 gpu 1
I0316 11:03:31.527032 504153 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 11:03:31.527165 504153 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 11:03:31.527231 504153 utils.py:162] NumExpr defaulting to 16 threads.
I0316 11:03:31.735805 504153 config.py:58] PyTorch version 2.4.0 available.
I0316 11:03:33.948656 504153 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0316 11:03:34.218288 503455 finetune.py:68] layer 19_down @ epoch 3 new loss 4.9642858357401565e-05 old loss 4.964310210198164e-05 BETTER
W0316 11:03:34.339488 504153 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:53,  1.72s/it]  6%|▋         | 2/32 [00:02<00:27,  1.10it/s]  9%|▉         | 3/32 [00:02<00:18,  1.53it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.89it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.17it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.65it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.83it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.86it/s] 41%|████      | 13/32 [00:05<00:06,  2.91it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.93it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.95it/s] 50%|█████     | 16/32 [00:06<00:05,  2.96it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.96it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.96it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.96it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.96it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.97it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.97it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.98it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.98it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.99it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.95it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.96it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.97it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.98it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.99it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.99it/s]100%|██████████| 32/32 [00:12<00:00,  2.99it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
W0316 11:03:50.352000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:03:50.353000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:03:50.353000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:03:50.353000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:03:50.353000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:03:50.353000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:03:50.353000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:03:50.396000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:03:50.396000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:03:50.396000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:03:50.396000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:03:50.396000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:03:50.692000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:03:50.692000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:03:50.692000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:03:50.692000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:03:50.692000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:03:51.322000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:03:51.322000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:03:51.322000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:03:51.322000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:03:51.322000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:03:51.322000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:03:51.322000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:03:51.340000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:03:51.340000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:03:51.340000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:03:51.340000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:03:51.341000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:03:51.540000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:03:51.540000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:03:51.540000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:03:51.540000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:03:51.540000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:03:52.694000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:03:52.694000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:03:52.694000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:03:52.695000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:03:52.695000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:03:52.695000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:03:52.695000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:03:52.713000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:03:52.713000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:03:52.713000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:03:52.713000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:03:52.713000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:03:53.359000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:03:53.359000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:03:53.359000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:03:53.359000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:03:53.359000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 11:03:59.848231 504153 finetune.py:45] layer 20_v initial loss 1.3031363778281957e-05
W0316 11:03:59.848456 504153 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:04:05.431212 503455 finetune.py:76] layer 19_down @ epoch 4 new loss 4.964305480825715e-05 old loss 4.9642858357401565e-05 WORSE
W0316 11:04:05.985756 503455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

19_down proxy err 0.003918192815035582 tr(WHW.T) 22.01154136657715
I0316 11:04:32.742267 460347 quantize_finetune_llama.py:186] computed original embedding for layer 21 in 62.57158875465393s
I0316 11:04:33.151603 460347 quantize_finetune_llama.py:159] layer 22 gpu 2
I0316 11:04:35.238817 504657 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 11:04:35.239012 504657 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 11:04:35.239111 504657 utils.py:162] NumExpr defaulting to 16 threads.
I0316 11:04:35.452312 504657 config.py:58] PyTorch version 2.4.0 available.
I0316 11:04:36.275936 504153 finetune.py:68] layer 20_v @ epoch 0 new loss 3.547555024852045e-06 old loss 1.3031363778281957e-05 BETTER
I0316 11:04:37.680423 504657 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 11:04:38.102274 504657 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.94s/it]  6%|▋         | 2/32 [00:02<00:30,  1.02s/it]  9%|▉         | 3/32 [00:02<00:20,  1.40it/s] 12%|█▎        | 4/32 [00:03<00:15,  1.75it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.03it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.26it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.54it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.66it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.72it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.74it/s] 41%|████      | 13/32 [00:06<00:06,  2.77it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.81it/s] 50%|█████     | 16/32 [00:07<00:05,  2.80it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.82it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.82it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.81it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.83it/s] 66%|██████▌   | 21/32 [00:09<00:03,  2.82it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.82it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.81it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.81it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.82it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.81it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.83it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.84it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.81it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
W0316 11:04:54.860000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:04:54.860000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:04:54.860000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:04:54.860000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:04:54.860000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:04:54.860000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:04:54.860000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:04:54.885000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:04:54.885000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:04:54.885000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:04:54.885000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:04:54.885000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:04:55.178000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:04:55.178000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:04:55.178000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:04:55.178000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:04:55.178000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:04:55.791000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:04:55.792000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:04:55.792000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:04:55.792000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:04:55.792000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:04:55.792000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:04:55.792000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:04:55.809000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:04:55.809000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:04:55.809000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:04:55.809000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:04:55.809000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:04:56.005000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:04:56.005000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:04:56.006000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:04:56.006000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:04:56.006000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:04:57.161000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:04:57.162000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:04:57.162000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:04:57.162000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:04:57.162000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:04:57.162000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:04:57.162000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:04:57.180000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:04:57.180000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:04:57.180000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:04:57.180000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:04:57.180000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:04:57.793000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:04:57.794000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:04:57.794000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:04:57.794000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:04:57.794000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 11:05:03.913750 504657 finetune.py:45] layer 21_v initial loss 1.3453287465381436e-05
W0316 11:05:03.913970 504657 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:05:13.780182 504153 finetune.py:68] layer 20_v @ epoch 1 new loss 3.129839569737669e-06 old loss 3.547555024852045e-06 BETTER
I0316 11:05:35.421015 460347 quantize_finetune_llama.py:186] computed original embedding for layer 22 in 61.75376796722412s
I0316 11:05:35.815152 460347 quantize_finetune_llama.py:159] layer 23 gpu 3
I0316 11:05:37.922069 505163 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 11:05:37.922228 505163 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 11:05:37.922305 505163 utils.py:162] NumExpr defaulting to 16 threads.
I0316 11:05:38.148596 505163 config.py:58] PyTorch version 2.4.0 available.
I0316 11:05:38.294450 504657 finetune.py:68] layer 21_v @ epoch 0 new loss 4.407967935549095e-06 old loss 1.3453287465381436e-05 BETTER
I0316 11:05:40.401424 505163 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 11:05:41.087983 505163 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:02,  2.02s/it]  6%|▋         | 2/32 [00:02<00:31,  1.04s/it]  9%|▉         | 3/32 [00:02<00:21,  1.38it/s] 12%|█▎        | 4/32 [00:03<00:16,  1.73it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.02it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.22it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.39it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.54it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.63it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.70it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.76it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.80it/s] 41%|████      | 13/32 [00:06<00:06,  2.81it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.82it/s] 50%|█████     | 16/32 [00:07<00:05,  2.84it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.85it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.86it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.87it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.88it/s] 66%|██████▌   | 21/32 [00:09<00:03,  2.82it/s]I0316 11:05:51.523680 504153 finetune.py:68] layer 20_v @ epoch 2 new loss 2.9554000775533495e-06 old loss 3.129839569737669e-06 BETTER
 69%|██████▉   | 22/32 [00:09<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.89it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.89it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.89it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.90it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.93it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.89it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.91it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.91it/s]100%|██████████| 32/32 [00:12<00:00,  2.90it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
W0316 11:05:57.913000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:05:57.913000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:05:57.913000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:05:57.913000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:05:57.913000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:05:57.913000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:05:57.913000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:05:57.940000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:05:57.940000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:05:57.940000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:05:57.940000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:05:57.941000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:05:58.246000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:05:58.246000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:05:58.246000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:05:58.246000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:05:58.246000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:05:58.878000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:05:58.878000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:05:58.878000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:05:58.878000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:05:58.878000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:05:58.878000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:05:58.878000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:05:58.896000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:05:58.897000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:05:58.897000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:05:58.897000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:05:58.897000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:05:59.107000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:05:59.107000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:05:59.107000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:05:59.108000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:05:59.108000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:06:00.262000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:06:00.262000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:06:00.262000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:06:00.262000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:06:00.262000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:06:00.262000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:06:00.262000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:06:00.280000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:06:00.280000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:06:00.280000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:06:00.280000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:06:00.280000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:06:00.928000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:06:00.929000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:06:00.929000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:06:00.929000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:06:00.929000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 11:06:07.770406 505163 finetune.py:45] layer 22_v initial loss 1.6178137229871936e-05
W0316 11:06:07.770816 505163 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:06:13.981506 504657 finetune.py:68] layer 21_v @ epoch 1 new loss 3.900503998011118e-06 old loss 4.407967935549095e-06 BETTER
I0316 11:06:29.388309 504153 finetune.py:68] layer 20_v @ epoch 3 new loss 2.863001554942457e-06 old loss 2.9554000775533495e-06 BETTER
I0316 11:06:37.408173 460347 quantize_finetune_llama.py:186] computed original embedding for layer 23 in 61.14116358757019s
I0316 11:06:37.791494 460347 quantize_finetune_llama.py:159] layer 24 gpu 0
I0316 11:06:39.997866 505669 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 11:06:39.998015 505669 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 11:06:39.998086 505669 utils.py:162] NumExpr defaulting to 16 threads.
I0316 11:06:40.224219 505669 config.py:58] PyTorch version 2.4.0 available.
I0316 11:06:42.462055 505163 finetune.py:68] layer 22_v @ epoch 0 new loss 3.982477210229263e-06 old loss 1.6178137229871936e-05 BETTER
I0316 11:06:42.638080 505669 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 11:06:43.080822 505669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:03,  2.06s/it]  6%|▋         | 2/32 [00:02<00:31,  1.06s/it]  9%|▉         | 3/32 [00:02<00:21,  1.35it/s] 12%|█▎        | 4/32 [00:03<00:16,  1.69it/s] 16%|█▌        | 5/32 [00:03<00:13,  1.97it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.20it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.37it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.58it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.64it/s]I0316 11:06:49.872740 504657 finetune.py:68] layer 21_v @ epoch 2 new loss 3.7020136005594395e-06 old loss 3.900503998011118e-06 BETTER
 34%|███▍      | 11/32 [00:05<00:07,  2.69it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.73it/s] 41%|████      | 13/32 [00:06<00:06,  2.75it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.76it/s] 47%|████▋     | 15/32 [00:07<00:06,  2.77it/s] 50%|█████     | 16/32 [00:07<00:05,  2.78it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.79it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.80it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.81it/s] 66%|██████▌   | 21/32 [00:09<00:03,  2.80it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.80it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.82it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.81it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.81it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.81it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.81it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.82it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.81it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.81it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:13<00:00,  2.81it/s]100%|██████████| 32/32 [00:13<00:00,  2.44it/s]
W0316 11:07:00.324000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:07:00.324000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:07:00.324000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:07:00.324000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:07:00.324000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:07:00.324000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:07:00.324000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:07:00.352000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:07:00.352000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:07:00.352000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:07:00.353000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:07:00.353000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:07:00.652000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:07:00.652000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:07:00.652000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:07:00.652000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:07:00.652000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:07:01.282000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:07:01.283000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:07:01.283000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:07:01.283000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:07:01.283000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:07:01.283000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:07:01.283000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:07:01.301000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:07:01.302000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:07:01.302000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:07:01.302000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:07:01.302000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:07:01.505000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:07:01.505000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:07:01.505000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:07:01.505000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:07:01.506000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:07:02.678000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:07:02.678000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:07:02.678000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:07:02.679000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:07:02.679000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:07:02.679000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:07:02.679000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:07:02.697000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:07:02.697000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:07:02.698000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:07:02.698000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:07:02.698000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:07:03.344000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:07:03.344000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:07:03.344000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:07:03.344000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:07:03.345000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 11:07:07.255403 504153 finetune.py:68] layer 20_v @ epoch 4 new loss 2.788098299788544e-06 old loss 2.863001554942457e-06 BETTER
W0316 11:07:09.036610 504153 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 11:07:09.863900 505669 finetune.py:45] layer 23_v initial loss 2.0813458831980824e-05
W0316 11:07:09.864128 505669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

20_v proxy err 0.003212905488908291 tr(WHW.T) 90.61302185058594
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:44,  1.44s/it]  6%|▋         | 2/32 [00:01<00:24,  1.23it/s]  9%|▉         | 3/32 [00:02<00:17,  1.63it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.93it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.15it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.30it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.41it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.53it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.62it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.65it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 50%|█████     | 16/32 [00:07<00:05,  2.67it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.67it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s]I0316 11:07:18.286130 505163 finetune.py:68] layer 22_v @ epoch 1 new loss 3.4746233268379e-06 old loss 3.982477210229263e-06 BETTER
 59%|█████▉    | 19/32 [00:08<00:04,  2.67it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.67it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.66it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.68it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.70it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.70it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.69it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:13<00:00,  2.68it/s]100%|██████████| 32/32 [00:13<00:00,  2.46it/s]
I0316 11:07:25.756966 504657 finetune.py:68] layer 21_v @ epoch 3 new loss 3.571783508959925e-06 old loss 3.7020136005594395e-06 BETTER
W0316 11:07:29.156000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.156000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.156000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.156000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.156000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.156000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.156000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.187000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.187000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.187000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.187000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.188000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.355000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.355000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.355000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.355000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.355000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.584000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.584000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.584000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.584000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.584000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.584000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.584000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.607000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.607000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.607000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.607000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.607000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.674000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.674000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.674000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.674000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:07:29.674000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:07:30.741000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:07:31.054000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:07:31.055000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:07:31.055000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:07:31.055000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:07:31.055000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:07:31.055000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:07:31.055000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:07:31.080000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:07:31.080000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:07:31.080000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:07:31.080000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:07:31.080000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:07:31.339000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:07:31.339000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:07:31.340000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:07:31.340000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:07:31.340000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:07:31.813000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 11:07:38.796750 504153 finetune.py:45] layer 20_q initial loss 4.20607784690219e-06
W0316 11:07:38.797219 504153 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:07:44.049067 505669 finetune.py:68] layer 23_v @ epoch 0 new loss 4.748961146106012e-06 old loss 2.0813458831980824e-05 BETTER
I0316 11:07:54.747550 505163 finetune.py:68] layer 22_v @ epoch 2 new loss 3.2658736017765477e-06 old loss 3.4746233268379e-06 BETTER
I0316 11:08:02.254364 504657 finetune.py:68] layer 21_v @ epoch 4 new loss 3.499833155728993e-06 old loss 3.571783508959925e-06 BETTER
W0316 11:08:03.940552 504657 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

21_v proxy err 0.003180884523317218 tr(WHW.T) 95.41011047363281
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.33s/it]  6%|▋         | 2/32 [00:01<00:23,  1.28it/s]  9%|▉         | 3/32 [00:02<00:17,  1.66it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.93it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.13it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.27it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.36it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.43it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.48it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.50it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.53it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.52it/s] 41%|████      | 13/32 [00:05<00:07,  2.54it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.55it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.56it/s] 50%|█████     | 16/32 [00:07<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.57it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.58it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.58it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.60it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.57it/s]I0316 11:08:15.663688 504153 finetune.py:68] layer 20_q @ epoch 0 new loss 4.0363775042351335e-06 old loss 4.20607784690219e-06 BETTER
 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.57it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.58it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.58it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:13<00:00,  2.59it/s]100%|██████████| 32/32 [00:13<00:00,  2.40it/s]
I0316 11:08:19.330057 505669 finetune.py:68] layer 23_v @ epoch 1 new loss 3.998518423031783e-06 old loss 4.748961146106012e-06 BETTER
W0316 11:08:24.339000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.339000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.339000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.339000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.340000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.340000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.340000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.370000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.370000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.371000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.371000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.371000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.542000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.542000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.542000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.542000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.542000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.766000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.766000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.766000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.766000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.766000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.766000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.766000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.788000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.788000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.788000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.788000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.788000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.855000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.855000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.855000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.855000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:08:24.855000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:08:25.895000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:08:26.207000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:08:26.208000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:08:26.208000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:08:26.208000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:08:26.208000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:08:26.208000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:08:26.208000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:08:26.229000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:08:26.230000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:08:26.230000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:08:26.230000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:08:26.230000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:08:26.479000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:08:26.479000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:08:26.480000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:08:26.480000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:08:26.480000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:08:26.939000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 11:08:31.271254 505163 finetune.py:68] layer 22_v @ epoch 3 new loss 3.1447898436454125e-06 old loss 3.2658736017765477e-06 BETTER
I0316 11:08:33.644924 504657 finetune.py:45] layer 21_q initial loss 5.739620064559858e-06
W0316 11:08:33.645307 504657 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:08:53.572983 504153 finetune.py:68] layer 20_q @ epoch 1 new loss 3.9464539440814406e-06 old loss 4.0363775042351335e-06 BETTER
I0316 11:08:55.060773 505669 finetune.py:68] layer 23_v @ epoch 2 new loss 3.725758006112301e-06 old loss 3.998518423031783e-06 BETTER
I0316 11:09:07.970239 505163 finetune.py:68] layer 22_v @ epoch 4 new loss 3.071997298320639e-06 old loss 3.1447898436454125e-06 BETTER
I0316 11:09:08.875723 504657 finetune.py:68] layer 21_q @ epoch 0 new loss 5.484601842908887e-06 old loss 5.739620064559858e-06 BETTER
W0316 11:09:09.745917 505163 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_v proxy err 0.0034664738923311234 tr(WHW.T) 101.29380798339844
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.38s/it]  6%|▋         | 2/32 [00:01<00:23,  1.25it/s]  9%|▉         | 3/32 [00:02<00:17,  1.64it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.92it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.11it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.25it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.35it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.42it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.47it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.50it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.53it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:06<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.57it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:07<00:06,  2.60it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.61it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.56it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.57it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.57it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.59it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.59it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:13<00:00,  2.59it/s]100%|██████████| 32/32 [00:13<00:00,  2.39it/s]
W0316 11:09:30.513000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.514000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.514000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.514000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.514000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.514000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.514000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.544000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.544000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.544000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.544000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.545000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.714000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.714000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.714000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.714000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.714000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.943000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.943000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.944000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.944000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.944000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.944000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.944000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.964000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.964000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.964000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.964000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:09:30.964000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
I0316 11:09:31.019950 505669 finetune.py:68] layer 23_v @ epoch 3 new loss 3.5854357065545628e-06 old loss 3.725758006112301e-06 BETTER
W0316 11:09:31.029000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:09:31.029000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:09:31.029000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:09:31.029000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:09:31.030000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
I0316 11:09:31.726876 504153 finetune.py:68] layer 20_q @ epoch 2 new loss 3.87915952160256e-06 old loss 3.9464539440814406e-06 BETTER
W0316 11:09:32.089000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:09:32.401000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:09:32.401000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:09:32.401000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:09:32.401000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:09:32.401000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:09:32.401000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:09:32.401000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:09:32.424000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:09:32.424000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:09:32.424000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:09:32.424000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:09:32.424000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:09:32.678000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:09:32.678000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:09:32.678000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:09:32.678000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:09:32.678000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:09:33.143000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 11:09:39.939571 505163 finetune.py:45] layer 22_q initial loss 4.847575837629847e-06
W0316 11:09:39.939945 505163 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:09:45.231370 504657 finetune.py:68] layer 21_q @ epoch 1 new loss 5.334509751264704e-06 old loss 5.484601842908887e-06 BETTER
I0316 11:10:07.624347 505669 finetune.py:68] layer 23_v @ epoch 4 new loss 3.4908910038211616e-06 old loss 3.5854357065545628e-06 BETTER
W0316 11:10:09.550942 505669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 11:10:09.690654 504153 finetune.py:68] layer 20_q @ epoch 3 new loss 3.828424269158859e-06 old loss 3.87915952160256e-06 BETTER
23_v proxy err 0.0036339566577225924 tr(WHW.T) 112.7859115600586
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.38s/it]  6%|▋         | 2/32 [00:01<00:24,  1.25it/s]  9%|▉         | 3/32 [00:02<00:17,  1.62it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.89it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.08it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.21it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.31it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.37it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.41it/s] 31%|███▏      | 10/32 [00:04<00:09,  2.44it/s]I0316 11:10:15.760099 505163 finetune.py:68] layer 22_q @ epoch 0 new loss 4.675742275139783e-06 old loss 4.847575837629847e-06 BETTER
 34%|███▍      | 11/32 [00:05<00:08,  2.47it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.50it/s] 41%|████      | 13/32 [00:06<00:07,  2.51it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.52it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.52it/s] 50%|█████     | 16/32 [00:07<00:06,  2.53it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.53it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.53it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.53it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.53it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.52it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.52it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.52it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.53it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.53it/s]I0316 11:10:21.857698 504657 finetune.py:68] layer 21_q @ epoch 2 new loss 5.224902452027891e-06 old loss 5.334509751264704e-06 BETTER
 81%|████████▏ | 26/32 [00:11<00:02,  2.54it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.55it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.55it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.55it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.55it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.53it/s]100%|██████████| 32/32 [00:13<00:00,  2.53it/s]100%|██████████| 32/32 [00:13<00:00,  2.35it/s]
W0316 11:10:30.499000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.500000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.500000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.500000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.500000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.500000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.500000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.530000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.530000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.531000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.531000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.531000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.700000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.701000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.701000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.701000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.701000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.927000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.927000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.927000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.927000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.927000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.928000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.928000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.950000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.951000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.951000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.951000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:10:30.951000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:10:31.018000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:10:31.018000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:10:31.018000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:10:31.018000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:10:31.019000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:10:32.095000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:10:32.424000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:10:32.424000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:10:32.424000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:10:32.424000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:10:32.424000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:10:32.424000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:10:32.425000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:10:32.448000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:10:32.448000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:10:32.448000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:10:32.448000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:10:32.448000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:10:32.706000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:10:32.706000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:10:32.706000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:10:32.707000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:10:32.707000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:10:33.177000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 11:10:40.185186 505669 finetune.py:45] layer 23_q initial loss 5.1438805712678e-06
W0316 11:10:40.185604 505669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:10:48.008162 504153 finetune.py:68] layer 20_q @ epoch 4 new loss 3.7900222196185496e-06 old loss 3.828424269158859e-06 BETTER
W0316 11:10:49.763144 504153 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_q proxy err 0.0007005485822446644 tr(WHW.T) 5693.5478515625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:38,  1.26s/it]I0316 11:10:52.566400 505163 finetune.py:68] layer 22_q @ epoch 1 new loss 4.577101208269596e-06 old loss 4.675742275139783e-06 BETTER
  6%|▋         | 2/32 [00:01<00:22,  1.36it/s]  9%|▉         | 3/32 [00:01<00:16,  1.76it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.06it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.25it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.39it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.59it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.64it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.67it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.69it/s]I0316 11:10:58.509908 504657 finetune.py:68] layer 21_q @ epoch 3 new loss 5.136933850735659e-06 old loss 5.224902452027891e-06 BETTER
 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.71it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.71it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.70it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.69it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.67it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.68it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.69it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.69it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.70it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
I0316 11:11:11.094861 504153 finetune.py:45] layer 20_k initial loss 4.814640760741895e-06
W0316 11:11:11.095256 504153 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:11:14.959491 505669 finetune.py:68] layer 23_q @ epoch 0 new loss 4.979937784810318e-06 old loss 5.1438805712678e-06 BETTER
I0316 11:11:28.784399 505163 finetune.py:68] layer 22_q @ epoch 2 new loss 4.50824973086128e-06 old loss 4.577101208269596e-06 BETTER
I0316 11:11:35.078210 504657 finetune.py:68] layer 21_q @ epoch 4 new loss 5.06897367813508e-06 old loss 5.136933850735659e-06 BETTER
W0316 11:11:36.688308 504657 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

21_q proxy err 0.0005579113494604826 tr(WHW.T) 6794.6064453125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:44,  1.45s/it]  6%|▋         | 2/32 [00:01<00:24,  1.21it/s]  9%|▉         | 3/32 [00:02<00:18,  1.61it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.90it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.11it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.26it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.38it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.46it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.50it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.53it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.55it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.56it/s] 41%|████      | 13/32 [00:06<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:07<00:06,  2.61it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.61it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.61it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.59it/s]I0316 11:11:48.074155 504153 finetune.py:68] layer 20_k @ epoch 0 new loss 4.678648110711947e-06 old loss 4.814640760741895e-06 BETTER
 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.62it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.63it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.63it/s]I0316 11:11:50.584932 505669 finetune.py:68] layer 23_q @ epoch 1 new loss 4.86849194203387e-06 old loss 4.979937784810318e-06 BETTER
 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:13<00:00,  2.64it/s]100%|██████████| 32/32 [00:13<00:00,  2.41it/s]
I0316 11:11:58.041949 504657 finetune.py:45] layer 21_k initial loss 6.656980986008421e-06
W0316 11:11:58.042426 504657 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:12:05.238110 505163 finetune.py:68] layer 22_q @ epoch 3 new loss 4.454409463505726e-06 old loss 4.50824973086128e-06 BETTER
I0316 11:12:26.052725 504153 finetune.py:68] layer 20_k @ epoch 1 new loss 4.636060566554079e-06 old loss 4.678648110711947e-06 BETTER
I0316 11:12:26.311081 505669 finetune.py:68] layer 23_q @ epoch 2 new loss 4.793830612470629e-06 old loss 4.86849194203387e-06 BETTER
I0316 11:12:33.477882 504657 finetune.py:68] layer 21_k @ epoch 0 new loss 6.493854016298428e-06 old loss 6.656980986008421e-06 BETTER
I0316 11:12:41.736271 505163 finetune.py:68] layer 22_q @ epoch 4 new loss 4.409825123730116e-06 old loss 4.454409463505726e-06 BETTER
W0316 11:12:43.546783 505163 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_q proxy err 0.0006298695807345212 tr(WHW.T) 5948.1865234375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it]  6%|▋         | 2/32 [00:01<00:22,  1.33it/s]  9%|▉         | 3/32 [00:02<00:16,  1.73it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.00it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.20it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.34it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.42it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.51it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.58it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 50%|█████     | 16/32 [00:06<00:06,  2.64it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.64it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.64it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.63it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.63it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.63it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.65it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.66it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.66it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.67it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.46it/s]
I0316 11:13:02.401488 505669 finetune.py:68] layer 23_q @ epoch 3 new loss 4.735958100354765e-06 old loss 4.793830612470629e-06 BETTER
I0316 11:13:04.037923 504153 finetune.py:68] layer 20_k @ epoch 2 new loss 4.609976258507231e-06 old loss 4.636060566554079e-06 BETTER
I0316 11:13:05.209782 505163 finetune.py:45] layer 22_k initial loss 5.851344667462399e-06
W0316 11:13:05.210125 505163 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:13:09.905870 504657 finetune.py:68] layer 21_k @ epoch 1 new loss 6.434424449253129e-06 old loss 6.493854016298428e-06 BETTER
I0316 11:13:39.034245 505669 finetune.py:68] layer 23_q @ epoch 4 new loss 4.694604740507202e-06 old loss 4.735958100354765e-06 BETTER
W0316 11:13:40.950144 505669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 11:13:41.001399 505163 finetune.py:68] layer 22_k @ epoch 0 new loss 5.5731020438543055e-06 old loss 5.851344667462399e-06 BETTER
23_q proxy err 0.0006201075157150626 tr(WHW.T) 6409.53271484375
  0%|          | 0/32 [00:00<?, ?it/s]I0316 11:13:42.705755 504153 finetune.py:68] layer 20_k @ epoch 3 new loss 4.582626388582867e-06 old loss 4.609976258507231e-06 BETTER
  3%|▎         | 1/32 [00:01<00:40,  1.32s/it]  6%|▋         | 2/32 [00:01<00:23,  1.29it/s]  9%|▉         | 3/32 [00:02<00:17,  1.67it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.94it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.13it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.27it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.37it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.43it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.48it/s]I0316 11:13:46.666096 504657 finetune.py:68] layer 21_k @ epoch 2 new loss 6.377576482918812e-06 old loss 6.434424449253129e-06 BETTER
 31%|███▏      | 10/32 [00:04<00:08,  2.52it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.53it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.54it/s] 41%|████      | 13/32 [00:05<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.56it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.57it/s] 50%|█████     | 16/32 [00:07<00:06,  2.58it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.58it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.59it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.59it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.58it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.58it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.59it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.60it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:13<00:00,  2.60it/s]100%|██████████| 32/32 [00:13<00:00,  2.41it/s]
I0316 11:14:02.491890 505669 finetune.py:45] layer 23_k initial loss 6.48546256343252e-06
W0316 11:14:02.492295 505669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:14:17.578618 505163 finetune.py:68] layer 22_k @ epoch 1 new loss 5.5342266023217235e-06 old loss 5.5731020438543055e-06 BETTER
I0316 11:14:20.552407 504153 finetune.py:68] layer 20_k @ epoch 4 new loss 4.561003606795566e-06 old loss 4.582626388582867e-06 BETTER
W0316 11:14:22.445035 504153 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 11:14:23.303928 504657 finetune.py:68] layer 21_k @ epoch 3 new loss 6.343185759760672e-06 old loss 6.377576482918812e-06 BETTER
20_k proxy err 0.000528804783243686 tr(WHW.T) 4224.31787109375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.23it/s]  6%|▋         | 2/32 [00:01<00:16,  1.80it/s]  9%|▉         | 3/32 [00:01<00:13,  2.11it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.31it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.65it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.68it/s] 41%|████      | 13/32 [00:05<00:07,  2.67it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.67it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.67it/s] 50%|█████     | 16/32 [00:06<00:05,  2.67it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.68it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.69it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.69it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.67it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.66it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.67it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.67it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
I0316 11:14:37.658205 505669 finetune.py:68] layer 23_k @ epoch 0 new loss 6.080785624362761e-06 old loss 6.48546256343252e-06 BETTER
I0316 11:14:43.692511 504153 finetune.py:45] layer 20_o initial loss 7.907571671239566e-06
W0316 11:14:43.692933 504153 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:14:54.387253 505163 finetune.py:68] layer 22_k @ epoch 2 new loss 5.502658495970536e-06 old loss 5.5342266023217235e-06 BETTER
I0316 11:15:00.002161 504657 finetune.py:68] layer 21_k @ epoch 4 new loss 6.307249350356869e-06 old loss 6.343185759760672e-06 BETTER
W0316 11:15:01.713975 504657 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

21_k proxy err 0.0004973555915057659 tr(WHW.T) 4412.3466796875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.24it/s]  6%|▋         | 2/32 [00:01<00:16,  1.78it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.53it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.56it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.57it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.57it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.57it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.58it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.59it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.59it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.59it/s]I0316 11:15:13.203399 505669 finetune.py:68] layer 23_k @ epoch 1 new loss 6.035331352904905e-06 old loss 6.080785624362761e-06 BETTER
 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.59it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.59it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.58it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.56it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.56it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
I0316 11:15:20.941685 504153 finetune.py:68] layer 20_o @ epoch 0 new loss 7.708058546995744e-06 old loss 7.907571671239566e-06 BETTER
I0316 11:15:23.100453 504657 finetune.py:45] layer 21_o initial loss 1.0758573807834182e-05
W0316 11:15:23.101023 504657 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:15:31.089601 505163 finetune.py:68] layer 22_k @ epoch 3 new loss 5.48372099729022e-06 old loss 5.502658495970536e-06 BETTER
I0316 11:15:48.967178 505669 finetune.py:68] layer 23_k @ epoch 2 new loss 6.003311682434287e-06 old loss 6.035331352904905e-06 BETTER
I0316 11:15:58.535209 504657 finetune.py:68] layer 21_o @ epoch 0 new loss 1.044742930389475e-05 old loss 1.0758573807834182e-05 BETTER
I0316 11:15:58.821124 504153 finetune.py:68] layer 20_o @ epoch 1 new loss 7.627246304764412e-06 old loss 7.708058546995744e-06 BETTER
I0316 11:16:08.382301 505163 finetune.py:68] layer 22_k @ epoch 4 new loss 5.4615579756500665e-06 old loss 5.48372099729022e-06 BETTER
W0316 11:16:10.142481 505163 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_k proxy err 0.0005096603417769074 tr(WHW.T) 4307.3203125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.22it/s]  6%|▋         | 2/32 [00:01<00:16,  1.78it/s]  9%|▉         | 3/32 [00:01<00:13,  2.09it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.27it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.54it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.58it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.61it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.62it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.61it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.60it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.60it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.62it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.62it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.62it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
I0316 11:16:25.266549 505669 finetune.py:68] layer 23_k @ epoch 3 new loss 5.974569830868859e-06 old loss 6.003311682434287e-06 BETTER
I0316 11:16:31.683609 505163 finetune.py:45] layer 22_o initial loss 1.0056854080175981e-05
W0316 11:16:31.684144 505163 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:16:35.238890 504657 finetune.py:68] layer 21_o @ epoch 1 new loss 1.030114435707219e-05 old loss 1.044742930389475e-05 BETTER
I0316 11:16:37.055575 504153 finetune.py:68] layer 20_o @ epoch 2 new loss 7.566933163616341e-06 old loss 7.627246304764412e-06 BETTER
I0316 11:17:01.485391 505669 finetune.py:68] layer 23_k @ epoch 4 new loss 5.953533218416851e-06 old loss 5.974569830868859e-06 BETTER
W0316 11:17:03.273716 505669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

23_k proxy err 0.0005451322067528963 tr(WHW.T) 4209.6669921875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.18it/s]  6%|▋         | 2/32 [00:01<00:17,  1.72it/s]  9%|▉         | 3/32 [00:01<00:14,  2.01it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.20it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.31it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.38it/s]I0316 11:17:07.650200 505163 finetune.py:68] layer 22_o @ epoch 0 new loss 9.835398486757185e-06 old loss 1.0056854080175981e-05 BETTER
 22%|██▏       | 7/32 [00:03<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.47it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.48it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.49it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.49it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.50it/s] 41%|████      | 13/32 [00:05<00:07,  2.51it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.52it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.53it/s] 50%|█████     | 16/32 [00:06<00:06,  2.54it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.55it/s]I0316 11:17:11.832040 504657 finetune.py:68] layer 21_o @ epoch 2 new loss 1.020668787532486e-05 old loss 1.030114435707219e-05 BETTER
 56%|█████▋    | 18/32 [00:07<00:05,  2.56it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.57it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.56it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.55it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.53it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.53it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.52it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.53it/s]I0316 11:17:15.125951 504153 finetune.py:68] layer 20_o @ epoch 3 new loss 7.518478014389984e-06 old loss 7.566933163616341e-06 BETTER
 81%|████████▏ | 26/32 [00:10<00:02,  2.54it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.54it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.55it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.55it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.55it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.56it/s]100%|██████████| 32/32 [00:13<00:00,  2.55it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
I0316 11:17:24.953045 505669 finetune.py:45] layer 23_o initial loss 1.0816341273311991e-05
W0316 11:17:24.953433 505669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:17:44.203356 505163 finetune.py:68] layer 22_o @ epoch 1 new loss 9.736041647556704e-06 old loss 9.835398486757185e-06 BETTER
I0316 11:17:48.495660 504657 finetune.py:68] layer 21_o @ epoch 3 new loss 1.0131477210961748e-05 old loss 1.020668787532486e-05 BETTER
I0316 11:17:53.194421 504153 finetune.py:68] layer 20_o @ epoch 4 new loss 7.483070930902613e-06 old loss 7.518478014389984e-06 BETTER
W0316 11:17:54.714187 504153 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_o proxy err 0.0033029543701559305 tr(WHW.T) 4.080405235290527
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:02,  2.01s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it]I0316 11:17:59.849773 505669 finetune.py:68] layer 23_o @ epoch 0 new loss 1.0544982615101617e-05 old loss 1.0816341273311991e-05 BETTER
  9%|▉         | 3/32 [00:04<00:46,  1.61s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.50s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.48s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.48s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it]I0316 11:18:20.942483 505163 finetune.py:68] layer 22_o @ epoch 2 new loss 9.676829904492479e-06 old loss 9.736041647556704e-06 BETTER
 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it]I0316 11:18:24.968398 504657 finetune.py:68] layer 21_o @ epoch 4 new loss 1.0074266356241424e-05 old loss 1.0131477210961748e-05 BETTER
 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it]W0316 11:18:26.508947 504657 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

21_o proxy err 0.0023832861334085464 tr(WHW.T) 7.590857982635498
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.48s/it]  3%|▎         | 1/32 [00:02<01:02,  2.03s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it]  6%|▋         | 2/32 [00:03<00:51,  1.72s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it]  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.48s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.48s/it]I0316 11:18:35.463886 505669 finetune.py:68] layer 23_o @ epoch 1 new loss 1.0450810805195943e-05 old loss 1.0544982615101617e-05 BETTER
 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.48s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.48s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.50s/it]
 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.52s/it] 41%|████      | 13/32 [00:20<00:28,  1.52s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it]I0316 11:18:52.639625 504153 finetune.py:45] layer 20_up initial loss 2.4078844944597222e-05
W0316 11:18:52.640048 504153 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it]I0316 11:18:57.236551 505163 finetune.py:68] layer 22_o @ epoch 3 new loss 9.626715836930089e-06 old loss 9.676829904492479e-06 BETTER
 62%|██████▎   | 20/32 [00:30<00:18,  1.52s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.52s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.52s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.52s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.52s/it]I0316 11:19:11.554405 505669 finetune.py:68] layer 23_o @ epoch 2 new loss 1.038470600178698e-05 old loss 1.0450810805195943e-05 BETTER
 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.52s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]
I0316 11:19:25.318981 504657 finetune.py:45] layer 21_up initial loss 2.9053275284240954e-05
W0316 11:19:25.319390 504657 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:19:29.117258 504153 finetune.py:68] layer 20_up @ epoch 0 new loss 2.3556887754239142e-05 old loss 2.4078844944597222e-05 BETTER
I0316 11:19:33.906731 505163 finetune.py:68] layer 22_o @ epoch 4 new loss 9.589138244336937e-06 old loss 9.626715836930089e-06 BETTER
W0316 11:19:35.515018 505163 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_o proxy err 0.0035595870576798916 tr(WHW.T) 5.0080084800720215
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:03,  2.03s/it]  6%|▋         | 2/32 [00:03<00:51,  1.72s/it]  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 16%|█▌        | 5/32 [00:08<00:41,  1.56s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it]I0316 11:19:47.267961 505669 finetune.py:68] layer 23_o @ epoch 3 new loss 1.033625903801294e-05 old loss 1.038470600178698e-05 BETTER
 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.52s/it] 41%|████      | 13/32 [00:20<00:28,  1.52s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it]I0316 11:19:59.810831 504657 finetune.py:68] layer 21_up @ epoch 0 new loss 2.840404340531677e-05 old loss 2.9053275284240954e-05 BETTER
 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.52s/it]I0316 11:20:05.693511 504153 finetune.py:68] layer 20_up @ epoch 1 new loss 2.32154707191512e-05 old loss 2.3556887754239142e-05 BETTER
 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.51s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it]I0316 11:20:22.825158 505669 finetune.py:68] layer 23_o @ epoch 4 new loss 1.0306195690645836e-05 old loss 1.033625903801294e-05 BETTER
 94%|█████████▍| 30/32 [00:45<00:03,  1.51s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it]W0316 11:20:24.443527 505669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

23_o proxy err 0.003026779042556882 tr(WHW.T) 6.166111946105957
  0%|          | 0/32 [00:00<?, ?it/s]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]
  3%|▎         | 1/32 [00:02<01:04,  2.07s/it]  6%|▋         | 2/32 [00:03<00:52,  1.77s/it]  9%|▉         | 3/32 [00:05<00:48,  1.68s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.63s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.60s/it]I0316 11:20:34.673276 505163 finetune.py:45] layer 22_up initial loss 2.9622777219628915e-05
W0316 11:20:34.673610 505163 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:20:34.841116 504657 finetune.py:68] layer 21_up @ epoch 1 new loss 2.7973681426374242e-05 old loss 2.840404340531677e-05 BETTER
 19%|█▉        | 6/32 [00:09<00:41,  1.59s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.57s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.56s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.56s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.56s/it]I0316 11:20:42.438760 504153 finetune.py:68] layer 20_up @ epoch 2 new loss 2.2928728867555037e-05 old loss 2.32154707191512e-05 BETTER
 34%|███▍      | 11/32 [00:17<00:32,  1.55s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.55s/it] 41%|████      | 13/32 [00:20<00:29,  1.55s/it] 44%|████▍     | 14/32 [00:22<00:27,  1.55s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.55s/it] 50%|█████     | 16/32 [00:25<00:24,  1.55s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.55s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.55s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.55s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.55s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.55s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.55s/it] 72%|███████▏  | 23/32 [00:36<00:13,  1.55s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.55s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.55s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.55s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.55s/it]I0316 11:21:09.060836 505163 finetune.py:68] layer 22_up @ epoch 0 new loss 2.901054176618345e-05 old loss 2.9622777219628915e-05 BETTER
I0316 11:21:09.711341 504657 finetune.py:68] layer 21_up @ epoch 2 new loss 2.7630077966023237e-05 old loss 2.7973681426374242e-05 BETTER
 88%|████████▊ | 28/32 [00:43<00:06,  1.55s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.55s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.55s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.54s/it]100%|██████████| 32/32 [00:50<00:00,  1.55s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]
I0316 11:21:19.604769 504153 finetune.py:68] layer 20_up @ epoch 3 new loss 2.2687003365717828e-05 old loss 2.2928728867555037e-05 BETTER
I0316 11:21:24.794944 505669 finetune.py:45] layer 23_up initial loss 3.2206298783421516e-05
W0316 11:21:24.795414 505669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:21:44.635666 505163 finetune.py:68] layer 22_up @ epoch 1 new loss 2.8607799322344363e-05 old loss 2.901054176618345e-05 BETTER
I0316 11:21:45.116551 504657 finetune.py:68] layer 21_up @ epoch 3 new loss 2.7337935534887947e-05 old loss 2.7630077966023237e-05 BETTER
I0316 11:21:56.579459 504153 finetune.py:68] layer 20_up @ epoch 4 new loss 2.2481934138340876e-05 old loss 2.2687003365717828e-05 BETTER
W0316 11:21:58.101402 504153 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 11:21:59.047835 505669 finetune.py:68] layer 23_up @ epoch 0 new loss 3.1587504054186866e-05 old loss 3.2206298783421516e-05 BETTER
20_up proxy err 0.003574975533410907 tr(WHW.T) 1127.572509765625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it]  6%|▋         | 2/32 [00:03<00:49,  1.66s/it]  9%|▉         | 3/32 [00:04<00:45,  1.58s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.52s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.48s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.48s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it]I0316 11:22:19.823119 504657 finetune.py:68] layer 21_up @ epoch 4 new loss 2.709190266614314e-05 old loss 2.7337935534887947e-05 BETTER
I0316 11:22:19.836137 505163 finetune.py:68] layer 22_up @ epoch 2 new loss 2.8287842724239454e-05 old loss 2.8607799322344363e-05 BETTER
 44%|████▍     | 14/32 [00:21<00:26,  1.47s/it]W0316 11:22:21.215120 504657 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 15/32 [00:22<00:25,  1.47s/it]21_up proxy err 0.0034513240680098534 tr(WHW.T) 1237.5087890625
  0%|          | 0/32 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:24<00:23,  1.47s/it]  3%|▎         | 1/32 [00:01<01:00,  1.95s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.47s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.47s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it]I0316 11:22:33.239936 505669 finetune.py:68] layer 23_up @ epoch 1 new loss 3.120614928775467e-05 old loss 3.1587504054186866e-05 BETTER
 22%|██▏       | 7/32 [00:11<00:38,  1.56s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.54s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.54s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.55s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
 50%|█████     | 16/32 [00:25<00:24,  1.54s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.55s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.54s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.54s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.55s/it]I0316 11:22:54.864552 505163 finetune.py:68] layer 22_up @ epoch 3 new loss 2.80186486634193e-05 old loss 2.8287842724239454e-05 BETTER
 66%|██████▌   | 21/32 [00:32<00:16,  1.54s/it]I0316 11:22:55.696000 504153 finetune.py:45] layer 20_gate initial loss 3.371589264133945e-05
W0316 11:22:55.696804 504153 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 69%|██████▉   | 22/32 [00:34<00:15,  1.55s/it] 72%|███████▏  | 23/32 [00:35<00:14,  1.56s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.56s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.56s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.56s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.56s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.56s/it]I0316 11:23:07.653025 505669 finetune.py:68] layer 23_up @ epoch 2 new loss 3.089927486144006e-05 old loss 3.120614928775467e-05 BETTER
 91%|█████████ | 29/32 [00:45<00:04,  1.57s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.56s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.56s/it]100%|██████████| 32/32 [00:49<00:00,  1.56s/it]100%|██████████| 32/32 [00:49<00:00,  1.56s/it]
I0316 11:23:20.908915 504657 finetune.py:45] layer 21_gate initial loss 4.0085349610308185e-05
W0316 11:23:20.909317 504657 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:23:30.091333 505163 finetune.py:68] layer 22_up @ epoch 4 new loss 2.7791655156761408e-05 old loss 2.80186486634193e-05 BETTER
I0316 11:23:30.132744 504153 finetune.py:68] layer 20_gate @ epoch 0 new loss 3.335009387228638e-05 old loss 3.371589264133945e-05 BETTER
W0316 11:23:31.660112 505163 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_up proxy err 0.0036076412070542574 tr(WHW.T) 1250.346923828125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it]  9%|▉         | 3/32 [00:04<00:46,  1.61s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it]I0316 11:23:41.827213 505669 finetune.py:68] layer 23_up @ epoch 3 new loss 3.0649880500277504e-05 old loss 3.089927486144006e-05 BETTER
 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.53s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.52s/it] 41%|████      | 13/32 [00:20<00:28,  1.52s/it]I0316 11:23:53.708638 504657 finetune.py:68] layer 21_gate @ epoch 0 new loss 3.9636550354771316e-05 old loss 4.0085349610308185e-05 BETTER
 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it]I0316 11:24:05.566231 504153 finetune.py:68] layer 20_gate @ epoch 1 new loss 3.313550041639246e-05 old loss 3.335009387228638e-05 BETTER
 69%|██████▉   | 22/32 [00:33<00:15,  1.52s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.51s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.52s/it]I0316 11:24:16.021402 505669 finetune.py:68] layer 23_up @ epoch 4 new loss 3.04355507978471e-05 old loss 3.0649880500277504e-05 BETTER
 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it]W0316 11:24:17.527097 505669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

23_up proxy err 0.003692005993798375 tr(WHW.T) 1298.693359375
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:45<00:03,  1.52s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.52s/it]  3%|▎         | 1/32 [00:01<01:01,  1.99s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]
  6%|▋         | 2/32 [00:03<00:52,  1.74s/it]  9%|▉         | 3/32 [00:05<00:48,  1.66s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.61s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.60s/it]I0316 11:24:27.511551 504657 finetune.py:68] layer 21_gate @ epoch 1 new loss 3.938128793379292e-05 old loss 3.9636550354771316e-05 BETTER
 19%|█▉        | 6/32 [00:09<00:41,  1.59s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.58s/it]I0316 11:24:30.803527 505163 finetune.py:45] layer 22_gate initial loss 4.188712773611769e-05
W0316 11:24:30.803885 505163 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:12<00:37,  1.57s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.57s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.56s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.55s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.55s/it] 41%|████      | 13/32 [00:20<00:29,  1.55s/it]I0316 11:24:40.993313 504153 finetune.py:68] layer 20_gate @ epoch 2 new loss 3.29543327097781e-05 old loss 3.313550041639246e-05 BETTER
 44%|████▍     | 14/32 [00:22<00:27,  1.55s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.55s/it] 50%|█████     | 16/32 [00:25<00:24,  1.55s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.54s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.55s/it] 59%|█████▉    | 19/32 [00:29<00:20,  1.54s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.55s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.54s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it] 72%|███████▏  | 23/32 [00:36<00:13,  1.54s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.54s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.54s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.54s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.54s/it]I0316 11:25:01.367197 504657 finetune.py:68] layer 21_gate @ epoch 2 new loss 3.916460264008492e-05 old loss 3.938128793379292e-05 BETTER
 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it]I0316 11:25:04.014178 505163 finetune.py:68] layer 22_gate @ epoch 0 new loss 4.147888103034347e-05 old loss 4.188712773611769e-05 BETTER
 91%|█████████ | 29/32 [00:45<00:04,  1.54s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.56s/it]
I0316 11:25:16.268387 504153 finetune.py:68] layer 20_gate @ epoch 3 new loss 3.279337033745833e-05 old loss 3.29543327097781e-05 BETTER
I0316 11:25:17.412246 505669 finetune.py:45] layer 23_gate initial loss 4.635685763787478e-05
W0316 11:25:17.412730 505669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:25:35.148663 504657 finetune.py:68] layer 21_gate @ epoch 3 new loss 3.897913120454177e-05 old loss 3.916460264008492e-05 BETTER
I0316 11:25:37.791780 505163 finetune.py:68] layer 22_gate @ epoch 1 new loss 4.123829057789408e-05 old loss 4.147888103034347e-05 BETTER
I0316 11:25:49.811375 505669 finetune.py:68] layer 23_gate @ epoch 0 new loss 4.59444236184936e-05 old loss 4.635685763787478e-05 BETTER
I0316 11:25:51.434480 504153 finetune.py:68] layer 20_gate @ epoch 4 new loss 3.265644772909582e-05 old loss 3.279337033745833e-05 BETTER
W0316 11:25:52.628933 504153 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_gate proxy err 0.00162307929713279 tr(WHW.T) 4544.00390625
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:39,  1.11it/s]  2%|▏         | 2/112 [00:01<01:05,  1.68it/s]  3%|▎         | 3/112 [00:01<00:53,  2.03it/s]  4%|▎         | 4/112 [00:02<00:48,  2.24it/s]  4%|▍         | 5/112 [00:02<00:45,  2.36it/s]  5%|▌         | 6/112 [00:02<00:43,  2.45it/s]  6%|▋         | 7/112 [00:03<00:41,  2.51it/s]  7%|▋         | 8/112 [00:03<00:40,  2.56it/s]  8%|▊         | 9/112 [00:03<00:39,  2.60it/s]  9%|▉         | 10/112 [00:04<00:38,  2.62it/s] 10%|▉         | 11/112 [00:04<00:38,  2.64it/s] 11%|█         | 12/112 [00:05<00:37,  2.66it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.66it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.65it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.65it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.62it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.64it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.65it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.66it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.68it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.68it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.68it/s] 21%|██        | 23/112 [00:09<00:33,  2.68it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.68it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.68it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.67it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.66it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.67it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.63it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.64it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.65it/s]I0316 11:26:08.628458 504657 finetune.py:68] layer 21_gate @ epoch 4 new loss 3.8818325265310705e-05 old loss 3.897913120454177e-05 BETTER
 29%|██▊       | 32/112 [00:12<00:30,  2.66it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.66it/s] 30%|███       | 34/112 [00:13<00:29,  2.66it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.67it/s]W0316 11:26:09.892982 504657 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 32%|███▏      | 36/112 [00:14<00:28,  2.67it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.67it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.68it/s]I0316 11:26:11.151205 505163 finetune.py:68] layer 22_gate @ epoch 2 new loss 4.1042312659556046e-05 old loss 4.123829057789408e-05 BETTER
 35%|███▍      | 39/112 [00:15<00:27,  2.68it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.66it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.63it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.64it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.65it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.67it/s]21_gate proxy err 0.0015723760006949306 tr(WHW.T) 5022.93017578125
  0%|          | 0/112 [00:00<?, ?it/s] 40%|████      | 45/112 [00:17<00:24,  2.68it/s] 41%|████      | 46/112 [00:17<00:24,  2.69it/s]  1%|          | 1/112 [00:00<01:36,  1.15it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.69it/s]  2%|▏         | 2/112 [00:01<01:04,  1.71it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.68it/s]  3%|▎         | 3/112 [00:01<00:53,  2.02it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.69it/s]  4%|▎         | 4/112 [00:02<00:49,  2.20it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.68it/s]  4%|▍         | 5/112 [00:02<00:46,  2.32it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.68it/s]  5%|▌         | 6/112 [00:02<00:44,  2.40it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.68it/s]  6%|▋         | 7/112 [00:03<00:42,  2.45it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.64it/s]  7%|▋         | 8/112 [00:03<00:41,  2.48it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.66it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.66it/s]  8%|▊         | 9/112 [00:03<00:41,  2.50it/s] 50%|█████     | 56/112 [00:21<00:21,  2.67it/s]  9%|▉         | 10/112 [00:04<00:40,  2.52it/s] 51%|█████     | 57/112 [00:21<00:20,  2.67it/s] 10%|▉         | 11/112 [00:04<00:40,  2.52it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.67it/s] 11%|█         | 12/112 [00:05<00:39,  2.53it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.68it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.54it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.68it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.55it/s] 54%|█████▍    | 61/112 [00:23<00:18,  2.69it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.56it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.69it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.57it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.68it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.58it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.68it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.58it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.64it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.58it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.65it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.57it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.66it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.57it/s] 61%|██████    | 68/112 [00:26<00:16,  2.67it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.53it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.67it/s] 21%|██        | 23/112 [00:09<00:35,  2.54it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.67it/s]I0316 11:26:23.139403 505669 finetune.py:68] layer 23_gate @ epoch 1 new loss 4.572856778395362e-05 old loss 4.59444236184936e-05 BETTER
 21%|██▏       | 24/112 [00:09<00:34,  2.54it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.68it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.54it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.68it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.54it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.68it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.54it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.69it/s] 25%|██▌       | 28/112 [00:11<00:33,  2.54it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.68it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.54it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.67it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.67it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.53it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.64it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.54it/s] 71%|███████   | 79/112 [00:30<00:12,  2.66it/s] 29%|██▊       | 32/112 [00:13<00:31,  2.53it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.67it/s] 29%|██▉       | 33/112 [00:13<00:31,  2.54it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.68it/s] 30%|███       | 34/112 [00:13<00:30,  2.52it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.68it/s] 31%|███▏      | 35/112 [00:14<00:30,  2.52it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.69it/s] 32%|███▏      | 36/112 [00:14<00:30,  2.52it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.69it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.53it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.69it/s] 34%|███▍      | 38/112 [00:15<00:29,  2.54it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.70it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.55it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.70it/s] 36%|███▌      | 40/112 [00:16<00:28,  2.55it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.69it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.55it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.68it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.54it/s] 80%|████████  | 90/112 [00:34<00:08,  2.65it/s] 38%|███▊      | 43/112 [00:17<00:27,  2.54it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.66it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.53it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.67it/s] 40%|████      | 45/112 [00:18<00:26,  2.51it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.68it/s] 41%|████      | 46/112 [00:18<00:26,  2.53it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.68it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.68it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.53it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.70it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.54it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.71it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.54it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.70it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.54it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.70it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.56it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.70it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.55it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.68it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.56it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.67it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.56it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.69it/s] 49%|████▉     | 55/112 [00:22<00:22,  2.56it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.68it/s] 50%|█████     | 56/112 [00:22<00:21,  2.56it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.69it/s] 51%|█████     | 57/112 [00:22<00:21,  2.52it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.69it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.53it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.69it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.54it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.70it/s] 54%|█████▎    | 60/112 [00:24<00:20,  2.55it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.70it/s] 54%|█████▍    | 61/112 [00:24<00:20,  2.55it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.70it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.56it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.71it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.55it/s]100%|██████████| 112/112 [00:42<00:00,  2.71it/s]100%|██████████| 112/112 [00:42<00:00,  2.64it/s]
 57%|█████▋    | 64/112 [00:25<00:18,  2.55it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.55it/s] 59%|█████▉    | 66/112 [00:26<00:18,  2.54it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.53it/s] 61%|██████    | 68/112 [00:27<00:17,  2.53it/s] 62%|██████▏   | 69/112 [00:27<00:17,  2.48it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.51it/s] 63%|██████▎   | 71/112 [00:28<00:16,  2.52it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.53it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.53it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.53it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.53it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.53it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.54it/s] 70%|██████▉   | 78/112 [00:31<00:13,  2.53it/s]I0316 11:26:44.771679 505163 finetune.py:68] layer 22_gate @ epoch 3 new loss 4.087073102709837e-05 old loss 4.1042312659556046e-05 BETTER
 71%|███████   | 79/112 [00:31<00:13,  2.54it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.53it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.51it/s]W0316 11:26:45.825000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:26:45.825000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:26:45.825000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:26:45.825000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:26:45.826000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:26:45.826000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:26:45.826000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:26:45.868000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:26:45.868000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:26:45.868000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:26:45.868000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:26:45.868000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.059000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.059000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.059000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.059000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.059000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 73%|███████▎  | 82/112 [00:32<00:11,  2.53it/s]W0316 11:26:46.383000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.383000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.383000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.383000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.384000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.384000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.384000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.418000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.418000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.418000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.418000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.418000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.489000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.489000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.490000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.490000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:26:46.490000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 74%|███████▍  | 83/112 [00:33<00:11,  2.54it/s] 75%|███████▌  | 84/112 [00:33<00:11,  2.54it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.54it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.54it/s]W0316 11:26:47.850000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 87/112 [00:34<00:09,  2.54it/s]W0316 11:26:48.311000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:26:48.311000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:26:48.312000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:26:48.312000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:26:48.312000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:26:48.312000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:26:48.312000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:26:48.343000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:26:48.343000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:26:48.343000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:26:48.343000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:26:48.343000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 79%|███████▊  | 88/112 [00:35<00:09,  2.54it/s]W0316 11:26:48.730000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:26:48.730000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:26:48.730000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:26:48.730000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:26:48.730000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 79%|███████▉  | 89/112 [00:35<00:09,  2.54it/s]W0316 11:26:49.279000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 80%|████████  | 90/112 [00:35<00:08,  2.53it/s]W0316 11:26:49.285000 140127922337600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 91/112 [00:36<00:08,  2.53it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.53it/s] 83%|████████▎ | 93/112 [00:37<00:07,  2.50it/s] 84%|████████▍ | 94/112 [00:37<00:07,  2.51it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.52it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.53it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.53it/s] 88%|████████▊ | 98/112 [00:39<00:05,  2.54it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.53it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.54it/s] 90%|█████████ | 101/112 [00:40<00:04,  2.53it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.53it/s] 92%|█████████▏| 103/112 [00:41<00:03,  2.52it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.49it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.51it/s] 95%|█████████▍| 106/112 [00:42<00:02,  2.52it/s]I0316 11:26:55.796090 505669 finetune.py:68] layer 23_gate @ epoch 2 new loss 4.5556826080428436e-05 old loss 4.572856778395362e-05 BETTER
 96%|█████████▌| 107/112 [00:42<00:01,  2.53it/s] 96%|█████████▋| 108/112 [00:43<00:01,  2.54it/s]I0316 11:26:56.592614 504153 finetune.py:45] layer 20_down initial loss 5.4616601119050756e-05
W0316 11:26:56.592978 504153 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 109/112 [00:43<00:01,  2.55it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.55it/s] 99%|█████████▉| 111/112 [00:44<00:00,  2.55it/s]100%|██████████| 112/112 [00:44<00:00,  2.55it/s]100%|██████████| 112/112 [00:44<00:00,  2.51it/s]
W0316 11:27:05.273000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.273000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.273000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.273000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.273000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.273000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.273000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.318000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.318000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.318000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.318000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.318000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.498000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.499000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.499000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.499000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.499000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.825000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.825000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.825000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.826000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.826000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.826000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.826000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.858000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.859000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.859000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.859000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.859000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.929000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.929000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.929000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.929000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:27:05.930000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:27:07.286000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:27:07.746000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:27:07.747000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:27:07.747000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:27:07.747000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:27:07.747000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:27:07.747000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:27:07.747000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:27:07.777000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:27:07.777000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:27:07.777000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:27:07.777000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:27:07.777000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:27:08.166000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:27:08.166000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:27:08.166000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:27:08.166000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:27:08.166000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:27:08.716000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:27:08.721000 140029781186368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0316 11:27:15.833555 504657 finetune.py:45] layer 21_down initial loss 6.398852565325797e-05
W0316 11:27:15.833966 504657 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:27:18.114843 505163 finetune.py:68] layer 22_gate @ epoch 4 new loss 4.07238767365925e-05 old loss 4.087073102709837e-05 BETTER
W0316 11:27:19.298003 505163 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_gate proxy err 0.0016923837829381227 tr(WHW.T) 4895.65380859375
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:40,  1.10it/s]  2%|▏         | 2/112 [00:01<01:05,  1.67it/s]  3%|▎         | 3/112 [00:01<00:54,  1.98it/s]  4%|▎         | 4/112 [00:02<00:49,  2.19it/s]  4%|▍         | 5/112 [00:02<00:46,  2.33it/s]  5%|▌         | 6/112 [00:02<00:43,  2.42it/s]  6%|▋         | 7/112 [00:03<00:42,  2.48it/s]  7%|▋         | 8/112 [00:03<00:41,  2.53it/s]  8%|▊         | 9/112 [00:03<00:40,  2.56it/s]  9%|▉         | 10/112 [00:04<00:39,  2.57it/s] 10%|▉         | 11/112 [00:04<00:39,  2.58it/s] 11%|█         | 12/112 [00:05<00:38,  2.59it/s]I0316 11:27:28.361573 505669 finetune.py:68] layer 23_gate @ epoch 3 new loss 4.541441012406722e-05 old loss 4.5556826080428436e-05 BETTER
 12%|█▏        | 13/112 [00:05<00:38,  2.60it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.57it/s]I0316 11:27:29.057812 504153 finetune.py:68] layer 20_down @ epoch 0 new loss 5.459736712509766e-05 old loss 5.4616601119050756e-05 BETTER
 13%|█▎        | 15/112 [00:06<00:37,  2.59it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.59it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.60it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.61it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.61it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.61it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.61it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.61it/s] 21%|██        | 23/112 [00:09<00:34,  2.61it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.61it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.57it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.58it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.60it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.60it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.61it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.61it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.61it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.61it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.62it/s] 30%|███       | 34/112 [00:13<00:29,  2.62it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.61it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.61it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.58it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.59it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.60it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.60it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.61it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.61it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.61it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.62it/s] 40%|████      | 45/112 [00:17<00:25,  2.62it/s] 41%|████      | 46/112 [00:18<00:25,  2.62it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.62it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.61it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.58it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.59it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.60it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.61it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.61it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.62it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.62it/s] 50%|█████     | 56/112 [00:22<00:21,  2.62it/s] 51%|█████     | 57/112 [00:22<00:20,  2.63it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.62it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.62it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.62it/s]I0316 11:27:46.883003 504657 finetune.py:68] layer 21_down @ epoch 0 new loss 6.396169919753447e-05 old loss 6.398852565325797e-05 BETTER
 54%|█████▍    | 61/112 [00:23<00:19,  2.58it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.60it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.60it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.61it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.62it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.63it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.63it/s] 61%|██████    | 68/112 [00:26<00:16,  2.63it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.63it/s] 62%|██████▎   | 70/112 [00:27<00:15,  2.63it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.62it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.62it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.59it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.60it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.61it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.61it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.62it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.62it/s] 71%|███████   | 79/112 [00:30<00:12,  2.62it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.63it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.64it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.64it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.63it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.63it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.60it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.61it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.61it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.62it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.62it/s] 80%|████████  | 90/112 [00:34<00:08,  2.63it/s] 81%|████████▏ | 91/112 [00:35<00:07,  2.63it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.63it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.63it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.62it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.62it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.62it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.58it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.60it/s]I0316 11:28:01.240327 505669 finetune.py:68] layer 23_gate @ epoch 4 new loss 4.52752283308655e-05 old loss 4.541441012406722e-05 BETTER
 88%|████████▊ | 99/112 [00:38<00:04,  2.61it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.61it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.62it/s]I0316 11:28:02.407724 504153 finetune.py:68] layer 20_down @ epoch 1 new loss 5.459312524180859e-05 old loss 5.459736712509766e-05 BETTER
W0316 11:28:02.452434 505669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 102/112 [00:39<00:03,  2.64it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.63it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.64it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.65it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.64it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.63it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.64it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.61it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.62it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.62it/s]23_gate proxy err 0.0018337680958211422 tr(WHW.T) 4777.37890625
  0%|          | 0/112 [00:00<?, ?it/s]100%|██████████| 112/112 [00:43<00:00,  2.63it/s]100%|██████████| 112/112 [00:43<00:00,  2.58it/s]
  1%|          | 1/112 [00:00<01:39,  1.12it/s]  2%|▏         | 2/112 [00:01<01:05,  1.67it/s]  3%|▎         | 3/112 [00:01<00:55,  1.97it/s]  4%|▎         | 4/112 [00:02<00:49,  2.17it/s]  4%|▍         | 5/112 [00:02<00:46,  2.29it/s]  5%|▌         | 6/112 [00:02<00:44,  2.36it/s]  6%|▋         | 7/112 [00:03<00:43,  2.41it/s]  7%|▋         | 8/112 [00:03<00:42,  2.45it/s]  8%|▊         | 9/112 [00:04<00:41,  2.48it/s]  9%|▉         | 10/112 [00:04<00:40,  2.49it/s] 10%|▉         | 11/112 [00:04<00:40,  2.51it/s] 11%|█         | 12/112 [00:05<00:39,  2.51it/s] 12%|█▏        | 13/112 [00:05<00:39,  2.52it/s] 12%|█▎        | 14/112 [00:06<00:38,  2.54it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.55it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.56it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.57it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.56it/s]W0316 11:28:13.831000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:28:13.831000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:28:13.832000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:28:13.832000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:28:13.832000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:28:13.832000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:28:13.832000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:28:13.873000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:28:13.873000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:28:13.873000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:28:13.873000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:28:13.874000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.056000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.056000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.056000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.056000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.056000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 17%|█▋        | 19/112 [00:07<00:36,  2.56it/s]W0316 11:28:14.382000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.382000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.382000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.382000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.382000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.382000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.382000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.414000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.414000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.414000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.414000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.414000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 18%|█▊        | 20/112 [00:08<00:36,  2.55it/s]W0316 11:28:14.487000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.487000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.487000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.488000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:28:14.488000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 21/112 [00:08<00:35,  2.55it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.55it/s] 21%|██        | 23/112 [00:09<00:34,  2.54it/s]W0316 11:28:15.874000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 21%|██▏       | 24/112 [00:09<00:34,  2.54it/s]W0316 11:28:16.344000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:28:16.344000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:28:16.344000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:28:16.344000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:28:16.344000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:28:16.344000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:28:16.344000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:28:16.377000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:28:16.377000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:28:16.378000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:28:16.378000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:28:16.378000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 25/112 [00:10<00:34,  2.52it/s]W0316 11:28:16.772000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:28:16.772000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:28:16.772000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:28:16.772000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:28:16.772000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 23%|██▎       | 26/112 [00:10<00:33,  2.53it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.53it/s]W0316 11:28:17.330000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:28:17.335000 140333859387200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 28/112 [00:11<00:33,  2.53it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.54it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.54it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.54it/s]I0316 11:28:18.855526 504657 finetune.py:68] layer 21_down @ epoch 1 new loss 6.396004027919844e-05 old loss 6.396169919753447e-05 BETTER
 29%|██▊       | 32/112 [00:13<00:31,  2.57it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.56it/s] 30%|███       | 34/112 [00:13<00:30,  2.55it/s] 31%|███▏      | 35/112 [00:14<00:30,  2.55it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.54it/s] 33%|███▎      | 37/112 [00:15<00:29,  2.54it/s] 34%|███▍      | 38/112 [00:15<00:29,  2.54it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.52it/s] 36%|███▌      | 40/112 [00:16<00:28,  2.53it/s] 37%|███▋      | 41/112 [00:16<00:28,  2.53it/s] 38%|███▊      | 42/112 [00:17<00:27,  2.54it/s] 38%|███▊      | 43/112 [00:17<00:27,  2.54it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.54it/s] 40%|████      | 45/112 [00:18<00:26,  2.55it/s]I0316 11:28:24.599745 505163 finetune.py:45] layer 22_down initial loss 6.756372022209689e-05
W0316 11:28:24.600234 505163 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 46/112 [00:18<00:25,  2.55it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.54it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.54it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.54it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.54it/s] 46%|████▌     | 51/112 [00:20<00:24,  2.53it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.54it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.52it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.53it/s] 49%|████▉     | 55/112 [00:22<00:22,  2.50it/s] 50%|█████     | 56/112 [00:22<00:22,  2.52it/s] 51%|█████     | 57/112 [00:22<00:21,  2.54it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.55it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.55it/s] 54%|█████▎    | 60/112 [00:24<00:20,  2.56it/s] 54%|█████▍    | 61/112 [00:24<00:20,  2.55it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.53it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.50it/s] 57%|█████▋    | 64/112 [00:25<00:19,  2.45it/s] 58%|█████▊    | 65/112 [00:26<00:19,  2.43it/s] 59%|█████▉    | 66/112 [00:26<00:19,  2.41it/s] 60%|█████▉    | 67/112 [00:26<00:18,  2.42it/s] 61%|██████    | 68/112 [00:27<00:18,  2.42it/s] 62%|██████▏   | 69/112 [00:27<00:17,  2.44it/s] 62%|██████▎   | 70/112 [00:28<00:16,  2.47it/s] 63%|██████▎   | 71/112 [00:28<00:16,  2.49it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.51it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.53it/s]I0316 11:28:35.756619 504153 finetune.py:68] layer 20_down @ epoch 2 new loss 5.459230305859819e-05 old loss 5.459312524180859e-05 BETTER
 66%|██████▌   | 74/112 [00:29<00:14,  2.53it/s] 67%|██████▋   | 75/112 [00:30<00:14,  2.54it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.54it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.53it/s] 70%|██████▉   | 78/112 [00:31<00:13,  2.54it/s] 71%|███████   | 79/112 [00:31<00:13,  2.54it/s] 71%|███████▏  | 80/112 [00:32<00:12,  2.52it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.53it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.53it/s] 74%|███████▍  | 83/112 [00:33<00:11,  2.54it/s] 75%|███████▌  | 84/112 [00:33<00:11,  2.54it/s] 76%|███████▌  | 85/112 [00:34<00:10,  2.54it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.55it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.56it/s] 79%|███████▊  | 88/112 [00:35<00:09,  2.55it/s] 79%|███████▉  | 89/112 [00:35<00:09,  2.55it/s] 80%|████████  | 90/112 [00:36<00:08,  2.54it/s] 81%|████████▏ | 91/112 [00:36<00:08,  2.54it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.54it/s] 83%|████████▎ | 93/112 [00:37<00:07,  2.53it/s] 84%|████████▍ | 94/112 [00:37<00:07,  2.54it/s] 85%|████████▍ | 95/112 [00:38<00:06,  2.54it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.54it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.55it/s] 88%|████████▊ | 98/112 [00:39<00:05,  2.56it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.57it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.57it/s] 90%|█████████ | 101/112 [00:40<00:04,  2.55it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.51it/s] 92%|█████████▏| 103/112 [00:41<00:03,  2.47it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.46it/s] 94%|█████████▍| 105/112 [00:42<00:02,  2.47it/s] 95%|█████████▍| 106/112 [00:42<00:02,  2.48it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.50it/s] 96%|█████████▋| 108/112 [00:43<00:01,  2.52it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.52it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.53it/s] 99%|█████████▉| 111/112 [00:44<00:00,  2.54it/s]100%|██████████| 112/112 [00:44<00:00,  2.55it/s]100%|██████████| 112/112 [00:44<00:00,  2.50it/s]
I0316 11:28:51.026314 504657 finetune.py:76] layer 21_down @ epoch 2 new loss 6.396082608262077e-05 old loss 6.396004027919844e-05 WORSE
I0316 11:28:56.277494 505163 finetune.py:68] layer 22_down @ epoch 0 new loss 6.753871275577694e-05 old loss 6.756372022209689e-05 BETTER
W0316 11:28:58.439000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.439000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.440000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.440000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.440000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.440000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.440000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.482000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.482000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.482000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.482000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.482000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.660000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.661000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.661000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.661000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.661000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.983000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.984000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.984000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.984000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.984000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.984000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:28:58.984000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:28:59.016000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:28:59.016000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:28:59.016000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:28:59.017000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:28:59.017000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:28:59.090000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:28:59.091000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:28:59.091000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:28:59.091000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:28:59.091000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:29:00.443000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:29:00.916000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:29:00.916000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:29:00.916000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:29:00.916000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:29:00.916000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:29:00.916000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:29:00.916000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:29:00.945000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:29:00.945000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:29:00.945000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:29:00.945000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:29:00.946000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:29:01.334000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:29:01.334000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:29:01.334000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:29:01.334000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:29:01.334000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:29:01.879000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:29:01.886000 140400995084096 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0316 11:29:09.096885 505669 finetune.py:45] layer 23_down initial loss 7.409643149003386e-05
W0316 11:29:09.097252 505669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:29:09.115825 504153 finetune.py:68] layer 20_down @ epoch 3 new loss 5.458901796373539e-05 old loss 5.459230305859819e-05 BETTER
I0316 11:29:22.664575 504657 finetune.py:68] layer 21_down @ epoch 3 new loss 6.395589298335835e-05 old loss 6.396004027919844e-05 BETTER
I0316 11:29:28.239330 505163 finetune.py:68] layer 22_down @ epoch 1 new loss 6.753767229383811e-05 old loss 6.753871275577694e-05 BETTER
I0316 11:29:39.733288 505669 finetune.py:68] layer 23_down @ epoch 0 new loss 7.406796794384718e-05 old loss 7.409643149003386e-05 BETTER
I0316 11:29:42.630039 504153 finetune.py:76] layer 20_down @ epoch 4 new loss 5.459446765598841e-05 old loss 5.458901796373539e-05 WORSE
W0316 11:29:43.271026 504153 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

20_down proxy err 0.0038823368959128857 tr(WHW.T) 23.955549240112305
I0316 11:29:54.665581 504657 finetune.py:68] layer 21_down @ epoch 4 new loss 6.395410309778526e-05 old loss 6.395589298335835e-05 BETTER
W0316 11:29:55.485846 504657 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

21_down proxy err 0.0036630178801715374 tr(WHW.T) 29.168224334716797
I0316 11:30:00.346963 505163 finetune.py:76] layer 22_down @ epoch 2 new loss 6.753834895789623e-05 old loss 6.753767229383811e-05 WORSE
I0316 11:30:11.100880 505669 finetune.py:68] layer 23_down @ epoch 1 new loss 7.406680379062891e-05 old loss 7.406796794384718e-05 BETTER
I0316 11:30:31.825620 505163 finetune.py:68] layer 22_down @ epoch 3 new loss 6.753719208063558e-05 old loss 6.753767229383811e-05 BETTER
I0316 11:30:42.281380 505669 finetune.py:68] layer 23_down @ epoch 2 new loss 7.406360236927867e-05 old loss 7.406680379062891e-05 BETTER
I0316 11:31:03.681614 505163 finetune.py:68] layer 22_down @ epoch 4 new loss 6.753118213964626e-05 old loss 6.753719208063558e-05 BETTER
W0316 11:31:04.445003 505163 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

22_down proxy err 0.003691398538649082 tr(WHW.T) 30.796499252319336
I0316 11:31:05.127202 460347 quantize_finetune_llama.py:186] computed original embedding for layer 24 in 65.24789524078369s
I0316 11:31:05.636857 460347 quantize_finetune_llama.py:159] layer 25 gpu 1
I0316 11:31:07.758047 506367 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 11:31:07.758151 506367 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 11:31:07.758208 506367 utils.py:162] NumExpr defaulting to 16 threads.
I0316 11:31:07.956807 506367 config.py:58] PyTorch version 2.4.0 available.
I0316 11:31:10.204102 506367 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 11:31:10.625099 506367 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:55,  1.78s/it]I0316 11:31:13.529740 505669 finetune.py:76] layer 23_down @ epoch 3 new loss 7.40638715797104e-05 old loss 7.406360236927867e-05 WORSE
  6%|▋         | 2/32 [00:02<00:28,  1.07it/s]  9%|▉         | 3/32 [00:02<00:19,  1.51it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.85it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.15it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.37it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.64it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.74it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.81it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.87it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.85it/s] 41%|████      | 13/32 [00:05<00:06,  2.88it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.90it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.91it/s] 50%|█████     | 16/32 [00:06<00:05,  2.94it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.96it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.97it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.91it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.91it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.92it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.94it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.96it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.97it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.98it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.94it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.92it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.94it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.96it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.97it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.98it/s]100%|██████████| 32/32 [00:12<00:00,  2.98it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
W0316 11:31:26.859000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:31:26.860000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:31:26.860000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:31:26.860000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:31:26.860000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:31:26.860000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:31:26.860000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:31:26.885000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:31:26.885000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:31:26.885000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:31:26.885000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:31:26.885000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:31:27.196000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:31:27.196000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:31:27.196000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:31:27.196000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:31:27.196000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:31:27.818000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:31:27.818000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:31:27.819000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:31:27.819000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:31:27.819000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:31:27.819000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:31:27.819000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:31:27.837000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:31:27.837000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:31:27.837000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:31:27.837000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:31:27.837000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:31:28.039000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:31:28.039000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:31:28.039000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:31:28.039000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:31:28.039000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:31:29.192000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:31:29.192000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:31:29.192000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:31:29.192000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:31:29.192000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:31:29.192000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:31:29.193000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:31:29.210000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:31:29.210000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:31:29.210000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:31:29.210000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:31:29.210000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:31:29.860000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:31:29.860000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:31:29.860000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:31:29.860000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:31:29.860000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 11:31:36.236816 506367 finetune.py:45] layer 24_v initial loss 2.8199829102959484e-05
W0316 11:31:36.237139 506367 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:31:44.520605 505669 finetune.py:68] layer 23_down @ epoch 4 new loss 7.406275835819542e-05 old loss 7.406360236927867e-05 BETTER
W0316 11:31:45.297694 505669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

23_down proxy err 0.0037211691960692406 tr(WHW.T) 32.77033233642578
I0316 11:32:11.806664 460347 quantize_finetune_llama.py:186] computed original embedding for layer 25 in 62.47555994987488s
I0316 11:32:12.176178 460347 quantize_finetune_llama.py:159] layer 26 gpu 2
I0316 11:32:12.707244 506367 finetune.py:68] layer 24_v @ epoch 0 new loss 5.741494987887563e-06 old loss 2.8199829102959484e-05 BETTER
I0316 11:32:14.182661 506871 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 11:32:14.182796 506871 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 11:32:14.182864 506871 utils.py:162] NumExpr defaulting to 16 threads.
I0316 11:32:14.376917 506871 config.py:58] PyTorch version 2.4.0 available.
I0316 11:32:16.573243 506871 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 11:32:17.046960 506871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:58,  1.89s/it]  6%|▋         | 2/32 [00:02<00:29,  1.00it/s]  9%|▉         | 3/32 [00:02<00:20,  1.42it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.78it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.06it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.28it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.58it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.67it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.69it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.73it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.75it/s] 41%|████      | 13/32 [00:06<00:06,  2.77it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.81it/s] 50%|█████     | 16/32 [00:07<00:05,  2.81it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.82it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.80it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.81it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.81it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.82it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.82it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.81it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.82it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.83it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.81it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.80it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.80it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.81it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.81it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
W0316 11:32:33.669000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:32:33.669000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:32:33.669000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:32:33.669000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:32:33.669000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:32:33.669000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:32:33.669000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:32:33.694000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:32:33.694000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:32:33.694000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:32:33.694000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:32:33.694000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:32:33.999000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:32:33.999000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.000000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.000000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.000000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.610000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.610000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.610000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.610000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.610000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.610000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.610000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.627000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.627000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.627000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.627000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.627000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.824000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.824000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.825000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.825000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:32:34.825000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:32:35.986000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:32:35.986000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:32:35.987000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:32:35.987000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:32:35.987000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:32:35.987000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:32:35.987000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:32:36.004000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:32:36.005000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:32:36.005000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:32:36.005000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:32:36.005000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:32:36.649000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:32:36.650000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:32:36.650000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:32:36.650000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:32:36.650000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 11:32:42.854154 506871 finetune.py:45] layer 25_v initial loss 3.5221848520450294e-05
W0316 11:32:42.854571 506871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:32:50.157281 506367 finetune.py:68] layer 24_v @ epoch 1 new loss 4.8415149649372324e-06 old loss 5.741494987887563e-06 BETTER
I0316 11:33:16.029827 460347 quantize_finetune_llama.py:186] computed original embedding for layer 26 in 63.43444800376892s
I0316 11:33:16.416480 460347 quantize_finetune_llama.py:159] layer 27 gpu 3
I0316 11:33:17.501967 506871 finetune.py:68] layer 25_v @ epoch 0 new loss 7.029747393971775e-06 old loss 3.5221848520450294e-05 BETTER
I0316 11:33:18.491322 507377 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 11:33:18.491428 507377 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 11:33:18.491491 507377 utils.py:162] NumExpr defaulting to 16 threads.
I0316 11:33:18.671900 507377 config.py:58] PyTorch version 2.4.0 available.
I0316 11:33:20.923454 507377 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 11:33:21.312227 507377 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:59,  1.91s/it]  6%|▋         | 2/32 [00:02<00:29,  1.01it/s]  9%|▉         | 3/32 [00:02<00:20,  1.43it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.78it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.05it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.26it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.42it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.54it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.67it/s]I0316 11:33:27.680782 506367 finetune.py:68] layer 24_v @ epoch 2 new loss 4.551138772512786e-06 old loss 4.8415149649372324e-06 BETTER
 34%|███▍      | 11/32 [00:05<00:07,  2.73it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.78it/s] 41%|████      | 13/32 [00:06<00:06,  2.79it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.81it/s] 50%|█████     | 16/32 [00:07<00:05,  2.80it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.83it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.83it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.84it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.85it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.86it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.82it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.82it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.82it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.83it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.83it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.84it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.83it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.83it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
W0316 11:33:37.962000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:33:37.962000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:33:37.962000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:33:37.962000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:33:37.963000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:33:37.963000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:33:37.963000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:33:37.991000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:33:37.991000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:33:37.991000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:33:37.991000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:33:37.991000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:33:38.289000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:33:38.289000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:33:38.289000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:33:38.290000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:33:38.290000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:33:38.896000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:33:38.896000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:33:38.896000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:33:38.896000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:33:38.896000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:33:38.896000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:33:38.896000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:33:38.913000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:33:38.913000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:33:38.913000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:33:38.914000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:33:38.914000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:33:39.120000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:33:39.120000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:33:39.120000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:33:39.120000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:33:39.120000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:33:40.263000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:33:40.263000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:33:40.263000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:33:40.263000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:33:40.263000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:33:40.263000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:33:40.263000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:33:40.281000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:33:40.281000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:33:40.281000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:33:40.281000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:33:40.281000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:33:40.910000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:33:40.910000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:33:40.910000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:33:40.910000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:33:40.910000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 11:33:47.152392 507377 finetune.py:45] layer 26_v initial loss 3.380683119758032e-05
W0316 11:33:47.152569 507377 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:33:53.063397 506871 finetune.py:68] layer 25_v @ epoch 1 new loss 5.928538485022727e-06 old loss 7.029747393971775e-06 BETTER
I0316 11:34:05.461790 506367 finetune.py:68] layer 24_v @ epoch 3 new loss 4.366350822238019e-06 old loss 4.551138772512786e-06 BETTER
I0316 11:34:17.931810 460347 quantize_finetune_llama.py:186] computed original embedding for layer 27 in 60.99906921386719s
I0316 11:34:18.356794 460347 quantize_finetune_llama.py:159] layer 28 gpu 0
I0316 11:34:20.491386 507883 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 11:34:20.491509 507883 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 11:34:20.491575 507883 utils.py:162] NumExpr defaulting to 16 threads.
I0316 11:34:20.711381 507883 config.py:58] PyTorch version 2.4.0 available.
I0316 11:34:21.865507 507377 finetune.py:68] layer 26_v @ epoch 0 new loss 9.810686606215313e-06 old loss 3.380683119758032e-05 BETTER
I0316 11:34:22.954536 507883 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 11:34:23.285794 507883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.99s/it]  6%|▋         | 2/32 [00:02<00:30,  1.03s/it]  9%|▉         | 3/32 [00:02<00:20,  1.39it/s] 12%|█▎        | 4/32 [00:03<00:16,  1.73it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.00it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.22it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.38it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.50it/s]I0316 11:34:28.990139 506871 finetune.py:68] layer 25_v @ epoch 2 new loss 5.570083430939121e-06 old loss 5.928538485022727e-06 BETTER
 28%|██▊       | 9/32 [00:04<00:08,  2.58it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.66it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.70it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.72it/s] 41%|████      | 13/32 [00:06<00:06,  2.75it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.76it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.77it/s] 50%|█████     | 16/32 [00:07<00:05,  2.77it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.78it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.78it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.79it/s] 66%|██████▌   | 21/32 [00:09<00:03,  2.79it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.79it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.80it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.80it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.80it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.81it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.81it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.81it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.81it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.82it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.82it/s]100%|██████████| 32/32 [00:13<00:00,  2.82it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
W0316 11:34:40.185000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:34:40.186000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:34:40.186000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:34:40.186000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:34:40.186000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:34:40.186000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:34:40.186000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:34:40.213000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:34:40.214000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:34:40.214000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:34:40.214000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:34:40.214000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:34:40.515000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:34:40.515000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:34:40.515000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:34:40.515000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:34:40.515000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:34:41.145000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:34:41.145000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:34:41.146000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:34:41.146000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:34:41.146000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:34:41.146000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:34:41.146000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:34:41.164000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:34:41.164000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:34:41.164000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:34:41.164000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:34:41.164000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:34:41.371000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:34:41.371000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:34:41.371000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:34:41.371000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:34:41.371000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:34:42.549000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:34:42.549000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:34:42.549000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:34:42.549000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:34:42.550000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:34:42.550000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:34:42.550000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:34:42.568000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:34:42.568000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:34:42.569000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:34:42.569000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:34:42.569000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:34:43.214000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:34:43.214000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:34:43.214000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:34:43.214000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:34:43.214000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
I0316 11:34:43.673958 506367 finetune.py:68] layer 24_v @ epoch 4 new loss 4.275438641343499e-06 old loss 4.366350822238019e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0316 11:34:45.382370 506367 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_v proxy err 0.0035998483654111624 tr(WHW.T) 136.39785766601562
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.35s/it]  6%|▋         | 2/32 [00:01<00:23,  1.29it/s]  9%|▉         | 3/32 [00:02<00:17,  1.69it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.98it/s]I0316 11:34:49.280887 507883 finetune.py:45] layer 27_v initial loss 4.067450936418027e-05
W0316 11:34:49.281102 507883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:02<00:12,  2.19it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.34it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.55it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.62it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.64it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.68it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.69it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.69it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.69it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.69it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.67it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.68it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.67it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.68it/s]I0316 11:34:57.980191 507377 finetune.py:68] layer 26_v @ epoch 1 new loss 8.78998889675131e-06 old loss 9.810686606215313e-06 BETTER
 88%|████████▊ | 28/32 [00:11<00:01,  2.69it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.71it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.71it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
W0316 11:35:05.557000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.558000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.558000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.558000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.558000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.558000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.558000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
I0316 11:35:05.579141 506871 finetune.py:68] layer 25_v @ epoch 3 new loss 5.519051228475291e-06 old loss 5.570083430939121e-06 BETTER
W0316 11:35:05.589000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.589000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.589000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.590000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.590000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.757000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.757000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.757000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.758000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.758000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.979000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.979000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.979000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.979000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.979000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.979000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:35:05.979000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:35:06.000000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:35:06.000000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:35:06.000000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:35:06.000000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:35:06.000000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:35:06.068000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:35:06.068000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:35:06.068000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:35:06.068000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:35:06.068000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:35:07.141000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:35:07.456000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:35:07.456000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:35:07.456000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:35:07.456000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:35:07.456000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:35:07.456000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:35:07.456000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:35:07.480000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:35:07.480000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:35:07.480000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:35:07.480000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:35:07.480000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:35:07.735000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:35:07.735000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:35:07.735000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:35:07.735000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:35:07.735000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:35:08.198000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 11:35:15.122907 506367 finetune.py:45] layer 24_q initial loss 6.142973688838538e-06
W0316 11:35:15.123237 506367 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:35:23.395519 507883 finetune.py:68] layer 27_v @ epoch 0 new loss 1.0328762982680928e-05 old loss 4.067450936418027e-05 BETTER
I0316 11:35:34.493389 507377 finetune.py:68] layer 26_v @ epoch 2 new loss 8.500404874212109e-06 old loss 8.78998889675131e-06 BETTER
I0316 11:35:42.194665 506871 finetune.py:68] layer 25_v @ epoch 4 new loss 5.3682983889302704e-06 old loss 5.519051228475291e-06 BETTER
W0316 11:35:43.929809 506871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

25_v proxy err 0.0031582436058670282 tr(WHW.T) 164.04367065429688
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.36s/it]  6%|▋         | 2/32 [00:01<00:23,  1.26it/s]  9%|▉         | 3/32 [00:02<00:17,  1.62it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.88it/s] 16%|█▌        | 5/32 [00:02<00:13,  2.06it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.19it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.28it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.35it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.40it/s] 31%|███▏      | 10/32 [00:04<00:09,  2.44it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.46it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.48it/s] 41%|████      | 13/32 [00:06<00:07,  2.49it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.50it/s]I0316 11:35:51.939032 506367 finetune.py:68] layer 24_q @ epoch 0 new loss 5.914185749134049e-06 old loss 6.142973688838538e-06 BETTER
 47%|████▋     | 15/32 [00:06<00:06,  2.51it/s] 50%|█████     | 16/32 [00:07<00:06,  2.52it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.51it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.50it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.51it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.52it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.53it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.54it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.54it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.53it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.54it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.53it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.53it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.53it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.51it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.51it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.51it/s]I0316 11:35:58.664419 507883 finetune.py:68] layer 27_v @ epoch 1 new loss 1.027758844429627e-05 old loss 1.0328762982680928e-05 BETTER
100%|██████████| 32/32 [00:13<00:00,  2.52it/s]100%|██████████| 32/32 [00:13<00:00,  2.34it/s]
W0316 11:36:04.720000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:36:04.721000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:36:04.721000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:36:04.721000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:36:04.721000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:36:04.721000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:36:04.721000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:36:04.752000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:36:04.752000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:36:04.752000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:36:04.752000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:36:04.752000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:36:04.918000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:36:04.918000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:36:04.918000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:36:04.918000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:36:04.918000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:36:05.145000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:36:05.145000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:36:05.145000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:36:05.145000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:36:05.145000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:36:05.145000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:36:05.145000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:36:05.168000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:36:05.168000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:36:05.168000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:36:05.168000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:36:05.168000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:36:05.234000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:36:05.234000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:36:05.234000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:36:05.234000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:36:05.234000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:36:06.300000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:36:06.630000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:36:06.630000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:36:06.630000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:36:06.630000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:36:06.631000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:36:06.631000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:36:06.631000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:36:06.652000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:36:06.653000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:36:06.653000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:36:06.653000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:36:06.653000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:36:06.912000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:36:06.912000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:36:06.912000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:36:06.912000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:36:06.912000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:36:07.386000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 11:36:10.886041 507377 finetune.py:68] layer 26_v @ epoch 3 new loss 8.3115173765691e-06 old loss 8.500404874212109e-06 BETTER
I0316 11:36:14.164608 506871 finetune.py:45] layer 25_q initial loss 9.054950169229414e-06
W0316 11:36:14.165097 506871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:36:29.534958 506367 finetune.py:68] layer 24_q @ epoch 1 new loss 5.795739525638055e-06 old loss 5.914185749134049e-06 BETTER
I0316 11:36:34.063611 507883 finetune.py:76] layer 27_v @ epoch 2 new loss 1.0550287697697058e-05 old loss 1.027758844429627e-05 WORSE
I0316 11:36:47.656011 507377 finetune.py:68] layer 26_v @ epoch 4 new loss 8.164665814547334e-06 old loss 8.3115173765691e-06 BETTER
I0316 11:36:49.496102 506871 finetune.py:68] layer 25_q @ epoch 0 new loss 8.514747605659068e-06 old loss 9.054950169229414e-06 BETTER
W0316 11:36:49.541327 507377 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

26_v proxy err 0.004185671918094158 tr(WHW.T) 123.81900024414062
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.37s/it]  6%|▋         | 2/32 [00:01<00:23,  1.26it/s]  9%|▉         | 3/32 [00:02<00:17,  1.64it/s] 12%|█▎        | 4/32 [00:02<00:16,  1.71it/s] 16%|█▌        | 5/32 [00:03<00:14,  1.93it/s] 19%|█▉        | 6/32 [00:03<00:12,  2.09it/s] 22%|██▏       | 7/32 [00:03<00:11,  2.20it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.30it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.37it/s] 31%|███▏      | 10/32 [00:05<00:09,  2.43it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.47it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.50it/s] 41%|████      | 13/32 [00:06<00:07,  2.52it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.53it/s] 47%|████▋     | 15/32 [00:07<00:06,  2.55it/s] 50%|█████     | 16/32 [00:07<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.58it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.56it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.54it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.55it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.55it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.56it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.55it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.56it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.56it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.55it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.54it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.55it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.54it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.53it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.52it/s]100%|██████████| 32/32 [00:13<00:00,  2.52it/s]100%|██████████| 32/32 [00:13<00:00,  2.33it/s]
I0316 11:37:07.556234 506367 finetune.py:68] layer 24_q @ epoch 2 new loss 5.71910641156137e-06 old loss 5.795739525638055e-06 BETTER
I0316 11:37:09.015553 507883 finetune.py:76] layer 27_v @ epoch 3 new loss 1.046679972205311e-05 old loss 1.027758844429627e-05 WORSE
W0316 11:37:10.674000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:37:10.675000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:37:10.675000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:37:10.675000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:37:10.675000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:37:10.675000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:37:10.675000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:37:10.706000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:37:10.706000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:37:10.706000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:37:10.706000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:37:10.706000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:37:10.868000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:37:10.869000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:37:10.869000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:37:10.869000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:37:10.869000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:37:11.097000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:37:11.097000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:37:11.098000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:37:11.098000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:37:11.098000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:37:11.098000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:37:11.098000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:37:11.118000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:37:11.118000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:37:11.118000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:37:11.118000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:37:11.118000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:37:11.183000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:37:11.183000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:37:11.184000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:37:11.184000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:37:11.184000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:37:12.244000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:37:12.569000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:37:12.570000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:37:12.570000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:37:12.570000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:37:12.570000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:37:12.570000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:37:12.570000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:37:12.595000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:37:12.595000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:37:12.595000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:37:12.595000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:37:12.595000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:37:12.848000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:37:12.848000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:37:12.849000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:37:12.849000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:37:12.849000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:37:13.318000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 11:37:20.048354 507377 finetune.py:45] layer 26_q initial loss 1.104228704207344e-05
W0316 11:37:20.048806 507377 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:37:25.760358 506871 finetune.py:68] layer 25_q @ epoch 1 new loss 8.245273420470767e-06 old loss 8.514747605659068e-06 BETTER
I0316 11:37:44.615376 507883 finetune.py:68] layer 27_v @ epoch 4 new loss 9.492022400081623e-06 old loss 1.027758844429627e-05 BETTER
I0316 11:37:45.935514 506367 finetune.py:68] layer 24_q @ epoch 3 new loss 5.6587800827401225e-06 old loss 5.71910641156137e-06 BETTER
W0316 11:37:46.347948 507883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

27_v proxy err 0.0033272842410951853 tr(WHW.T) 203.18115234375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.33s/it]  6%|▋         | 2/32 [00:01<00:23,  1.28it/s]  9%|▉         | 3/32 [00:02<00:17,  1.65it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.90it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.09it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.22it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.31it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.37it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.41it/s] 31%|███▏      | 10/32 [00:04<00:09,  2.44it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.47it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.49it/s] 41%|████      | 13/32 [00:06<00:07,  2.50it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.51it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.50it/s] 50%|█████     | 16/32 [00:07<00:06,  2.51it/s] 53%|█████▎    | 17/32 [00:07<00:06,  2.49it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.49it/s]I0316 11:37:55.554314 507377 finetune.py:68] layer 26_q @ epoch 0 new loss 1.0490799468243495e-05 old loss 1.104228704207344e-05 BETTER
 59%|█████▉    | 19/32 [00:08<00:05,  2.50it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.51it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.51it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.51it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.52it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.52it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.52it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.51it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.50it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.48it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.49it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.49it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.49it/s]100%|██████████| 32/32 [00:13<00:00,  2.50it/s]100%|██████████| 32/32 [00:13<00:00,  2.34it/s]
I0316 11:38:02.329274 506871 finetune.py:68] layer 25_q @ epoch 2 new loss 8.059086212597322e-06 old loss 8.245273420470767e-06 BETTER
W0316 11:38:07.141000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.141000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.141000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.142000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.142000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.142000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.142000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.172000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.172000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.172000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.172000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.173000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.338000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.338000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.338000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.338000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.338000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.568000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.568000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.568000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.568000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.568000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.569000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.569000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.591000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.591000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.591000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.591000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.591000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.657000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.657000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.657000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.657000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:38:07.657000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:38:08.735000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:38:09.059000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:38:09.059000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:38:09.059000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:38:09.059000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:38:09.059000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:38:09.059000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:38:09.060000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:38:09.083000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:38:09.083000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:38:09.083000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:38:09.083000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:38:09.083000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:38:09.334000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:38:09.334000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:38:09.334000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:38:09.334000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:38:09.334000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:38:09.800000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 11:38:16.389647 507883 finetune.py:45] layer 27_q initial loss 1.387338215863565e-05
W0316 11:38:16.390067 507883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:38:23.821481 506367 finetune.py:68] layer 24_q @ epoch 4 new loss 5.61663682674407e-06 old loss 5.6587800827401225e-06 BETTER
W0316 11:38:25.529359 506367 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_q proxy err 0.0005868662265129387 tr(WHW.T) 6551.501953125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:40,  1.32s/it]  6%|▋         | 2/32 [00:01<00:23,  1.30it/s]  9%|▉         | 3/32 [00:02<00:17,  1.68it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.96it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.15it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.28it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.37it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.43it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.46it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.52it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.53it/s]I0316 11:38:32.180184 507377 finetune.py:68] layer 26_q @ epoch 1 new loss 1.0270070561091416e-05 old loss 1.0490799468243495e-05 BETTER
 38%|███▊      | 12/32 [00:05<00:07,  2.53it/s] 41%|████      | 13/32 [00:05<00:07,  2.53it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.54it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.53it/s] 50%|█████     | 16/32 [00:07<00:06,  2.55it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.56it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.58it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.57it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.58it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.56it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.59it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.65it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.68it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.72it/s]I0316 11:38:38.891281 506871 finetune.py:68] layer 25_q @ epoch 3 new loss 7.950517101562582e-06 old loss 8.059086212597322e-06 BETTER
 94%|█████████▍| 30/32 [00:12<00:00,  2.73it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.76it/s]100%|██████████| 32/32 [00:13<00:00,  2.76it/s]100%|██████████| 32/32 [00:13<00:00,  2.43it/s]
I0316 11:38:47.092536 506367 finetune.py:45] layer 24_k initial loss 7.707955774094444e-06
W0316 11:38:47.093049 506367 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:38:51.128621 507883 finetune.py:68] layer 27_q @ epoch 0 new loss 1.273073758056853e-05 old loss 1.387338215863565e-05 BETTER
I0316 11:39:08.919236 507377 finetune.py:68] layer 26_q @ epoch 2 new loss 1.0170370842388365e-05 old loss 1.0270070561091416e-05 BETTER
I0316 11:39:15.795977 506871 finetune.py:68] layer 25_q @ epoch 4 new loss 7.831191396689974e-06 old loss 7.950517101562582e-06 BETTER
W0316 11:39:17.502461 506871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

25_q proxy err 0.0005039926036261022 tr(WHW.T) 7680.9423828125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it]  6%|▋         | 2/32 [00:01<00:22,  1.32it/s]  9%|▉         | 3/32 [00:02<00:17,  1.70it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.97it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.15it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.28it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.37it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.43it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.47it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.54it/s]I0316 11:39:24.086952 506367 finetune.py:68] layer 24_k @ epoch 0 new loss 7.396406545012724e-06 old loss 7.707955774094444e-06 BETTER
 38%|███▊      | 12/32 [00:05<00:07,  2.57it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.56it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.56it/s] 50%|█████     | 16/32 [00:07<00:06,  2.54it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.54it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.56it/s]I0316 11:39:26.873884 507883 finetune.py:68] layer 27_q @ epoch 1 new loss 1.2439370948413853e-05 old loss 1.273073758056853e-05 BETTER
 59%|█████▉    | 19/32 [00:08<00:05,  2.57it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.60it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.61it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.60it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.59it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.57it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.57it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.57it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.58it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:13<00:00,  2.60it/s]100%|██████████| 32/32 [00:13<00:00,  2.41it/s]
I0316 11:39:38.855371 506871 finetune.py:45] layer 25_k initial loss 1.1338695003360044e-05
W0316 11:39:38.855649 506871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:39:45.395008 507377 finetune.py:68] layer 26_q @ epoch 3 new loss 1.0056674909719732e-05 old loss 1.0170370842388365e-05 BETTER
I0316 11:40:02.096496 506367 finetune.py:68] layer 24_k @ epoch 1 new loss 7.358934908552328e-06 old loss 7.396406545012724e-06 BETTER
I0316 11:40:02.583016 507883 finetune.py:68] layer 27_q @ epoch 2 new loss 1.2436781616997905e-05 old loss 1.2439370948413853e-05 BETTER
I0316 11:40:14.407375 506871 finetune.py:68] layer 25_k @ epoch 0 new loss 1.098607390304096e-05 old loss 1.1338695003360044e-05 BETTER
I0316 11:40:22.081892 507377 finetune.py:68] layer 26_q @ epoch 4 new loss 1.0027502867160365e-05 old loss 1.0056674909719732e-05 BETTER
W0316 11:40:23.716740 507377 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

26_q proxy err 0.0005864259437657893 tr(WHW.T) 6100.2529296875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:40,  1.29s/it]  6%|▋         | 2/32 [00:01<00:22,  1.32it/s]  9%|▉         | 3/32 [00:02<00:16,  1.71it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.98it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.17it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.30it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.39it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.43it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.47it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.52it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.55it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.57it/s] 41%|████      | 13/32 [00:05<00:07,  2.58it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 50%|█████     | 16/32 [00:07<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.61it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.59it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.59it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.60it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.60it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.56it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.57it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:13<00:00,  2.57it/s]100%|██████████| 32/32 [00:13<00:00,  2.42it/s]
I0316 11:40:38.188823 507883 finetune.py:68] layer 27_q @ epoch 3 new loss 1.2347030860837549e-05 old loss 1.2436781616997905e-05 BETTER
I0316 11:40:40.087759 506367 finetune.py:68] layer 24_k @ epoch 2 new loss 7.32071794118383e-06 old loss 7.358934908552328e-06 BETTER
I0316 11:40:45.183133 507377 finetune.py:45] layer 26_k initial loss 1.2509448424680158e-05
W0316 11:40:45.183552 507377 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:40:50.722096 506871 finetune.py:68] layer 25_k @ epoch 1 new loss 1.087075361283496e-05 old loss 1.098607390304096e-05 BETTER
I0316 11:41:14.170436 507883 finetune.py:76] layer 27_q @ epoch 4 new loss 1.2391785276122391e-05 old loss 1.2347030860837549e-05 WORSE
W0316 11:41:15.423072 507883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

27_q proxy err 0.0006088115042075515 tr(WHW.T) 6389.54248046875
  0%|          | 0/32 [00:00<?, ?it/s]I0316 11:41:17.997514 506367 finetune.py:68] layer 24_k @ epoch 3 new loss 7.3029227678489406e-06 old loss 7.32071794118383e-06 BETTER
  3%|▎         | 1/32 [00:01<00:41,  1.32s/it]  6%|▋         | 2/32 [00:01<00:23,  1.29it/s]  9%|▉         | 3/32 [00:02<00:17,  1.67it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.93it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.12it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.25it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.34it/s]I0316 11:41:20.553273 507377 finetune.py:68] layer 26_k @ epoch 0 new loss 1.2199091543152463e-05 old loss 1.2509448424680158e-05 BETTER
 25%|██▌       | 8/32 [00:04<00:09,  2.43it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.47it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.49it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.50it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.51it/s] 41%|████      | 13/32 [00:06<00:07,  2.51it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.53it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.52it/s] 50%|█████     | 16/32 [00:07<00:06,  2.53it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.54it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.56it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.56it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.57it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.58it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.57it/s]I0316 11:41:27.182771 506871 finetune.py:68] layer 25_k @ epoch 2 new loss 1.0829066013684496e-05 old loss 1.087075361283496e-05 BETTER
 78%|███████▊  | 25/32 [00:10<00:02,  2.56it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.57it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.56it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.56it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.58it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.58it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.59it/s]100%|██████████| 32/32 [00:13<00:00,  2.59it/s]100%|██████████| 32/32 [00:13<00:00,  2.39it/s]
I0316 11:41:37.262443 507883 finetune.py:45] layer 27_k initial loss 1.672016514930874e-05
W0316 11:41:37.262899 507883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:41:56.250225 506367 finetune.py:68] layer 24_k @ epoch 4 new loss 7.283115337486379e-06 old loss 7.3029227678489406e-06 BETTER
I0316 11:41:57.498313 507377 finetune.py:68] layer 26_k @ epoch 1 new loss 1.210747541335877e-05 old loss 1.2199091543152463e-05 BETTER
W0316 11:41:57.940582 506367 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_k proxy err 0.0005021430552005768 tr(WHW.T) 4139.076171875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.18it/s]  6%|▋         | 2/32 [00:01<00:17,  1.73it/s]  9%|▉         | 3/32 [00:01<00:14,  2.04it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.41it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.47it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.49it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s]I0316 11:42:03.928649 506871 finetune.py:68] layer 25_k @ epoch 3 new loss 1.0774533620860893e-05 old loss 1.0829066013684496e-05 BETTER
 34%|███▍      | 11/32 [00:04<00:08,  2.52it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.54it/s] 41%|████      | 13/32 [00:05<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:06<00:08,  2.23it/s] 47%|████▋     | 15/32 [00:06<00:07,  2.32it/s] 50%|█████     | 16/32 [00:06<00:06,  2.38it/s] 53%|█████▎    | 17/32 [00:07<00:06,  2.45it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.52it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.57it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.57it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.60it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.65it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.66it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.69it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.70it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.70it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
I0316 11:42:12.104491 507883 finetune.py:68] layer 27_k @ epoch 0 new loss 1.616839290363714e-05 old loss 1.672016514930874e-05 BETTER
I0316 11:42:19.593998 506367 finetune.py:45] layer 24_o initial loss 1.288000748900231e-05
W0316 11:42:19.594386 506367 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:42:33.954043 507377 finetune.py:68] layer 26_k @ epoch 2 new loss 1.1973816981480923e-05 old loss 1.210747541335877e-05 BETTER
I0316 11:42:40.521219 506871 finetune.py:68] layer 25_k @ epoch 4 new loss 1.0676100828277413e-05 old loss 1.0774533620860893e-05 BETTER
W0316 11:42:42.189471 506871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

25_k proxy err 0.0005018287338316441 tr(WHW.T) 4237.599609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.18it/s]  6%|▋         | 2/32 [00:01<00:17,  1.73it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.20it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.40it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.46it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.49it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.50it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.52it/s]I0316 11:42:48.256982 507883 finetune.py:68] layer 27_k @ epoch 1 new loss 1.598353082954418e-05 old loss 1.616839290363714e-05 BETTER
 38%|███▊      | 12/32 [00:05<00:07,  2.54it/s] 41%|████      | 13/32 [00:05<00:07,  2.56it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.55it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.55it/s] 50%|█████     | 16/32 [00:06<00:06,  2.55it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.55it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.55it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.54it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.53it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.53it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.53it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.53it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.53it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.54it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.54it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.55it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.55it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.54it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.53it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.52it/s]100%|██████████| 32/32 [00:13<00:00,  2.53it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
I0316 11:42:56.560710 506367 finetune.py:68] layer 24_o @ epoch 0 new loss 1.2576632798300125e-05 old loss 1.288000748900231e-05 BETTER
I0316 11:43:03.868658 506871 finetune.py:45] layer 25_o initial loss 1.694780439720489e-05
W0316 11:43:03.869050 506871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:43:10.630430 507377 finetune.py:68] layer 26_k @ epoch 3 new loss 1.1916104995179921e-05 old loss 1.1973816981480923e-05 BETTER
I0316 11:43:24.242714 507883 finetune.py:76] layer 27_k @ epoch 2 new loss 1.603556847840082e-05 old loss 1.598353082954418e-05 WORSE
I0316 11:43:34.359780 506367 finetune.py:68] layer 24_o @ epoch 1 new loss 1.2494277143559884e-05 old loss 1.2576632798300125e-05 BETTER
I0316 11:43:39.864251 506871 finetune.py:68] layer 25_o @ epoch 0 new loss 1.6535625036340207e-05 old loss 1.694780439720489e-05 BETTER
I0316 11:43:47.501128 507377 finetune.py:68] layer 26_k @ epoch 4 new loss 1.187591169582447e-05 old loss 1.1916104995179921e-05 BETTER
W0316 11:43:49.221199 507377 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

26_k proxy err 0.0004728438798338175 tr(WHW.T) 4394.07421875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.17it/s]  6%|▋         | 2/32 [00:01<00:17,  1.71it/s]  9%|▉         | 3/32 [00:01<00:14,  2.03it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.20it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.31it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.39it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.51it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.55it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.56it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.57it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.57it/s] 50%|█████     | 16/32 [00:06<00:06,  2.54it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.55it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.55it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.56it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.56it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.57it/s]I0316 11:43:59.388711 507883 finetune.py:76] layer 27_k @ epoch 3 new loss 1.6098818377940916e-05 old loss 1.598353082954418e-05 WORSE
 69%|██████▉   | 22/32 [00:09<00:03,  2.57it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.58it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.57it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.57it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.55it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.54it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.55it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.56it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
I0316 11:44:10.770736 507377 finetune.py:45] layer 26_o initial loss 2.076950295304414e-05
W0316 11:44:10.771144 507377 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:44:12.740840 506367 finetune.py:68] layer 24_o @ epoch 2 new loss 1.243952374352375e-05 old loss 1.2494277143559884e-05 BETTER
I0316 11:44:16.392192 506871 finetune.py:68] layer 25_o @ epoch 1 new loss 1.6400948879891075e-05 old loss 1.6535625036340207e-05 BETTER
I0316 11:44:34.683310 507883 finetune.py:76] layer 27_k @ epoch 4 new loss 1.608431921340525e-05 old loss 1.598353082954418e-05 WORSE
W0316 11:44:35.758211 507883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

27_k proxy err 0.0005442497204057872 tr(WHW.T) 4200.1494140625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.21it/s]  6%|▋         | 2/32 [00:01<00:17,  1.74it/s]  9%|▉         | 3/32 [00:01<00:14,  2.01it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.25it/s] 19%|█▉        | 6/32 [00:02<00:11,  2.33it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.38it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.42it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.45it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.47it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.48it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.49it/s] 41%|████      | 13/32 [00:05<00:07,  2.50it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.50it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.48it/s] 50%|█████     | 16/32 [00:06<00:06,  2.49it/s] 53%|█████▎    | 17/32 [00:07<00:06,  2.49it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.49it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.50it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.51it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.51it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.50it/s]I0316 11:44:46.407464 507377 finetune.py:68] layer 26_o @ epoch 0 new loss 2.0236800992279314e-05 old loss 2.076950295304414e-05 BETTER
 72%|███████▏  | 23/32 [00:09<00:03,  2.50it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.49it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.49it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.49it/s] 84%|████████▍ | 27/32 [00:11<00:02,  2.47it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.48it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.48it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.48it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.48it/s]100%|██████████| 32/32 [00:13<00:00,  2.49it/s]100%|██████████| 32/32 [00:13<00:00,  2.41it/s]
I0316 11:44:50.587774 506367 finetune.py:68] layer 24_o @ epoch 3 new loss 1.2394884834066033e-05 old loss 1.243952374352375e-05 BETTER
I0316 11:44:52.757109 506871 finetune.py:68] layer 25_o @ epoch 2 new loss 1.62962096510455e-05 old loss 1.6400948879891075e-05 BETTER
I0316 11:44:57.618954 507883 finetune.py:45] layer 27_o initial loss 2.6334058929933235e-05
W0316 11:44:57.619318 507883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:45:22.576257 507377 finetune.py:68] layer 26_o @ epoch 1 new loss 2.0032068277942017e-05 old loss 2.0236800992279314e-05 BETTER
I0316 11:45:28.315448 506367 finetune.py:68] layer 24_o @ epoch 4 new loss 1.2371237971819937e-05 old loss 1.2394884834066033e-05 BETTER
I0316 11:45:29.115173 506871 finetune.py:68] layer 25_o @ epoch 3 new loss 1.6234678696491756e-05 old loss 1.62962096510455e-05 BETTER
W0316 11:45:30.265795 506367 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_o proxy err 0.003231590613722801 tr(WHW.T) 6.535715579986572
  0%|          | 0/32 [00:00<?, ?it/s]I0316 11:45:32.524856 507883 finetune.py:68] layer 27_o @ epoch 0 new loss 2.582993511168752e-05 old loss 2.6334058929933235e-05 BETTER
  3%|▎         | 1/32 [00:01<01:01,  1.98s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.51s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.48s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.48s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.48s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.47s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 41%|████      | 13/32 [00:19<00:27,  1.47s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.47s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 50%|█████     | 16/32 [00:24<00:23,  1.47s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.47s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.47s/it]I0316 11:45:58.989989 507377 finetune.py:68] layer 26_o @ epoch 2 new loss 1.9925020751543343e-05 old loss 2.0032068277942017e-05 BETTER
 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.47s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it]I0316 11:46:05.653905 506871 finetune.py:68] layer 25_o @ epoch 4 new loss 1.6180892998818308e-05 old loss 1.6234678696491756e-05 BETTER
 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it]W0316 11:46:07.435135 506871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it]I0316 11:46:08.621034 507883 finetune.py:68] layer 27_o @ epoch 1 new loss 2.566430703154765e-05 old loss 2.582993511168752e-05 BETTER
25_o proxy err 0.002605301095172763 tr(WHW.T) 8.829571723937988
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it]  3%|▎         | 1/32 [00:02<01:04,  2.09s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it]  6%|▋         | 2/32 [00:03<00:53,  1.79s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it]  9%|▉         | 3/32 [00:05<00:48,  1.69s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.46s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.64s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.46s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.61s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.46s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.59s/it]100%|██████████| 32/32 [00:47<00:00,  1.46s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
 22%|██▏       | 7/32 [00:11<00:39,  1.58s/it] 25%|██▌       | 8/32 [00:13<00:37,  1.58s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.57s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.57s/it] 34%|███▍      | 11/32 [00:17<00:33,  1.58s/it]I0316 11:46:27.688006 506367 finetune.py:45] layer 24_up initial loss 3.628413469414227e-05
W0316 11:46:27.688406 506367 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:19<00:31,  1.57s/it] 41%|████      | 13/32 [00:20<00:29,  1.57s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.58s/it] 47%|████▋     | 15/32 [00:24<00:26,  1.57s/it] 50%|█████     | 16/32 [00:25<00:25,  1.58s/it]I0316 11:46:35.310086 507377 finetune.py:68] layer 26_o @ epoch 3 new loss 1.9791177692241035e-05 old loss 1.9925020751543343e-05 BETTER
 53%|█████▎    | 17/32 [00:27<00:23,  1.57s/it] 56%|█████▋    | 18/32 [00:28<00:22,  1.57s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.57s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.57s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.57s/it] 69%|██████▉   | 22/32 [00:35<00:15,  1.58s/it]I0316 11:46:44.479045 507883 finetune.py:68] layer 27_o @ epoch 2 new loss 2.533193037379533e-05 old loss 2.566430703154765e-05 BETTER
 72%|███████▏  | 23/32 [00:36<00:14,  1.58s/it] 75%|███████▌  | 24/32 [00:38<00:12,  1.57s/it] 78%|███████▊  | 25/32 [00:39<00:11,  1.58s/it] 81%|████████▏ | 26/32 [00:41<00:09,  1.57s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.57s/it] 88%|████████▊ | 28/32 [00:44<00:06,  1.58s/it] 91%|█████████ | 29/32 [00:46<00:04,  1.57s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.57s/it] 97%|█████████▋| 31/32 [00:49<00:01,  1.58s/it]100%|██████████| 32/32 [00:50<00:00,  1.58s/it]100%|██████████| 32/32 [00:50<00:00,  1.59s/it]
I0316 11:47:03.798698 506367 finetune.py:68] layer 24_up @ epoch 0 new loss 3.56829768861644e-05 old loss 3.628413469414227e-05 BETTER
I0316 11:47:08.241034 506871 finetune.py:45] layer 25_up initial loss 4.343521504779346e-05
W0316 11:47:08.241409 506871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:47:11.816301 507377 finetune.py:68] layer 26_o @ epoch 4 new loss 1.9779803551500663e-05 old loss 1.9791177692241035e-05 BETTER
W0316 11:47:13.446176 507377 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

26_o proxy err 0.0019741372670978308 tr(WHW.T) 16.225059509277344
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:03,  2.04s/it]  6%|▋         | 2/32 [00:03<00:55,  1.84s/it]  9%|▉         | 3/32 [00:05<00:49,  1.71s/it]I0316 11:47:20.768820 507883 finetune.py:68] layer 27_o @ epoch 3 new loss 2.520358066249173e-05 old loss 2.533193037379533e-05 BETTER
 12%|█▎        | 4/32 [00:06<00:45,  1.64s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.60s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.59s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.57s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.56s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it] 31%|███▏      | 10/32 [00:16<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:19<00:30,  1.54s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:22<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.54s/it] 50%|█████     | 16/32 [00:25<00:24,  1.53s/it]I0316 11:47:40.362729 506367 finetune.py:68] layer 24_up @ epoch 1 new loss 3.529979949234985e-05 old loss 3.56829768861644e-05 BETTER
 53%|█████▎    | 17/32 [00:26<00:23,  1.53s/it]I0316 11:47:42.395425 506871 finetune.py:68] layer 25_up @ epoch 0 new loss 4.2741536162793636e-05 old loss 4.343521504779346e-05 BETTER
 56%|█████▋    | 18/32 [00:28<00:21,  1.54s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.54s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.54s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.54s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it] 72%|███████▏  | 23/32 [00:36<00:13,  1.54s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.54s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.54s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.54s/it]I0316 11:47:56.117628 507883 finetune.py:68] layer 27_o @ epoch 4 new loss 2.5160194127238356e-05 old loss 2.520358066249173e-05 BETTER
 84%|████████▍ | 27/32 [00:42<00:07,  1.54s/it]W0316 11:47:57.703783 507883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it]27_o proxy err 0.0023402783554047346 tr(WHW.T) 16.177593231201172
  0%|          | 0/32 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:45<00:04,  1.54s/it]  3%|▎         | 1/32 [00:02<01:03,  2.06s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it]  6%|▋         | 2/32 [00:03<00:52,  1.76s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.54s/it]  9%|▉         | 3/32 [00:05<00:48,  1.67s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.56s/it]
 12%|█▎        | 4/32 [00:06<00:45,  1.63s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.60s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.59s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.57s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.57s/it]I0316 11:48:12.731151 507377 finetune.py:45] layer 26_up initial loss 5.2161845815135166e-05
W0316 11:48:12.731848 507377 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:14<00:36,  1.58s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.57s/it] 34%|███▍      | 11/32 [00:17<00:33,  1.57s/it]I0316 11:48:17.173126 506367 finetune.py:68] layer 24_up @ epoch 2 new loss 3.500800812616944e-05 old loss 3.529979949234985e-05 BETTER
I0316 11:48:17.450623 506871 finetune.py:68] layer 25_up @ epoch 1 new loss 4.2329629650339484e-05 old loss 4.2741536162793636e-05 BETTER
 38%|███▊      | 12/32 [00:19<00:31,  1.58s/it] 41%|████      | 13/32 [00:20<00:29,  1.57s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.57s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.57s/it] 50%|█████     | 16/32 [00:25<00:24,  1.56s/it] 53%|█████▎    | 17/32 [00:27<00:23,  1.56s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.56s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.55s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.55s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.56s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.55s/it] 72%|███████▏  | 23/32 [00:36<00:13,  1.55s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.56s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.55s/it] 81%|████████▏ | 26/32 [00:41<00:09,  1.55s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.56s/it] 88%|████████▊ | 28/32 [00:44<00:06,  1.56s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.56s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.56s/it]I0316 11:48:47.150136 507377 finetune.py:68] layer 26_up @ epoch 0 new loss 5.125165625941008e-05 old loss 5.2161845815135166e-05 BETTER
 97%|█████████▋| 31/32 [00:48<00:01,  1.56s/it]100%|██████████| 32/32 [00:50<00:00,  1.56s/it]100%|██████████| 32/32 [00:50<00:00,  1.58s/it]
I0316 11:48:52.749268 506871 finetune.py:68] layer 25_up @ epoch 2 new loss 4.19900388806127e-05 old loss 4.2329629650339484e-05 BETTER
I0316 11:48:53.951232 506367 finetune.py:68] layer 24_up @ epoch 3 new loss 3.475717312539928e-05 old loss 3.500800812616944e-05 BETTER
I0316 11:48:57.894277 507883 finetune.py:45] layer 27_up initial loss 6.387056782841682e-05
W0316 11:48:57.894625 507883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:49:22.259666 507377 finetune.py:68] layer 26_up @ epoch 1 new loss 5.072786007076502e-05 old loss 5.125165625941008e-05 BETTER
I0316 11:49:28.348072 506871 finetune.py:68] layer 25_up @ epoch 3 new loss 4.172647822997533e-05 old loss 4.19900388806127e-05 BETTER
I0316 11:49:30.818150 506367 finetune.py:68] layer 24_up @ epoch 4 new loss 3.454116085777059e-05 old loss 3.475717312539928e-05 BETTER
I0316 11:49:31.874236 507883 finetune.py:68] layer 27_up @ epoch 0 new loss 6.276835483731702e-05 old loss 6.387056782841682e-05 BETTER
W0316 11:49:32.228933 506367 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_up proxy err 0.0038163987919688225 tr(WHW.T) 1342.89111328125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:58,  1.90s/it]  6%|▋         | 2/32 [00:03<00:49,  1.65s/it]  9%|▉         | 3/32 [00:04<00:45,  1.57s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.52s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.50s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.48s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.48s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.48s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.47s/it] 41%|████      | 13/32 [00:19<00:28,  1.47s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.47s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.47s/it]I0316 11:49:57.227329 507377 finetune.py:68] layer 26_up @ epoch 2 new loss 5.031640102970414e-05 old loss 5.072786007076502e-05 BETTER
 50%|█████     | 16/32 [00:24<00:23,  1.47s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.47s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.47s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.46s/it]I0316 11:50:03.339360 506871 finetune.py:68] layer 25_up @ epoch 4 new loss 4.148503649048507e-05 old loss 4.172647822997533e-05 BETTER
 62%|██████▎   | 20/32 [00:29<00:17,  1.47s/it]W0316 11:50:04.830654 506871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it]25_up proxy err 0.0038314543198794127 tr(WHW.T) 1429.37060546875
  0%|          | 0/32 [00:00<?, ?it/s] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it]I0316 11:50:06.464343 507883 finetune.py:68] layer 27_up @ epoch 1 new loss 6.209858111105859e-05 old loss 6.276835483731702e-05 BETTER
 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it]  3%|▎         | 1/32 [00:01<01:01,  1.99s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it]  6%|▋         | 2/32 [00:03<00:52,  1.74s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it]  9%|▉         | 3/32 [00:05<00:48,  1.66s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.62s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.60s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.46s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.58s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.57s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.46s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.57s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.46s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.56s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
 31%|███▏      | 10/32 [00:16<00:34,  1.56s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.56s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.57s/it] 41%|████      | 13/32 [00:20<00:29,  1.56s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.57s/it]I0316 11:50:29.440036 506367 finetune.py:45] layer 24_gate initial loss 5.289237014949322e-05
W0316 11:50:29.440473 506367 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 47%|████▋     | 15/32 [00:23<00:26,  1.56s/it] 50%|█████     | 16/32 [00:25<00:24,  1.56s/it]I0316 11:50:32.000402 507377 finetune.py:68] layer 26_up @ epoch 3 new loss 4.997711948817596e-05 old loss 5.031640102970414e-05 BETTER
 53%|█████▎    | 17/32 [00:26<00:23,  1.56s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.56s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.56s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.56s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.56s/it]I0316 11:50:40.602162 507883 finetune.py:68] layer 27_up @ epoch 2 new loss 6.157207826618105e-05 old loss 6.209858111105859e-05 BETTER
 69%|██████▉   | 22/32 [00:34<00:15,  1.56s/it] 72%|███████▏  | 23/32 [00:36<00:14,  1.56s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.56s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.56s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.56s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.56s/it] 88%|████████▊ | 28/32 [00:44<00:06,  1.56s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.57s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.57s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.57s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]
I0316 11:51:04.520496 506367 finetune.py:68] layer 24_gate @ epoch 0 new loss 5.2497100114123896e-05 old loss 5.289237014949322e-05 BETTER
I0316 11:51:05.182919 506871 finetune.py:45] layer 25_gate initial loss 6.305953138507903e-05
W0316 11:51:05.183317 506871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:51:07.572141 507377 finetune.py:68] layer 26_up @ epoch 4 new loss 4.9691105232341215e-05 old loss 4.997711948817596e-05 BETTER
W0316 11:51:09.002416 507377 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

26_up proxy err 0.003763841697946191 tr(WHW.T) 1584.2598876953125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  2.00s/it]  6%|▋         | 2/32 [00:03<00:51,  1.73s/it]I0316 11:51:14.658390 507883 finetune.py:68] layer 27_up @ epoch 3 new loss 6.115469295764342e-05 old loss 6.157207826618105e-05 BETTER
  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it] 50%|█████     | 16/32 [00:25<00:24,  1.54s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.54s/it]I0316 11:51:38.199050 506871 finetune.py:68] layer 25_gate @ epoch 0 new loss 6.261953240027651e-05 old loss 6.305953138507903e-05 BETTER
 56%|█████▋    | 18/32 [00:28<00:21,  1.53s/it]I0316 11:51:39.627146 506367 finetune.py:68] layer 24_gate @ epoch 1 new loss 5.2289877203293145e-05 old loss 5.2497100114123896e-05 BETTER
 59%|█████▉    | 19/32 [00:29<00:19,  1.54s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.54s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.54s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it]I0316 11:51:48.776983 507883 finetune.py:68] layer 27_up @ epoch 4 new loss 6.0813290474470705e-05 old loss 6.115469295764342e-05 BETTER
 78%|███████▊  | 25/32 [00:38<00:10,  1.54s/it]W0316 11:51:50.158752 507883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it]27_up proxy err 0.0035088665317744017 tr(WHW.T) 1872.06298828125
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it]  3%|▎         | 1/32 [00:02<01:02,  2.03s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.53s/it]  6%|▋         | 2/32 [00:03<00:52,  1.76s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.53s/it]  9%|▉         | 3/32 [00:05<00:48,  1.68s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.53s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.64s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.53s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.61s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
 19%|█▉        | 6/32 [00:09<00:41,  1.60s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.60s/it] 25%|██▌       | 8/32 [00:13<00:38,  1.59s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.59s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.59s/it]I0316 11:52:07.903389 507377 finetune.py:45] layer 26_gate initial loss 7.572951290057972e-05
W0316 11:52:07.904006 507377 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:17<00:33,  1.58s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.58s/it]I0316 11:52:11.758899 506871 finetune.py:68] layer 25_gate @ epoch 1 new loss 6.238462083274499e-05 old loss 6.261953240027651e-05 BETTER
 41%|████      | 13/32 [00:20<00:30,  1.58s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.58s/it]I0316 11:52:14.546025 506367 finetune.py:68] layer 24_gate @ epoch 2 new loss 5.211303141550161e-05 old loss 5.2289877203293145e-05 BETTER
 47%|████▋     | 15/32 [00:24<00:26,  1.58s/it] 50%|█████     | 16/32 [00:25<00:25,  1.58s/it] 53%|█████▎    | 17/32 [00:27<00:23,  1.57s/it] 56%|█████▋    | 18/32 [00:28<00:22,  1.58s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.57s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.57s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.58s/it] 69%|██████▉   | 22/32 [00:35<00:15,  1.58s/it] 72%|███████▏  | 23/32 [00:36<00:14,  1.57s/it] 75%|███████▌  | 24/32 [00:38<00:12,  1.58s/it] 78%|███████▊  | 25/32 [00:39<00:11,  1.57s/it] 81%|████████▏ | 26/32 [00:41<00:09,  1.57s/it] 84%|████████▍ | 27/32 [00:43<00:07,  1.57s/it] 88%|████████▊ | 28/32 [00:44<00:06,  1.57s/it] 91%|█████████ | 29/32 [00:46<00:04,  1.57s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.57s/it] 97%|█████████▋| 31/32 [00:49<00:01,  1.58s/it]I0316 11:52:41.097133 507377 finetune.py:68] layer 26_gate @ epoch 0 new loss 7.520687358919531e-05 old loss 7.572951290057972e-05 BETTER
100%|██████████| 32/32 [00:50<00:00,  1.58s/it]100%|██████████| 32/32 [00:50<00:00,  1.59s/it]
I0316 11:52:45.568052 506871 finetune.py:68] layer 25_gate @ epoch 2 new loss 6.221017974894494e-05 old loss 6.238462083274499e-05 BETTER
I0316 11:52:49.627105 506367 finetune.py:68] layer 24_gate @ epoch 3 new loss 5.196591519052163e-05 old loss 5.211303141550161e-05 BETTER
I0316 11:52:50.832544 507883 finetune.py:45] layer 27_gate initial loss 9.359265095554292e-05
W0316 11:52:50.832904 507883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:53:14.816307 507377 finetune.py:68] layer 26_gate @ epoch 1 new loss 7.492421718779951e-05 old loss 7.520687358919531e-05 BETTER
I0316 11:53:18.665900 506871 finetune.py:68] layer 25_gate @ epoch 3 new loss 6.204702367540449e-05 old loss 6.221017974894494e-05 BETTER
I0316 11:53:23.069323 507883 finetune.py:68] layer 27_gate @ epoch 0 new loss 9.287786815548316e-05 old loss 9.359265095554292e-05 BETTER
I0316 11:53:24.750694 506367 finetune.py:68] layer 24_gate @ epoch 4 new loss 5.184944166103378e-05 old loss 5.196591519052163e-05 BETTER
W0316 11:53:26.021726 506367 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_gate proxy err 0.001969525357708335 tr(WHW.T) 4750.44091796875
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:41,  1.09it/s]  2%|▏         | 2/112 [00:01<01:05,  1.67it/s]  3%|▎         | 3/112 [00:01<00:53,  2.02it/s]  4%|▎         | 4/112 [00:02<00:48,  2.23it/s]  4%|▍         | 5/112 [00:02<00:45,  2.36it/s]  5%|▌         | 6/112 [00:02<00:43,  2.45it/s]  6%|▋         | 7/112 [00:03<00:42,  2.50it/s]  7%|▋         | 8/112 [00:03<00:40,  2.54it/s]  8%|▊         | 9/112 [00:03<00:40,  2.55it/s]  9%|▉         | 10/112 [00:04<00:39,  2.56it/s] 10%|▉         | 11/112 [00:04<00:39,  2.58it/s] 11%|█         | 12/112 [00:05<00:38,  2.57it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.57it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.56it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.57it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.58it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.58it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.60it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.63it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.62it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.60it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.60it/s] 21%|██        | 23/112 [00:09<00:34,  2.59it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.60it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.61it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.58it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.61it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.61it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.60it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.59it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.60it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.60it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.61it/s] 30%|███       | 34/112 [00:13<00:29,  2.60it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.60it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.59it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.61it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.59it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.60it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.61it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.63it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.64it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.64it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.65it/s] 40%|████      | 45/112 [00:17<00:25,  2.65it/s] 41%|████      | 46/112 [00:18<00:24,  2.64it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.62it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.63it/s]I0316 11:53:48.558063 507377 finetune.py:68] layer 26_gate @ epoch 2 new loss 7.470083801308647e-05 old loss 7.492421718779951e-05 BETTER
 44%|████▍     | 49/112 [00:19<00:23,  2.63it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.60it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.61it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.62it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.61it/s] 48%|████▊     | 54/112 [00:21<00:21,  2.64it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.65it/s] 50%|█████     | 56/112 [00:21<00:21,  2.66it/s] 51%|█████     | 57/112 [00:22<00:20,  2.67it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.67it/s]I0316 11:53:52.374432 506871 finetune.py:68] layer 25_gate @ epoch 4 new loss 6.189973646542057e-05 old loss 6.204702367540449e-05 BETTER
 53%|█████▎    | 59/112 [00:23<00:19,  2.67it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.67it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.68it/s] 55%|█████▌    | 62/112 [00:24<00:18,  2.65it/s]W0316 11:53:53.564614 506871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 56%|█████▋    | 63/112 [00:24<00:18,  2.67it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.68it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.69it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.69it/s] 60%|█████▉    | 67/112 [00:26<00:16,  2.69it/s] 61%|██████    | 68/112 [00:26<00:16,  2.70it/s]I0316 11:53:55.971481 507883 finetune.py:68] layer 27_gate @ epoch 1 new loss 9.250912989955395e-05 old loss 9.287786815548316e-05 BETTER
 62%|██████▏   | 69/112 [00:26<00:15,  2.70it/s] 62%|██████▎   | 70/112 [00:27<00:15,  2.71it/s]25_gate proxy err 0.0019596153870224953 tr(WHW.T) 5093.5966796875
  0%|          | 0/112 [00:00<?, ?it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.70it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.71it/s]  1%|          | 1/112 [00:00<01:36,  1.14it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.72it/s]  2%|▏         | 2/112 [00:01<01:04,  1.70it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.71it/s]  3%|▎         | 3/112 [00:01<00:54,  1.99it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.72it/s]  4%|▎         | 4/112 [00:02<00:49,  2.17it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.73it/s] 69%|██████▉   | 77/112 [00:29<00:12,  2.73it/s]  4%|▍         | 5/112 [00:02<00:46,  2.29it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.73it/s]  5%|▌         | 6/112 [00:02<00:44,  2.37it/s] 71%|███████   | 79/112 [00:30<00:12,  2.73it/s]  6%|▋         | 7/112 [00:03<00:43,  2.41it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.73it/s]  7%|▋         | 8/112 [00:03<00:42,  2.44it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.74it/s]  8%|▊         | 9/112 [00:04<00:41,  2.46it/s] 73%|███████▎  | 82/112 [00:31<00:10,  2.75it/s]  9%|▉         | 10/112 [00:04<00:41,  2.48it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.74it/s] 10%|▉         | 11/112 [00:04<00:40,  2.50it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.74it/s] 11%|█         | 12/112 [00:05<00:39,  2.52it/s] 76%|███████▌  | 85/112 [00:32<00:09,  2.74it/s] 12%|█▏        | 13/112 [00:05<00:39,  2.52it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.73it/s] 12%|█▎        | 14/112 [00:06<00:38,  2.53it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.71it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.54it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.73it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.54it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.74it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.53it/s] 80%|████████  | 90/112 [00:34<00:08,  2.73it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.72it/s] 16%|█▌        | 18/112 [00:07<00:37,  2.52it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.73it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.53it/s] 83%|████████▎ | 93/112 [00:35<00:06,  2.73it/s] 18%|█▊        | 20/112 [00:08<00:36,  2.51it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.73it/s] 19%|█▉        | 21/112 [00:08<00:36,  2.50it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.71it/s] 20%|█▉        | 22/112 [00:09<00:36,  2.49it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.72it/s] 21%|██        | 23/112 [00:09<00:35,  2.50it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.72it/s] 21%|██▏       | 24/112 [00:09<00:35,  2.51it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.74it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.52it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.72it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.53it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.73it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.54it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.73it/s] 25%|██▌       | 28/112 [00:11<00:33,  2.54it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.74it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.53it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.72it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.74it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.53it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.75it/s] 28%|██▊       | 31/112 [00:12<00:32,  2.49it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.74it/s] 29%|██▊       | 32/112 [00:13<00:31,  2.50it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.75it/s] 29%|██▉       | 33/112 [00:13<00:31,  2.52it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.74it/s] 30%|███       | 34/112 [00:13<00:31,  2.51it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.72it/s] 31%|███▏      | 35/112 [00:14<00:30,  2.52it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.73it/s] 32%|███▏      | 36/112 [00:14<00:30,  2.53it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.70it/s] 33%|███▎      | 37/112 [00:15<00:29,  2.53it/s]100%|██████████| 112/112 [00:42<00:00,  2.72it/s]100%|██████████| 112/112 [00:42<00:00,  2.63it/s]
 34%|███▍      | 38/112 [00:15<00:29,  2.54it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.53it/s] 36%|███▌      | 40/112 [00:16<00:28,  2.52it/s] 37%|███▋      | 41/112 [00:16<00:28,  2.51it/s] 38%|███▊      | 42/112 [00:17<00:27,  2.52it/s] 38%|███▊      | 43/112 [00:17<00:27,  2.48it/s] 39%|███▉      | 44/112 [00:17<00:27,  2.51it/s] 40%|████      | 45/112 [00:18<00:26,  2.52it/s] 41%|████      | 46/112 [00:18<00:26,  2.53it/s] 42%|████▏     | 47/112 [00:19<00:25,  2.53it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.54it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.54it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.54it/s] 46%|████▌     | 51/112 [00:20<00:24,  2.53it/s] 46%|████▋     | 52/112 [00:21<00:23,  2.52it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.51it/s] 48%|████▊     | 54/112 [00:21<00:23,  2.47it/s]W0316 11:54:18.853000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:54:18.853000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:54:18.853000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:54:18.853000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:54:18.853000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:54:18.853000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:54:18.854000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:54:18.896000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:54:18.896000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:54:18.897000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:54:18.897000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:54:18.897000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 49%|████▉     | 55/112 [00:22<00:22,  2.48it/s]W0316 11:54:19.076000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.076000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.076000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.076000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.077000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 56/112 [00:22<00:22,  2.49it/s]W0316 11:54:19.405000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.405000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.405000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.405000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.405000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.406000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.406000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.437000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.437000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.437000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.437000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.437000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.509000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.509000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.509000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.509000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:54:19.510000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 51%|█████     | 57/112 [00:23<00:22,  2.50it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.50it/s] 53%|█████▎    | 59/112 [00:23<00:21,  2.50it/s]W0316 11:54:20.873000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 54%|█████▎    | 60/112 [00:24<00:20,  2.50it/s] 54%|█████▍    | 61/112 [00:24<00:20,  2.51it/s]W0316 11:54:21.352000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:54:21.352000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:54:21.353000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:54:21.353000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:54:21.353000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:54:21.353000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:54:21.353000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:54:21.385000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:54:21.385000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:54:21.385000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:54:21.385000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:54:21.385000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 55%|█████▌    | 62/112 [00:25<00:19,  2.51it/s]W0316 11:54:21.774000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:54:21.774000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:54:21.774000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:54:21.775000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:54:21.775000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
I0316 11:54:21.840316 507377 finetune.py:68] layer 26_gate @ epoch 3 new loss 7.452085992554203e-05 old loss 7.470083801308647e-05 BETTER
 56%|█████▋    | 63/112 [00:25<00:19,  2.51it/s]W0316 11:54:22.337000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:54:22.343000 139757268076352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 57%|█████▋    | 64/112 [00:25<00:19,  2.50it/s] 58%|█████▊    | 65/112 [00:26<00:18,  2.50it/s] 59%|█████▉    | 66/112 [00:26<00:18,  2.49it/s] 60%|█████▉    | 67/112 [00:27<00:17,  2.50it/s] 61%|██████    | 68/112 [00:27<00:17,  2.51it/s] 62%|██████▏   | 69/112 [00:27<00:17,  2.52it/s] 62%|██████▎   | 70/112 [00:28<00:16,  2.53it/s] 63%|██████▎   | 71/112 [00:28<00:16,  2.54it/s] 64%|██████▍   | 72/112 [00:29<00:15,  2.54it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.54it/s] 66%|██████▌   | 74/112 [00:29<00:15,  2.53it/s] 67%|██████▋   | 75/112 [00:30<00:14,  2.53it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.53it/s] 69%|██████▉   | 77/112 [00:31<00:14,  2.49it/s] 70%|██████▉   | 78/112 [00:31<00:13,  2.50it/s] 71%|███████   | 79/112 [00:31<00:13,  2.51it/s] 71%|███████▏  | 80/112 [00:32<00:12,  2.51it/s]I0316 11:54:28.897129 507883 finetune.py:68] layer 27_gate @ epoch 2 new loss 9.220997162628919e-05 old loss 9.250912989955395e-05 BETTER
 72%|███████▏  | 81/112 [00:32<00:12,  2.52it/s] 73%|███████▎  | 82/112 [00:33<00:11,  2.52it/s]I0316 11:54:29.709608 506367 finetune.py:45] layer 24_down initial loss 8.263099880423397e-05
W0316 11:54:29.710097 506367 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 74%|███████▍  | 83/112 [00:33<00:11,  2.53it/s] 75%|███████▌  | 84/112 [00:33<00:11,  2.53it/s] 76%|███████▌  | 85/112 [00:34<00:10,  2.54it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.54it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.55it/s] 79%|███████▊  | 88/112 [00:35<00:09,  2.54it/s] 79%|███████▉  | 89/112 [00:35<00:09,  2.52it/s] 80%|████████  | 90/112 [00:36<00:08,  2.53it/s] 81%|████████▏ | 91/112 [00:36<00:08,  2.53it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.55it/s] 83%|████████▎ | 93/112 [00:37<00:07,  2.55it/s] 84%|████████▍ | 94/112 [00:37<00:07,  2.55it/s] 85%|████████▍ | 95/112 [00:38<00:06,  2.56it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.56it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.57it/s] 88%|████████▊ | 98/112 [00:39<00:05,  2.56it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.55it/s] 89%|████████▉ | 100/112 [00:40<00:04,  2.56it/s] 90%|█████████ | 101/112 [00:40<00:04,  2.53it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.53it/s] 92%|█████████▏| 103/112 [00:41<00:03,  2.53it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.53it/s] 94%|█████████▍| 105/112 [00:42<00:02,  2.54it/s] 95%|█████████▍| 106/112 [00:42<00:02,  2.54it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.54it/s] 96%|█████████▋| 108/112 [00:43<00:01,  2.54it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.54it/s] 98%|█████████▊| 110/112 [00:44<00:00,  2.54it/s] 99%|█████████▉| 111/112 [00:44<00:00,  2.53it/s]100%|██████████| 112/112 [00:44<00:00,  2.50it/s]100%|██████████| 112/112 [00:44<00:00,  2.50it/s]
W0316 11:54:48.406000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.406000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.406000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.406000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.406000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.406000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.407000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.448000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.448000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.448000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.448000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.448000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.629000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.629000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.629000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.630000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.630000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.954000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.954000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.954000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.954000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.954000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.954000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.954000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.990000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.990000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.990000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.990000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:54:48.990000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:54:49.061000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:54:49.061000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:54:49.061000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:54:49.061000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:54:49.061000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:54:50.400000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:54:50.867000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:54:50.867000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:54:50.867000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:54:50.867000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:54:50.867000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:54:50.867000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:54:50.867000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:54:50.899000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:54:50.899000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:54:50.900000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:54:50.900000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:54:50.900000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:54:51.285000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:54:51.285000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:54:51.285000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:54:51.285000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:54:51.286000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:54:51.835000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:54:51.840000 139691852089152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0316 11:54:55.602986 507377 finetune.py:68] layer 26_gate @ epoch 4 new loss 7.435769657604396e-05 old loss 7.452085992554203e-05 BETTER
W0316 11:54:57.099883 507377 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 11:54:59.267938 506871 finetune.py:45] layer 25_down initial loss 9.566690278006718e-05
W0316 11:54:59.268250 506871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

26_gate proxy err 0.0018080498557537794 tr(WHW.T) 5985.15576171875
  0%|          | 0/112 [00:00<?, ?it/s]I0316 11:55:01.856676 507883 finetune.py:68] layer 27_gate @ epoch 3 new loss 9.198466432280838e-05 old loss 9.220997162628919e-05 BETTER
  1%|          | 1/112 [00:00<01:40,  1.10it/s]  2%|▏         | 2/112 [00:01<01:06,  1.66it/s]I0316 11:55:02.351234 506367 finetune.py:68] layer 24_down @ epoch 0 new loss 8.260181493824348e-05 old loss 8.263099880423397e-05 BETTER
  3%|▎         | 3/112 [00:01<00:55,  1.97it/s]  4%|▎         | 4/112 [00:02<00:52,  2.07it/s]  4%|▍         | 5/112 [00:02<00:49,  2.16it/s]  5%|▌         | 6/112 [00:02<00:47,  2.23it/s]  6%|▋         | 7/112 [00:03<00:45,  2.31it/s]  7%|▋         | 8/112 [00:03<00:43,  2.38it/s]  8%|▊         | 9/112 [00:04<00:42,  2.44it/s]  9%|▉         | 10/112 [00:04<00:41,  2.49it/s] 10%|▉         | 11/112 [00:04<00:40,  2.49it/s] 11%|█         | 12/112 [00:05<00:40,  2.47it/s] 12%|█▏        | 13/112 [00:05<00:39,  2.51it/s] 12%|█▎        | 14/112 [00:06<00:38,  2.53it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.54it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.54it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.53it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.54it/s] 17%|█▋        | 19/112 [00:08<00:36,  2.55it/s] 18%|█▊        | 20/112 [00:08<00:36,  2.53it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.54it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.55it/s] 21%|██        | 23/112 [00:09<00:34,  2.56it/s] 21%|██▏       | 24/112 [00:10<00:34,  2.56it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.56it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.55it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.56it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.56it/s] 26%|██▌       | 29/112 [00:12<00:32,  2.53it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.54it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.55it/s] 29%|██▊       | 32/112 [00:13<00:31,  2.57it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.57it/s] 30%|███       | 34/112 [00:13<00:30,  2.58it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.59it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.58it/s] 33%|███▎      | 37/112 [00:15<00:29,  2.57it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.57it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.56it/s] 36%|███▌      | 40/112 [00:16<00:28,  2.57it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.54it/s] 38%|███▊      | 42/112 [00:17<00:27,  2.55it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.56it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.56it/s] 40%|████      | 45/112 [00:18<00:26,  2.57it/s] 41%|████      | 46/112 [00:18<00:25,  2.57it/s] 42%|████▏     | 47/112 [00:19<00:25,  2.57it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.57it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.58it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.57it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.57it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.57it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.53it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.55it/s] 49%|████▉     | 55/112 [00:22<00:22,  2.56it/s] 50%|█████     | 56/112 [00:22<00:21,  2.57it/s] 51%|█████     | 57/112 [00:22<00:21,  2.57it/s] 52%|█████▏    | 58/112 [00:23<00:20,  2.58it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.58it/s] 54%|█████▎    | 60/112 [00:24<00:20,  2.59it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.59it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.54it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.53it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.54it/s] 58%|█████▊    | 65/112 [00:26<00:18,  2.53it/s] 59%|█████▉    | 66/112 [00:26<00:18,  2.54it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.56it/s] 61%|██████    | 68/112 [00:27<00:17,  2.56it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.57it/s] 62%|██████▎   | 70/112 [00:28<00:16,  2.55it/s] 63%|██████▎   | 71/112 [00:28<00:15,  2.56it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.57it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.57it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.57it/s]I0316 11:55:30.703697 506871 finetune.py:68] layer 25_down @ epoch 0 new loss 9.564541687723249e-05 old loss 9.566690278006718e-05 BETTER
 67%|██████▋   | 75/112 [00:29<00:14,  2.55it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.55it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.53it/s] 70%|██████▉   | 78/112 [00:31<00:13,  2.55it/s] 71%|███████   | 79/112 [00:31<00:12,  2.55it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.56it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.57it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.57it/s] 74%|███████▍  | 83/112 [00:33<00:11,  2.58it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.58it/s]I0316 11:55:34.576492 507883 finetune.py:68] layer 27_gate @ epoch 4 new loss 9.178683103527874e-05 old loss 9.198466432280838e-05 BETTER
 76%|███████▌  | 85/112 [00:33<00:10,  2.58it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.58it/s]I0316 11:55:35.355522 506367 finetune.py:68] layer 24_down @ epoch 1 new loss 8.25929528218694e-05 old loss 8.260181493824348e-05 BETTER
 78%|███████▊  | 87/112 [00:34<00:09,  2.58it/s]W0316 11:55:35.652410 507883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 79%|███████▊  | 88/112 [00:35<00:09,  2.56it/s] 79%|███████▉  | 89/112 [00:35<00:09,  2.54it/s] 80%|████████  | 90/112 [00:35<00:08,  2.56it/s] 81%|████████▏ | 91/112 [00:36<00:08,  2.57it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.57it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.57it/s] 84%|████████▍ | 94/112 [00:37<00:06,  2.58it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.58it/s]27_gate proxy err 0.001630741753615439 tr(WHW.T) 7249.4248046875
  0%|          | 0/112 [00:00<?, ?it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.59it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.59it/s]  1%|          | 1/112 [00:00<01:38,  1.13it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.58it/s]  2%|▏         | 2/112 [00:01<01:05,  1.68it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.58it/s]  3%|▎         | 3/112 [00:01<00:54,  1.99it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.58it/s]  4%|▎         | 4/112 [00:02<00:49,  2.18it/s] 90%|█████████ | 101/112 [00:40<00:04,  2.56it/s]  4%|▍         | 5/112 [00:02<00:46,  2.30it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.57it/s]  5%|▌         | 6/112 [00:02<00:44,  2.37it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.57it/s]  6%|▋         | 7/112 [00:03<00:43,  2.42it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.58it/s]  7%|▋         | 8/112 [00:03<00:42,  2.46it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.58it/s]  8%|▊         | 9/112 [00:04<00:41,  2.48it/s] 95%|█████████▍| 106/112 [00:42<00:02,  2.58it/s]  9%|▉         | 10/112 [00:04<00:41,  2.48it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.58it/s] 10%|▉         | 11/112 [00:04<00:40,  2.50it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.57it/s] 11%|█         | 12/112 [00:05<00:39,  2.50it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.57it/s] 12%|█▏        | 13/112 [00:05<00:39,  2.52it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.56it/s] 12%|█▎        | 14/112 [00:06<00:38,  2.53it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.57it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.54it/s]100%|██████████| 112/112 [00:44<00:00,  2.53it/s]100%|██████████| 112/112 [00:44<00:00,  2.52it/s]
 14%|█▍        | 16/112 [00:06<00:37,  2.53it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.53it/s] 16%|█▌        | 18/112 [00:07<00:37,  2.53it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.53it/s] 18%|█▊        | 20/112 [00:08<00:36,  2.53it/s] 19%|█▉        | 21/112 [00:08<00:36,  2.51it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.52it/s] 21%|██        | 23/112 [00:09<00:35,  2.54it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.54it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.55it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.56it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.57it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.57it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.57it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.56it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.55it/s] 29%|██▊       | 32/112 [00:13<00:31,  2.55it/s] 29%|██▉       | 33/112 [00:13<00:31,  2.52it/s]W0316 11:55:52.425000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.425000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.425000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.426000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.426000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.426000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.426000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.470000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.470000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.470000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.470000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.470000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.653000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.653000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.653000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.653000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.653000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 30%|███       | 34/112 [00:13<00:30,  2.53it/s]W0316 11:55:52.976000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.976000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.976000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.977000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.977000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.977000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:55:52.977000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:55:53.010000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:55:53.011000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:55:53.011000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:55:53.011000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:55:53.011000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:55:53.082000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:55:53.082000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:55:53.082000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:55:53.082000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:55:53.082000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 35/112 [00:14<00:30,  2.53it/s] 32%|███▏      | 36/112 [00:14<00:30,  2.52it/s] 33%|███▎      | 37/112 [00:15<00:29,  2.53it/s] 34%|███▍      | 38/112 [00:15<00:29,  2.53it/s]W0316 11:55:54.441000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 35%|███▍      | 39/112 [00:15<00:28,  2.54it/s]W0316 11:55:54.913000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:55:54.913000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:55:54.913000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:55:54.914000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:55:54.914000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:55:54.914000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:55:54.914000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:55:54.947000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:55:54.947000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:55:54.947000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:55:54.947000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:55:54.947000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 36%|███▌      | 40/112 [00:16<00:28,  2.54it/s]W0316 11:55:55.357000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:55:55.357000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:55:55.357000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:55:55.357000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:55:55.357000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 37%|███▋      | 41/112 [00:16<00:28,  2.53it/s] 38%|███▊      | 42/112 [00:17<00:27,  2.53it/s]W0316 11:55:55.910000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:55:55.917000 140322994964288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 43/112 [00:17<00:27,  2.52it/s] 39%|███▉      | 44/112 [00:17<00:27,  2.50it/s] 40%|████      | 45/112 [00:18<00:26,  2.52it/s] 41%|████      | 46/112 [00:18<00:26,  2.53it/s] 42%|████▏     | 47/112 [00:19<00:25,  2.53it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.53it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.53it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.53it/s] 46%|████▌     | 51/112 [00:20<00:24,  2.53it/s] 46%|████▋     | 52/112 [00:21<00:23,  2.53it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.53it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.53it/s] 49%|████▉     | 55/112 [00:22<00:22,  2.54it/s] 50%|█████     | 56/112 [00:22<00:22,  2.51it/s] 51%|█████     | 57/112 [00:22<00:21,  2.52it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.54it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.54it/s]I0316 11:56:02.667309 506871 finetune.py:68] layer 25_down @ epoch 1 new loss 9.564417996443808e-05 old loss 9.564541687723249e-05 BETTER
 54%|█████▎    | 60/112 [00:24<00:20,  2.55it/s]I0316 11:56:03.356003 507377 finetune.py:45] layer 26_down initial loss 0.00011350939894327894
W0316 11:56:03.356506 507377 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 54%|█████▍    | 61/112 [00:24<00:19,  2.55it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.56it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.56it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.56it/s] 58%|█████▊    | 65/112 [00:26<00:18,  2.56it/s] 59%|█████▉    | 66/112 [00:26<00:18,  2.55it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.55it/s] 61%|██████    | 68/112 [00:27<00:17,  2.53it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.54it/s] 62%|██████▎   | 70/112 [00:28<00:16,  2.54it/s] 63%|██████▎   | 71/112 [00:28<00:16,  2.55it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.55it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.56it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.56it/s]I0316 11:56:08.740469 506367 finetune.py:76] layer 24_down @ epoch 2 new loss 8.259396417997777e-05 old loss 8.25929528218694e-05 WORSE
 67%|██████▋   | 75/112 [00:30<00:14,  2.57it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.56it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.56it/s] 70%|██████▉   | 78/112 [00:31<00:13,  2.56it/s] 71%|███████   | 79/112 [00:31<00:13,  2.52it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.53it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.54it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.54it/s] 74%|███████▍  | 83/112 [00:33<00:11,  2.55it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.55it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.55it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.55it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.56it/s] 79%|███████▊  | 88/112 [00:35<00:09,  2.56it/s] 79%|███████▉  | 89/112 [00:35<00:09,  2.55it/s] 80%|████████  | 90/112 [00:35<00:08,  2.55it/s] 81%|████████▏ | 91/112 [00:36<00:08,  2.52it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.53it/s] 83%|████████▎ | 93/112 [00:37<00:07,  2.55it/s] 84%|████████▍ | 94/112 [00:37<00:07,  2.55it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.56it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.57it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.57it/s] 88%|████████▊ | 98/112 [00:39<00:05,  2.57it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.57it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.56it/s] 90%|█████████ | 101/112 [00:40<00:04,  2.56it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.55it/s] 92%|█████████▏| 103/112 [00:41<00:03,  2.53it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.54it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.55it/s] 95%|█████████▍| 106/112 [00:42<00:02,  2.55it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.55it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.55it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.56it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.57it/s] 99%|█████████▉| 111/112 [00:44<00:00,  2.57it/s]100%|██████████| 112/112 [00:44<00:00,  2.56it/s]100%|██████████| 112/112 [00:44<00:00,  2.52it/s]
W0316 11:56:30.615000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:56:30.615000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:56:30.615000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:56:30.615000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:56:30.616000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:56:30.616000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:56:30.616000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:56:30.657000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:56:30.658000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:56:30.658000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:56:30.658000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:56:30.658000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:56:30.845000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:56:30.845000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:56:30.846000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:56:30.846000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:56:30.846000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:56:31.177000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:56:31.177000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:56:31.178000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:56:31.178000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:56:31.178000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:56:31.178000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:56:31.178000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:56:31.215000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:56:31.215000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:56:31.215000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:56:31.216000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:56:31.216000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:56:31.287000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:56:31.287000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:56:31.287000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:56:31.287000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:56:31.287000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:56:32.679000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:56:33.154000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:56:33.155000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:56:33.155000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:56:33.155000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:56:33.155000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:56:33.155000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:56:33.155000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:56:33.188000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:56:33.188000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:56:33.188000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:56:33.188000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:56:33.188000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:56:33.582000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:56:33.582000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:56:33.582000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:56:33.582000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:56:33.582000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:56:34.144000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 11:56:34.149000 140144047765312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0316 11:56:34.431102 506871 finetune.py:68] layer 25_down @ epoch 2 new loss 9.563643106957898e-05 old loss 9.564417996443808e-05 BETTER
I0316 11:56:34.944694 507377 finetune.py:68] layer 26_down @ epoch 0 new loss 0.0001134792692027986 old loss 0.00011350939894327894 BETTER
I0316 11:56:41.212390 507883 finetune.py:45] layer 27_down initial loss 0.00013815710553899407
W0316 11:56:41.212870 507883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:56:41.696566 506367 finetune.py:76] layer 24_down @ epoch 3 new loss 8.259466994786635e-05 old loss 8.25929528218694e-05 WORSE
I0316 11:57:06.528799 506871 finetune.py:68] layer 25_down @ epoch 3 new loss 9.563253843225539e-05 old loss 9.563643106957898e-05 BETTER
I0316 11:57:07.400851 507377 finetune.py:68] layer 26_down @ epoch 1 new loss 0.000113475471152924 old loss 0.0001134792692027986 BETTER
I0316 11:57:11.828136 507883 finetune.py:68] layer 27_down @ epoch 0 new loss 0.00013810860400553793 old loss 0.00013815710553899407 BETTER
I0316 11:57:14.576380 506367 finetune.py:68] layer 24_down @ epoch 4 new loss 8.25899260235019e-05 old loss 8.25929528218694e-05 BETTER
W0316 11:57:15.390673 506367 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

24_down proxy err 0.0037282751873135567 tr(WHW.T) 34.989952087402344
I0316 11:57:38.601117 506871 finetune.py:68] layer 25_down @ epoch 4 new loss 9.56306466832757e-05 old loss 9.563253843225539e-05 BETTER
I0316 11:57:39.400559 507377 finetune.py:76] layer 26_down @ epoch 2 new loss 0.00011347707913955674 old loss 0.000113475471152924 WORSE
W0316 11:57:39.481954 506871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

25_down proxy err 0.0037597373593598604 tr(WHW.T) 37.95758819580078
I0316 11:57:43.657711 507883 finetune.py:68] layer 27_down @ epoch 1 new loss 0.00013809943629894406 old loss 0.00013810860400553793 BETTER
I0316 11:58:10.704561 507377 finetune.py:76] layer 26_down @ epoch 3 new loss 0.00011347561667207628 old loss 0.000113475471152924 WORSE
I0316 11:58:14.636920 507883 finetune.py:76] layer 27_down @ epoch 2 new loss 0.00013810236123390496 old loss 0.00013809943629894406 WORSE
I0316 11:58:42.258638 507377 finetune.py:68] layer 26_down @ epoch 4 new loss 0.00011347232066327706 old loss 0.000113475471152924 BETTER
W0316 11:58:43.104805 507377 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

26_down proxy err 0.0038062932435423136 tr(WHW.T) 43.31587219238281
I0316 11:58:45.519901 507883 finetune.py:76] layer 27_down @ epoch 3 new loss 0.00013810065865982324 old loss 0.00013809943629894406 WORSE
I0316 11:58:48.992121 460347 quantize_finetune_llama.py:186] computed original embedding for layer 28 in 65.13001704216003s
I0316 11:58:49.401952 460347 quantize_finetune_llama.py:159] layer 29 gpu 1
I0316 11:58:51.429845 508581 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 11:58:51.429958 508581 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 11:58:51.430022 508581 utils.py:162] NumExpr defaulting to 16 threads.
I0316 11:58:51.622961 508581 config.py:58] PyTorch version 2.4.0 available.
I0316 11:58:53.851423 508581 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 11:58:54.258561 508581 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:55,  1.78s/it]  6%|▋         | 2/32 [00:02<00:27,  1.07it/s]  9%|▉         | 3/32 [00:02<00:19,  1.51it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.85it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.13it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.35it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.64it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.80it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.86it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.84it/s] 41%|████      | 13/32 [00:05<00:06,  2.89it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.95it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.99it/s] 50%|█████     | 16/32 [00:06<00:05,  3.01it/s] 53%|█████▎    | 17/32 [00:07<00:04,  3.02it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.04it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.07it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.99it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.94it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.94it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.93it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.93it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.92it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.92it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.88it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.89it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.90it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
W0316 11:59:10.526000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:59:10.526000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:59:10.526000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:59:10.526000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:59:10.527000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 11:59:10.527000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 11:59:10.527000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:59:10.555000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:59:10.555000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:59:10.555000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:59:10.555000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:59:10.555000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:59:10.860000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 11:59:10.860000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 11:59:10.860000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 11:59:10.860000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 11:59:10.860000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 11:59:11.484000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:59:11.484000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:59:11.484000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 11:59:11.484000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:59:11.484000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 11:59:11.484000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:59:11.485000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:59:11.503000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:59:11.503000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:59:11.503000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:59:11.503000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:59:11.503000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:59:11.713000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 11:59:11.713000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 11:59:11.713000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 11:59:11.713000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 11:59:11.713000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 11:59:12.901000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:59:12.901000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:59:12.901000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:59:12.901000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:59:12.901000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:59:12.902000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 11:59:12.902000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 11:59:12.920000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:59:12.920000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:59:12.920000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:59:12.920000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:59:12.921000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 11:59:13.578000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 11:59:13.578000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 11:59:13.578000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 11:59:13.578000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 11:59:13.578000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 11:59:16.547233 507883 finetune.py:68] layer 27_down @ epoch 4 new loss 0.0001380869362037629 old loss 0.00013809943629894406 BETTER
W0316 11:59:17.370111 507883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

27_down proxy err 0.0031985740642994642 tr(WHW.T) 61.274600982666016
I0316 11:59:20.140876 508581 finetune.py:45] layer 28_v initial loss 5.6756754929665476e-05
W0316 11:59:20.141159 508581 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 11:59:52.817337 460347 quantize_finetune_llama.py:186] computed original embedding for layer 29 in 62.9697380065918s
I0316 11:59:53.177515 460347 quantize_finetune_llama.py:159] layer 30 gpu 2
I0316 11:59:55.290981 509085 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 11:59:55.291105 509085 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 11:59:55.291166 509085 utils.py:162] NumExpr defaulting to 16 threads.
I0316 11:59:55.515659 509085 config.py:58] PyTorch version 2.4.0 available.
I0316 11:59:56.479145 508581 finetune.py:68] layer 28_v @ epoch 0 new loss 1.5569872630294412e-05 old loss 5.6756754929665476e-05 BETTER
I0316 11:59:57.982428 509085 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 11:59:58.426566 509085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.99s/it]  6%|▋         | 2/32 [00:02<00:30,  1.02s/it]  9%|▉         | 3/32 [00:02<00:20,  1.40it/s] 12%|█▎        | 4/32 [00:03<00:15,  1.76it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.05it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.28it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.64it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.70it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.76it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.81it/s] 41%|████      | 13/32 [00:06<00:06,  2.84it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.87it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.86it/s] 50%|█████     | 16/32 [00:07<00:05,  2.87it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.90it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.90it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.93it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.94it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.93it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.93it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.94it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.93it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.92it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.94it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.92it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.91it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.94it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.90it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.90it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
W0316 12:00:15.184000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:00:15.185000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:00:15.185000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:00:15.185000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 12:00:15.185000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:00:15.185000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:00:15.185000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 12:00:15.214000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:00:15.214000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:00:15.214000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:00:15.214000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:00:15.214000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:00:15.508000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:00:15.508000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:00:15.508000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:00:15.508000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:00:15.508000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:00:16.128000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:00:16.129000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 12:00:16.129000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 12:00:16.129000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:00:16.129000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:00:16.129000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:00:16.129000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:00:16.147000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:00:16.147000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:00:16.148000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:00:16.148000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:00:16.148000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:00:16.358000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:00:16.358000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:00:16.358000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:00:16.359000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:00:16.359000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:00:17.534000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:00:17.534000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:00:17.534000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:00:17.534000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:00:17.534000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:00:17.534000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 12:00:17.534000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 12:00:17.552000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:00:17.552000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:00:17.552000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:00:17.552000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:00:17.552000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:00:18.190000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:00:18.190000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:00:18.191000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:00:18.191000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:00:18.191000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 12:00:24.696094 509085 finetune.py:45] layer 29_v initial loss 8.006591815501451e-05
W0316 12:00:24.696385 509085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:00:33.931365 508581 finetune.py:68] layer 28_v @ epoch 1 new loss 1.4540863048750907e-05 old loss 1.5569872630294412e-05 BETTER
I0316 12:00:55.888605 460347 quantize_finetune_llama.py:186] computed original embedding for layer 30 in 62.22558355331421s
I0316 12:00:56.291066 460347 quantize_finetune_llama.py:159] layer 31 gpu 3
I0316 12:00:58.377777 509591 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 12:00:58.377908 509591 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 12:00:58.377973 509591 utils.py:162] NumExpr defaulting to 16 threads.
I0316 12:00:58.592163 509591 config.py:58] PyTorch version 2.4.0 available.
I0316 12:00:59.577349 509085 finetune.py:68] layer 29_v @ epoch 0 new loss 1.976820385607425e-05 old loss 8.006591815501451e-05 BETTER
I0316 12:01:00.795143 509591 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 12:01:01.170117 509591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:58,  1.90s/it]  6%|▋         | 2/32 [00:02<00:29,  1.00it/s]  9%|▉         | 3/32 [00:02<00:20,  1.42it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.77it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.05it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.27it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.63it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.69it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.74it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.77it/s] 41%|████      | 13/32 [00:06<00:06,  2.80it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.81it/s] 50%|█████     | 16/32 [00:07<00:05,  2.81it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.81it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.82it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.82it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.82it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.82it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.83it/s]I0316 12:01:11.634392 508581 finetune.py:68] layer 28_v @ epoch 2 new loss 1.432755288988119e-05 old loss 1.4540863048750907e-05 BETTER
 72%|███████▏  | 23/32 [00:09<00:03,  2.82it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.84it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.84it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.84it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.84it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.84it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.85it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.84it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
W0316 12:01:17.739000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:01:17.739000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:01:17.739000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:01:17.739000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 12:01:17.739000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:01:17.739000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:01:17.739000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 12:01:17.766000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:01:17.766000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:01:17.766000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:01:17.766000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:01:17.766000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.067000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.068000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.068000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.068000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.068000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.697000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.698000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.698000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.698000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.698000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.698000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.698000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.716000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.716000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.716000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.716000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.716000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.925000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.925000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.925000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.925000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:01:18.926000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:01:20.099000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:01:20.099000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:01:20.100000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:01:20.100000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 12:01:20.100000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 12:01:20.100000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:01:20.100000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:01:20.119000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:01:20.119000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:01:20.119000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:01:20.119000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:01:20.119000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:01:20.754000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:01:20.754000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:01:20.754000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:01:20.754000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:01:20.754000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 12:01:27.058008 509591 finetune.py:45] layer 30_v initial loss 0.00010801883036037907
W0316 12:01:27.058406 509591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:01:35.225989 509085 finetune.py:76] layer 29_v @ epoch 1 new loss 2.1293506506481208e-05 old loss 1.976820385607425e-05 WORSE
I0316 12:01:49.632426 508581 finetune.py:76] layer 28_v @ epoch 3 new loss 1.4598215784644708e-05 old loss 1.432755288988119e-05 WORSE
I0316 12:01:57.633756 460347 quantize_finetune_llama.py:186] computed original embedding for layer 31 in 60.86284613609314s
I0316 12:02:00.154799 510097 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 12:02:00.154921 510097 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 12:02:00.154984 510097 utils.py:162] NumExpr defaulting to 16 threads.
I0316 12:02:00.377386 510097 config.py:58] PyTorch version 2.4.0 available.
I0316 12:02:01.810083 509591 finetune.py:68] layer 30_v @ epoch 0 new loss 4.412871930981055e-05 old loss 0.00010801883036037907 BETTER
I0316 12:02:02.818253 510097 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0316 12:02:03.215776 510097 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it]  6%|▋         | 2/32 [00:02<00:29,  1.00it/s]  9%|▉         | 3/32 [00:02<00:20,  1.43it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.78it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.02it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.21it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.36it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.48it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.56it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.64it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.70it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.73it/s] 41%|████      | 13/32 [00:06<00:06,  2.77it/s]I0316 12:02:10.524777 509085 finetune.py:76] layer 29_v @ epoch 2 new loss 2.34221042774152e-05 old loss 1.976820385607425e-05 WORSE
 44%|████▍     | 14/32 [00:06<00:06,  2.79it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.80it/s] 50%|█████     | 16/32 [00:07<00:05,  2.82it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.82it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.82it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.83it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.84it/s] 66%|██████▌   | 21/32 [00:09<00:03,  2.84it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.85it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.86it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.85it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.87it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.85it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.85it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.87it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.86it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
W0316 12:02:19.973000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0316 12:02:19.973000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:02:19.973000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0316 12:02:19.973000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:02:19.973000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:02:19.974000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:02:19.974000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.001000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.001000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.001000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.001000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.001000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.304000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.304000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.304000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.305000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.305000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.944000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.945000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.945000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.945000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.945000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.945000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.945000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.963000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.963000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.963000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.964000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:02:20.964000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:02:21.173000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:02:21.173000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:02:21.173000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:02:21.173000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:02:21.173000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:02:22.370000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:02:22.370000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0316 12:02:22.370000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:02:22.370000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:02:22.370000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0316 12:02:22.371000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:02:22.371000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:02:22.390000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:02:22.390000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:02:22.390000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:02:22.390000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:02:22.390000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:02:23.037000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:02:23.037000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:02:23.037000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:02:23.037000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:02:23.038000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0316 12:02:27.216343 508581 finetune.py:76] layer 28_v @ epoch 4 new loss 1.4747607565368526e-05 old loss 1.432755288988119e-05 WORSE
W0316 12:02:28.470974 508581 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

28_v proxy err 0.004000305663794279 tr(WHW.T) 175.40756225585938
  0%|          | 0/32 [00:00<?, ?it/s]I0316 12:02:29.757056 510097 finetune.py:45] layer 31_v initial loss 0.00014017797366250306
W0316 12:02:29.757374 510097 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:41,  1.32s/it]  6%|▋         | 2/32 [00:01<00:23,  1.30it/s]  9%|▉         | 3/32 [00:02<00:17,  1.69it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.97it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.17it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.31it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.41it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.46it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.58it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.62it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.63it/s] 50%|█████     | 16/32 [00:07<00:06,  2.64it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.63it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.61it/s]I0316 12:02:38.099811 509591 finetune.py:76] layer 30_v @ epoch 1 new loss 4.517907291301526e-05 old loss 4.412871930981055e-05 WORSE
 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.64it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.64it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.64it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.64it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:13<00:00,  2.63it/s]100%|██████████| 32/32 [00:13<00:00,  2.44it/s]
I0316 12:02:46.257398 509085 finetune.py:76] layer 29_v @ epoch 3 new loss 2.04831321752863e-05 old loss 1.976820385607425e-05 WORSE
W0316 12:02:48.918000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:02:48.918000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:02:48.919000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:02:48.919000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:02:48.919000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 12:02:48.919000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:02:48.919000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 12:02:48.950000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:02:48.950000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:02:48.950000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:02:48.950000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:02:48.950000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.114000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.114000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.114000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.114000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.114000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.341000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.341000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.341000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.341000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.341000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.341000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.341000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.362000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.362000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.362000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.362000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.362000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.428000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.428000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.428000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.428000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:02:49.428000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:02:50.495000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 12:02:50.827000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:02:50.827000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 12:02:50.827000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 12:02:50.827000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:02:50.827000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:02:50.827000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:02:50.827000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:02:50.850000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:02:50.850000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:02:50.850000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:02:50.850000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:02:50.851000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:02:51.109000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:02:51.109000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:02:51.109000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:02:51.109000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:02:51.109000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:02:51.589000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 12:02:58.783606 508581 finetune.py:45] layer 28_q initial loss 1.9753386368392967e-05
W0316 12:02:58.784103 508581 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:03:04.059911 510097 finetune.py:68] layer 31_v @ epoch 0 new loss 7.697912951698527e-05 old loss 0.00014017797366250306 BETTER
I0316 12:03:13.913849 509591 finetune.py:76] layer 30_v @ epoch 2 new loss 5.257105658529326e-05 old loss 4.412871930981055e-05 WORSE
I0316 12:03:21.938552 509085 finetune.py:68] layer 29_v @ epoch 4 new loss 1.942408925970085e-05 old loss 1.976820385607425e-05 BETTER
W0316 12:03:23.618918 509085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

29_v proxy err 0.003329054219648242 tr(WHW.T) 249.68582153320312
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.33s/it]  6%|▋         | 2/32 [00:01<00:23,  1.29it/s]  9%|▉         | 3/32 [00:02<00:17,  1.67it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.95it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.16it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.30it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.40it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.46it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.51it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.58it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.58it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.59it/s] 50%|█████     | 16/32 [00:07<00:06,  2.60it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.61it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.61it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.61it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.62it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.61it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.61it/s]I0316 12:03:35.470271 508581 finetune.py:68] layer 28_q @ epoch 0 new loss 1.8896113033406436e-05 old loss 1.9753386368392967e-05 BETTER
 81%|████████▏ | 26/32 [00:10<00:02,  2.60it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.62it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.62it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:13<00:00,  2.64it/s]100%|██████████| 32/32 [00:13<00:00,  2.43it/s]
I0316 12:03:39.158154 510097 finetune.py:76] layer 31_v @ epoch 1 new loss 0.00011838261707453057 old loss 7.697912951698527e-05 WORSE
W0316 12:03:43.830000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:03:43.830000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 12:03:43.830000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:03:43.830000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 12:03:43.830000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:03:43.830000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:03:43.830000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:03:43.859000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:03:43.860000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:03:43.860000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:03:43.860000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:03:43.860000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.023000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.023000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.023000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.023000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.024000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.250000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.250000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.250000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.250000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.250000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.250000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.250000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.270000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.271000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.271000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.271000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.271000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.336000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.336000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.336000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.336000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:03:44.336000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:03:45.451000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 12:03:45.789000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:03:45.789000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:03:45.789000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:03:45.789000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:03:45.789000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 12:03:45.789000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 12:03:45.789000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:03:45.816000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:03:45.817000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:03:45.817000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:03:45.817000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:03:45.817000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:03:46.079000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:03:46.079000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:03:46.079000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:03:46.079000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:03:46.079000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:03:46.564000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 12:03:49.359570 509591 finetune.py:76] layer 30_v @ epoch 3 new loss 4.7609304601792246e-05 old loss 4.412871930981055e-05 WORSE
I0316 12:03:53.660710 509085 finetune.py:45] layer 29_q initial loss 3.4570828574942425e-05
W0316 12:03:53.661047 509085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:04:13.411979 508581 finetune.py:68] layer 28_q @ epoch 1 new loss 1.833351416280493e-05 old loss 1.8896113033406436e-05 BETTER
I0316 12:04:14.381657 510097 finetune.py:76] layer 31_v @ epoch 2 new loss 0.00011192486272193491 old loss 7.697912951698527e-05 WORSE
I0316 12:04:25.164263 509591 finetune.py:76] layer 30_v @ epoch 4 new loss 4.73510917800013e-05 old loss 4.412871930981055e-05 WORSE
W0316 12:04:26.327397 509591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

30_v proxy err 0.0041311089880764484 tr(WHW.T) 252.81201171875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:43,  1.40s/it]I0316 12:04:29.185309 509085 finetune.py:68] layer 29_q @ epoch 0 new loss 3.1336549000116065e-05 old loss 3.4570828574942425e-05 BETTER
  6%|▋         | 2/32 [00:01<00:24,  1.24it/s]  9%|▉         | 3/32 [00:02<00:17,  1.61it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.88it/s] 16%|█▌        | 5/32 [00:02<00:13,  2.07it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.20it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.30it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.36it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.40it/s] 31%|███▏      | 10/32 [00:04<00:09,  2.42it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.44it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.47it/s] 41%|████      | 13/32 [00:06<00:07,  2.49it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.50it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.51it/s] 50%|█████     | 16/32 [00:07<00:06,  2.52it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.53it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.53it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.53it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.53it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.51it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.52it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.51it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.52it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.53it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.53it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.54it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.54it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.54it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.54it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.53it/s]100%|██████████| 32/32 [00:13<00:00,  2.53it/s]100%|██████████| 32/32 [00:13<00:00,  2.34it/s]
W0316 12:04:47.109000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.109000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.109000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.109000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.110000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.110000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.110000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.140000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.140000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.140000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.140000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.140000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.307000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.307000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.307000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.307000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.307000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.535000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.535000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.535000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.535000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.535000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.535000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.536000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.558000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.558000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.558000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.558000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.558000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.624000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.624000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.624000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.624000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:04:47.624000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:04:48.701000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 12:04:49.026000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:04:49.026000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 12:04:49.027000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 12:04:49.027000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:04:49.027000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:04:49.027000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:04:49.027000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:04:49.049000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:04:49.049000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:04:49.050000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:04:49.050000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:04:49.050000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:04:49.307000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:04:49.307000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:04:49.307000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:04:49.307000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:04:49.307000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
I0316 12:04:49.680385 510097 finetune.py:76] layer 31_v @ epoch 3 new loss 0.000143607787322253 old loss 7.697912951698527e-05 WORSE
W0316 12:04:49.779000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 12:04:52.065771 508581 finetune.py:76] layer 28_q @ epoch 2 new loss 1.849289947131183e-05 old loss 1.833351416280493e-05 WORSE
I0316 12:04:56.560642 509591 finetune.py:45] layer 30_q initial loss 5.241567487246357e-05
W0316 12:04:56.560999 509591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:05:05.639812 509085 finetune.py:68] layer 29_q @ epoch 1 new loss 3.0193339625839144e-05 old loss 3.1336549000116065e-05 BETTER
I0316 12:05:24.904336 510097 finetune.py:76] layer 31_v @ epoch 4 new loss 0.00014318354078568518 old loss 7.697912951698527e-05 WORSE
W0316 12:05:26.080259 510097 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

31_v proxy err 0.0016904684016481042 tr(WHW.T) 365.68487548828125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.36s/it]  6%|▋         | 2/32 [00:01<00:23,  1.28it/s]  9%|▉         | 3/32 [00:02<00:17,  1.67it/s]I0316 12:05:29.638607 508581 finetune.py:76] layer 28_q @ epoch 3 new loss 1.842275378294289e-05 old loss 1.833351416280493e-05 WORSE
 12%|█▎        | 4/32 [00:02<00:14,  1.94it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.13it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.27it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.37it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.43it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.48it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.50it/s]I0316 12:05:32.171606 509591 finetune.py:68] layer 30_q @ epoch 0 new loss 4.9332033086102456e-05 old loss 5.241567487246357e-05 BETTER
 34%|███▍      | 11/32 [00:05<00:08,  2.53it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:05<00:07,  2.56it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.58it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:07<00:06,  2.61it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.61it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.61it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.61it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.61it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.56it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.58it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.59it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.59it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.59it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:13<00:00,  2.58it/s]100%|██████████| 32/32 [00:13<00:00,  2.40it/s]
I0316 12:05:42.462708 509085 finetune.py:68] layer 29_q @ epoch 2 new loss 3.018868483195547e-05 old loss 3.0193339625839144e-05 BETTER
W0316 12:05:46.535000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.536000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.536000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.536000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.536000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.536000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.536000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.567000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.567000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.568000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.568000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.568000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.734000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.734000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.734000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.734000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.734000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.959000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.959000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.959000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.959000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.959000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.959000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.959000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.981000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.981000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.981000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.981000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:05:46.981000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:05:47.044000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:05:47.044000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:05:47.045000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:05:47.045000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:05:47.045000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:05:48.087000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0316 12:05:48.404000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:05:48.404000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0316 12:05:48.404000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:05:48.404000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:05:48.404000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0316 12:05:48.405000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:05:48.405000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:05:48.426000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:05:48.426000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:05:48.426000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:05:48.427000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:05:48.427000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:05:48.672000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:05:48.673000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:05:48.673000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:05:48.673000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:05:48.673000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:05:49.134000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0316 12:05:55.717031 510097 finetune.py:45] layer 31_q initial loss 0.0001064251919160597
W0316 12:05:55.717501 510097 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:06:07.387104 508581 finetune.py:76] layer 28_q @ epoch 4 new loss 1.8345119315199554e-05 old loss 1.833351416280493e-05 WORSE
W0316 12:06:08.706108 508581 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 12:06:09.105925 509591 finetune.py:76] layer 30_q @ epoch 1 new loss 5.410247467807494e-05 old loss 4.9332033086102456e-05 WORSE
28_q proxy err 0.0005542786093428731 tr(WHW.T) 6756.87109375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:38,  1.24s/it]  6%|▋         | 2/32 [00:01<00:21,  1.37it/s]  9%|▉         | 3/32 [00:01<00:16,  1.76it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.04it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.25it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.58it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.63it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.64it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.67it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.67it/s] 50%|█████     | 16/32 [00:06<00:05,  2.67it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.67it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.66it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.67it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.68it/s]I0316 12:06:19.080978 509085 finetune.py:68] layer 29_q @ epoch 3 new loss 2.986067374877166e-05 old loss 3.018868483195547e-05 BETTER
 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.69it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.69it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.68it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.69it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.67it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
I0316 12:06:30.089719 508581 finetune.py:45] layer 28_k initial loss 2.2996340703684837e-05
W0316 12:06:30.090301 508581 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:06:31.022945 510097 finetune.py:68] layer 31_q @ epoch 0 new loss 0.00010179451783187687 old loss 0.0001064251919160597 BETTER
I0316 12:06:45.318092 509591 finetune.py:68] layer 30_q @ epoch 2 new loss 4.871842611464672e-05 old loss 4.9332033086102456e-05 BETTER
I0316 12:06:55.708613 509085 finetune.py:68] layer 29_q @ epoch 4 new loss 2.880615102185402e-05 old loss 2.986067374877166e-05 BETTER
W0316 12:06:57.288644 509085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

29_q proxy err 0.0006651452858932316 tr(WHW.T) 6065.3994140625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.26s/it]  6%|▋         | 2/32 [00:01<00:22,  1.34it/s]  9%|▉         | 3/32 [00:02<00:16,  1.73it/s] 12%|█▎        | 4/32 [00:02<00:14,  2.00it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.18it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.42it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.53it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.58it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.63it/s] 50%|█████     | 16/32 [00:06<00:06,  2.63it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.63it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.61it/s]I0316 12:07:06.665679 510097 finetune.py:76] layer 31_q @ epoch 1 new loss 0.00013144759577699006 old loss 0.00010179451783187687 WORSE
 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s]I0316 12:07:06.867787 508581 finetune.py:68] layer 28_k @ epoch 0 new loss 2.240373578388244e-05 old loss 2.2996340703684837e-05 BETTER
 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.66it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.66it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.65it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.64it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.64it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:13<00:00,  2.63it/s]100%|██████████| 32/32 [00:13<00:00,  2.46it/s]
I0316 12:07:18.324963 509085 finetune.py:45] layer 29_k initial loss 3.6283337976783514e-05
W0316 12:07:18.325339 509085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:07:21.660110 509591 finetune.py:76] layer 30_q @ epoch 3 new loss 5.446092109195888e-05 old loss 4.871842611464672e-05 WORSE
I0316 12:07:42.407000 510097 finetune.py:76] layer 31_q @ epoch 2 new loss 0.00014514880604110658 old loss 0.00010179451783187687 WORSE
I0316 12:07:45.482780 508581 finetune.py:76] layer 28_k @ epoch 1 new loss 2.2456239094026387e-05 old loss 2.240373578388244e-05 WORSE
I0316 12:07:54.466477 509085 finetune.py:68] layer 29_k @ epoch 0 new loss 3.554809154593386e-05 old loss 3.6283337976783514e-05 BETTER
I0316 12:07:57.646926 509591 finetune.py:76] layer 30_q @ epoch 4 new loss 5.0052629376295954e-05 old loss 4.871842611464672e-05 WORSE
W0316 12:07:58.778486 509591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

30_q proxy err 0.000455778616014868 tr(WHW.T) 7046.23779296875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.26s/it]  6%|▋         | 2/32 [00:01<00:22,  1.34it/s]  9%|▉         | 3/32 [00:02<00:16,  1.72it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.97it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.16it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.29it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.39it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.45it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.50it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.52it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.55it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.57it/s] 41%|████      | 13/32 [00:05<00:07,  2.58it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.59it/s] 50%|█████     | 16/32 [00:07<00:06,  2.58it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.61it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.63it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.63it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.59it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:13<00:00,  2.54it/s]100%|██████████| 32/32 [00:13<00:00,  2.43it/s]
I0316 12:08:17.480955 510097 finetune.py:76] layer 31_q @ epoch 3 new loss 0.0001441949134459719 old loss 0.00010179451783187687 WORSE
I0316 12:08:20.640900 509591 finetune.py:45] layer 30_k initial loss 5.745238740928471e-05
W0316 12:08:20.641739 509591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:08:23.798541 508581 finetune.py:68] layer 28_k @ epoch 2 new loss 2.2278893084148876e-05 old loss 2.240373578388244e-05 BETTER
I0316 12:08:30.796949 509085 finetune.py:68] layer 29_k @ epoch 1 new loss 3.5303768527228385e-05 old loss 3.554809154593386e-05 BETTER
I0316 12:08:53.072803 510097 finetune.py:76] layer 31_q @ epoch 4 new loss 0.0001099828805308789 old loss 0.00010179451783187687 WORSE
W0316 12:08:54.389509 510097 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

31_q proxy err 0.0003095381543971598 tr(WHW.T) 9336.708984375
  0%|          | 0/32 [00:00<?, ?it/s]I0316 12:08:56.563433 509591 finetune.py:68] layer 30_k @ epoch 0 new loss 5.711439735023305e-05 old loss 5.745238740928471e-05 BETTER
  3%|▎         | 1/32 [00:01<00:41,  1.32s/it]  6%|▋         | 2/32 [00:01<00:23,  1.30it/s]  9%|▉         | 3/32 [00:02<00:17,  1.69it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.97it/s] 16%|█▌        | 5/32 [00:03<00:14,  1.90it/s] 19%|█▉        | 6/32 [00:03<00:12,  2.10it/s] 22%|██▏       | 7/32 [00:03<00:11,  2.25it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.36it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.44it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.49it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.51it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:06<00:07,  2.56it/s]I0316 12:09:02.138874 508581 finetune.py:76] layer 28_k @ epoch 3 new loss 2.247303018521052e-05 old loss 2.2278893084148876e-05 WORSE
 44%|████▍     | 14/32 [00:06<00:06,  2.58it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:07<00:06,  2.61it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.64it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.64it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.63it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.62it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.62it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.60it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.62it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s]I0316 12:09:07.546369 509085 finetune.py:68] layer 29_k @ epoch 2 new loss 3.5058805224252865e-05 old loss 3.5303768527228385e-05 BETTER
 91%|█████████ | 29/32 [00:12<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.65it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:13<00:00,  2.64it/s]100%|██████████| 32/32 [00:13<00:00,  2.41it/s]
I0316 12:09:16.025982 510097 finetune.py:45] layer 31_k initial loss 0.0001419471955159679
W0316 12:09:16.026332 510097 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:09:33.030650 509591 finetune.py:68] layer 30_k @ epoch 1 new loss 5.650567982229404e-05 old loss 5.711439735023305e-05 BETTER
I0316 12:09:39.968629 508581 finetune.py:76] layer 28_k @ epoch 4 new loss 2.2296395400189795e-05 old loss 2.2278893084148876e-05 WORSE
W0316 12:09:41.213288 508581 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

28_k proxy err 0.0004565189010463655 tr(WHW.T) 4372.890625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.25it/s]  6%|▋         | 2/32 [00:01<00:16,  1.81it/s]I0316 12:09:43.942656 509085 finetune.py:68] layer 29_k @ epoch 3 new loss 3.457817365415394e-05 old loss 3.5058805224252865e-05 BETTER
  9%|▉         | 3/32 [00:01<00:13,  2.11it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.29it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.55it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.63it/s] 50%|█████     | 16/32 [00:06<00:06,  2.64it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.63it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.61it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s]I0316 12:09:50.914601 510097 finetune.py:76] layer 31_k @ epoch 0 new loss 0.000142108227009885 old loss 0.0001419471955159679 WORSE
 66%|██████▌   | 21/32 [00:08<00:04,  2.63it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.63it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.65it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.65it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.66it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
I0316 12:10:02.751166 508581 finetune.py:45] layer 28_o initial loss 3.834383096545935e-05
W0316 12:10:02.751528 508581 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:10:09.706541 509591 finetune.py:76] layer 30_k @ epoch 2 new loss 5.8078960137208924e-05 old loss 5.650567982229404e-05 WORSE
I0316 12:10:20.677254 509085 finetune.py:68] layer 29_k @ epoch 4 new loss 3.382578870514408e-05 old loss 3.457817365415394e-05 BETTER
W0316 12:10:22.293074 509085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

29_k proxy err 0.0004950394504703581 tr(WHW.T) 4802.4140625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.20it/s]  6%|▋         | 2/32 [00:01<00:17,  1.76it/s]  9%|▉         | 3/32 [00:01<00:14,  2.07it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s]I0316 12:10:25.876154 510097 finetune.py:68] layer 31_k @ epoch 1 new loss 0.00013818146544508636 old loss 0.0001419471955159679 BETTER
 19%|█▉        | 6/32 [00:02<00:10,  2.47it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.54it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.57it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s] 41%|████      | 13/32 [00:05<00:07,  2.58it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.61it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.61it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.63it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.63it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.63it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.62it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.63it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
I0316 12:10:39.744461 508581 finetune.py:68] layer 28_o @ epoch 0 new loss 3.676847700262442e-05 old loss 3.834383096545935e-05 BETTER
I0316 12:10:43.812773 509085 finetune.py:45] layer 29_o initial loss 4.898770566796884e-05
W0316 12:10:43.813575 509085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:10:45.527682 509591 finetune.py:68] layer 30_k @ epoch 3 new loss 5.3653107897844166e-05 old loss 5.650567982229404e-05 BETTER
I0316 12:11:01.814944 510097 finetune.py:76] layer 31_k @ epoch 2 new loss 0.00017334824951831251 old loss 0.00013818146544508636 WORSE
I0316 12:11:18.154202 508581 finetune.py:68] layer 28_o @ epoch 1 new loss 3.635890607256442e-05 old loss 3.676847700262442e-05 BETTER
I0316 12:11:19.598470 509085 finetune.py:68] layer 29_o @ epoch 0 new loss 4.735306720249355e-05 old loss 4.898770566796884e-05 BETTER
I0316 12:11:22.483305 509591 finetune.py:68] layer 30_k @ epoch 4 new loss 5.360178693081252e-05 old loss 5.3653107897844166e-05 BETTER
W0316 12:11:24.146492 509591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

30_k proxy err 0.00039134264807216823 tr(WHW.T) 4108.470703125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.20it/s]  6%|▋         | 2/32 [00:01<00:17,  1.76it/s]  9%|▉         | 3/32 [00:01<00:14,  2.07it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.26it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.52it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.54it/s] 41%|████      | 13/32 [00:05<00:07,  2.53it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.54it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.55it/s] 50%|█████     | 16/32 [00:06<00:06,  2.56it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.56it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.57it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.57it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.58it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.58it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.57it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.55it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.55it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.55it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.56it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.56it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.57it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.57it/s]I0316 12:11:37.365733 510097 finetune.py:68] layer 31_k @ epoch 3 new loss 0.00013181610847823322 old loss 0.00013818146544508636 BETTER
 94%|█████████▍| 30/32 [00:12<00:00,  2.58it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
I0316 12:11:45.597122 509591 finetune.py:45] layer 30_o initial loss 8.545843593310565e-05
W0316 12:11:45.597483 509591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:11:55.894588 508581 finetune.py:68] layer 28_o @ epoch 2 new loss 3.601058779167943e-05 old loss 3.635890607256442e-05 BETTER
I0316 12:11:56.067335 509085 finetune.py:68] layer 29_o @ epoch 1 new loss 4.6650126023450866e-05 old loss 4.735306720249355e-05 BETTER
I0316 12:12:13.387805 510097 finetune.py:76] layer 31_k @ epoch 4 new loss 0.0001468904665671289 old loss 0.00013181610847823322 WORSE
W0316 12:12:14.552689 510097 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

31_k proxy err 0.00034169130958616734 tr(WHW.T) 4138.2861328125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.21it/s]  6%|▋         | 2/32 [00:01<00:17,  1.76it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.24it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.54it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.56it/s] 41%|████      | 13/32 [00:05<00:07,  2.55it/s]I0316 12:12:21.644102 509591 finetune.py:68] layer 30_o @ epoch 0 new loss 8.284908108180389e-05 old loss 8.545843593310565e-05 BETTER
 44%|████▍     | 14/32 [00:05<00:07,  2.56it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:06<00:06,  2.58it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.58it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.56it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.56it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.57it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.58it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.59it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.59it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
I0316 12:12:32.738681 509085 finetune.py:76] layer 29_o @ epoch 2 new loss 4.709357745014131e-05 old loss 4.6650126023450866e-05 WORSE
I0316 12:12:33.776417 508581 finetune.py:76] layer 28_o @ epoch 3 new loss 3.617219044826925e-05 old loss 3.601058779167943e-05 WORSE
I0316 12:12:36.257176 510097 finetune.py:45] layer 31_o initial loss 0.0001820406032493338
W0316 12:12:36.257595 510097 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:12:58.212522 509591 finetune.py:68] layer 30_o @ epoch 1 new loss 8.213615365093574e-05 old loss 8.284908108180389e-05 BETTER
I0316 12:13:08.620885 509085 finetune.py:76] layer 29_o @ epoch 3 new loss 4.672116119763814e-05 old loss 4.6650126023450866e-05 WORSE
I0316 12:13:11.166539 508581 finetune.py:76] layer 28_o @ epoch 4 new loss 3.628926060628146e-05 old loss 3.601058779167943e-05 WORSE
I0316 12:13:11.473431 510097 finetune.py:68] layer 31_o @ epoch 0 new loss 0.0001514895266154781 old loss 0.0001820406032493338 BETTER
W0316 12:13:12.290522 508581 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

28_o proxy err 0.0022675027139484882 tr(WHW.T) 24.530946731567383
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.94s/it]  6%|▋         | 2/32 [00:03<00:48,  1.62s/it]  9%|▉         | 3/32 [00:04<00:44,  1.53s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.45s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.43s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.42s/it] 25%|██▌       | 8/32 [00:11<00:33,  1.41s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.40s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.40s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.39s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it] 41%|████      | 13/32 [00:18<00:27,  1.45s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it]I0316 12:13:34.753981 509591 finetune.py:76] layer 30_o @ epoch 2 new loss 8.297850581584498e-05 old loss 8.213615365093574e-05 WORSE
 47%|████▋     | 15/32 [00:21<00:24,  1.45s/it] 50%|█████     | 16/32 [00:23<00:22,  1.43s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.40s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.40s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.39s/it]I0316 12:13:44.289756 509085 finetune.py:68] layer 29_o @ epoch 4 new loss 4.662719220505096e-05 old loss 4.6650126023450866e-05 BETTER
 69%|██████▉   | 22/32 [00:31<00:13,  1.39s/it]W0316 12:13:45.854230 509085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:32<00:12,  1.39s/it]29_o proxy err 0.001405019429512322 tr(WHW.T) 36.708797454833984
  0%|          | 0/32 [00:00<?, ?it/s]I0316 12:13:47.241935 510097 finetune.py:68] layer 31_o @ epoch 1 new loss 0.0001395459403283894 old loss 0.0001514895266154781 BETTER
 75%|███████▌  | 24/32 [00:34<00:11,  1.39s/it]  3%|▎         | 1/32 [00:02<01:02,  2.01s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.39s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.40s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.41s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.41s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.43s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.45s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.52s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.48s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.50s/it]100%|██████████| 32/32 [00:45<00:00,  1.49s/it]100%|██████████| 32/32 [00:45<00:00,  1.44s/it]
 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.50s/it]I0316 12:14:09.721993 509591 finetune.py:76] layer 30_o @ epoch 3 new loss 8.402623643632978e-05 old loss 8.213615365093574e-05 WORSE
 47%|████▋     | 15/32 [00:22<00:25,  1.50s/it]I0316 12:14:10.328246 508581 finetune.py:45] layer 28_up initial loss 8.625173359178007e-05
W0316 12:14:10.328663 508581 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.50s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.50s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.50s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it]I0316 12:14:23.048933 510097 finetune.py:76] layer 31_o @ epoch 2 new loss 0.0001474950258852914 old loss 0.0001395459403283894 WORSE
 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.50s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.49s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.50s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
I0316 12:14:44.009492 509085 finetune.py:45] layer 29_up initial loss 0.00011852540046675131
W0316 12:14:44.009809 509085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:14:45.686632 509591 finetune.py:68] layer 30_o @ epoch 4 new loss 8.200809679692611e-05 old loss 8.213615365093574e-05 BETTER
I0316 12:14:46.392929 508581 finetune.py:68] layer 28_up @ epoch 0 new loss 8.453697228105739e-05 old loss 8.625173359178007e-05 BETTER
W0316 12:14:47.347640 509591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

30_o proxy err 0.001266289735212922 tr(WHW.T) 84.19990539550781
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:02,  2.01s/it]  6%|▋         | 2/32 [00:03<00:51,  1.73s/it]  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it]I0316 12:14:58.524288 510097 finetune.py:76] layer 31_o @ epoch 3 new loss 0.00015372440975625068 old loss 0.0001395459403283894 WORSE
 22%|██▏       | 7/32 [00:11<00:37,  1.52s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.51s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 41%|████      | 13/32 [00:20<00:28,  1.50s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 50%|█████     | 16/32 [00:24<00:24,  1.50s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it]I0316 12:15:18.037476 509085 finetune.py:68] layer 29_up @ epoch 0 new loss 0.00011542446009116247 old loss 0.00011852540046675131 BETTER
 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.50s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.50s/it]I0316 12:15:23.057038 508581 finetune.py:68] layer 28_up @ epoch 1 new loss 8.357025944860652e-05 old loss 8.453697228105739e-05 BETTER
 72%|███████▏  | 23/32 [00:35<00:13,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.50s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.50s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.50s/it]I0316 12:15:33.959706 510097 finetune.py:76] layer 31_o @ epoch 4 new loss 0.0001567529106978327 old loss 0.0001395459403283894 WORSE
 94%|█████████▍| 30/32 [00:45<00:03,  1.50s/it]W0316 12:15:34.976578 510097 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:47<00:01,  1.50s/it]31_o proxy err 0.0008354168967343867 tr(WHW.T) 182.29603576660156
  0%|          | 0/32 [00:00<?, ?it/s]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
  3%|▎         | 1/32 [00:01<01:00,  1.96s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it]  9%|▉         | 3/32 [00:04<00:45,  1.58s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.53s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.51s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.50s/it]I0316 12:15:45.479273 509591 finetune.py:45] layer 30_up initial loss 0.0002292550343554467
W0316 12:15:45.479542 509591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.48s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.48s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it]I0316 12:15:52.529880 509085 finetune.py:68] layer 29_up @ epoch 1 new loss 0.00011378047929611057 old loss 0.00011542446009116247 BETTER
 34%|███▍      | 11/32 [00:16<00:30,  1.47s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.47s/it] 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.50s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.51s/it]I0316 12:15:59.796716 508581 finetune.py:68] layer 28_up @ epoch 2 new loss 8.279980829684064e-05 old loss 8.357025944860652e-05 BETTER
 50%|█████     | 16/32 [00:24<00:23,  1.50s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.47s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.47s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.48s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it]I0316 12:16:19.901367 509591 finetune.py:68] layer 30_up @ epoch 0 new loss 0.0002211482496932149 old loss 0.0002292550343554467 BETTER
 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
I0316 12:16:27.698175 509085 finetune.py:68] layer 29_up @ epoch 2 new loss 0.00011252205149503425 old loss 0.00011378047929611057 BETTER
I0316 12:16:31.958439 510097 finetune.py:45] layer 31_up initial loss 0.000775457825511694
W0316 12:16:31.958820 510097 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:16:36.642812 508581 finetune.py:68] layer 28_up @ epoch 3 new loss 8.219147275667638e-05 old loss 8.279980829684064e-05 BETTER
I0316 12:16:54.847645 509591 finetune.py:68] layer 30_up @ epoch 1 new loss 0.00021658687910530716 old loss 0.0002211482496932149 BETTER
I0316 12:17:02.496008 509085 finetune.py:68] layer 29_up @ epoch 3 new loss 0.00011162903683725744 old loss 0.00011252205149503425 BETTER
I0316 12:17:05.450672 510097 finetune.py:68] layer 31_up @ epoch 0 new loss 0.0007231237832456827 old loss 0.000775457825511694 BETTER
I0316 12:17:13.149226 508581 finetune.py:68] layer 28_up @ epoch 4 new loss 8.164954488165677e-05 old loss 8.219147275667638e-05 BETTER
W0316 12:17:14.518568 508581 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

28_up proxy err 0.0029460247606039047 tr(WHW.T) 2478.915771484375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:57,  1.87s/it]  6%|▋         | 2/32 [00:03<00:48,  1.62s/it]  9%|▉         | 3/32 [00:04<00:44,  1.54s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.50s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.45s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.44s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it]I0316 12:17:29.809220 509591 finetune.py:68] layer 30_up @ epoch 2 new loss 0.00021296531485859305 old loss 0.00021658687910530716 BETTER
 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.43s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it] 41%|████      | 13/32 [00:19<00:27,  1.43s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.44s/it]I0316 12:17:37.238874 509085 finetune.py:68] layer 29_up @ epoch 4 new loss 0.00011081029515480623 old loss 0.00011162903683725744 BETTER
 47%|████▋     | 15/32 [00:21<00:24,  1.45s/it]W0316 12:17:38.539208 509085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 50%|█████     | 16/32 [00:23<00:23,  1.47s/it]29_up proxy err 0.0024105254560709 tr(WHW.T) 3245.844482421875
  0%|          | 0/32 [00:00<?, ?it/s]I0316 12:17:39.868265 510097 finetune.py:68] layer 31_up @ epoch 1 new loss 0.000688579399138689 old loss 0.0007231237832456827 BETTER
 53%|█████▎    | 17/32 [00:24<00:21,  1.46s/it]  3%|▎         | 1/32 [00:01<01:00,  1.94s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.44s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.44s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.57s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.44s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.55s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.44s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.44s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.44s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.44s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.44s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.51s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.44s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.44s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.45s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.44s/it] 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.44s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]
 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it]I0316 12:18:04.591882 509591 finetune.py:68] layer 30_up @ epoch 3 new loss 0.00021019275300204754 old loss 0.00021296531485859305 BETTER
 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.50s/it]I0316 12:18:10.572009 508581 finetune.py:45] layer 28_gate initial loss 0.00012377816892694682
W0316 12:18:10.572387 508581 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 66%|██████▌   | 21/32 [00:32<00:16,  1.50s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.50s/it]I0316 12:18:14.159335 510097 finetune.py:68] layer 31_up @ epoch 2 new loss 0.000662457721773535 old loss 0.000688579399138689 BETTER
 72%|███████▏  | 23/32 [00:35<00:13,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.50s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.50s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.50s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.50s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.50s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
I0316 12:18:36.777967 509085 finetune.py:45] layer 29_gate initial loss 0.00016602806863375008
W0316 12:18:36.778329 509085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:18:39.549996 509591 finetune.py:68] layer 30_up @ epoch 4 new loss 0.0002077276585623622 old loss 0.00021019275300204754 BETTER
W0316 12:18:40.864222 509591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

30_up proxy err 0.0014844384277239442 tr(WHW.T) 5505.69921875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it]I0316 12:18:45.228148 508581 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.0001227385364472866 old loss 0.00012377816892694682 BETTER
  6%|▋         | 2/32 [00:03<00:50,  1.68s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it]I0316 12:18:48.525458 510097 finetune.py:68] layer 31_up @ epoch 3 new loss 0.0006427193875424564 old loss 0.000662457721773535 BETTER
 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it] 22%|██▏       | 7/32 [00:10<00:38,  1.52s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.51s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.51s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it]I0316 12:19:09.312196 509085 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.0001646609598537907 old loss 0.00016602806863375008 BETTER
 56%|█████▋    | 18/32 [00:27<00:21,  1.52s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.50s/it]I0316 12:19:20.018673 508581 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.0001222125138156116 old loss 0.0001227385364472866 BETTER
 78%|███████▊  | 25/32 [00:38<00:10,  1.50s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.50s/it]I0316 12:19:22.430329 510097 finetune.py:68] layer 31_up @ epoch 4 new loss 0.0006262867245823145 old loss 0.0006427193875424564 BETTER
 84%|████████▍ | 27/32 [00:41<00:07,  1.50s/it]W0316 12:19:23.844264 510097 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 88%|████████▊ | 28/32 [00:42<00:05,  1.50s/it]31_up proxy err 0.0005860932869836688 tr(WHW.T) 12264.05078125
  0%|          | 0/32 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:44<00:04,  1.50s/it]  3%|▎         | 1/32 [00:01<00:58,  1.89s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.50s/it]  6%|▋         | 2/32 [00:03<00:49,  1.64s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.50s/it]  9%|▉         | 3/32 [00:04<00:45,  1.57s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
 12%|█▎        | 4/32 [00:06<00:42,  1.53s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.51s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.50s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.48s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.48s/it]I0316 12:19:39.201113 509591 finetune.py:45] layer 30_gate initial loss 0.00029194646049290895
W0316 12:19:39.201535 509591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:15<00:32,  1.48s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.48s/it]I0316 12:19:42.828898 509085 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.00016392499674111605 old loss 0.0001646609598537907 BETTER
 38%|███▊      | 12/32 [00:18<00:29,  1.47s/it] 41%|████      | 13/32 [00:19<00:28,  1.47s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.47s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.47s/it] 50%|█████     | 16/32 [00:23<00:23,  1.48s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.51s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it]I0316 12:19:55.258634 508581 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.00012179210898466408 old loss 0.0001222125138156116 BETTER
 66%|██████▌   | 21/32 [00:31<00:16,  1.50s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.48s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.48s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.48s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.48s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it]I0316 12:20:12.344501 509591 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.000288386014290154 old loss 0.00029194646049290895 BETTER
100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
I0316 12:20:16.439313 509085 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.00016332988161593676 old loss 0.00016392499674111605 BETTER
I0316 12:20:20.845100 510097 finetune.py:45] layer 31_gate initial loss 0.0008181582670658827
W0316 12:20:20.845573 510097 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:20:30.332744 508581 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.00012144555512350053 old loss 0.00012179210898466408 BETTER
I0316 12:20:45.215997 509591 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.00028625395498238504 old loss 0.000288386014290154 BETTER
I0316 12:20:49.362401 509085 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.0001628434838494286 old loss 0.00016332988161593676 BETTER
I0316 12:20:52.993610 510097 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.0007935284520499408 old loss 0.0008181582670658827 BETTER
I0316 12:21:05.398131 508581 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.0001211525741382502 old loss 0.00012144555512350053 BETTER
W0316 12:21:06.519742 508581 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

28_gate proxy err 0.001435043290257454 tr(WHW.T) 8695.0234375
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:34,  1.18it/s]  2%|▏         | 2/112 [00:01<01:02,  1.76it/s]  3%|▎         | 3/112 [00:01<00:52,  2.08it/s]  4%|▎         | 4/112 [00:01<00:47,  2.30it/s]  4%|▍         | 5/112 [00:02<00:44,  2.43it/s]  5%|▌         | 6/112 [00:02<00:42,  2.52it/s]  6%|▋         | 7/112 [00:03<00:40,  2.59it/s]  7%|▋         | 8/112 [00:03<00:39,  2.63it/s]  8%|▊         | 9/112 [00:03<00:38,  2.66it/s]  9%|▉         | 10/112 [00:04<00:38,  2.68it/s] 10%|▉         | 11/112 [00:04<00:37,  2.70it/s] 11%|█         | 12/112 [00:04<00:36,  2.72it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.73it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.74it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.76it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.75it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.74it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.73it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.72it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.69it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.71it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.73it/s]I0316 12:21:18.392688 509591 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.00028456145082600415 old loss 0.00028625395498238504 BETTER
 21%|██        | 23/112 [00:08<00:32,  2.73it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.73it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.74it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.74it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.75it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.75it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.75it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.71it/s] 28%|██▊       | 31/112 [00:11<00:30,  2.69it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.68it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.65it/s]I0316 12:21:22.446260 509085 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.0001624104188522324 old loss 0.0001628434838494286 BETTER
 30%|███       | 34/112 [00:12<00:29,  2.66it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.67it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.67it/s]W0316 12:21:23.505700 509085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 33%|███▎      | 37/112 [00:14<00:27,  2.68it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.68it/s] 35%|███▍      | 39/112 [00:14<00:27,  2.69it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.69it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.69it/s] 38%|███▊      | 42/112 [00:15<00:26,  2.69it/s]I0316 12:21:25.832322 510097 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.0007789936498738825 old loss 0.0007935284520499408 BETTER
 38%|███▊      | 43/112 [00:16<00:25,  2.71it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.73it/s]29_gate proxy err 0.0012982103507965803 tr(WHW.T) 9683.66015625
  0%|          | 0/112 [00:00<?, ?it/s] 40%|████      | 45/112 [00:17<00:24,  2.69it/s] 41%|████      | 46/112 [00:17<00:24,  2.67it/s] 42%|████▏     | 47/112 [00:17<00:24,  2.69it/s]  1%|          | 1/112 [00:00<01:37,  1.14it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.71it/s]  2%|▏         | 2/112 [00:01<01:03,  1.73it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.73it/s]  3%|▎         | 3/112 [00:01<00:52,  2.07it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.74it/s]  4%|▎         | 4/112 [00:01<00:47,  2.28it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.74it/s]  4%|▍         | 5/112 [00:02<00:44,  2.41it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.75it/s]  5%|▌         | 6/112 [00:02<00:42,  2.50it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.74it/s]  6%|▋         | 7/112 [00:03<00:40,  2.57it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.75it/s]  7%|▋         | 8/112 [00:03<00:39,  2.60it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.74it/s]  8%|▊         | 9/112 [00:03<00:39,  2.61it/s] 50%|█████     | 56/112 [00:21<00:20,  2.74it/s]  9%|▉         | 10/112 [00:04<00:38,  2.62it/s] 51%|█████     | 57/112 [00:21<00:20,  2.73it/s] 10%|▉         | 11/112 [00:04<00:38,  2.64it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.72it/s] 11%|█         | 12/112 [00:04<00:38,  2.61it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.68it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.64it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.69it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.65it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.70it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.67it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.71it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.69it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.73it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.69it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.74it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.69it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.73it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.68it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.73it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.68it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.72it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.67it/s] 61%|██████    | 68/112 [00:25<00:16,  2.72it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.68it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.72it/s] 21%|██        | 23/112 [00:09<00:33,  2.67it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.71it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.63it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.71it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.65it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.68it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.66it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.70it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.67it/s] 66%|██████▌   | 74/112 [00:27<00:14,  2.70it/s] 25%|██▌       | 28/112 [00:10<00:31,  2.68it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.70it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.68it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.70it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.69it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.71it/s] 28%|██▊       | 31/112 [00:12<00:29,  2.71it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.72it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.71it/s] 71%|███████   | 79/112 [00:29<00:12,  2.73it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.70it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.73it/s] 30%|███       | 34/112 [00:13<00:28,  2.70it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.71it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.69it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.70it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.69it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.68it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.65it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.70it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.66it/s] 76%|███████▌  | 85/112 [00:31<00:10,  2.67it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.67it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.69it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.68it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.70it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.68it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.70it/s] 38%|███▊      | 42/112 [00:16<00:25,  2.70it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.69it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.70it/s] 80%|████████  | 90/112 [00:33<00:08,  2.70it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.70it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.71it/s] 40%|████      | 45/112 [00:17<00:24,  2.70it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.72it/s] 41%|████      | 46/112 [00:17<00:24,  2.68it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.72it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.68it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.73it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.66it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.72it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.64it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.70it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.66it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.69it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.66it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.67it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.67it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.68it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.66it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.70it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.66it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.71it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.65it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.72it/s] 50%|█████     | 56/112 [00:21<00:21,  2.64it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.73it/s] 51%|█████     | 57/112 [00:21<00:20,  2.65it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.73it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.64it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.72it/s] 53%|█████▎    | 59/112 [00:22<00:20,  2.64it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.72it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.72it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.63it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.72it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.59it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.72it/s] 55%|█████▌    | 62/112 [00:23<00:19,  2.61it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.73it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.61it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.69it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.62it/s]100%|██████████| 112/112 [00:41<00:00,  2.70it/s]100%|██████████| 112/112 [00:41<00:00,  2.68it/s]
 58%|█████▊    | 65/112 [00:24<00:17,  2.62it/s]I0316 12:21:51.737414 509591 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.00028299709083512425 old loss 0.00028456145082600415 BETTER
 59%|█████▉    | 66/112 [00:25<00:17,  2.64it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.67it/s] 61%|██████    | 68/112 [00:25<00:16,  2.66it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.66it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.67it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.69it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.70it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.67it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.67it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.68it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.69it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.69it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.71it/s] 71%|███████   | 79/112 [00:30<00:12,  2.71it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.71it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.71it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.70it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.70it/s]W0316 12:21:58.423000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.423000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.423000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.423000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.424000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.424000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.424000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.470000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.470000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.471000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.471000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.471000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 84/112 [00:31<00:10,  2.70it/s]W0316 12:21:58.648000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.649000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.649000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.649000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.649000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
I0316 12:21:58.675132 510097 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.0007676617824472487 old loss 0.0007789936498738825 BETTER
 76%|███████▌  | 85/112 [00:32<00:10,  2.67it/s]W0316 12:21:58.948000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.948000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.948000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.949000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.949000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.949000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.949000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.977000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.978000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.978000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.978000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:21:58.978000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:21:59.043000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:21:59.043000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:21:59.043000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:21:59.043000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:21:59.043000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 77%|███████▋  | 86/112 [00:32<00:09,  2.68it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.69it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.68it/s]W0316 12:22:00.366000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 79%|███████▉  | 89/112 [00:33<00:08,  2.69it/s] 80%|████████  | 90/112 [00:34<00:08,  2.68it/s]W0316 12:22:00.829000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:22:00.829000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 12:22:00.830000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 12:22:00.830000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:22:00.830000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:22:00.830000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:22:00.830000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:22:00.862000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:22:00.862000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:22:00.862000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:22:00.862000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:22:00.862000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 91/112 [00:34<00:07,  2.68it/s]W0316 12:22:01.240000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:22:01.241000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:22:01.241000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:22:01.241000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:22:01.241000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 82%|████████▏ | 92/112 [00:34<00:07,  2.69it/s]W0316 12:22:01.787000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 12:22:01.792000 140110556706624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 83%|████████▎ | 93/112 [00:35<00:07,  2.69it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.68it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.67it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.67it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.65it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.67it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.69it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.69it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.68it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.69it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.71it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.70it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.69it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.69it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.68it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.67it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.68it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.66it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.68it/s]100%|██████████| 112/112 [00:42<00:00,  2.68it/s]100%|██████████| 112/112 [00:42<00:00,  2.64it/s]
I0316 12:22:09.220017 508581 finetune.py:45] layer 28_down initial loss 0.00018079319852404296
W0316 12:22:09.220529 508581 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0316 12:22:16.242000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.243000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.243000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.243000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.243000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.243000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.243000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.285000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.285000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.285000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.285000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.285000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.459000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.459000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.459000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.459000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.459000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.778000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.778000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.778000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.779000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.779000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.779000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.779000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.808000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.808000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.808000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.808000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.808000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.876000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.876000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.876000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.876000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:22:16.876000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:22:18.210000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 12:22:18.661000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:22:18.661000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:22:18.661000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:22:18.661000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:22:18.661000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 12:22:18.661000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 12:22:18.662000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:22:18.691000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:22:18.692000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:22:18.692000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:22:18.692000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:22:18.692000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:22:19.071000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:22:19.071000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:22:19.071000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:22:19.071000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:22:19.071000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:22:19.619000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 12:22:19.625000 139633039947584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0316 12:22:24.700620 509591 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.0002816666092257947 old loss 0.00028299709083512425 BETTER
W0316 12:22:25.834038 509591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0316 12:22:26.840664 509085 finetune.py:45] layer 29_down initial loss 0.00024235594901256263
W0316 12:22:26.840982 509085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

30_gate proxy err 0.0010155682684853673 tr(WHW.T) 13195.501953125
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:36,  1.15it/s]  2%|▏         | 2/112 [00:01<01:04,  1.70it/s]  3%|▎         | 3/112 [00:01<00:53,  2.03it/s]  4%|▎         | 4/112 [00:02<00:48,  2.23it/s]  4%|▍         | 5/112 [00:02<00:45,  2.36it/s]I0316 12:22:31.425525 510097 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.0007568293949589133 old loss 0.0007676617824472487 BETTER
  5%|▌         | 6/112 [00:02<00:43,  2.44it/s]  6%|▋         | 7/112 [00:03<00:41,  2.51it/s]  7%|▋         | 8/112 [00:03<00:40,  2.54it/s]  8%|▊         | 9/112 [00:03<00:40,  2.57it/s]  9%|▉         | 10/112 [00:04<00:39,  2.58it/s] 10%|▉         | 11/112 [00:04<00:39,  2.59it/s] 11%|█         | 12/112 [00:05<00:38,  2.60it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.57it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.58it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.59it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.59it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.60it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.60it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.60it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.62it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.63it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.62it/s] 21%|██        | 23/112 [00:09<00:33,  2.63it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.63it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.60it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.61it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.61it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.62it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.63it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.64it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.64it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.64it/s]I0316 12:22:41.822089 508581 finetune.py:68] layer 28_down @ epoch 0 new loss 0.00018072487728204578 old loss 0.00018079319852404296 BETTER
 29%|██▉       | 33/112 [00:13<00:29,  2.63it/s] 30%|███       | 34/112 [00:13<00:29,  2.62it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.62it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.62it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.59it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.61it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.62it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.63it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.62it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.63it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.64it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.64it/s] 40%|████      | 45/112 [00:17<00:25,  2.64it/s] 41%|████      | 46/112 [00:18<00:25,  2.64it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.64it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.64it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.60it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.60it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.62it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.63it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.64it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.63it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.63it/s] 50%|█████     | 56/112 [00:21<00:21,  2.63it/s] 51%|█████     | 57/112 [00:22<00:20,  2.64it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.63it/s] 53%|█████▎    | 59/112 [00:22<00:20,  2.64it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.64it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.60it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.60it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.60it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.60it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.60it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.60it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.60it/s] 61%|██████    | 68/112 [00:26<00:16,  2.60it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.60it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.59it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.60it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.60it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.57it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.58it/s]I0316 12:22:57.750333 509085 finetune.py:68] layer 29_down @ epoch 0 new loss 0.0002422627730993554 old loss 0.00024235594901256263 BETTER
 67%|██████▋   | 75/112 [00:29<00:14,  2.61it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.62it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.62it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.62it/s] 71%|███████   | 79/112 [00:30<00:12,  2.64it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.64it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.64it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.64it/s] 74%|███████▍  | 83/112 [00:32<00:10,  2.65it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.65it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.62it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.63it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.62it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.63it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.64it/s] 80%|████████  | 90/112 [00:34<00:08,  2.64it/s] 81%|████████▏ | 91/112 [00:35<00:07,  2.64it/s]I0316 12:23:04.093606 510097 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.0007473377627320588 old loss 0.0007568293949589133 BETTER
 82%|████████▏ | 92/112 [00:35<00:07,  2.65it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.64it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.63it/s]W0316 12:23:05.245058 510097 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 85%|████████▍ | 95/112 [00:36<00:06,  2.63it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.63it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.60it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.61it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.61it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.62it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.62it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.63it/s]31_gate proxy err 0.0004433422291185707 tr(WHW.T) 25669.189453125
  0%|          | 0/112 [00:00<?, ?it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.63it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.62it/s]  1%|          | 1/112 [00:00<01:34,  1.17it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.62it/s]  2%|▏         | 2/112 [00:01<01:02,  1.75it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.62it/s]  3%|▎         | 3/112 [00:01<00:53,  2.05it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.62it/s]  4%|▎         | 4/112 [00:02<00:48,  2.23it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.60it/s]  4%|▍         | 5/112 [00:02<00:45,  2.35it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.61it/s]  5%|▌         | 6/112 [00:02<00:43,  2.43it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.60it/s]  6%|▋         | 7/112 [00:03<00:42,  2.49it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.61it/s]  7%|▋         | 8/112 [00:03<00:41,  2.53it/s]100%|██████████| 112/112 [00:43<00:00,  2.63it/s]100%|██████████| 112/112 [00:43<00:00,  2.59it/s]
  8%|▊         | 9/112 [00:03<00:40,  2.55it/s]  9%|▉         | 10/112 [00:04<00:39,  2.57it/s] 10%|▉         | 11/112 [00:04<00:39,  2.58it/s] 11%|█         | 12/112 [00:05<00:38,  2.57it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.55it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.57it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.58it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.59it/s]I0316 12:23:15.155608 508581 finetune.py:68] layer 28_down @ epoch 1 new loss 0.00018072096281684935 old loss 0.00018072487728204578 BETTER
 15%|█▌        | 17/112 [00:06<00:36,  2.63it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.65it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.65it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.66it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.66it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.67it/s] 21%|██        | 23/112 [00:09<00:33,  2.67it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.67it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.68it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.66it/s]W0316 12:23:18.882000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:23:18.882000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 12:23:18.882000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:23:18.882000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:23:18.882000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:23:18.882000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 12:23:18.883000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:23:18.927000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:23:18.927000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:23:18.927000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:23:18.927000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:23:18.927000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 24%|██▍       | 27/112 [00:10<00:31,  2.66it/s]W0316 12:23:19.105000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.105000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.105000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.105000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.105000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 28/112 [00:11<00:31,  2.67it/s]W0316 12:23:19.422000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.423000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.423000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.423000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.423000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.423000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.423000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.457000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.457000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.457000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.457000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.457000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.528000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.528000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.528000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.528000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:23:19.528000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 26%|██▌       | 29/112 [00:11<00:31,  2.67it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.67it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.68it/s]W0316 12:23:20.868000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 29%|██▊       | 32/112 [00:12<00:29,  2.69it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.69it/s]W0316 12:23:21.331000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:23:21.332000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 12:23:21.332000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 12:23:21.332000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:23:21.332000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:23:21.332000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:23:21.332000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:23:21.364000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:23:21.365000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:23:21.365000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:23:21.365000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:23:21.365000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 30%|███       | 34/112 [00:13<00:28,  2.69it/s]W0316 12:23:21.751000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:23:21.751000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:23:21.751000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:23:21.751000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:23:21.751000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 35/112 [00:13<00:28,  2.67it/s]W0316 12:23:22.295000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 12:23:22.301000 139802982561600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 32%|███▏      | 36/112 [00:14<00:28,  2.66it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.66it/s] 34%|███▍      | 38/112 [00:14<00:28,  2.63it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.64it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.66it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.68it/s] 38%|███▊      | 42/112 [00:16<00:25,  2.70it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.70it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.70it/s] 40%|████      | 45/112 [00:17<00:24,  2.71it/s] 41%|████      | 46/112 [00:17<00:24,  2.71it/s] 42%|████▏     | 47/112 [00:18<00:23,  2.71it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.71it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.68it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.68it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.65it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.65it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.66it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.65it/s]I0316 12:23:29.357507 509085 finetune.py:68] layer 29_down @ epoch 1 new loss 0.00024225031665991992 old loss 0.0002422627730993554 BETTER
 49%|████▉     | 55/112 [00:21<00:21,  2.66it/s]I0316 12:23:29.563607 509591 finetune.py:45] layer 30_down initial loss 0.0004002769710496068
W0316 12:23:29.565633 509591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 50%|█████     | 56/112 [00:21<00:21,  2.66it/s] 51%|█████     | 57/112 [00:21<00:20,  2.67it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.68it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.70it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.70it/s] 54%|█████▍    | 61/112 [00:23<00:18,  2.69it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.67it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.67it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.64it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.66it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.67it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.67it/s] 61%|██████    | 68/112 [00:26<00:16,  2.66it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.66it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.65it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.67it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.69it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.69it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.68it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.67it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.63it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.64it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.64it/s] 71%|███████   | 79/112 [00:30<00:12,  2.65it/s] 71%|███████▏  | 80/112 [00:30<00:12,  2.65it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.66it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.67it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.68it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.68it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.69it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.66it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.65it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.65it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.61it/s] 80%|████████  | 90/112 [00:34<00:08,  2.63it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.65it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.63it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.61it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.61it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.60it/s] 86%|████████▌ | 96/112 [00:36<00:06,  2.61it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.61it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.60it/s] 88%|████████▊ | 99/112 [00:37<00:05,  2.59it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.58it/s] 90%|█████████ | 101/112 [00:38<00:05,  2.03it/s] 91%|█████████ | 102/112 [00:39<00:04,  2.17it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.29it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.38it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.45it/s]I0316 12:23:48.911750 508581 finetune.py:68] layer 28_down @ epoch 2 new loss 0.00018071472004521638 old loss 0.00018072096281684935 BETTER
 95%|█████████▍| 106/112 [00:40<00:02,  2.53it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.56it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.60it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.62it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.64it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.64it/s]100%|██████████| 112/112 [00:43<00:00,  2.65it/s]100%|██████████| 112/112 [00:43<00:00,  2.60it/s]
W0316 12:23:58.166000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.166000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.166000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.166000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.167000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.167000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.167000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.215000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.215000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.215000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.215000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.216000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.380000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.380000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.380000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.380000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.380000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.700000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.700000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.700000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.700000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.701000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.701000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.701000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.732000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.732000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.732000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.732000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.732000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.812000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.812000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.812000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.813000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0316 12:23:58.813000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0316 12:24:00.132000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 12:24:00.581000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:24:00.582000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0316 12:24:00.582000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:24:00.582000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:24:00.582000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0316 12:24:00.582000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:24:00.583000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:24:00.614000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:24:00.614000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:24:00.614000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:24:00.614000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:24:00.614000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0316 12:24:00.991000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0316 12:24:00.991000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0316 12:24:00.991000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0316 12:24:00.991000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0316 12:24:00.991000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
I0316 12:24:01.153454 509591 finetune.py:68] layer 30_down @ epoch 0 new loss 0.00040006887866184115 old loss 0.0004002769710496068 BETTER
W0316 12:24:01.533000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0316 12:24:01.538000 139944357209920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0316 12:24:01.660165 509085 finetune.py:76] layer 29_down @ epoch 2 new loss 0.00024225603556260467 old loss 0.00024225031665991992 WORSE
I0316 12:24:08.629615 510097 finetune.py:45] layer 31_down initial loss 0.0009888516506180167
W0316 12:24:08.630030 510097 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0316 12:24:22.187594 508581 finetune.py:76] layer 28_down @ epoch 3 new loss 0.00018071482190862298 old loss 0.00018071472004521638 WORSE
I0316 12:24:32.911603 509085 finetune.py:68] layer 29_down @ epoch 3 new loss 0.00024221671628765762 old loss 0.00024225031665991992 BETTER
I0316 12:24:33.006435 509591 finetune.py:68] layer 30_down @ epoch 1 new loss 0.00040003436151891947 old loss 0.00040006887866184115 BETTER
I0316 12:24:39.139207 510097 finetune.py:68] layer 31_down @ epoch 0 new loss 0.0009880127618089318 old loss 0.0009888516506180167 BETTER
I0316 12:24:54.931282 508581 finetune.py:68] layer 28_down @ epoch 4 new loss 0.00018070398073177785 old loss 0.00018071472004521638 BETTER
W0316 12:24:55.677112 508581 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

28_down proxy err 0.0030661174096167088 tr(WHW.T) 82.26544952392578
I0316 12:25:04.436012 509085 finetune.py:76] layer 29_down @ epoch 4 new loss 0.00024222982756327838 old loss 0.00024221671628765762 WORSE
I0316 12:25:04.784446 509591 finetune.py:68] layer 30_down @ epoch 2 new loss 0.00039999597356654704 old loss 0.00040003436151891947 BETTER
W0316 12:25:04.953670 509085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

29_down proxy err 0.002568507567048073 tr(WHW.T) 132.44154357910156
I0316 12:25:10.320567 510097 finetune.py:68] layer 31_down @ epoch 1 new loss 0.0009880007710307837 old loss 0.0009880127618089318 BETTER
I0316 12:25:36.460798 509591 finetune.py:68] layer 30_down @ epoch 3 new loss 0.00039998930878937244 old loss 0.00039999597356654704 BETTER
I0316 12:25:41.718060 510097 finetune.py:68] layer 31_down @ epoch 2 new loss 0.0009878651471808553 old loss 0.0009880007710307837 BETTER
I0316 12:26:08.370773 509591 finetune.py:68] layer 30_down @ epoch 4 new loss 0.0003999604086857289 old loss 0.00039998930878937244 BETTER
W0316 12:26:09.083632 509591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

30_down proxy err 0.0013876133598387241 tr(WHW.T) 368.70623779296875
I0316 12:26:13.533938 510097 finetune.py:68] layer 31_down @ epoch 3 new loss 0.0009874834213405848 old loss 0.0009878651471808553 BETTER
I0316 12:26:45.017700 510097 finetune.py:68] layer 31_down @ epoch 4 new loss 0.0009872978553175926 old loss 0.0009874834213405848 BETTER
W0316 12:26:45.753857 510097 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

31_down proxy err 0.0004259551642462611 tr(WHW.T) 2765.154052734375
I0316 12:27:17.097168 510795 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0316 12:27:17.097309 510795 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0316 12:27:17.097351 510795 utils.py:162] NumExpr defaulting to 16 threads.
I0316 12:27:17.277792 510795 config.py:58] PyTorch version 2.4.0 available.
W0316 12:27:19.056162 510795 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0316 12:27:19.056833 510795 hfize_llama.py:25] LlamaConfig {
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {
    "K": 4,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.56it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.99it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  6.22it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  4.45it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  3.54it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  3.09it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  3.05it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  3.67it/s]
Some weights of the model checkpoint at ../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B and are newly initialized: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.tp_rank', 'model.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  3.15it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  3.28it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:01,  3.37it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:01<00:00,  3.50it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  3.56it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  3.65it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  3.77it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  3.59it/s]
W0316 12:27:24.462439 510795 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(f'{args.quantized_path}/{ii}_layernorm.pt',

W0316 12:27:24.463790 510795 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',

W0316 12:27:24.491867 510795 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_k.pt',

W0316 12:27:24.497474 510795 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_v.pt',

W0316 12:27:24.501715 510795 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_o.pt',

W0316 12:27:24.521656 510795 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',

W0316 12:27:24.551025 510795 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_gate.pt',

W0316 12:27:24.575337 510795 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_down.pt',

I0316 12:27:24.592161 510795 hfize_llama.py:113] loaded layer 0
I0316 12:27:24.692726 510795 hfize_llama.py:113] loaded layer 1
I0316 12:27:24.835990 510795 hfize_llama.py:113] loaded layer 2
I0316 12:27:24.942819 510795 hfize_llama.py:113] loaded layer 3
I0316 12:27:25.054176 510795 hfize_llama.py:113] loaded layer 4
I0316 12:27:25.153215 510795 hfize_llama.py:113] loaded layer 5
I0316 12:27:25.278105 510795 hfize_llama.py:113] loaded layer 6
I0316 12:27:25.378498 510795 hfize_llama.py:113] loaded layer 7
I0316 12:27:25.463193 510795 hfize_llama.py:113] loaded layer 8
I0316 12:27:25.579893 510795 hfize_llama.py:113] loaded layer 9
I0316 12:27:25.685152 510795 hfize_llama.py:113] loaded layer 10
I0316 12:27:25.773219 510795 hfize_llama.py:113] loaded layer 11
I0316 12:27:25.894088 510795 hfize_llama.py:113] loaded layer 12
I0316 12:27:26.005966 510795 hfize_llama.py:113] loaded layer 13
I0316 12:27:26.136036 510795 hfize_llama.py:113] loaded layer 14
I0316 12:27:26.245160 510795 hfize_llama.py:113] loaded layer 15
I0316 12:27:26.353870 510795 hfize_llama.py:113] loaded layer 16
I0316 12:27:26.537577 510795 hfize_llama.py:113] loaded layer 17
I0316 12:27:26.655966 510795 hfize_llama.py:113] loaded layer 18
I0316 12:27:26.758151 510795 hfize_llama.py:113] loaded layer 19
I0316 12:27:26.881054 510795 hfize_llama.py:113] loaded layer 20
I0316 12:27:27.010569 510795 hfize_llama.py:113] loaded layer 21
I0316 12:27:27.105760 510795 hfize_llama.py:113] loaded layer 22
I0316 12:27:27.240036 510795 hfize_llama.py:113] loaded layer 23
I0316 12:27:27.384569 510795 hfize_llama.py:113] loaded layer 24
I0316 12:27:27.540487 510795 hfize_llama.py:113] loaded layer 25
I0316 12:27:27.668981 510795 hfize_llama.py:113] loaded layer 26
I0316 12:27:27.800232 510795 hfize_llama.py:113] loaded layer 27
I0316 12:27:27.944640 510795 hfize_llama.py:113] loaded layer 28
I0316 12:27:28.084946 510795 hfize_llama.py:113] loaded layer 29
I0316 12:27:28.215605 510795 hfize_llama.py:113] loaded layer 30
I0316 12:27:28.355979 510795 hfize_llama.py:113] loaded layer 31
I0316 12:27:28.356076 510795 hfize_llama.py:115] saving model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.63it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.84it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.61it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]
I0316 12:27:42.375301 510795 hfize_llama.py:122] successfully loaded hfized model
