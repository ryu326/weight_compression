I0402 09:59:58.458645 1606370 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:59:58.458778 1606370 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:59:58.458818 1606370 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:59:58.645383 1606370 config.py:58] PyTorch version 2.4.0 available.
W0402 10:00:00.799486 1606370 warnings.py:110] /workspace/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:02,  2.62it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  3.20it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:01,  3.66it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:01<00:00,  3.78it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  4.05it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  4.10it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  4.37it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  3.94it/s]
I0402 10:00:03.407709 1606370 quantize_finetune_llama.py:135] loaded model
I0402 10:00:23.978206 1606370 quantize_finetune_llama.py:139] loaded dataset and devset
I0402 10:00:30.269127 1606370 quantize_finetune_llama.py:159] layer 0 gpu 0
I0402 10:00:34.155910 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 0 in 3.748865842819214s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0402 10:00:54.290221 1606370 quantize_finetune_llama.py:159] layer 1 gpu 1
I0402 10:00:56.200613 1607628 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:00:56.200720 1607628 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:00:56.200780 1607628 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:00:56.386986 1607628 config.py:58] PyTorch version 2.4.0 available.
I0402 10:00:58.049841 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 1 in 3.5962915420532227s
I0402 10:00:58.530686 1606370 quantize_finetune_llama.py:159] layer 2 gpu 2
I0402 10:00:58.653425 1607628 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 10:00:59.012731 1607628 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:01:00.488256 1608016 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:01:00.488369 1608016 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:01:00.488437 1608016 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:01:00.681675 1608016 config.py:58] PyTorch version 2.4.0 available.
  3%|▎         | 1/32 [00:01<00:51,  1.67s/it]I0402 10:01:01.961060 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 2 in 3.2849674224853516s
  6%|▋         | 2/32 [00:02<00:26,  1.13it/s]  9%|▉         | 3/32 [00:02<00:18,  1.58it/s]I0402 10:01:02.472504 1606370 quantize_finetune_llama.py:159] layer 3 gpu 3
 12%|█▎        | 4/32 [00:02<00:14,  1.92it/s]I0402 10:01:02.925155 1608016 data_utils.py:336] using 256 training seqs, 128 validation seqs
 16%|█▌        | 5/32 [00:03<00:12,  2.11it/s]W0402 10:01:03.318340 1608016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:04<00:08,  2.71it/s]  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.86it/s]I0402 10:01:04.460550 1608429 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:01:04.460657 1608429 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:01:04.460725 1608429 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:01:04.652968 1608429 config.py:58] PyTorch version 2.4.0 available.
 31%|███▏      | 10/32 [00:04<00:07,  2.96it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.03it/s] 38%|███▊      | 12/32 [00:05<00:06,  3.07it/s] 41%|████      | 13/32 [00:05<00:06,  3.08it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.13it/s]  3%|▎         | 1/32 [00:01<00:56,  1.82s/it]I0402 10:01:06.216568 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 3 in 3.601280450820923s
 47%|████▋     | 15/32 [00:06<00:05,  3.13it/s]  6%|▋         | 2/32 [00:02<00:28,  1.04it/s] 50%|█████     | 16/32 [00:06<00:05,  3.00it/s]I0402 10:01:06.718019 1606370 quantize_finetune_llama.py:159] layer 4 gpu 0
I0402 10:01:06.756530 1608429 data_utils.py:336] using 256 training seqs, 128 validation seqs
  9%|▉         | 3/32 [00:02<00:19,  1.47it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.99it/s]W0402 10:01:07.112187 1608429 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:02<00:15,  1.82it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.99it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.08it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.99it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.04it/s]  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.06it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.63it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.06it/s]I0402 10:01:08.706968 1609098 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:01:08.707142 1609098 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:01:08.707199 1609098 utils.py:162] NumExpr defaulting to 16 threads.
 28%|██▊       | 9/32 [00:04<00:08,  2.69it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.05it/s]I0402 10:01:09.005523 1609098 config.py:58] PyTorch version 2.4.0 available.
 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.08it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.83it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.10it/s]  3%|▎         | 1/32 [00:01<00:55,  1.79s/it] 38%|███▊      | 12/32 [00:05<00:06,  2.88it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.11it/s]  6%|▋         | 2/32 [00:02<00:28,  1.07it/s] 41%|████      | 13/32 [00:05<00:06,  2.92it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.13it/s]  9%|▉         | 3/32 [00:02<00:19,  1.50it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.13it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.93it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.85it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.10it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.92it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.11it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.07it/s] 50%|█████     | 16/32 [00:06<00:05,  2.89it/s]I0402 10:01:11.456503 1609098 data_utils.py:336] using 256 training seqs, 128 validation seqs
 19%|█▉        | 6/32 [00:03<00:11,  2.31it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.05it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.88it/s]W0402 10:01:11.857167 1609098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:11<00:00,  3.07it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.89it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.59it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.88it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.68it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.90it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.89it/s]  0%|          | 0/32 [00:00<?, ?it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.78it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.91it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.84it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.94it/s] 41%|████      | 13/32 [00:05<00:06,  2.89it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.94it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.90it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.94it/s]W0402 10:01:14.552000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:14.552000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:14.552000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:14.552000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:14.552000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:14.552000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:14.553000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:14.578000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:14.578000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:14.578000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:14.578000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:14.578000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 15/32 [00:06<00:05,  2.94it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.97it/s]W0402 10:01:14.861000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:14.861000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:14.861000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:14.861000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:14.861000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:57,  1.84s/it] 50%|█████     | 16/32 [00:06<00:05,  2.93it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.95it/s]  6%|▋         | 2/32 [00:02<00:28,  1.04it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.93it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.96it/s]W0402 10:01:15.457000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:15.457000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:15.457000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:15.458000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:15.458000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:15.458000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:15.458000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:15.474000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:15.475000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:15.475000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:15.475000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:15.475000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:02<00:19,  1.48it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.95it/s]W0402 10:01:15.669000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:15.669000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:15.669000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:15.669000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:15.669000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.97it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.85it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.97it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.98it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.15it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.98it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.98it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.37it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.98it/s]100%|██████████| 32/32 [00:12<00:00,  2.99it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
W0402 10:01:16.768000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:16.769000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:16.769000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:16.769000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:16.769000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:16.769000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:16.769000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:16.786000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:16.786000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:16.786000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:16.786000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:16.786000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.96it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.93it/s]W0402 10:01:17.412000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:17.412000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:17.413000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:17.413000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:17.413000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:04<00:08,  2.71it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.94it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.91it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 34%|███▍      | 11/32 [00:05<00:07,  2.81it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.93it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.87it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.96it/s] 41%|████      | 13/32 [00:05<00:06,  2.86it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.93it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.84it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.90it/s]W0402 10:01:19.463000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:19.463000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:19.463000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:19.463000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:19.463000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:19.463000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:19.463000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:19.490000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:19.490000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:19.491000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:19.491000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:19.491000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 15/32 [00:06<00:05,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.89it/s]W0402 10:01:19.789000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:19.789000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:19.789000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:19.789000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:19.789000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:05,  2.84it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.87it/s]W0402 10:01:20.411000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:20.411000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:20.411000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:20.411000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:20.411000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:20.411000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:20.411000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:07<00:05,  2.84it/s]W0402 10:01:20.429000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:20.429000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:20.429000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:20.429000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:20.429000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
W0402 10:01:20.634000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:20.634000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:20.634000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:20.634000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:20.634000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:07<00:04,  2.81it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.82it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.82it/s]W0402 10:01:21.795000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:21.795000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:21.795000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:21.795000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:21.795000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:21.795000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:21.795000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:21.814000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:21.814000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:21.814000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:21.814000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:21.814000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:03,  2.82it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.84it/s]W0402 10:01:22.456000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:22.456000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:22.456000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:22.456000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:22.456000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:09<00:03,  2.85it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.86it/s]W0402 10:01:23.211000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:23.212000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:23.212000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:23.212000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:23.212000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:23.212000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:23.212000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 10:01:23.239000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:23.240000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:23.240000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:23.240000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:23.240000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:10<00:02,  2.85it/s]W0402 10:01:23.541000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:23.541000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:23.541000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:23.541000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:23.541000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:10<00:02,  2.86it/s]I0402 10:01:23.615437 1607628 finetune.py:45] layer 0_v initial loss 0.00013404896890278906
W0402 10:01:23.615765 1607628 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 84%|████████▍ | 27/32 [00:10<00:01,  2.91it/s]W0402 10:01:24.161000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:24.162000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:24.162000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:24.162000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:24.162000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:24.162000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:24.162000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:24.180000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:24.180000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:24.180000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:24.181000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:24.181000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:11<00:01,  2.90it/s]W0402 10:01:24.387000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:24.387000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:24.388000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:24.388000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:24.388000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:24.601092 1607628 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:11<00:01,  2.90it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.89it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.88it/s]W0402 10:01:25.561000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:25.561000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:25.561000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:25.561000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:25.561000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:25.561000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:25.561000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:25.580000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:25.580000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:25.580000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:25.580000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:25.580000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.88it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
0_v proxy err 0.0013809925876557827 tr(WHW.T) 1.3175290822982788
  0%|          | 0/32 [00:00<?, ?it/s]W0402 10:01:26.221000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:26.222000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:26.222000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:26.222000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:26.222000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
  3%|▎         | 1/32 [00:01<00:41,  1.33s/it]  6%|▋         | 2/32 [00:01<00:22,  1.31it/s]  9%|▉         | 3/32 [00:02<00:16,  1.72it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.01it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.23it/s]W0402 10:01:28.785000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:28.786000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:28.786000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:28.786000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:28.786000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:28.786000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:28.786000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:28.811000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:28.811000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:28.811000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:28.811000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:28.812000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:03<00:10,  2.38it/s]I0402 10:01:28.955058 1608016 finetune.py:45] layer 1_v initial loss 0.004391693510115147
W0402 10:01:28.955473 1608016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:01:29.092000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.092000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.092000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.092000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.093000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.56it/s]W0402 10:01:29.691000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.692000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.692000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.692000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.692000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.692000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.692000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.710000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.710000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.710000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.710000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.710000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.912000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.912000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.912000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.912000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:29.912000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:04<00:08,  2.60it/s]W0402 10:01:30.062227 1608016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.65it/s]W0402 10:01:31.030000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:31.030000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:31.030000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:31.030000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:31.030000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:31.030000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:31.030000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:31.048000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:31.048000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:31.048000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:31.048000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:31.048000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
1_v proxy err 0.001383635331876576 tr(WHW.T) 5.699800491333008
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.67it/s] 41%|████      | 13/32 [00:05<00:07,  2.67it/s]W0402 10:01:31.645000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:31.645000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:31.645000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:31.645000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:31.645000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:06<00:06,  2.69it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s]  3%|▎         | 1/32 [00:01<00:41,  1.33s/it]I0402 10:01:32.492705 1608429 finetune.py:45] layer 2_v initial loss 0.0006899290601722896
W0402 10:01:32.492937 1608429 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 50%|█████     | 16/32 [00:06<00:05,  2.72it/s]  6%|▋         | 2/32 [00:01<00:22,  1.31it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.76it/s]  9%|▉         | 3/32 [00:02<00:17,  1.70it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s]W0402 10:01:33.529747 1608429 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:02<00:14,  1.97it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.16it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.74it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.30it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.74it/s]2_v proxy err 0.0019992440938949585 tr(WHW.T) 26.839576721191406
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.40it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.73it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.46it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.51it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.74it/s]  3%|▎         | 1/32 [00:01<00:43,  1.42s/it] 34%|███▍      | 11/32 [00:05<00:08,  2.57it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.74it/s]  6%|▋         | 2/32 [00:01<00:24,  1.24it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s]  9%|▉         | 3/32 [00:02<00:17,  1.63it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.74it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.92it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.76it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.63it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.13it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.77it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.28it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.77it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.40it/s]100%|██████████| 32/32 [00:12<00:00,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
 53%|█████▎    | 17/32 [00:07<00:05,  2.69it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.47it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.64it/s]I0402 10:01:38.945443 1609098 finetune.py:45] layer 3_v initial loss 0.0013833277625963092
W0402 10:01:38.945895 1609098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:04<00:09,  2.49it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.65it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.52it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.65it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.53it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.66it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s]W0402 10:01:40.348193 1609098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:09<00:03,  2.70it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.70it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.64it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.73it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.67it/s]3_v proxy err 0.0025086288806051016 tr(WHW.T) 40.46714782714844
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.74it/s] 50%|█████     | 16/32 [00:07<00:05,  2.67it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.73it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.67it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.75it/s]  3%|▎         | 1/32 [00:01<00:40,  1.29s/it] 59%|█████▉    | 19/32 [00:08<00:04,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s]  6%|▋         | 2/32 [00:01<00:22,  1.33it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.73it/s]  9%|▉         | 3/32 [00:02<00:16,  1.73it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.72it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.01it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.69it/s]W0402 10:01:43.959000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:43.959000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:43.959000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:43.959000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:43.959000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:43.960000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:43.960000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:43.987000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:43.988000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:43.988000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:43.988000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:43.988000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
W0402 10:01:44.140000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.141000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.141000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.141000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.141000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:12,  2.21it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s]W0402 10:01:44.360000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.360000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.360000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.360000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.360000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.360000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.360000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.383000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.383000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.383000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.383000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.383000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.448000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.448000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.448000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.449000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:44.449000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:03<00:11,  2.36it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.67it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.64it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.65it/s]W0402 10:01:45.486000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:04<00:08,  2.57it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.64it/s]W0402 10:01:45.799000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:45.800000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:45.800000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:45.800000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:45.800000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:45.800000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:45.800000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:45.821000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:45.821000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:45.821000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:45.822000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:45.822000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:46.057000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:46.057000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:46.057000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:46.057000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:46.057000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:08,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.64it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.64it/s]W0402 10:01:46.495000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.67it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.66it/s] 41%|████      | 13/32 [00:05<00:07,  2.67it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.67it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.67it/s]100%|██████████| 32/32 [00:13<00:00,  2.68it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
 47%|████▋     | 15/32 [00:06<00:06,  2.67it/s] 50%|█████     | 16/32 [00:06<00:05,  2.70it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.70it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.72it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.76it/s]W0402 10:01:49.817000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:49.817000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:49.817000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:49.818000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:49.818000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:49.818000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:49.818000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:49.848000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:49.848000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:49.849000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:49.849000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:49.849000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.015000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.015000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.015000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.015000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.015000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:03,  2.77it/s]W0402 10:01:50.241000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.241000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.241000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.241000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.242000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.242000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.242000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.264000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.264000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.264000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.264000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.264000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.329000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.329000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.329000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.329000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:50.330000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:09<00:03,  2.77it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.76it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.74it/s]W0402 10:01:51.382000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:10<00:02,  2.74it/s]W0402 10:01:51.700000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:51.700000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:51.700000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:51.700000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:51.701000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:51.701000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:51.701000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:51.724000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:51.725000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:51.725000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:51.725000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:51.725000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s]W0402 10:01:51.976000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:51.976000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:51.976000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:51.976000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:51.977000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  2.71it/s]W0402 10:01:52.435000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:11<00:01,  2.69it/s]I0402 10:01:52.865367 1607628 finetune.py:45] layer 0_q initial loss 0.00013394605775829405
W0402 10:01:52.865575 1607628 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s]W0402 10:01:53.581000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:53.581000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:53.582000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:53.582000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:53.582000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:53.582000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:53.582000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:53.611000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:53.611000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:53.611000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:53.611000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:53.611000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:12<00:00,  2.75it/s]W0402 10:01:53.763614 1607628 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 10:01:53.776000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:53.776000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:53.776000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:53.776000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:53.777000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:54.004000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:54.004000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:54.004000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:54.005000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:54.005000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:54.005000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:54.005000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:54.026000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:54.026000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:54.026000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:54.026000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:54.026000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:54.095000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:54.095000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:54.096000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:54.096000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:54.096000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
0_q proxy err 6.565259536728263e-05 tr(WHW.T) 6234.0732421875
  0%|          | 0/32 [00:00<?, ?it/s]W0402 10:01:55.179000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:55.506000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:55.506000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:55.506000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:55.506000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:55.506000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:55.506000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:55.506000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:55.528000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:55.529000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:55.529000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:55.529000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:55.529000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:55.787000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:55.787000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:55.787000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:55.787000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:55.787000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:38,  1.23s/it]W0402 10:01:56.259000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:21,  1.39it/s]  9%|▉         | 3/32 [00:01<00:16,  1.81it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.09it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.45it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s]I0402 10:01:58.738954 1608016 finetune.py:45] layer 1_q initial loss 0.004396774806082249
W0402 10:01:58.739329 1608016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:04<00:08,  2.66it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.72it/s]W0402 10:01:59.686000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:59.686000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:01:59.686000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:59.686000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:01:59.686000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:59.686000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:59.686000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:59.714000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:59.714000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:59.714000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:59.714000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:59.714000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:01:59.794724 1608016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 10:01:59.869000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:01:59.869000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:01:59.869000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:01:59.869000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:01:59.869000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:05<00:07,  2.74it/s]W0402 10:02:00.076000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:02:00.076000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:02:00.076000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:02:00.076000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:02:00.076000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:02:00.077000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:02:00.077000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:02:00.095000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:02:00.095000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:02:00.095000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:02:00.095000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:02:00.095000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:02:00.155000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:02:00.155000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:02:00.155000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:02:00.155000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:02:00.155000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.75it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.76it/s]1_q proxy err 2.3907912691356614e-05 tr(WHW.T) 7568.30322265625
  0%|          | 0/32 [00:00<?, ?it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.77it/s]W0402 10:02:01.129000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:05,  2.77it/s]W0402 10:02:01.429000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:02:01.429000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:02:01.429000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:02:01.429000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:02:01.429000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:02:01.429000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:02:01.429000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:02:01.452000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:02:01.452000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:02:01.452000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:02:01.452000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:02:01.452000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:02:01.694000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:02:01.694000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:02:01.694000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:02:01.694000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:02:01.694000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.78it/s]W0402 10:02:02.126000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:39,  1.27s/it] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s]  6%|▋         | 2/32 [00:01<00:22,  1.35it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.78it/s]  9%|▉         | 3/32 [00:02<00:16,  1.74it/s]I0402 10:02:02.900783 1608429 finetune.py:45] layer 2_q initial loss 0.0006894723628647625
W0402 10:02:02.901165 1608429 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 66%|██████▌   | 21/32 [00:08<00:03,  2.82it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.05it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.26it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.84it/s]W0402 10:02:03.890177 1608429 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:03<00:10,  2.40it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.84it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.88it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s]2_q proxy err 0.00021791479957755655 tr(WHW.T) 7137.59423828125
  0%|          | 0/32 [00:00<?, ?it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.91it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.70it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.94it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.93it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.76it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.92it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.77it/s]  3%|▎         | 1/32 [00:01<00:37,  1.22s/it] 94%|█████████▍| 30/32 [00:11<00:00,  2.95it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s]  6%|▋         | 2/32 [00:01<00:21,  1.41it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.95it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.82it/s]  9%|▉         | 3/32 [00:01<00:15,  1.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.95it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 47%|████▋     | 15/32 [00:06<00:06,  2.82it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.10it/s] 50%|█████     | 16/32 [00:06<00:05,  2.79it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.27it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.79it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.40it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.78it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s]I0402 10:02:08.413248 1609098 finetune.py:45] layer 3_q initial loss 0.0013834965648129582
W0402 10:02:08.413442 1609098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 59%|█████▉    | 19/32 [00:07<00:04,  2.80it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.79it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.75it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s]W0402 10:02:09.530629 1609098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.63it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.64it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s]3_q proxy err 0.00031451942049898207 tr(WHW.T) 6654.2763671875
  0%|          | 0/32 [00:00<?, ?it/s] 41%|████      | 13/32 [00:05<00:07,  2.63it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.67it/s] 50%|█████     | 16/32 [00:06<00:06,  2.63it/s]  3%|▎         | 1/32 [00:01<00:37,  1.21s/it] 88%|████████▊ | 28/32 [00:11<00:01,  2.68it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.64it/s]  6%|▋         | 2/32 [00:01<00:21,  1.41it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.69it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s]  9%|▉         | 3/32 [00:01<00:16,  1.81it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.69it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.08it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.69it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
I0402 10:02:13.452905 1607628 finetune.py:45] layer 0_k initial loss 0.00013393524568527937
W0402 10:02:13.453120 1607628 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:03<00:10,  2.45it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.66it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.67it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.68it/s]W0402 10:02:14.574966 1607628 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:04<00:08,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.67it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.69it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.74it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.71it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.72it/s]0_k proxy err 2.4896931790863164e-05 tr(WHW.T) 2167.80859375
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.69it/s] 41%|████      | 13/32 [00:05<00:06,  2.75it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.68it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.76it/s]  3%|▎         | 1/32 [00:00<00:24,  1.25it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.78it/s]  6%|▋         | 2/32 [00:01<00:16,  1.84it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.65it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s]  9%|▉         | 3/32 [00:01<00:13,  2.16it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.64it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.79it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.36it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
 56%|█████▋    | 18/32 [00:07<00:05,  2.80it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.48it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.79it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.56it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.80it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.61it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.67it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.77it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.78it/s]I0402 10:02:20.104553 1608016 finetune.py:45] layer 1_k initial loss 0.004415975417941809
W0402 10:02:20.105002 1608016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:04<00:07,  2.70it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.71it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.82it/s] 41%|████      | 13/32 [00:05<00:07,  2.71it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.82it/s]W0402 10:02:21.231595 1608016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:05<00:06,  2.72it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.83it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.72it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.84it/s] 50%|█████     | 16/32 [00:06<00:05,  2.72it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.83it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.72it/s]1_k proxy err 2.3573264115839265e-05 tr(WHW.T) 3947.499267578125
  0%|          | 0/32 [00:00<?, ?it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.81it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s]  3%|▎         | 1/32 [00:00<00:25,  1.20it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.72it/s]  6%|▋         | 2/32 [00:01<00:17,  1.75it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s]  9%|▉         | 3/32 [00:01<00:14,  2.05it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.72it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.23it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.72it/s]I0402 10:02:24.598692 1608429 finetune.py:45] layer 2_k initial loss 0.0006893260288052261
W0402 10:02:24.598869 1608429 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.77it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.40it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.50it/s]W0402 10:02:25.590201 1608429 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 81%|████████▏ | 26/32 [00:09<00:02,  2.80it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.57it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.81it/s]2_k proxy err 0.00023496942594647408 tr(WHW.T) 3891.80859375
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.88it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.67it/s]  3%|▎         | 1/32 [00:00<00:23,  1.31it/s]100%|██████████| 32/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]
 41%|████      | 13/32 [00:05<00:07,  2.70it/s]  6%|▋         | 2/32 [00:01<00:15,  1.91it/s]  9%|▉         | 3/32 [00:01<00:13,  2.20it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.35it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.48it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.56it/s]I0402 10:02:29.286453 1609098 finetune.py:45] layer 3_k initial loss 0.001383663504384458
W0402 10:02:29.286782 1609098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 53%|█████▎    | 17/32 [00:06<00:05,  2.67it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.64it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s]W0402 10:02:30.271422 1609098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:03<00:08,  2.63it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.65it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.62it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.61it/s]3_k proxy err 0.00033460219856351614 tr(WHW.T) 3660.859619140625
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.61it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.57it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.57it/s]  3%|▎         | 1/32 [00:00<00:24,  1.28it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.60it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.57it/s]  6%|▋         | 2/32 [00:01<00:16,  1.87it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.58it/s]  9%|▉         | 3/32 [00:01<00:13,  2.18it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.58it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.36it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.59it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.59it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.48it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.59it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.57it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.58it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.63it/s]I0402 10:02:34.495464 1607628 finetune.py:45] layer 0_o initial loss 0.00013380312884692103
W0402 10:02:34.495747 1607628 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 20/32 [00:07<00:04,  2.61it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s]W0402 10:02:35.393234 1607628 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.73it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.67it/s] 41%|████      | 13/32 [00:05<00:06,  2.76it/s]0_o proxy err 0.00030140142189338803 tr(WHW.T) 0.23014843463897705
  0%|          | 0/32 [00:00<?, ?it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.76it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.77it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.68it/s] 50%|█████     | 16/32 [00:06<00:05,  2.77it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.71it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.73it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.78it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.71it/s]  3%|▎         | 1/32 [00:01<00:57,  1.85s/it] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.66it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
 66%|██████▌   | 21/32 [00:08<00:03,  2.77it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s]  6%|▋         | 2/32 [00:03<00:48,  1.62s/it] 72%|███████▏  | 23/32 [00:08<00:03,  2.75it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.77it/s]  9%|▉         | 3/32 [00:04<00:44,  1.54s/it] 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.78it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.75it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s] 12%|█▎        | 4/32 [00:06<00:42,  1.51s/it] 97%|█████████▋| 31/32 [00:11<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
I0402 10:02:43.564707 1608016 finetune.py:45] layer 1_o initial loss 0.004062773194164038
W0402 10:02:43.566108 1608016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it]W0402 10:02:45.236470 1608016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:09<00:38,  1.47s/it]I0402 10:02:46.163556 1608429 finetune.py:45] layer 2_o initial loss 0.0006964720669202507
W0402 10:02:46.163775 1608429 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

1_o proxy err 0.0011716936714947224 tr(WHW.T) 0.31429046392440796
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it]W0402 10:02:47.053928 1608429 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_o proxy err 0.0011449179146438837 tr(WHW.T) 0.5573468208312988
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:11<00:34,  1.42s/it]  3%|▎         | 1/32 [00:02<01:04,  2.09s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.40s/it]I0402 10:02:50.007711 1609098 finetune.py:45] layer 3_o initial loss 0.0014003405813127756
W0402 10:02:50.007923 1609098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:59,  1.92s/it]  6%|▋         | 2/32 [00:03<00:52,  1.76s/it]W0402 10:02:51.024592 1609098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:14<00:30,  1.40s/it]  6%|▋         | 2/32 [00:03<00:49,  1.64s/it]  9%|▉         | 3/32 [00:05<00:47,  1.65s/it]3_o proxy err 0.0018263363745063543 tr(WHW.T) 0.9357947111129761
  0%|          | 0/32 [00:00<?, ?it/s] 34%|███▍      | 11/32 [00:15<00:29,  1.39s/it]  9%|▉         | 3/32 [00:04<00:45,  1.55s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 38%|███▊      | 12/32 [00:17<00:27,  1.39s/it]  3%|▎         | 1/32 [00:01<01:00,  1.94s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.51s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it] 41%|████      | 13/32 [00:18<00:26,  1.39s/it]  6%|▋         | 2/32 [00:03<00:49,  1.66s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 44%|████▍     | 14/32 [00:20<00:24,  1.39s/it]  9%|▉         | 3/32 [00:04<00:45,  1.56s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.47s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.39s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 50%|█████     | 16/32 [00:22<00:22,  1.38s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.50s/it] 25%|██▌       | 8/32 [00:12<00:34,  1.46s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.50s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.38s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.45s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.48s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.38s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.48s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.45s/it] 34%|███▍      | 11/32 [00:17<00:30,  1.47s/it] 59%|█████▉    | 19/32 [00:27<00:17,  1.38s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.44s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.38s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.44s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.38s/it] 41%|████      | 13/32 [00:19<00:27,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.46s/it] 41%|████      | 13/32 [00:19<00:27,  1.44s/it] 69%|██████▉   | 22/32 [00:31<00:13,  1.38s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.46s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.44s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.38s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.44s/it] 75%|███████▌  | 24/32 [00:33<00:11,  1.38s/it] 50%|█████     | 16/32 [00:24<00:23,  1.47s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it] 50%|█████     | 16/32 [00:23<00:23,  1.44s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.38s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.45s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.38s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.45s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it] 84%|████████▍ | 27/32 [00:38<00:06,  1.38s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.45s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.45s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.38s/it] 50%|█████     | 16/32 [00:23<00:23,  1.46s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.44s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.46s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.38s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 66%|██████▌   | 21/32 [00:31<00:15,  1.45s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.39s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.45s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.45s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.39s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.46s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.45s/it] 72%|███████▏  | 23/32 [00:34<00:12,  1.44s/it]100%|██████████| 32/32 [00:45<00:00,  1.39s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]
 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.44s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.45s/it] 66%|██████▌   | 21/32 [00:31<00:15,  1.45s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.46s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.45s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.48s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.46s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.44s/it] 81%|████████▏ | 26/32 [00:38<00:09,  1.50s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.45s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.52s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.50s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.44s/it] 88%|████████▊ | 28/32 [00:41<00:06,  1.53s/it]I0402 10:03:29.970765 1607628 finetune.py:45] layer 0_up initial loss 0.00013301325088832527
W0402 10:03:29.970976 1607628 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:43<00:04,  1.51s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.45s/it]W0402 10:03:30.814896 1607628 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:42<00:04,  1.51s/it] 94%|█████████▍| 30/32 [00:44<00:03,  1.51s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.45s/it]0_up proxy err 0.002578686224296689 tr(WHW.T) 101.63944244384766
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:44<00:03,  1.50s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.48s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.45s/it]  3%|▎         | 1/32 [00:01<00:56,  1.81s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.48s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
 91%|█████████ | 29/32 [00:42<00:04,  1.46s/it]  6%|▋         | 2/32 [00:03<00:47,  1.58s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
 94%|█████████▍| 30/32 [00:44<00:02,  1.45s/it]  9%|▉         | 3/32 [00:04<00:44,  1.53s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.46s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.51s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]100%|██████████| 32/32 [00:46<00:00,  1.47s/it]
 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.48s/it]I0402 10:03:43.486325 1608016 finetune.py:45] layer 1_up initial loss 0.004027043469250202
W0402 10:03:43.487426 1608016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 10:03:43.779201 1608429 finetune.py:45] layer 2_up initial loss 0.0006936431745998561
W0402 10:03:43.779609 1608429 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:12<00:35,  1.48s/it]W0402 10:03:44.475804 1608016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 10:03:44.671461 1608429 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it]1_up proxy err 0.003046379890292883 tr(WHW.T) 159.80691528320312
  0%|          | 0/32 [00:00<?, ?it/s]2_up proxy err 0.0035589532926678658 tr(WHW.T) 225.97427368164062
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it]I0402 10:03:47.047452 1609098 finetune.py:45] layer 3_up initial loss 0.0013952483423054218
W0402 10:03:47.047843 1609098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:56,  1.84s/it]  3%|▎         | 1/32 [00:01<00:57,  1.87s/it]W0402 10:03:47.938665 1609098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:16<00:29,  1.43s/it]  6%|▋         | 2/32 [00:03<00:48,  1.60s/it]  6%|▋         | 2/32 [00:03<00:49,  1.64s/it]3_up proxy err 0.0034292417112737894 tr(WHW.T) 316.0293273925781
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it]  9%|▉         | 3/32 [00:04<00:44,  1.52s/it]  9%|▉         | 3/32 [00:04<00:45,  1.56s/it] 41%|████      | 13/32 [00:19<00:26,  1.41s/it]  3%|▎         | 1/32 [00:01<00:57,  1.87s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.53s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.41s/it]  6%|▋         | 2/32 [00:03<00:48,  1.62s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.50s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.41s/it]  9%|▉         | 3/32 [00:04<00:44,  1.55s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.46s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.50s/it] 50%|█████     | 16/32 [00:23<00:22,  1.41s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.51s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.40s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.45s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.48s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.40s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.40s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.45s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.40s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.46s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.44s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.40s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.47s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.44s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.40s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 31%|███▏      | 10/32 [00:14<00:32,  1.46s/it] 41%|████      | 13/32 [00:19<00:27,  1.44s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.40s/it] 41%|████      | 13/32 [00:19<00:27,  1.47s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.44s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.40s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.47s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.45s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.44s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.41s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.47s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 50%|█████     | 16/32 [00:23<00:23,  1.44s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.40s/it] 50%|█████     | 16/32 [00:23<00:23,  1.47s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.44s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.40s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.45s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.44s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.40s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it] 50%|█████     | 16/32 [00:23<00:23,  1.46s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.45s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.40s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.46s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.39s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.46s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.46s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.39s/it] 66%|██████▌   | 21/32 [00:30<00:16,  1.47s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.46s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.46s/it]100%|██████████| 32/32 [00:45<00:00,  1.39s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]
 69%|██████▉   | 22/32 [00:32<00:14,  1.46s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.46s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.47s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it] 66%|██████▌   | 21/32 [00:30<00:16,  1.46s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.46s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.46s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.46s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.48s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.45s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.45s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.50s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.45s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.45s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.45s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.50s/it]I0402 10:04:26.160988 1607628 finetune.py:45] layer 0_gate initial loss 0.00013216305524110794
W0402 10:04:26.161232 1607628 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 28/32 [00:40<00:05,  1.45s/it]W0402 10:04:26.914687 1607628 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 81%|████████▏ | 26/32 [00:38<00:08,  1.45s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.49s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.45s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.46s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.45s/it]0_gate proxy err 0.0017805562820285559 tr(WHW.T) 179.7012481689453
  0%|          | 0/112 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:41<00:05,  1.46s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it]  1%|          | 1/112 [00:00<01:28,  1.25it/s] 97%|█████████▋| 31/32 [00:45<00:01,  1.46s/it]  2%|▏         | 2/112 [00:01<00:58,  1.87it/s]  3%|▎         | 3/112 [00:01<00:48,  2.22it/s] 91%|█████████ | 29/32 [00:42<00:04,  1.45s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.46s/it]  4%|▎         | 4/112 [00:01<00:44,  2.44it/s]  4%|▍         | 5/112 [00:02<00:42,  2.55it/s]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
  5%|▌         | 6/112 [00:02<00:39,  2.65it/s]  6%|▋         | 7/112 [00:02<00:38,  2.71it/s] 94%|█████████▍| 30/32 [00:44<00:02,  1.45s/it]100%|██████████| 32/32 [00:47<00:00,  1.46s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
  7%|▋         | 8/112 [00:03<00:38,  2.70it/s]  8%|▊         | 9/112 [00:03<00:38,  2.70it/s]  9%|▉         | 10/112 [00:04<00:37,  2.69it/s] 10%|▉         | 11/112 [00:04<00:37,  2.69it/s] 97%|█████████▋| 31/32 [00:45<00:01,  1.46s/it] 11%|█         | 12/112 [00:04<00:37,  2.69it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.71it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.75it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.77it/s]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]100%|██████████| 32/32 [00:46<00:00,  1.47s/it]
 14%|█▍        | 16/112 [00:06<00:34,  2.78it/s] 15%|█▌        | 17/112 [00:06<00:33,  2.80it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.77it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.75it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.74it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.73it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.72it/s] 21%|██        | 23/112 [00:09<00:39,  2.25it/s] 21%|██▏       | 24/112 [00:09<00:37,  2.37it/s] 22%|██▏       | 25/112 [00:09<00:35,  2.46it/s] 23%|██▎       | 26/112 [00:10<00:34,  2.52it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.58it/s] 25%|██▌       | 28/112 [00:10<00:32,  2.61it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.63it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.65it/s]I0402 10:04:41.990813 1608429 finetune.py:45] layer 2_gate initial loss 0.0006890447693876922
W0402 10:04:41.991086 1608429 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 31/112 [00:11<00:30,  2.67it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.68it/s]I0402 10:04:42.490661 1608016 finetune.py:45] layer 1_gate initial loss 0.003958530258387327
W0402 10:04:42.491154 1608016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 29%|██▉       | 33/112 [00:12<00:29,  2.71it/s]W0402 10:04:42.866777 1608429 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 30%|███       | 34/112 [00:13<00:28,  2.76it/s]W0402 10:04:43.353239 1608016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 35/112 [00:13<00:27,  2.79it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.78it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.75it/s]I0402 10:04:44.508373 1609098 finetune.py:45] layer 3_gate initial loss 0.0013865692308172584
W0402 10:04:44.508596 1609098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 38/112 [00:14<00:26,  2.75it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.76it/s] 36%|███▌      | 40/112 [00:15<00:25,  2.78it/s]W0402 10:04:45.286788 1609098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_gate proxy err 0.002310567069798708 tr(WHW.T) 449.34307861328125
  0%|          | 0/112 [00:00<?, ?it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.81it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.83it/s] 38%|███▊      | 43/112 [00:16<00:24,  2.84it/s]  1%|          | 1/112 [00:00<01:31,  1.22it/s] 39%|███▉      | 44/112 [00:16<00:23,  2.84it/s]  2%|▏         | 2/112 [00:01<01:00,  1.80it/s]1_gate proxy err 0.0021768473088741302 tr(WHW.T) 270.8945617675781
  0%|          | 0/112 [00:00<?, ?it/s] 40%|████      | 45/112 [00:16<00:23,  2.83it/s]  3%|▎         | 3/112 [00:01<00:51,  2.12it/s] 41%|████      | 46/112 [00:17<00:23,  2.84it/s]  4%|▎         | 4/112 [00:01<00:46,  2.33it/s]  1%|          | 1/112 [00:00<01:34,  1.18it/s] 42%|████▏     | 47/112 [00:17<00:22,  2.85it/s]  4%|▍         | 5/112 [00:02<00:43,  2.46it/s]  2%|▏         | 2/112 [00:01<01:03,  1.74it/s] 43%|████▎     | 48/112 [00:18<00:22,  2.85it/s]3_gate proxy err 0.0018716187914833426 tr(WHW.T) 875.7509765625
  0%|          | 0/112 [00:00<?, ?it/s]  5%|▌         | 6/112 [00:02<00:41,  2.54it/s]  3%|▎         | 3/112 [00:01<00:52,  2.09it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.85it/s]  6%|▋         | 7/112 [00:03<00:40,  2.60it/s]  4%|▎         | 4/112 [00:01<00:46,  2.30it/s] 45%|████▍     | 50/112 [00:18<00:21,  2.83it/s]  7%|▋         | 8/112 [00:03<00:39,  2.63it/s]  1%|          | 1/112 [00:00<01:30,  1.22it/s] 46%|████▌     | 51/112 [00:19<00:21,  2.84it/s]  4%|▍         | 5/112 [00:02<00:43,  2.43it/s]  8%|▊         | 9/112 [00:03<00:38,  2.65it/s]  2%|▏         | 2/112 [00:01<01:00,  1.81it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.84it/s]  5%|▌         | 6/112 [00:02<00:41,  2.53it/s]  9%|▉         | 10/112 [00:04<00:37,  2.69it/s]  3%|▎         | 3/112 [00:01<00:50,  2.14it/s] 47%|████▋     | 53/112 [00:19<00:20,  2.85it/s]  6%|▋         | 7/112 [00:03<00:40,  2.59it/s] 10%|▉         | 11/112 [00:04<00:37,  2.69it/s]  4%|▎         | 4/112 [00:01<00:46,  2.34it/s] 48%|████▊     | 54/112 [00:20<00:20,  2.83it/s]  7%|▋         | 8/112 [00:03<00:39,  2.63it/s] 11%|█         | 12/112 [00:04<00:37,  2.69it/s]  4%|▍         | 5/112 [00:02<00:43,  2.46it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.83it/s]  8%|▊         | 9/112 [00:03<00:38,  2.65it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.69it/s]  5%|▌         | 6/112 [00:02<00:41,  2.53it/s] 50%|█████     | 56/112 [00:20<00:19,  2.83it/s]  9%|▉         | 10/112 [00:04<00:38,  2.66it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.70it/s]  6%|▋         | 7/112 [00:03<00:40,  2.57it/s] 51%|█████     | 57/112 [00:21<00:19,  2.84it/s] 10%|▉         | 11/112 [00:04<00:37,  2.68it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.71it/s]  7%|▋         | 8/112 [00:03<00:39,  2.63it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.84it/s] 11%|█         | 12/112 [00:04<00:37,  2.70it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.72it/s]  8%|▊         | 9/112 [00:03<00:38,  2.66it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.84it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.71it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.70it/s]  9%|▉         | 10/112 [00:04<00:38,  2.68it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.82it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.71it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.83it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.71it/s] 10%|▉         | 11/112 [00:04<00:37,  2.69it/s] 13%|█▎        | 15/112 [00:06<00:35,  2.72it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.83it/s] 11%|█         | 12/112 [00:04<00:36,  2.71it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.71it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.72it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.84it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.72it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.71it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.72it/s] 57%|█████▋    | 64/112 [00:23<00:16,  2.85it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.73it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.72it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.72it/s] 58%|█████▊    | 65/112 [00:24<00:16,  2.85it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.73it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.71it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.69it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.83it/s] 21%|██        | 23/112 [00:08<00:32,  2.71it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.68it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.66it/s] 60%|█████▉    | 67/112 [00:24<00:15,  2.84it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.72it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.68it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.67it/s] 61%|██████    | 68/112 [00:25<00:15,  2.85it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.73it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.69it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.68it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.86it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.73it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.71it/s] 21%|██        | 23/112 [00:08<00:33,  2.68it/s] 62%|██████▎   | 70/112 [00:25<00:14,  2.87it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.73it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.71it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.87it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.70it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.74it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.72it/s] 64%|██████▍   | 72/112 [00:26<00:13,  2.86it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.70it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.73it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.72it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.86it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.70it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.73it/s] 21%|██        | 23/112 [00:08<00:32,  2.73it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.86it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.68it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.73it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.71it/s] 67%|██████▋   | 75/112 [00:27<00:12,  2.85it/s] 25%|██▌       | 28/112 [00:10<00:31,  2.69it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.74it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.72it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.86it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.69it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.74it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.70it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.86it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.71it/s] 30%|███       | 34/112 [00:12<00:28,  2.73it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.71it/s] 70%|██████▉   | 78/112 [00:28<00:11,  2.85it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.72it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.73it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.72it/s] 71%|███████   | 79/112 [00:28<00:11,  2.86it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.73it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.74it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.73it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.86it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.73it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.74it/s] 72%|███████▏  | 81/112 [00:29<00:10,  2.87it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.72it/s] 30%|███       | 34/112 [00:13<00:28,  2.74it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.75it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.87it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.73it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.74it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.74it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.87it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.73it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.74it/s] 75%|███████▌  | 84/112 [00:30<00:09,  2.85it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.73it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.71it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.73it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.85it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.73it/s] 30%|███       | 34/112 [00:12<00:28,  2.72it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.71it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.85it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.73it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.68it/s] 35%|███▍      | 39/112 [00:14<00:27,  2.69it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.85it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.73it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.70it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.71it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.85it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.73it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.70it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.72it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.85it/s] 40%|████      | 45/112 [00:16<00:24,  2.73it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.72it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.72it/s] 80%|████████  | 90/112 [00:32<00:07,  2.85it/s] 41%|████      | 46/112 [00:17<00:24,  2.73it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.72it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.72it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.84it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.72it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.71it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.85it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.74it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.73it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.72it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.86it/s] 40%|████      | 45/112 [00:17<00:24,  2.74it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.70it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.73it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.86it/s] 41%|████      | 46/112 [00:17<00:24,  2.73it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.72it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.71it/s] 85%|████████▍ | 95/112 [00:34<00:05,  2.87it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.74it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.73it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.72it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.87it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.72it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.73it/s] 40%|████      | 45/112 [00:17<00:24,  2.71it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.84it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.67it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.74it/s] 41%|████      | 46/112 [00:17<00:24,  2.71it/s] 88%|████████▊ | 98/112 [00:35<00:04,  2.86it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.70it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.74it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.72it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.87it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.71it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.74it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.72it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.87it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.72it/s] 50%|█████     | 56/112 [00:20<00:20,  2.73it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.73it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.86it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.71it/s] 51%|█████     | 57/112 [00:21<00:20,  2.72it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.84it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.72it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.72it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.72it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.83it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.72it/s] 49%|████▉     | 55/112 [00:20<00:21,  2.71it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.72it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.84it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.71it/s] 50%|█████     | 56/112 [00:21<00:20,  2.71it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.72it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.83it/s] 47%|████▋     | 53/112 [00:19<00:22,  2.67it/s] 51%|█████     | 57/112 [00:21<00:20,  2.68it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.72it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.85it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.68it/s] 52%|█████▏    | 58/112 [00:21<00:20,  2.69it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.86it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.73it/s] 49%|████▉     | 55/112 [00:20<00:21,  2.69it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.70it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.86it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.73it/s] 50%|█████     | 56/112 [00:21<00:20,  2.70it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.71it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.86it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.73it/s] 51%|█████     | 57/112 [00:21<00:20,  2.71it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.73it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.86it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.73it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.72it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.73it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.86it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.74it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.71it/s]100%|██████████| 112/112 [00:40<00:00,  2.86it/s]100%|██████████| 112/112 [00:40<00:00,  2.77it/s]
 56%|█████▋    | 63/112 [00:23<00:17,  2.73it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.73it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.72it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.73it/s] 61%|██████    | 68/112 [00:25<00:16,  2.71it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.71it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.72it/s] 62%|██████▏   | 69/112 [00:25<00:16,  2.67it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.71it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.73it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.65it/s] 56%|█████▋    | 63/112 [00:23<00:18,  2.72it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.74it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.63it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.71it/s] 61%|██████    | 68/112 [00:25<00:16,  2.72it/s] 64%|██████▍   | 72/112 [00:26<00:15,  2.65it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.71it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.73it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.65it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.72it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.74it/s] 66%|██████▌   | 74/112 [00:27<00:14,  2.66it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.72it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.73it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.68it/s] 61%|██████    | 68/112 [00:25<00:16,  2.73it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.74it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.70it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.73it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.74it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.71it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.72it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.72it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.71it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.70it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.69it/s] 71%|███████   | 79/112 [00:29<00:12,  2.72it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.70it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.70it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.72it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.71it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.71it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.73it/s] 66%|██████▌   | 74/112 [00:27<00:14,  2.71it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.71it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.73it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.71it/s] 71%|███████   | 79/112 [00:29<00:12,  2.70it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.71it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.70it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.70it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.71it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.71it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.71it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.72it/s]W0402 10:05:17.331000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.331000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.331000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.331000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.331000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.331000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.332000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 70%|██████▉   | 78/112 [00:29<00:12,  2.72it/s]W0402 10:05:17.372000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.372000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.372000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.372000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.372000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 73%|███████▎  | 82/112 [00:30<00:11,  2.71it/s]W0402 10:05:17.539000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.539000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.539000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.539000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.539000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 77%|███████▋  | 86/112 [00:32<00:09,  2.71it/s] 71%|███████   | 79/112 [00:29<00:12,  2.71it/s]W0402 10:05:17.837000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.837000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.837000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.838000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.838000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.838000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.838000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.869000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.869000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.869000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.869000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.869000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 74%|███████▍  | 83/112 [00:31<00:10,  2.66it/s]W0402 10:05:17.935000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.935000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.935000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.935000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:17.935000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 87/112 [00:32<00:09,  2.72it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.72it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.69it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.73it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.72it/s] 76%|███████▌  | 85/112 [00:31<00:10,  2.70it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.72it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.72it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.71it/s] 80%|████████  | 90/112 [00:33<00:08,  2.72it/s]W0402 10:05:19.185000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 74%|███████▍  | 83/112 [00:31<00:10,  2.71it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.69it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.73it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.71it/s]W0402 10:05:19.621000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:19.621000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:05:19.622000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:19.622000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:19.622000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:19.622000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:19.622000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:19.652000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:19.652000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:19.652000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:19.652000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:19.652000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 79%|███████▊  | 88/112 [00:32<00:08,  2.70it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.70it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.72it/s]W0402 10:05:20.007000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:20.007000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:20.007000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:20.007000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:20.007000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 79%|███████▉  | 89/112 [00:33<00:08,  2.71it/s] 83%|████████▎ | 93/112 [00:34<00:07,  2.71it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.73it/s] 80%|████████  | 90/112 [00:33<00:08,  2.72it/s]W0402 10:05:20.517000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:20.522000 139815443773248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 94/112 [00:35<00:06,  2.72it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.73it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.73it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.70it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.72it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.73it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.71it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.71it/s] 83%|████████▎ | 93/112 [00:34<00:07,  2.71it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.72it/s] 80%|████████  | 90/112 [00:33<00:08,  2.67it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.70it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.72it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.69it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.72it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.70it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.71it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.72it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.66it/s] 83%|████████▎ | 93/112 [00:34<00:07,  2.70it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.73it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.64it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.72it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.74it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.63it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.73it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.75it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.73it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.62it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.76it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.74it/s] 93%|█████████▎| 104/112 [00:38<00:03,  2.62it/s] 90%|█████████ | 101/112 [00:37<00:03,  2.75it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.74it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.61it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.76it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.73it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.60it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.77it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.74it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.60it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.77it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.74it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.60it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.77it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.75it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.60it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.76it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.73it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.75it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.60it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.74it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.75it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.60it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.75it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.77it/s]100%|██████████| 112/112 [00:41<00:00,  2.60it/s]100%|██████████| 112/112 [00:41<00:00,  2.67it/s]
 95%|█████████▍| 106/112 [00:39<00:02,  2.75it/s]I0402 10:05:27.688647 1607628 finetune.py:45] layer 0_down initial loss 0.0001316997513640672
W0402 10:05:27.688913 1607628 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 98%|█████████▊| 110/112 [00:40<00:00,  2.77it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.72it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.75it/s]W0402 10:05:28.188174 1607628 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 96%|█████████▋| 108/112 [00:40<00:01,  2.70it/s]100%|██████████| 112/112 [00:41<00:00,  2.72it/s]100%|██████████| 112/112 [00:41<00:00,  2.69it/s]
0_down proxy err 0.0019570032600313425 tr(WHW.T) 0.4814107120037079
 97%|█████████▋| 109/112 [00:40<00:01,  2.71it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.67it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.64it/s]100%|██████████| 112/112 [00:41<00:00,  2.62it/s]100%|██████████| 112/112 [00:41<00:00,  2.68it/s]
W0402 10:05:34.456000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.456000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.456000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.456000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.457000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.457000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.457000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.498000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.499000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.499000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.499000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.499000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.668000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.668000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.668000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.668000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.668000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.985000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.985000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.985000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.985000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.985000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.986000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:34.986000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.017000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.017000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.017000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.017000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.017000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.086000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.086000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.086000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.086000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.087000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.405000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.405000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.406000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.406000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.406000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.406000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.406000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.445000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.445000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.445000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.445000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.445000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.610000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.610000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.610000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.610000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.610000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.906000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.906000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.906000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.906000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.906000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.906000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.907000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.939000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.939000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.939000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.939000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:35.939000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.005000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.005000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.005000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.005000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.005000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.349000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.528000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.528000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.528000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.528000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.529000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.529000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.529000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.569000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.569000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.569000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.569000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.569000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.734000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.734000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.734000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.734000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.734000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.786000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.786000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.787000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.787000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.787000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.787000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.787000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.816000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.816000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.816000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.816000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:36.816000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.035000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.035000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.035000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.035000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.036000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.036000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.036000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.066000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.066000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.066000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.066000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.066000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.131000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.131000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.131000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.131000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.131000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.185000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.185000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.185000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.185000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.185000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.245000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.674000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.674000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.675000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.675000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.675000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.675000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.675000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.703000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.704000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.705000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.705000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.705000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.705000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:37.709000 140521970001728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.061000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.061000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.061000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.061000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.061000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.373000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.564000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.569000 140531481601856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.815000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.816000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.816000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.816000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.816000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.816000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.816000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.846000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.846000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.846000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.846000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:38.846000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:39.204000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:05:39.204000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:39.205000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:05:39.205000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:05:39.205000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:05:39.713000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:05:39.718000 139734914078528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0402 10:05:44.681351 1608429 finetune.py:45] layer 2_down initial loss 0.0006863057496957481
W0402 10:05:44.681510 1608429 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:05:45.142270 1608429 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

I0402 10:05:45.442089 1608016 finetune.py:45] layer 1_down initial loss 0.003956494852900505
W0402 10:05:45.442344 1608016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

2_down proxy err 0.0033020596019923687 tr(WHW.T) 1.2005820274353027
W0402 10:05:45.933296 1608016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

1_down proxy err 5.787615373264998e-05 tr(WHW.T) 67.10469055175781
I0402 10:05:46.618202 1609098 finetune.py:45] layer 3_down initial loss 0.0013775532133877277
W0402 10:05:46.618448 1609098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:05:47.155283 1609098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

3_down proxy err 0.0036684824153780937 tr(WHW.T) 2.1166553497314453
I0402 10:05:50.104082 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 4 in 1.5804035663604736s
I0402 10:05:50.620306 1606370 quantize_finetune_llama.py:159] layer 5 gpu 1
I0402 10:05:52.524795 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 5 in 1.4716849327087402s
I0402 10:05:52.592742 1613124 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:05:52.592858 1613124 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:05:52.592922 1613124 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:05:52.798618 1613124 config.py:58] PyTorch version 2.4.0 available.
I0402 10:05:53.050097 1606370 quantize_finetune_llama.py:159] layer 6 gpu 2
I0402 10:05:54.626652 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 6 in 1.161191701889038s
I0402 10:05:55.073760 1613124 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 10:05:55.122666 1606370 quantize_finetune_llama.py:159] layer 7 gpu 3
I0402 10:05:55.143247 1613291 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:05:55.143481 1613291 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:05:55.143553 1613291 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:05:55.417143 1613291 config.py:58] PyTorch version 2.4.0 available.
W0402 10:05:55.422603 1613124 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:05:56.722254 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 7 in 1.1435327529907227s
I0402 10:05:57.132455 1613698 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:05:57.132569 1613698 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:05:57.132646 1613698 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:05:57.162999 1606370 quantize_finetune_llama.py:159] layer 8 gpu 0
I0402 10:05:57.345773 1613698 config.py:58] PyTorch version 2.4.0 available.
I0402 10:05:57.709649 1613291 data_utils.py:336] using 256 training seqs, 128 validation seqs
  3%|▎         | 1/32 [00:01<00:52,  1.69s/it]W0402 10:05:58.167546 1613291 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:02<00:26,  1.12it/s]  9%|▉         | 3/32 [00:02<00:18,  1.57it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.93it/s]  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:05:59.149823 1614016 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:05:59.149927 1614016 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:05:59.149986 1614016 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:05:59.332103 1614016 config.py:58] PyTorch version 2.4.0 available.
 16%|█▌        | 5/32 [00:03<00:12,  2.21it/s]I0402 10:05:59.517792 1613698 data_utils.py:336] using 256 training seqs, 128 validation seqs
 19%|█▉        | 6/32 [00:03<00:10,  2.41it/s]W0402 10:05:59.893104 1613698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 25%|██▌       | 8/32 [00:04<00:08,  2.67it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.80it/s]  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:58,  1.88s/it] 31%|███▏      | 10/32 [00:04<00:07,  2.87it/s]  6%|▋         | 2/32 [00:02<00:29,  1.00it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.88it/s]I0402 10:06:01.437620 1614016 data_utils.py:336] using 256 training seqs, 128 validation seqs
  9%|▉         | 3/32 [00:02<00:20,  1.41it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.90it/s]W0402 10:06:01.795504 1614016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:02<00:16,  1.74it/s] 41%|████      | 13/32 [00:05<00:06,  2.88it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.93it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.02it/s]  3%|▎         | 1/32 [00:01<00:55,  1.80s/it]  0%|          | 0/32 [00:00<?, ?it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.96it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.26it/s]  6%|▋         | 2/32 [00:02<00:28,  1.07it/s] 50%|█████     | 16/32 [00:06<00:05,  2.99it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.42it/s]  9%|▉         | 3/32 [00:02<00:19,  1.51it/s] 53%|█████▎    | 17/32 [00:07<00:04,  3.02it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.54it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.88it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.03it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.63it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.18it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.04it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.70it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.40it/s] 62%|██████▎   | 20/32 [00:08<00:03,  3.04it/s]  3%|▎         | 1/32 [00:01<00:55,  1.79s/it] 34%|███▍      | 11/32 [00:05<00:07,  2.75it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.02it/s]  6%|▋         | 2/32 [00:02<00:28,  1.07it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.79it/s] 25%|██▌       | 8/32 [00:04<00:08,  2.71it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.03it/s]  9%|▉         | 3/32 [00:02<00:19,  1.52it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.79it/s] 41%|████      | 13/32 [00:06<00:06,  2.80it/s] 72%|███████▏  | 23/32 [00:09<00:02,  3.02it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.88it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.85it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.81it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.03it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.17it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.91it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.82it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.03it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.40it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.94it/s] 50%|█████     | 16/32 [00:07<00:05,  2.84it/s] 81%|████████▏ | 26/32 [00:10<00:01,  3.04it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 41%|████      | 13/32 [00:05<00:06,  2.96it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.84it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.04it/s] 25%|██▌       | 8/32 [00:04<00:08,  2.69it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.99it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.85it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.04it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.77it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.99it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.85it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.05it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.83it/s] 50%|█████     | 16/32 [00:06<00:05,  2.99it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.04it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.84it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.87it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.99it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.05it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.84it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.91it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.00it/s]100%|██████████| 32/32 [00:11<00:00,  3.07it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
 69%|██████▉   | 22/32 [00:09<00:03,  2.85it/s] 41%|████      | 13/32 [00:05<00:06,  2.94it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.00it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.82it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.90it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.95it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.82it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.91it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.95it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.79it/s] 50%|█████     | 16/32 [00:06<00:05,  2.89it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.93it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.79it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.90it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.94it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.82it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.93it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.97it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.95it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.84it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.98it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.97it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.86it/s] 81%|████████▏ | 26/32 [00:10<00:01,  3.00it/s]W0402 10:06:11.042000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.042000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.042000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.042000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.042000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.042000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.042000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.067000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.068000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.068000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.068000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.068000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:03,  2.98it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.87it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.01it/s]W0402 10:06:11.348000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.348000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.348000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.348000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.348000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  2.98it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.86it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.00it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.97it/s] 91%|█████████ | 29/32 [00:11<00:00,  3.00it/s]100%|██████████| 32/32 [00:12<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
W0402 10:06:11.949000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.949000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.949000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.949000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.949000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.949000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.950000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.970000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.970000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.970000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.970000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:11.970000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:12.168000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:12.168000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:12.168000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:12.168000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:12.168000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:09<00:02,  2.95it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.01it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.93it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.02it/s]100%|██████████| 32/32 [00:12<00:00,  3.04it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
 81%|████████▏ | 26/32 [00:10<00:02,  2.93it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.93it/s]W0402 10:06:13.319000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:13.319000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:13.319000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:13.319000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:13.319000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:13.319000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:13.319000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:13.338000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:13.338000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:13.338000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:13.338000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:13.338000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  2.92it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.93it/s]W0402 10:06:13.953000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:13.953000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:13.953000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:13.953000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:13.954000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.92it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.91it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 10:06:14.712000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:14.712000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:14.712000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:14.712000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:14.712000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:14.712000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:14.712000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:14.737000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:14.737000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:14.737000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:14.738000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:14.738000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.93it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
W0402 10:06:15.014000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.014000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.014000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.014000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.014000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.621000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.621000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.621000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.621000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.621000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.621000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.621000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.639000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.639000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.639000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.640000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.640000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.844000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.844000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.844000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.844000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:15.844000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.111000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.111000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.112000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.112000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.112000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.112000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.112000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.139000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.139000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.139000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.139000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.139000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.438000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.438000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.438000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.438000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.438000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.992000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.992000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.992000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.992000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.992000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.992000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:16.992000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.011000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.011000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.011000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.011000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.011000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.069000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.070000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.070000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.070000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.070000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.070000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.070000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.088000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.088000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.088000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.088000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.088000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.294000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.294000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.294000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.294000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.294000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.649000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.649000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.649000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.649000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:17.649000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.059000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.059000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.059000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.059000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.059000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.060000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.060000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.087000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.087000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.087000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.087000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.087000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.391000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.391000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.391000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.391000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.391000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 10:06:18.473000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.473000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.474000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.474000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.474000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.474000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.474000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.492000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.492000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.492000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.492000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:18.493000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.023000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.023000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.023000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.023000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.023000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.023000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.023000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.042000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.042000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.042000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.042000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.042000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.132000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.132000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.132000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.132000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.132000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.250000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.250000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.250000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.250000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:19.250000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0402 10:06:20.096795 1613124 finetune.py:45] layer 4_v initial loss 0.0014856344787403941
W0402 10:06:20.097136 1613124 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:06:20.431000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:20.431000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:20.431000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:20.431000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:20.431000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:20.431000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:20.432000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:20.450000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:20.450000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:20.450000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:20.450000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:20.450000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:21.101000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:21.101000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:21.101000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:21.102000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:21.102000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:21.282150 1613124 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
4_v proxy err 0.002547520911321044 tr(WHW.T) 38.379119873046875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.35s/it]I0402 10:06:23.938746 1613291 finetune.py:45] layer 5_v initial loss 0.0011536570964381099
W0402 10:06:23.939086 1613291 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:01<00:23,  1.29it/s]  9%|▉         | 3/32 [00:02<00:17,  1.69it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.98it/s]W0402 10:06:25.173808 1613291 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:02<00:12,  2.18it/s]I0402 10:06:25.617149 1613698 finetune.py:45] layer 6_v initial loss 0.0019046380184590816
W0402 10:06:25.617539 1613698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:03<00:11,  2.33it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s]5_v proxy err 0.002414981834590435 tr(WHW.T) 37.700050354003906
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s]W0402 10:06:26.587188 1613698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:04<00:09,  2.55it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s]I0402 10:06:27.450400 1614016 finetune.py:45] layer 7_v initial loss 0.0020045486744493246
W0402 10:06:27.450625 1614016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:05<00:08,  2.62it/s]6_v proxy err 0.002533421153202653 tr(WHW.T) 42.837894439697266
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.34s/it] 38%|███▊      | 12/32 [00:05<00:07,  2.68it/s]  6%|▋         | 2/32 [00:01<00:23,  1.29it/s] 41%|████      | 13/32 [00:05<00:07,  2.70it/s]W0402 10:06:28.394156 1614016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:02<00:17,  1.66it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.71it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.93it/s]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it] 47%|████▋     | 15/32 [00:06<00:06,  2.74it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.14it/s]  6%|▋         | 2/32 [00:01<00:22,  1.36it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s]7_v proxy err 0.0021123685874044895 tr(WHW.T) 53.14668655395508
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.29it/s]  9%|▉         | 3/32 [00:01<00:16,  1.77it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.78it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.39it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.06it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.80it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.47it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.27it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.80it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it] 19%|█▉        | 6/32 [00:03<00:10,  2.41it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.81it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s]  6%|▋         | 2/32 [00:01<00:22,  1.34it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.81it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s]  9%|▉         | 3/32 [00:02<00:16,  1.74it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.58it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.82it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.61it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.03it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.82it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.22it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.68it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.37it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.82it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.63it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.71it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.82it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.83it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 50%|█████     | 16/32 [00:07<00:06,  2.64it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.73it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.80it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.59it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.63it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.82it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.75it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.64it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s] 50%|█████     | 16/32 [00:06<00:05,  2.75it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.82it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.76it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.67it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
 56%|█████▋    | 18/32 [00:07<00:05,  2.77it/s] 41%|████      | 13/32 [00:05<00:07,  2.70it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.69it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.61it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.69it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.70it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.59it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.69it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.67it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.68it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.69it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.71it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.64it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.71it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.71it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.73it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.74it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.72it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.64it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.75it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.73it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.66it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.77it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s]100%|██████████| 32/32 [00:13<00:00,  2.66it/s]100%|██████████| 32/32 [00:13<00:00,  2.44it/s]
 94%|█████████▍| 30/32 [00:11<00:00,  2.78it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.73it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.74it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
 84%|████████▍ | 27/32 [00:10<00:01,  2.65it/s]W0402 10:06:40.524000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.524000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.524000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.524000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.525000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.525000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.525000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.556000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.556000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.556000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.556000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.556000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:11<00:01,  2.65it/s]W0402 10:06:40.724000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.725000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.725000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.725000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.725000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.955000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.955000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.955000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.955000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.955000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.955000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.955000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.976000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.976000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.976000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.976000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:40.976000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:41.042000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:41.042000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:41.042000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:41.042000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:41.042000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.66it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.64it/s]W0402 10:06:42.070000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
W0402 10:06:42.381000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:42.381000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:42.381000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:42.381000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:42.381000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:42.381000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:42.381000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:42.401000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:42.401000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:42.401000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:42.401000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:42.401000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:42.656000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:42.656000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:42.656000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:42.657000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:42.657000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:43.123000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.138000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.139000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.139000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.139000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.139000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.139000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.139000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.170000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.170000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.170000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.170000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.170000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.338000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.338000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.338000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.338000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.339000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.569000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.569000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.569000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.569000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.569000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.569000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.570000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.592000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.592000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.592000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.592000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.592000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.658000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.658000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.658000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.658000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.658000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.976000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.977000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.977000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.977000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.977000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.977000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:45.977000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.008000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.008000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.008000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.008000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.008000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.175000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.175000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.176000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.176000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.176000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.404000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.404000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.404000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.404000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.404000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.404000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.404000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.427000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.427000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.427000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.427000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.427000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.493000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.493000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.493000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.493000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.493000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:46.723000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.056000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.056000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.056000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.057000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.057000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.057000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.057000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.079000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.080000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.080000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.080000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.080000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.331000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.331000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.331000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.331000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.331000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.553000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.795000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.875000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.876000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.876000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.876000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.876000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.876000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.876000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.900000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.900000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.900000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.900000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.900000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.982000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.983000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.983000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.983000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.983000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.983000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:47.983000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.013000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.013000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.013000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.013000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.013000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.156000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.156000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.156000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.156000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.156000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.180000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.180000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.180000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.180000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.180000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.406000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.406000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.406000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.406000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.407000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.407000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.407000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.429000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.429000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.429000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.429000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.429000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.494000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.495000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.495000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.495000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.495000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:48.622000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0402 10:06:49.510668 1613124 finetune.py:45] layer 4_q initial loss 0.0014852371532469988
W0402 10:06:49.510991 1613124 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:06:49.555000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:49.877000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:06:49.878000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:49.878000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:49.878000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:49.878000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:06:49.878000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:49.878000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:49.901000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:49.901000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:49.902000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:49.902000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:49.902000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:50.155000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:06:50.155000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:06:50.155000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:06:50.155000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:06:50.155000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:06:50.525257 1613124 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 10:06:50.622000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
4_q proxy err 0.00025184600963257253 tr(WHW.T) 6741.380859375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.27s/it]  6%|▋         | 2/32 [00:01<00:22,  1.35it/s]  9%|▉         | 3/32 [00:02<00:16,  1.76it/s]I0402 10:06:53.906874 1613291 finetune.py:45] layer 5_q initial loss 0.001153207034803927
W0402 10:06:53.907129 1613291 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 4/32 [00:02<00:13,  2.04it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.25it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.40it/s]W0402 10:06:55.018060 1613291 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:03<00:10,  2.50it/s]I0402 10:06:55.428690 1613698 finetune.py:45] layer 6_q initial loss 0.001904107048176229
W0402 10:06:55.428932 1613698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.63it/s]5_q proxy err 0.0003501696919556707 tr(WHW.T) 6497.876953125
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.69it/s]W0402 10:06:56.577055 1613698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 38%|███▊      | 12/32 [00:05<00:07,  2.71it/s]I0402 10:06:57.210758 1614016 finetune.py:45] layer 7_q initial loss 0.0020050399471074343
W0402 10:06:57.211030 1614016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:05<00:06,  2.72it/s]  3%|▎         | 1/32 [00:01<00:39,  1.27s/it]6_q proxy err 0.0003953645355068147 tr(WHW.T) 6019.40673828125
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.76it/s]  6%|▋         | 2/32 [00:01<00:22,  1.34it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.77it/s]  9%|▉         | 3/32 [00:02<00:16,  1.72it/s]W0402 10:06:58.197714 1614016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.98it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.79it/s]  3%|▎         | 1/32 [00:01<00:37,  1.22s/it] 16%|█▌        | 5/32 [00:02<00:12,  2.19it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.83it/s]  6%|▋         | 2/32 [00:01<00:21,  1.41it/s]7_q proxy err 0.0003819434787146747 tr(WHW.T) 6036.439453125
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.34it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s]  9%|▉         | 3/32 [00:01<00:15,  1.82it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.86it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.12it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.86it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.31it/s]  3%|▎         | 1/32 [00:01<00:37,  1.21s/it] 28%|██▊       | 9/32 [00:04<00:09,  2.55it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.87it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.45it/s]  6%|▋         | 2/32 [00:01<00:21,  1.39it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.86it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s]  9%|▉         | 3/32 [00:01<00:16,  1.80it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.88it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.61it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.08it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.88it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.62it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.88it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s] 41%|████      | 13/32 [00:05<00:07,  2.63it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.40it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.87it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.50it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.74it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.86it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.56it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 41%|████      | 13/32 [00:05<00:06,  2.75it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.86it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.61it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.68it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.78it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.87it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
 34%|███▍      | 11/32 [00:04<00:07,  2.68it/s] 50%|█████     | 16/32 [00:06<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.68it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.66it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.63it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.64it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.77it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.66it/s] 50%|█████     | 16/32 [00:06<00:05,  2.67it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.75it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.60it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.70it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.59it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.69it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.62it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.59it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.68it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.58it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.63it/s]100%|██████████| 32/32 [00:13<00:00,  2.58it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
 94%|█████████▍| 30/32 [00:11<00:00,  2.68it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
 84%|████████▍ | 27/32 [00:10<00:01,  2.63it/s]I0402 10:07:10.433723 1613124 finetune.py:45] layer 4_k initial loss 0.0014847824349999428
W0402 10:07:10.434060 1613124 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.63it/s]W0402 10:07:11.483252 1613124 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
4_k proxy err 0.00025032766279764473 tr(WHW.T) 3941.66064453125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.18it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s]  9%|▉         | 3/32 [00:01<00:13,  2.09it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.29it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s]I0402 10:07:15.464686 1613291 finetune.py:45] layer 5_k initial loss 0.0011527173919603229
W0402 10:07:15.464878 1613291 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:02<00:10,  2.49it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s]W0402 10:07:16.544734 1613291 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:03<00:08,  2.63it/s]I0402 10:07:16.671405 1613698 finetune.py:45] layer 6_k initial loss 0.00190359924454242
W0402 10:07:16.671774 1613698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s]5_k proxy err 0.0003237711207475513 tr(WHW.T) 4151.982421875
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.67it/s]W0402 10:07:17.748098 1613698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 13/32 [00:05<00:07,  2.68it/s]  3%|▎         | 1/32 [00:00<00:25,  1.21it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.68it/s]I0402 10:07:18.725977 1614016 finetune.py:45] layer 7_k initial loss 0.002004750771448016
W0402 10:07:18.726191 1614016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s]  9%|▉         | 3/32 [00:01<00:13,  2.07it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s]6_k proxy err 0.00030531667289324105 tr(WHW.T) 4418.953125
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.28it/s]W0402 10:07:19.667308 1614016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.76it/s]  3%|▎         | 1/32 [00:00<00:24,  1.29it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s]  6%|▋         | 2/32 [00:01<00:15,  1.88it/s]7_k proxy err 0.0003150570555590093 tr(WHW.T) 4611.79638671875
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.80it/s]  9%|▉         | 3/32 [00:01<00:13,  2.18it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.82it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.34it/s]  3%|▎         | 1/32 [00:00<00:23,  1.30it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.82it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.48it/s]  6%|▋         | 2/32 [00:01<00:15,  1.88it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.82it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s]  9%|▉         | 3/32 [00:01<00:13,  2.17it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.58it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.36it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.63it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.81it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.47it/s] 41%|████      | 13/32 [00:05<00:07,  2.63it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.82it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.66it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.54it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.64it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.83it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.83it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.83it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.69it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.67it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.65it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s] 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.69it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.65it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.64it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.61it/s] 50%|█████     | 16/32 [00:06<00:05,  2.75it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.65it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.61it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.77it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.67it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.63it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.63it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.61it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.77it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.58it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.58it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.82it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.56it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.58it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.81it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.56it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.56it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.57it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
 78%|███████▊  | 25/32 [00:09<00:02,  2.57it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.57it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.60it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.56it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.57it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
I0402 10:07:31.814354 1613124 finetune.py:45] layer 4_o initial loss 0.0014674840494990349
W0402 10:07:31.814563 1613124 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:11<00:01,  2.60it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.60it/s]W0402 10:07:32.816973 1613124 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
4_o proxy err 0.002111291978508234 tr(WHW.T) 1.3690094947814941
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.99s/it]I0402 10:07:37.037629 1613291 finetune.py:45] layer 5_o initial loss 0.0011500605614855886
W0402 10:07:37.037898 1613291 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:03<00:50,  1.69s/it]W0402 10:07:38.042060 1613291 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

5_o proxy err 0.0020324077922850847 tr(WHW.T) 1.8119544982910156
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it]I0402 10:07:39.542905 1613698 finetune.py:45] layer 6_o initial loss 0.001900803647004068
W0402 10:07:39.543405 1613698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:07:40.587484 1613698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it]I0402 10:07:40.816811 1614016 finetune.py:45] layer 7_o initial loss 0.001966819865629077
W0402 10:07:40.817024 1614016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:02<01:02,  2.02s/it]W0402 10:07:41.692262 1614016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_o proxy err 0.002662677550688386 tr(WHW.T) 2.5981621742248535
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it]7_o proxy err 0.0025035839062184095 tr(WHW.T) 3.787720203399658
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:09<00:38,  1.46s/it]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it]  9%|▉         | 3/32 [00:05<00:47,  1.63s/it]  3%|▎         | 1/32 [00:01<01:00,  1.95s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it]  6%|▋         | 2/32 [00:03<00:49,  1.66s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it]  6%|▋         | 2/32 [00:03<00:50,  1.67s/it] 25%|██▌       | 8/32 [00:12<00:34,  1.44s/it]  9%|▉         | 3/32 [00:04<00:45,  1.57s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it]  9%|▉         | 3/32 [00:04<00:45,  1.58s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.53s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.43s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.48s/it] 41%|████      | 13/32 [00:19<00:27,  1.42s/it] 25%|██▌       | 8/32 [00:12<00:34,  1.45s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.43s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.45s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.43s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it] 31%|███▏      | 10/32 [00:14<00:32,  1.46s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 50%|█████     | 16/32 [00:23<00:22,  1.43s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.45s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.43s/it] 41%|████      | 13/32 [00:20<00:28,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.45s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.42s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.50s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.46s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.50s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.45s/it] 62%|██████▎   | 20/32 [00:29<00:16,  1.41s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it] 50%|█████     | 16/32 [00:24<00:23,  1.50s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.45s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.41s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.50s/it] 50%|█████     | 16/32 [00:23<00:23,  1.44s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 50%|█████     | 16/32 [00:23<00:23,  1.46s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.44s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.41s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.50s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.45s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.41s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.44s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.50s/it] 78%|███████▊  | 25/32 [00:36<00:09,  1.41s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.46s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.44s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.50s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.41s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.46s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.44s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.50s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.41s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.46s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.44s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.49s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.40s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.46s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.44s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.40s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.46s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.45s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.50s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.41s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.46s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.45s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.50s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.42s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.46s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.44s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.50s/it]100%|██████████| 32/32 [00:46<00:00,  1.41s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 81%|████████▏ | 26/32 [00:38<00:08,  1.46s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.44s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.44s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.44s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.48s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.52s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.43s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.48s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.52s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.43s/it] 94%|█████████▍| 30/32 [00:44<00:03,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
100%|██████████| 32/32 [00:46<00:00,  1.43s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
 97%|█████████▋| 31/32 [00:46<00:01,  1.52s/it]100%|██████████| 32/32 [00:47<00:00,  1.52s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
I0402 10:08:30.472641 1613124 finetune.py:45] layer 4_up initial loss 0.001456958008930087
W0402 10:08:30.472871 1613124 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:08:31.452782 1613124 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_up proxy err 0.003344194730743766 tr(WHW.T) 400.241455078125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:58,  1.89s/it]I0402 10:08:35.701884 1613291 finetune.py:45] layer 5_up initial loss 0.001144213485531509
W0402 10:08:35.702260 1613291 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:03<00:48,  1.62s/it]I0402 10:08:36.595753 1613698 finetune.py:45] layer 6_up initial loss 0.0018847963074222207
W0402 10:08:36.596133 1613698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:08:36.624646 1613291 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 10:08:37.453370 1613698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:04<00:44,  1.53s/it]5_up proxy err 0.0032687403727322817 tr(WHW.T) 497.4553527832031
  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:08:38.371809 1614016 finetune.py:45] layer 7_up initial loss 0.0019521998474374413
W0402 10:08:38.371996 1614016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

6_up proxy err 0.0030882256105542183 tr(WHW.T) 571.9508666992188
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:06<00:41,  1.48s/it]W0402 10:08:39.168823 1614016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:59,  1.92s/it]7_up proxy err 0.002878134371712804 tr(WHW.T) 651.590087890625
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:07<00:39,  1.45s/it]  3%|▎         | 1/32 [00:01<00:56,  1.82s/it]  6%|▋         | 2/32 [00:03<00:49,  1.66s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it]  6%|▋         | 2/32 [00:03<00:47,  1.59s/it]  3%|▎         | 1/32 [00:01<00:57,  1.86s/it]  9%|▉         | 3/32 [00:04<00:45,  1.58s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.43s/it]  9%|▉         | 3/32 [00:04<00:43,  1.52s/it]  6%|▋         | 2/32 [00:03<00:48,  1.63s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it]  9%|▉         | 3/32 [00:04<00:44,  1.55s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.52s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.42s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.51s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.45s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.42s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.44s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.48s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it] 41%|████      | 13/32 [00:18<00:26,  1.41s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.48s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.41s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.41s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.43s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it] 50%|█████     | 16/32 [00:23<00:22,  1.41s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 41%|████      | 13/32 [00:19<00:27,  1.44s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.43s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.40s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 50%|█████     | 16/32 [00:23<00:23,  1.46s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.42s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.40s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.40s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.47s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.46s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.40s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.46s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.40s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.42s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.41s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.46s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.42s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.41s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.46s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.42s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.40s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.45s/it] 78%|███████▊  | 25/32 [00:36<00:09,  1.42s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.42s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.45s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.43s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.48s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.41s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.45s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.43s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]100%|██████████| 32/32 [00:45<00:00,  1.42s/it]
 81%|████████▏ | 26/32 [00:38<00:08,  1.46s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.43s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.49s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.48s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.43s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.48s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.42s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.49s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.49s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.42s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]100%|██████████| 32/32 [00:45<00:00,  1.44s/it]
 94%|█████████▍| 30/32 [00:44<00:03,  1.51s/it]100%|██████████| 32/32 [00:47<00:00,  1.51s/it]100%|██████████| 32/32 [00:47<00:00,  1.50s/it]
 97%|█████████▋| 31/32 [00:45<00:01,  1.51s/it]I0402 10:09:27.207870 1613124 finetune.py:45] layer 4_gate initial loss 0.0014495017239823937
W0402 10:09:27.208071 1613124 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:47<00:00,  1.51s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
W0402 10:09:27.978117 1613124 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_gate proxy err 0.0015063975006341934 tr(WHW.T) 1579.121337890625
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:36,  1.14it/s]I0402 10:09:32.546431 1613698 finetune.py:45] layer 6_gate initial loss 0.0018730016890913248
W0402 10:09:32.546724 1613698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  2%|▏         | 2/112 [00:01<01:03,  1.73it/s]  3%|▎         | 3/112 [00:01<00:52,  2.09it/s]W0402 10:09:33.303080 1613698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  4%|▎         | 4/112 [00:01<00:46,  2.32it/s]I0402 10:09:33.528744 1613291 finetune.py:45] layer 5_gate initial loss 0.0011308059329167008
W0402 10:09:33.528971 1613291 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  4%|▍         | 5/112 [00:02<00:43,  2.46it/s]  5%|▌         | 6/112 [00:02<00:41,  2.58it/s]W0402 10:09:34.368377 1613291 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 7/112 [00:03<00:40,  2.62it/s]  7%|▋         | 8/112 [00:03<00:39,  2.63it/s]  8%|▊         | 9/112 [00:03<00:38,  2.67it/s]  9%|▉         | 10/112 [00:04<00:37,  2.72it/s]I0402 10:09:35.819224 1614016 finetune.py:45] layer 7_gate initial loss 0.0019317572005093098
W0402 10:09:35.819488 1614016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 10%|▉         | 11/112 [00:04<00:36,  2.74it/s] 11%|█         | 12/112 [00:04<00:36,  2.78it/s]6_gate proxy err 0.0012135925935581326 tr(WHW.T) 2580.454833984375
  0%|          | 0/112 [00:00<?, ?it/s]W0402 10:09:36.526049 1614016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▏        | 13/112 [00:05<00:35,  2.79it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.77it/s]  1%|          | 1/112 [00:00<01:31,  1.22it/s]5_gate proxy err 0.0014434821205213666 tr(WHW.T) 1974.3070068359375
  0%|          | 0/112 [00:00<?, ?it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.78it/s]  2%|▏         | 2/112 [00:01<01:00,  1.81it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.77it/s]  3%|▎         | 3/112 [00:01<00:51,  2.12it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.77it/s]  1%|          | 1/112 [00:00<01:38,  1.12it/s]  4%|▎         | 4/112 [00:01<00:46,  2.34it/s] 16%|█▌        | 18/112 [00:07<00:33,  2.77it/s]  2%|▏         | 2/112 [00:01<01:04,  1.70it/s]  4%|▍         | 5/112 [00:02<00:43,  2.47it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.79it/s]  3%|▎         | 3/112 [00:01<00:53,  2.03it/s]  5%|▌         | 6/112 [00:02<00:41,  2.56it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.81it/s]  6%|▋         | 7/112 [00:03<00:39,  2.63it/s]  4%|▎         | 4/112 [00:02<00:48,  2.25it/s]7_gate proxy err 0.0011821237858384848 tr(WHW.T) 2636.09716796875
  0%|          | 0/112 [00:00<?, ?it/s] 19%|█▉        | 21/112 [00:08<00:32,  2.82it/s]  7%|▋         | 8/112 [00:03<00:38,  2.68it/s]  4%|▍         | 5/112 [00:02<00:44,  2.38it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.80it/s]  8%|▊         | 9/112 [00:03<00:38,  2.69it/s]  5%|▌         | 6/112 [00:02<00:42,  2.48it/s] 21%|██        | 23/112 [00:08<00:31,  2.79it/s]  1%|          | 1/112 [00:00<01:32,  1.20it/s]  9%|▉         | 10/112 [00:04<00:37,  2.71it/s]  6%|▋         | 7/112 [00:03<00:41,  2.53it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.79it/s]  2%|▏         | 2/112 [00:01<01:01,  1.78it/s] 10%|▉         | 11/112 [00:04<00:37,  2.72it/s]  7%|▋         | 8/112 [00:03<00:40,  2.58it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.78it/s]  3%|▎         | 3/112 [00:01<00:51,  2.13it/s] 11%|█         | 12/112 [00:04<00:36,  2.74it/s]  8%|▊         | 9/112 [00:03<00:39,  2.60it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.80it/s]  4%|▎         | 4/112 [00:01<00:46,  2.32it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.75it/s]  9%|▉         | 10/112 [00:04<00:38,  2.62it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.80it/s]  4%|▍         | 5/112 [00:02<00:43,  2.46it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.74it/s] 10%|▉         | 11/112 [00:04<00:38,  2.64it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.79it/s]  5%|▌         | 6/112 [00:02<00:41,  2.54it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.75it/s] 11%|█         | 12/112 [00:05<00:37,  2.64it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.80it/s]  6%|▋         | 7/112 [00:03<00:40,  2.59it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.76it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.80it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.66it/s]  7%|▋         | 8/112 [00:03<00:39,  2.63it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.76it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.81it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.67it/s]  8%|▊         | 9/112 [00:03<00:38,  2.66it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.77it/s] 29%|██▊       | 32/112 [00:12<00:28,  2.81it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.67it/s]  9%|▉         | 10/112 [00:04<00:38,  2.67it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.74it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.79it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.67it/s] 10%|▉         | 11/112 [00:04<00:37,  2.70it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.75it/s] 30%|███       | 34/112 [00:12<00:27,  2.80it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.67it/s] 11%|█         | 12/112 [00:04<00:36,  2.71it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.75it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.78it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.66it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.71it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.72it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.77it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.67it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.72it/s] 21%|██        | 23/112 [00:08<00:32,  2.72it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.77it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.66it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.71it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.74it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.79it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.66it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.71it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.75it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.79it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.67it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.72it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.76it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.81it/s] 21%|██        | 23/112 [00:09<00:33,  2.65it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.71it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.76it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.80it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.65it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.71it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.74it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.78it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.65it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.71it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.74it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.79it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.64it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.70it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.76it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.80it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.65it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.71it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.75it/s] 40%|████      | 45/112 [00:16<00:23,  2.80it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.65it/s] 21%|██        | 23/112 [00:08<00:32,  2.70it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.76it/s] 41%|████      | 46/112 [00:17<00:23,  2.81it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.63it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.69it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.74it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.79it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.65it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.71it/s] 30%|███       | 34/112 [00:12<00:28,  2.76it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.80it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.71it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.64it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.74it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.79it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.71it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.79it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.64it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.75it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.73it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.81it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.75it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.65it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.72it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.79it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.74it/s] 30%|███       | 34/112 [00:13<00:29,  2.64it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.79it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.72it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.74it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.65it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.77it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.73it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.72it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.66it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.78it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.73it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.72it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.65it/s] 50%|█████     | 56/112 [00:20<00:20,  2.77it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.73it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.72it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.65it/s] 51%|█████     | 57/112 [00:20<00:19,  2.79it/s] 30%|███       | 34/112 [00:12<00:28,  2.75it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.74it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.66it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.79it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.74it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.73it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.66it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.79it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.74it/s] 40%|████      | 45/112 [00:16<00:24,  2.75it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.66it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.80it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.74it/s] 41%|████      | 46/112 [00:17<00:23,  2.76it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.67it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.80it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.74it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.76it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.67it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.78it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.74it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.74it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.67it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.78it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.74it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.76it/s] 40%|████      | 45/112 [00:17<00:25,  2.67it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.79it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.74it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.76it/s] 41%|████      | 46/112 [00:17<00:24,  2.67it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.80it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.75it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.74it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.66it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.75it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.74it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.75it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.65it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.77it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.74it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.76it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.66it/s] 61%|██████    | 68/112 [00:24<00:15,  2.79it/s] 40%|████      | 45/112 [00:16<00:24,  2.75it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.75it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.66it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.80it/s] 41%|████      | 46/112 [00:17<00:24,  2.74it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.76it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.65it/s] 62%|██████▎   | 70/112 [00:25<00:14,  2.80it/s] 50%|█████     | 56/112 [00:20<00:20,  2.76it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.72it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.66it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.81it/s] 51%|█████     | 57/112 [00:21<00:19,  2.77it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.74it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.66it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.79it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.75it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.74it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.76it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.66it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.74it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.72it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.78it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.67it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.75it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.74it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.78it/s] 50%|█████     | 56/112 [00:21<00:20,  2.67it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.74it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.73it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.80it/s] 51%|█████     | 57/112 [00:21<00:20,  2.66it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.74it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.75it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.81it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.67it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.75it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.76it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.81it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.68it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.76it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.74it/s] 71%|███████   | 79/112 [00:28<00:11,  2.82it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.68it/s] 50%|█████     | 56/112 [00:20<00:20,  2.74it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.76it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.82it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.67it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.76it/s] 51%|█████     | 57/112 [00:21<00:20,  2.75it/s] 72%|███████▏  | 81/112 [00:29<00:10,  2.82it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.68it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.74it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.75it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.80it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.68it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.76it/s] 61%|██████    | 68/112 [00:25<00:15,  2.76it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.81it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.76it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.68it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.76it/s] 75%|███████▌  | 84/112 [00:30<00:09,  2.81it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.74it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.75it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.67it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.79it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.76it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.75it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.68it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.80it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.76it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.75it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.68it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.81it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.77it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.74it/s] 61%|██████    | 68/112 [00:26<00:16,  2.67it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.81it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.77it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.74it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.67it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.81it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.77it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.74it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.67it/s] 80%|████████  | 90/112 [00:32<00:07,  2.81it/s] 68%|██████▊   | 76/112 [00:28<00:12,  2.78it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.74it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.67it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.82it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.74it/s] 61%|██████    | 68/112 [00:25<00:16,  2.73it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.67it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.76it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.74it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.74it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.68it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.78it/s] 71%|███████   | 79/112 [00:29<00:11,  2.75it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.74it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.67it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.78it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.76it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.71it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.79it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.67it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.78it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.72it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.81it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.68it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.78it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.73it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.83it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.68it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.78it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.73it/s] 88%|████████▊ | 98/112 [00:35<00:04,  2.83it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.67it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.78it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.73it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.83it/s] 71%|███████   | 79/112 [00:30<00:12,  2.67it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.79it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.72it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.82it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.67it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.79it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.81it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.73it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.66it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.77it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.80it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.72it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.66it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.78it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.81it/s] 71%|███████   | 79/112 [00:29<00:12,  2.72it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.65it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.76it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.79it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.73it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.66it/s] 80%|████████  | 90/112 [00:33<00:07,  2.77it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.80it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.73it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.66it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.77it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.80it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.74it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.66it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.77it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.80it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.74it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.65it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.77it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.81it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.74it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.66it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.77it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.81it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.75it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.66it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.78it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.81it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.75it/s] 80%|████████  | 90/112 [00:34<00:08,  2.66it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.73it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.76it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.74it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.66it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.73it/s]100%|██████████| 112/112 [00:40<00:00,  2.77it/s]100%|██████████| 112/112 [00:40<00:00,  2.76it/s]
 79%|███████▊  | 88/112 [00:32<00:08,  2.73it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.74it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.66it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.74it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.69it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.64it/s] 80%|████████  | 90/112 [00:33<00:08,  2.74it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.64it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.65it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.75it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.65it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.64it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.75it/s] 86%|████████▌ | 96/112 [00:36<00:06,  2.65it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.67it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.75it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.70it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.64it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.75it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.69it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.65it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.74it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.68it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.65it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.74it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.70it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.74it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.65it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.72it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.74it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.65it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.73it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.75it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.67it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.76it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.75it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.67it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.76it/s] 90%|█████████ | 101/112 [00:37<00:03,  2.75it/s] 93%|█████████▎| 104/112 [00:39<00:03,  2.66it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.77it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.75it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.67it/s]100%|██████████| 112/112 [00:41<00:00,  2.78it/s]100%|██████████| 112/112 [00:41<00:00,  2.72it/s]
 92%|█████████▏| 103/112 [00:38<00:03,  2.77it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.68it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.76it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.69it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.75it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.69it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.74it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.69it/s]W0402 10:10:18.938000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:18.938000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:18.938000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:18.938000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:18.938000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:18.939000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:10:18.939000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:18.982000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:18.982000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:18.982000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:18.982000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:18.982000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 96%|█████████▌| 107/112 [00:39<00:01,  2.72it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.67it/s]W0402 10:10:19.160000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.160000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.160000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.160000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.160000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 96%|█████████▋| 108/112 [00:39<00:01,  2.73it/s]W0402 10:10:19.463000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.463000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.463000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.463000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.463000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.463000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.464000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 99%|█████████▉| 111/112 [00:42<00:00,  2.67it/s]W0402 10:10:19.492000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.493000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.493000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.493000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.493000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.564000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.564000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.564000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.564000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:19.564000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 109/112 [00:40<00:01,  2.73it/s]100%|██████████| 112/112 [00:42<00:00,  2.67it/s]100%|██████████| 112/112 [00:42<00:00,  2.63it/s]
 98%|█████████▊| 110/112 [00:40<00:00,  2.74it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.70it/s]W0402 10:10:20.837000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
100%|██████████| 112/112 [00:41<00:00,  2.65it/s]100%|██████████| 112/112 [00:41<00:00,  2.70it/s]
W0402 10:10:21.272000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:21.272000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:10:21.272000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:21.272000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:21.272000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:21.272000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:21.272000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:21.301000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:21.301000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:21.301000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:21.301000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:21.301000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:21.658000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:21.658000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:21.658000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:21.658000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:21.658000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:22.179000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:22.184000 140074805479232 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.096000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.096000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.096000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.096000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.096000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.097000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.097000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.140000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.140000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.140000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.140000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.140000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.323000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.323000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.323000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.323000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.323000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.652000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.652000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.652000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.652000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.653000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.653000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.653000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.686000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.686000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.686000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.686000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.686000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.758000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.758000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.758000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.759000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:24.759000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.114000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.369000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.370000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.370000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.370000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.370000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.370000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.370000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.412000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.412000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.412000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.412000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.412000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.577000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.577000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.577000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.577000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.577000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.582000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.582000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.582000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.582000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.583000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.583000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.583000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.616000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.616000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.616000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.616000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.616000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.877000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.877000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.878000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.878000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.878000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.878000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.878000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.910000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.910000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.910000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.910000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.910000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.979000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.979000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.980000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.980000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:26.980000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.005000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.005000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.005000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.005000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.006000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.557000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.563000 139895431530304 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.812000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.812000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.812000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.812000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.812000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.812000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.813000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.854000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.854000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.854000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.854000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:27.854000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.022000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.022000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.022000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.022000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.022000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.229000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.322000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.322000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.322000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.323000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.323000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.323000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.323000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.356000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.356000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.357000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.357000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.357000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.423000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.423000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.423000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.423000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.423000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.663000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.663000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.663000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.663000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.663000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.664000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.664000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.692000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.692000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.693000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.693000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:28.693000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:29.066000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:29.066000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:29.067000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:29.067000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:29.067000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
I0402 10:10:29.551151 1613124 finetune.py:45] layer 4_down initial loss 0.0014394760364666581
W0402 10:10:29.551487 1613124 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:10:29.574000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:29.579000 140301899487040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:29.680000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:30.076636 1613124 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0402 10:10:30.119000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:10:30.120000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:30.120000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:30.120000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:30.120000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:10:30.120000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:30.120000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:30.147000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:30.148000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:30.148000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:30.148000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:30.148000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:10:30.513000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:10:30.513000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:10:30.513000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:30.513000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:10:30.513000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
4_down proxy err 0.0037967313546687365 tr(WHW.T) 3.4171693325042725
W0402 10:10:31.026000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:10:31.031000 139925310940992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0402 10:10:34.510507 1613698 finetune.py:45] layer 6_down initial loss 0.00185528548900038
W0402 10:10:34.510761 1613698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:10:35.028561 1613698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

6_down proxy err 0.003567006206139922 tr(WHW.T) 6.227620601654053
I0402 10:10:36.421670 1613291 finetune.py:45] layer 5_down initial loss 0.0011239554733037949
W0402 10:10:36.421880 1613291 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:10:36.992723 1613291 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

5_down proxy err 0.0037240379024297 tr(WHW.T) 4.908621311187744
I0402 10:10:38.380366 1614016 finetune.py:45] layer 7_down initial loss 0.0019146426348015666
W0402 10:10:38.380723 1614016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:10:38.854000 1614016 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

7_down proxy err 0.0036739555653184652 tr(WHW.T) 6.803934097290039
I0402 10:10:41.294489 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 8 in 1.7841296195983887s
I0402 10:10:41.807760 1606370 quantize_finetune_llama.py:159] layer 9 gpu 1
I0402 10:10:43.423558 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 9 in 1.1854455471038818s
I0402 10:10:43.803582 1618546 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:10:43.803712 1618546 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:10:43.803775 1618546 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:10:43.951158 1606370 quantize_finetune_llama.py:159] layer 10 gpu 2
I0402 10:10:44.017580 1618546 config.py:58] PyTorch version 2.4.0 available.
I0402 10:10:45.620366 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 10 in 1.2193667888641357s
I0402 10:10:45.987305 1618708 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:10:45.987427 1618708 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:10:45.987490 1618708 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:10:46.162030 1606370 quantize_finetune_llama.py:159] layer 11 gpu 3
I0402 10:10:46.184535 1618708 config.py:58] PyTorch version 2.4.0 available.
I0402 10:10:46.192130 1618546 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 10:10:46.527600 1618546 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:10:47.776181 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 11 in 1.1708588600158691s
I0402 10:10:48.107025 1619072 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:10:48.107146 1619072 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:10:48.107206 1619072 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:10:48.281339 1606370 quantize_finetune_llama.py:159] layer 12 gpu 0
I0402 10:10:48.319210 1619072 config.py:58] PyTorch version 2.4.0 available.
I0402 10:10:48.404243 1618708 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 10:10:48.804920 1618708 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:53,  1.74s/it]  6%|▋         | 2/32 [00:02<00:27,  1.09it/s]  9%|▉         | 3/32 [00:02<00:18,  1.54it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.90it/s]  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:10:50.299842 1619583 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:10:50.299981 1619583 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:10:50.300049 1619583 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:10:50.511365 1619583 config.py:58] PyTorch version 2.4.0 available.
I0402 10:10:50.513126 1619072 data_utils.py:336] using 256 training seqs, 128 validation seqs
 16%|█▌        | 5/32 [00:03<00:13,  2.07it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.29it/s]W0402 10:10:50.943427 1619072 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.58it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.66it/s]  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:58,  1.90s/it] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s]  6%|▋         | 2/32 [00:02<00:30,  1.01s/it]I0402 10:10:52.655522 1619583 data_utils.py:336] using 256 training seqs, 128 validation seqs
 34%|███▍      | 11/32 [00:05<00:07,  2.79it/s]  9%|▉         | 3/32 [00:02<00:20,  1.41it/s]W0402 10:10:53.005190 1619583 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 38%|███▊      | 12/32 [00:05<00:07,  2.82it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.77it/s] 41%|████      | 13/32 [00:05<00:06,  2.83it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.06it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.87it/s]  3%|▎         | 1/32 [00:01<00:55,  1.80s/it] 19%|█▉        | 6/32 [00:03<00:11,  2.31it/s]  0%|          | 0/32 [00:00<?, ?it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.89it/s]  6%|▋         | 2/32 [00:02<00:28,  1.06it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 50%|█████     | 16/32 [00:06<00:05,  2.90it/s]  9%|▉         | 3/32 [00:02<00:19,  1.49it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.63it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.93it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.85it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.75it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.95it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.13it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.81it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.96it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.34it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.87it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.96it/s]  3%|▎         | 1/32 [00:01<00:56,  1.81s/it] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.91it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.97it/s]  6%|▋         | 2/32 [00:02<00:28,  1.06it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.65it/s] 41%|████      | 13/32 [00:05<00:06,  2.94it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.98it/s]  9%|▉         | 3/32 [00:02<00:19,  1.49it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.74it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.97it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.99it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.86it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.99it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.80it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.98it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.13it/s] 50%|█████     | 16/32 [00:06<00:05,  3.00it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.99it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.36it/s] 53%|█████▎    | 17/32 [00:07<00:04,  3.00it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.89it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.99it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.01it/s] 41%|████      | 13/32 [00:05<00:06,  2.92it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.98it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.63it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.01it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.93it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.98it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.72it/s] 62%|██████▎   | 20/32 [00:08<00:03,  3.02it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.94it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.99it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.03it/s] 50%|█████     | 16/32 [00:06<00:05,  2.95it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.98it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.83it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.02it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.95it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.99it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.86it/s] 72%|███████▏  | 23/32 [00:09<00:02,  3.03it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.96it/s]100%|██████████| 32/32 [00:12<00:00,  2.99it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
 41%|████      | 13/32 [00:05<00:06,  2.88it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.02it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.96it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.86it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.97it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.91it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.87it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.97it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.92it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.94it/s] 50%|█████     | 16/32 [00:06<00:05,  2.85it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.89it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.96it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.87it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.90it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.98it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.89it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.92it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.00it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.90it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.93it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.98it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.89it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.92it/s]W0402 10:11:02.423000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:02.423000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:02.423000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:02.423000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:02.423000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:02.423000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:02.423000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:02.449000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:02.449000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:02.449000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:02.449000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:02.449000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.99it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
 66%|██████▌   | 21/32 [00:08<00:03,  2.92it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.94it/s]W0402 10:11:02.730000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:02.730000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:02.730000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:02.730000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:02.730000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:09<00:03,  2.90it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.91it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.89it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.91it/s]W0402 10:11:03.351000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:03.351000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:03.352000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:03.352000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:03.352000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:03.352000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:03.352000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:03.370000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:03.370000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:03.370000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:03.370000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:03.370000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:03.578000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:03.578000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:03.578000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:03.578000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:03.578000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:09<00:02,  2.87it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.89it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.89it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.91it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.91it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
 84%|████████▍ | 27/32 [00:10<00:01,  2.89it/s]W0402 10:11:04.708000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:04.708000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:04.708000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:04.708000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:04.708000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:04.708000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:04.709000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:04.727000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:04.727000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:04.727000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:04.728000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:04.728000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:11<00:01,  2.88it/s]W0402 10:11:05.259000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.259000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.260000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.260000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.260000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.260000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.260000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.287000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.287000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.287000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.288000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.288000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.357000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.357000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.357000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.357000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.357000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.87it/s]W0402 10:11:05.581000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.581000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.581000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.582000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:05.582000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 97%|█████████▋| 31/32 [00:12<00:00,  2.88it/s]W0402 10:11:06.187000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:06.187000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:06.187000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:06.187000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:06.187000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:06.187000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:06.187000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:06.205000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:06.206000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:06.206000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:06.206000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:06.206000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
W0402 10:11:06.405000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:06.405000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:06.406000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:06.406000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:06.406000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.056000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.057000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.057000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.057000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.057000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.057000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.057000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.085000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.085000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.085000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.085000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.085000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.386000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.386000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.386000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.386000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.386000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.575000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.575000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.575000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.575000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.575000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.575000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.575000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.594000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.594000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.594000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.594000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:07.594000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.035000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.035000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.035000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.035000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.035000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.035000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.036000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.054000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.054000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.054000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.054000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.054000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.241000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.241000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.241000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.241000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.241000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.263000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.263000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.263000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.263000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:08.263000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 10:11:09.399000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.400000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.400000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.400000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.400000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.400000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.400000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.426000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.426000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.426000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.426000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.426000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.448000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.448000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.448000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.448000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.449000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.449000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.449000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.467000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.467000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.467000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.467000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.467000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.713000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.714000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.714000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.714000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:09.714000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.120000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.120000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.120000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.120000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.120000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.298000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.298000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.298000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.298000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.298000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.298000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.299000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.316000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.316000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.316000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.316000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.316000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.525000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.526000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.526000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.526000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:10.526000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 10:11:11.722000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:11.722000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:11.723000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:11.723000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:11.723000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:11.723000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:11.723000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:11.742000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:11.742000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:11.743000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:11.743000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:11.743000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
I0402 10:11:11.939446 1618546 finetune.py:45] layer 8_v initial loss 0.00248919241130352
W0402 10:11:11.939756 1618546 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:11:12.395000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:12.395000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:12.396000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:12.396000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:12.396000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:13.016872 1618546 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
8_v proxy err 0.00274472963064909 tr(WHW.T) 53.42280197143555
  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:11:14.549058 1618708 finetune.py:45] layer 9_v initial loss 0.0031509408727288246
W0402 10:11:14.550512 1618708 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:41,  1.33s/it]  6%|▋         | 2/32 [00:01<00:23,  1.30it/s]W0402 10:11:16.092448 1618708 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:02<00:17,  1.68it/s]I0402 10:11:16.405278 1619072 finetune.py:45] layer 10_v initial loss 0.002035470213741064
W0402 10:11:16.405612 1619072 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 4/32 [00:02<00:14,  1.96it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.16it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.30it/s]9_v proxy err 0.0027735563926398754 tr(WHW.T) 72.69827270507812
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.41it/s]W0402 10:11:17.956882 1619072 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:03<00:09,  2.47it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s]  3%|▎         | 1/32 [00:01<00:39,  1.27s/it] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s]10_v proxy err 0.00260100606828928 tr(WHW.T) 60.08286666870117
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:01<00:21,  1.37it/s]I0402 10:11:19.112328 1619583 finetune.py:45] layer 11_v initial loss 0.0026545198634266853
W0402 10:11:19.113533 1619583 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:05<00:08,  2.60it/s]  9%|▉         | 3/32 [00:01<00:16,  1.78it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.07it/s] 41%|████      | 13/32 [00:05<00:07,  2.63it/s]W0402 10:11:20.189470 1619583 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:02<00:11,  2.27it/s]  3%|▎         | 1/32 [00:01<00:40,  1.30s/it] 44%|████▍     | 14/32 [00:06<00:06,  2.63it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.44it/s]  6%|▋         | 2/32 [00:01<00:22,  1.33it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s]  9%|▉         | 3/32 [00:02<00:16,  1.75it/s] 50%|█████     | 16/32 [00:06<00:05,  2.70it/s]11_v proxy err 0.002135798567906022 tr(WHW.T) 74.2698974609375
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.04it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.72it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.71it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.23it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.31it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.72it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.76it/s]  3%|▎         | 1/32 [00:01<00:39,  1.27s/it] 22%|██▏       | 7/32 [00:03<00:10,  2.43it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.78it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.70it/s]  6%|▋         | 2/32 [00:01<00:22,  1.36it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s]  9%|▉         | 3/32 [00:01<00:16,  1.77it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.57it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.81it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.73it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.07it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.80it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.75it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.64it/s] 50%|█████     | 16/32 [00:06<00:05,  2.81it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.74it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.42it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.65it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.80it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.74it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.81it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.76it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.62it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.64it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.75it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.76it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.77it/s] 50%|█████     | 16/32 [00:06<00:06,  2.63it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.73it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.66it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.72it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.69it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.79it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
 59%|█████▉    | 19/32 [00:08<00:04,  2.67it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.72it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.70it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.81it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.68it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.68it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.81it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.82it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.65it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.71it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.66it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.72it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.82it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
 84%|████████▍ | 27/32 [00:11<00:01,  2.68it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.66it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.68it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.72it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.69it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.72it/s]W0402 10:11:32.719000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:32.719000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:32.719000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:32.719000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:32.719000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:32.719000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:32.720000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.70it/s]W0402 10:11:32.748000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:32.748000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:32.748000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:32.748000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:32.748000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:32.906000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:32.906000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:32.906000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:32.906000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:32.906000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s]W0402 10:11:33.119000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:33.120000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:33.120000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:33.120000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:33.120000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:33.120000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:33.120000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:33.139000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:33.139000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:33.139000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:33.139000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:33.139000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:33.201000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:33.201000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:33.201000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:33.201000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:33.202000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
W0402 10:11:34.236000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:34.559000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:34.559000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:34.559000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:34.559000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:34.559000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:34.559000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:34.559000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:34.582000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:34.582000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:34.582000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:34.582000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:34.582000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:34.826000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:34.826000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:34.826000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:34.826000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:34.826000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:35.270000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:35.636000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:35.637000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:35.637000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:35.637000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:35.637000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:35.637000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:35.637000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:35.666000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:35.666000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:35.666000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:35.666000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:35.666000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:35.821000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:35.821000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:35.821000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:35.821000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:35.821000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:36.042000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:36.042000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:36.042000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:36.042000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:36.042000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:36.043000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:36.043000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:36.062000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:36.062000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:36.062000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:36.062000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:36.062000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:36.122000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:36.123000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:36.123000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:36.123000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:36.123000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:37.107000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:37.406000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:37.406000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:37.406000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:37.406000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:37.406000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:37.406000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:37.406000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:37.428000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:37.428000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:37.428000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:37.428000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:37.428000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:37.664000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:37.665000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:37.665000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:37.665000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:37.665000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.019000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.019000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.019000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.019000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.020000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.020000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.020000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.048000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.048000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.048000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.048000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.048000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.093000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.206000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.206000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.206000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.206000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.206000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.416000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.417000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.417000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.417000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.417000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.417000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.417000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.438000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.438000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.438000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.438000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.438000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.500000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.500000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.500000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.501000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:38.501000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.528000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.575000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.576000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.576000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.576000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.576000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.576000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.576000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.606000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.606000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.606000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.606000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.606000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.771000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.771000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.771000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.771000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.771000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.837000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.837000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.837000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.837000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.837000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.837000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.838000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.860000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.860000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.860000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.860000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.860000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.993000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.993000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.993000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.993000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.994000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.994000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:39.994000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:40.014000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:40.014000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:40.014000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:40.014000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:40.014000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:40.079000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:40.079000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:40.079000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:40.079000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:40.079000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:40.111000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:40.111000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:40.112000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:40.112000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:40.112000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:40.577000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:41.138000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:41.457000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:41.457000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:41.457000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:11:41.458000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:41.458000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:41.458000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:11:41.458000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:41.480000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:41.480000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:41.480000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:41.480000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:41.480000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
I0402 10:11:41.706838 1618546 finetune.py:45] layer 8_q initial loss 0.002488252706825733
W0402 10:11:41.707154 1618546 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:11:41.730000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:41.730000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:11:41.731000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:11:41.731000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:11:41.731000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:11:42.182000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:11:42.708678 1618546 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_q proxy err 0.0004965208936482668 tr(WHW.T) 5514.4345703125
  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:11:44.726355 1618708 finetune.py:45] layer 9_q initial loss 0.003150091739371419
W0402 10:11:44.726531 1618708 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:38,  1.26s/it]  6%|▋         | 2/32 [00:01<00:22,  1.36it/s]  9%|▉         | 3/32 [00:01<00:16,  1.77it/s]W0402 10:11:45.735452 1618708 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:02<00:13,  2.05it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.24it/s]9_q proxy err 0.0005250570829957724 tr(WHW.T) 5306.75244140625
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s]I0402 10:11:47.441082 1619072 finetune.py:45] layer 10_q initial loss 0.0020358727779239416
W0402 10:11:47.441411 1619072 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:09,  2.54it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.59it/s]  3%|▎         | 1/32 [00:01<00:38,  1.26s/it] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s]  6%|▋         | 2/32 [00:01<00:21,  1.38it/s]W0402 10:11:48.537478 1619072 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s]  9%|▉         | 3/32 [00:01<00:16,  1.81it/s]I0402 10:11:48.801311 1619583 finetune.py:45] layer 11_q initial loss 0.00265352800488472
W0402 10:11:48.801614 1619583 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:05<00:07,  2.69it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.11it/s] 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s]10_q proxy err 0.0005590853979811072 tr(WHW.T) 5567.9375
  0%|          | 0/32 [00:00<?, ?it/s]W0402 10:11:49.691220 1619583 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:06<00:06,  2.76it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.49it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.74it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 50%|█████     | 16/32 [00:06<00:05,  2.75it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.67it/s]11_q proxy err 0.000562729372177273 tr(WHW.T) 5157.30615234375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:37,  1.23s/it] 53%|█████▎    | 17/32 [00:07<00:05,  2.77it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.69it/s]  6%|▋         | 2/32 [00:01<00:21,  1.38it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.79it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s]  9%|▉         | 3/32 [00:01<00:16,  1.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.78it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.07it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.80it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.79it/s]  3%|▎         | 1/32 [00:01<00:37,  1.20s/it] 66%|██████▌   | 21/32 [00:08<00:03,  2.80it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.25it/s] 41%|████      | 13/32 [00:05<00:06,  2.79it/s]  6%|▋         | 2/32 [00:01<00:21,  1.42it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.80it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.80it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.38it/s]  9%|▉         | 3/32 [00:01<00:15,  1.83it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.81it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.12it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.82it/s] 50%|█████     | 16/32 [00:06<00:05,  2.82it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.83it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.82it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.84it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.82it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.83it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.84it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.82it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.71it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.83it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.83it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.83it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.84it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.71it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.84it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.85it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.80it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
 75%|███████▌  | 24/32 [00:09<00:02,  2.84it/s] 41%|████      | 13/32 [00:05<00:06,  2.82it/s] 50%|█████     | 16/32 [00:06<00:05,  2.73it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.85it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.78it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.74it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.84it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.77it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.72it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.82it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.77it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.74it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.83it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.80it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.75it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.86it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.76it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.88it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.75it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 66%|██████▌   | 21/32 [00:08<00:04,  2.73it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.72it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.68it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.68it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.70it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.65it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.62it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.71it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s]I0402 10:12:02.688317 1618546 finetune.py:45] layer 8_k initial loss 0.0024878717958927155
W0402 10:12:02.688550 1618546 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 31/32 [00:12<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
W0402 10:12:03.729755 1618546 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_k proxy err 0.00036068540066480637 tr(WHW.T) 4670.25390625
  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:12:05.509696 1618708 finetune.py:45] layer 9_k initial loss 0.0031504896469414234
W0402 10:12:05.509959 1618708 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:00<00:24,  1.24it/s]  6%|▋         | 2/32 [00:01<00:16,  1.81it/s]  9%|▉         | 3/32 [00:01<00:13,  2.12it/s]W0402 10:12:06.549080 1618708 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:01<00:12,  2.30it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s]9_k proxy err 0.0003952164261136204 tr(WHW.T) 4332.2958984375
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s]  3%|▎         | 1/32 [00:00<00:24,  1.25it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s]  6%|▋         | 2/32 [00:01<00:16,  1.81it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s]  9%|▉         | 3/32 [00:01<00:13,  2.12it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s]I0402 10:12:09.557271 1619583 finetune.py:45] layer 11_k initial loss 0.00265315268188715
W0402 10:12:09.557447 1619583 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 10:12:09.560094 1619072 finetune.py:45] layer 10_k initial loss 0.002035334473475814
W0402 10:12:09.560355 1619072 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 4/32 [00:01<00:12,  2.29it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.67it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.45it/s] 41%|████      | 13/32 [00:05<00:07,  2.71it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.73it/s]W0402 10:12:10.461939 1619583 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 10:12:10.560606 1619072 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:03<00:09,  2.62it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.72it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s] 50%|█████     | 16/32 [00:06<00:05,  2.75it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.68it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.77it/s]11_k proxy err 0.000452801410574466 tr(WHW.T) 4189.68798828125
  0%|          | 0/32 [00:00<?, ?it/s]10_k proxy err 0.0003977615851908922 tr(WHW.T) 4710.4462890625
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.77it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s]  3%|▎         | 1/32 [00:00<00:23,  1.30it/s]  3%|▎         | 1/32 [00:00<00:24,  1.28it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.72it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s]  6%|▋         | 2/32 [00:01<00:15,  1.88it/s]  6%|▋         | 2/32 [00:01<00:16,  1.84it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s]  9%|▉         | 3/32 [00:01<00:13,  2.19it/s]  9%|▉         | 3/32 [00:01<00:13,  2.15it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.38it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.34it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.80it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.50it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.59it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.63it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.73it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.80it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.67it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.70it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.75it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.80it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.74it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.76it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.74it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.77it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.80it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.67it/s] 41%|████      | 13/32 [00:05<00:06,  2.75it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
 41%|████      | 13/32 [00:05<00:07,  2.67it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.71it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.77it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.71it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.75it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.68it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.71it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.76it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.77it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.72it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.76it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.77it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.68it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.66it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.65it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.65it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.64it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.64it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.65it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.64it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.64it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.59it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.58it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.63it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.56it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.55it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
I0402 10:12:23.842273 1618546 finetune.py:45] layer 8_o initial loss 0.0024923270102590322
W0402 10:12:23.842579 1618546 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:12<00:00,  2.55it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
W0402 10:12:24.746866 1618546 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_o proxy err 0.003282911842688918 tr(WHW.T) 3.732954978942871
  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:12:26.755278 1618708 finetune.py:45] layer 9_o initial loss 0.0030941320583224297
W0402 10:12:26.755608 1618708 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<01:01,  1.98s/it]W0402 10:12:27.904464 1618708 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

9_o proxy err 0.003336124587804079 tr(WHW.T) 4.439431190490723
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:03<00:50,  1.70s/it]I0402 10:12:30.603887 1619583 finetune.py:45] layer 11_o initial loss 0.002611143747344613
W0402 10:12:30.604119 1619583 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:04<00:46,  1.60s/it]  3%|▎         | 1/32 [00:01<01:01,  2.00s/it]W0402 10:12:31.502484 1619583 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 10:12:31.665728 1619072 finetune.py:45] layer 10_o initial loss 0.002001603599637747
W0402 10:12:31.665955 1619072 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 4/32 [00:06<00:42,  1.53s/it]11_o proxy err 0.0034064140636473894 tr(WHW.T) 4.454067230224609
  0%|          | 0/32 [00:00<?, ?it/s]W0402 10:12:32.625992 1619072 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:03<00:50,  1.68s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it]10_o proxy err 0.003292123321443796 tr(WHW.T) 4.188048362731934
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:04<00:45,  1.57s/it]  3%|▎         | 1/32 [00:01<00:58,  1.89s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.46s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it]  3%|▎         | 1/32 [00:01<01:00,  1.96s/it]  6%|▋         | 2/32 [00:03<00:48,  1.62s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.48s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it]  9%|▉         | 3/32 [00:04<00:44,  1.52s/it] 25%|██▌       | 8/32 [00:12<00:34,  1.44s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.46s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.48s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 25%|██▌       | 8/32 [00:12<00:34,  1.44s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.43s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.43s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 41%|████      | 13/32 [00:19<00:26,  1.42s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.42s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.42s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.51s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 41%|████      | 13/32 [00:19<00:27,  1.42s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.42s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.50s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 41%|████      | 13/32 [00:18<00:26,  1.42s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.42s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.42s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.50s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.43s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.50s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.42s/it] 50%|█████     | 16/32 [00:24<00:24,  1.50s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.42s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.42s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.50s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.43s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.42s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.42s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 78%|███████▊  | 25/32 [00:36<00:09,  1.42s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.42s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.50s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.42s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.50s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.42s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.42s/it] 78%|███████▊  | 25/32 [00:36<00:09,  1.42s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.42s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.50s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.42s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.42s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.50s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.42s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.42s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.42s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.42s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.42s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.42s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.42s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.42s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it]100%|██████████| 32/32 [00:46<00:00,  1.43s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 94%|█████████▍| 30/32 [00:43<00:02,  1.42s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.43s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.50s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.42s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.44s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.50s/it]100%|██████████| 32/32 [00:46<00:00,  1.42s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 94%|█████████▍| 30/32 [00:43<00:02,  1.44s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.50s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.46s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.51s/it]100%|██████████| 32/32 [00:46<00:00,  1.47s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 94%|█████████▍| 30/32 [00:45<00:03,  1.51s/it]I0402 10:13:20.432986 1618546 finetune.py:45] layer 8_up initial loss 0.002468830905854702
W0402 10:13:20.433430 1618546 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 31/32 [00:47<00:01,  1.53s/it]W0402 10:13:21.212474 1618546 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_up proxy err 0.0029298458248376846 tr(WHW.T) 668.69921875
  0%|          | 0/32 [00:00<?, ?it/s]100%|██████████| 32/32 [00:48<00:00,  1.54s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
I0402 10:13:23.543331 1618708 finetune.py:45] layer 9_up initial loss 0.0030768595170229673
W0402 10:13:23.543672 1618708 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:59,  1.92s/it]W0402 10:13:24.408350 1618708 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

9_up proxy err 0.0028207250870764256 tr(WHW.T) 723.7196044921875
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:03<00:50,  1.67s/it]I0402 10:13:26.576475 1619583 finetune.py:45] layer 11_up initial loss 0.00259460904635489
W0402 10:13:26.576690 1619583 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:04<00:45,  1.58s/it]W0402 10:13:27.415504 1619583 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:59,  1.91s/it]11_up proxy err 0.0028417501598596573 tr(WHW.T) 789.370361328125
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it]  6%|▋         | 2/32 [00:03<00:50,  1.67s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.48s/it]  3%|▎         | 1/32 [00:01<00:57,  1.84s/it]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it]I0402 10:13:31.148157 1619072 finetune.py:45] layer 10_up initial loss 0.0019774388056248426
W0402 10:13:31.148465 1619072 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:09<00:37,  1.45s/it]  6%|▋         | 2/32 [00:03<00:47,  1.60s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it]W0402 10:13:32.126187 1619072 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:10<00:36,  1.44s/it]  9%|▉         | 3/32 [00:04<00:44,  1.53s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.52s/it]10_up proxy err 0.002857413375750184 tr(WHW.T) 749.0686645507812
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.50s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it]  3%|▎         | 1/32 [00:01<00:59,  1.91s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it]  6%|▋         | 2/32 [00:03<00:49,  1.67s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:34,  1.45s/it]  9%|▉         | 3/32 [00:04<00:45,  1.58s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.43s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.55s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.46s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.52s/it] 41%|████      | 13/32 [00:19<00:27,  1.43s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.43s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.43s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it] 31%|███▏      | 10/32 [00:14<00:32,  1.46s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.43s/it] 41%|████      | 13/32 [00:19<00:27,  1.42s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.45s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.43s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.43s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.43s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 50%|█████     | 16/32 [00:23<00:22,  1.43s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.43s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.45s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.43s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.42s/it] 50%|█████     | 16/32 [00:23<00:23,  1.45s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.43s/it] 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.45s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.43s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.42s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.44s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.42s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.44s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.42s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.42s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.44s/it] 78%|███████▊  | 25/32 [00:36<00:09,  1.42s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.42s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.44s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.42s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.44s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.42s/it] 78%|███████▊  | 25/32 [00:36<00:09,  1.42s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.49s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.45s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.42s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.44s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.43s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.42s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.44s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.42s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.41s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.48s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.44s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.42s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.42s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.44s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it]100%|██████████| 32/32 [00:46<00:00,  1.42s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 94%|█████████▍| 30/32 [00:43<00:02,  1.42s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.45s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.48s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.42s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.46s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it]100%|██████████| 32/32 [00:46<00:00,  1.41s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 94%|█████████▍| 30/32 [00:43<00:02,  1.47s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.49s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.50s/it]100%|██████████| 32/32 [00:46<00:00,  1.49s/it]100%|██████████| 32/32 [00:46<00:00,  1.47s/it]
 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it]I0402 10:14:16.991943 1618546 finetune.py:45] layer 8_gate initial loss 0.002433298621326685
W0402 10:14:16.992200 1618546 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:43<00:04,  1.52s/it]W0402 10:14:17.698246 1618546 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:45<00:03,  1.53s/it]I0402 10:14:19.881547 1618708 finetune.py:45] layer 9_gate initial loss 0.0030414441134780645
W0402 10:14:19.881848 1618708 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 31/32 [00:46<00:01,  1.53s/it]8_gate proxy err 0.0011329229455441236 tr(WHW.T) 2931.24951171875
  0%|          | 0/112 [00:00<?, ?it/s]W0402 10:14:20.633098 1618708 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  1%|          | 1/112 [00:00<01:36,  1.16it/s]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
  2%|▏         | 2/112 [00:01<01:03,  1.74it/s]  3%|▎         | 3/112 [00:01<00:52,  2.06it/s]  4%|▎         | 4/112 [00:01<00:47,  2.25it/s]  4%|▍         | 5/112 [00:02<00:44,  2.39it/s]  5%|▌         | 6/112 [00:02<00:42,  2.47it/s]I0402 10:14:23.446863 1619583 finetune.py:45] layer 11_gate initial loss 0.002569126198068261
W0402 10:14:23.447120 1619583 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

9_gate proxy err 0.0010931827127933502 tr(WHW.T) 3179.91943359375
  0%|          | 0/112 [00:00<?, ?it/s]  6%|▋         | 7/112 [00:03<00:41,  2.54it/s]  7%|▋         | 8/112 [00:03<00:39,  2.62it/s]W0402 10:14:24.229268 1619583 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  1%|          | 1/112 [00:00<01:33,  1.19it/s]  8%|▊         | 9/112 [00:03<00:38,  2.68it/s]  2%|▏         | 2/112 [00:01<01:00,  1.81it/s]  9%|▉         | 10/112 [00:04<00:38,  2.67it/s]  3%|▎         | 3/112 [00:01<00:51,  2.12it/s] 10%|▉         | 11/112 [00:04<00:37,  2.69it/s]  4%|▎         | 4/112 [00:01<00:46,  2.31it/s] 11%|█         | 12/112 [00:04<00:37,  2.67it/s]  4%|▍         | 5/112 [00:02<00:44,  2.41it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.68it/s]  5%|▌         | 6/112 [00:02<00:42,  2.50it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.71it/s]  6%|▋         | 7/112 [00:03<00:41,  2.55it/s] 13%|█▎        | 15/112 [00:06<00:35,  2.70it/s]  7%|▋         | 8/112 [00:03<00:40,  2.57it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.73it/s]11_gate proxy err 0.0010709259659051895 tr(WHW.T) 3167.24609375
  0%|          | 0/112 [00:00<?, ?it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.74it/s]  8%|▊         | 9/112 [00:03<00:39,  2.60it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.75it/s]  9%|▉         | 10/112 [00:04<00:38,  2.62it/s]  1%|          | 1/112 [00:00<01:30,  1.22it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.78it/s] 10%|▉         | 11/112 [00:04<00:38,  2.62it/s]  2%|▏         | 2/112 [00:01<01:00,  1.83it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.80it/s] 11%|█         | 12/112 [00:04<00:37,  2.64it/s]  3%|▎         | 3/112 [00:01<00:50,  2.17it/s] 19%|█▉        | 21/112 [00:08<00:32,  2.78it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.64it/s]  4%|▎         | 4/112 [00:01<00:45,  2.36it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.79it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.64it/s]  4%|▍         | 5/112 [00:02<00:42,  2.50it/s] 21%|██        | 23/112 [00:08<00:31,  2.80it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.65it/s]  5%|▌         | 6/112 [00:02<00:40,  2.61it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.81it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.65it/s]  6%|▋         | 7/112 [00:02<00:39,  2.67it/s]I0402 10:14:30.171169 1619072 finetune.py:45] layer 10_gate initial loss 0.001958745066076517
W0402 10:14:30.171436 1619072 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 25/112 [00:09<00:30,  2.81it/s]  7%|▋         | 8/112 [00:03<00:38,  2.71it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.67it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.81it/s]  8%|▊         | 9/112 [00:03<00:37,  2.72it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.69it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.78it/s]W0402 10:14:31.006105 1619072 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 17%|█▋        | 19/112 [00:07<00:34,  2.71it/s]  9%|▉         | 10/112 [00:04<00:37,  2.72it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.78it/s] 10%|▉         | 11/112 [00:04<00:37,  2.73it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.70it/s] 26%|██▌       | 29/112 [00:11<00:29,  2.78it/s] 11%|█         | 12/112 [00:04<00:36,  2.75it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.72it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.77it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.74it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.70it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.77it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.76it/s] 21%|██        | 23/112 [00:09<00:33,  2.67it/s] 29%|██▊       | 32/112 [00:12<00:28,  2.78it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.76it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.67it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.76it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.75it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.71it/s] 30%|███       | 34/112 [00:12<00:28,  2.77it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.76it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.73it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.79it/s] 16%|█▌        | 18/112 [00:06<00:34,  2.76it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.74it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.79it/s]10_gate proxy err 0.0011050974717363715 tr(WHW.T) 3045.35498046875
  0%|          | 0/112 [00:00<?, ?it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.75it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.76it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.80it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.75it/s] 26%|██▌       | 29/112 [00:11<00:29,  2.77it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.79it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.75it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.78it/s]  1%|          | 1/112 [00:00<01:33,  1.19it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.78it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.74it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.79it/s]  2%|▏         | 2/112 [00:01<01:02,  1.77it/s] 36%|███▌      | 40/112 [00:15<00:25,  2.78it/s] 21%|██        | 23/112 [00:08<00:32,  2.75it/s] 29%|██▊       | 32/112 [00:12<00:28,  2.80it/s]  3%|▎         | 3/112 [00:01<00:52,  2.09it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.78it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.81it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.75it/s]  4%|▎         | 4/112 [00:01<00:47,  2.29it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.78it/s] 30%|███       | 34/112 [00:12<00:27,  2.82it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.74it/s]  4%|▍         | 5/112 [00:02<00:44,  2.42it/s] 38%|███▊      | 43/112 [00:16<00:24,  2.78it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.82it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.75it/s]  5%|▌         | 6/112 [00:02<00:42,  2.50it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.77it/s] 32%|███▏      | 36/112 [00:13<00:26,  2.82it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.73it/s]  6%|▋         | 7/112 [00:03<00:41,  2.56it/s] 40%|████      | 45/112 [00:16<00:24,  2.76it/s] 33%|███▎      | 37/112 [00:14<00:26,  2.82it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.74it/s]  7%|▋         | 8/112 [00:03<00:40,  2.58it/s] 41%|████      | 46/112 [00:17<00:23,  2.77it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.81it/s] 26%|██▌       | 29/112 [00:10<00:30,  2.74it/s]  8%|▊         | 9/112 [00:03<00:39,  2.61it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.77it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.80it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.73it/s]  9%|▉         | 10/112 [00:04<00:38,  2.62it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.76it/s] 36%|███▌      | 40/112 [00:15<00:25,  2.82it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.74it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.76it/s] 10%|▉         | 11/112 [00:04<00:38,  2.64it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.82it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.74it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.76it/s] 11%|█         | 12/112 [00:04<00:37,  2.64it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.81it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.72it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.75it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.66it/s] 38%|███▊      | 43/112 [00:16<00:24,  2.82it/s] 30%|███       | 34/112 [00:12<00:28,  2.74it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.76it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.67it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.82it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.74it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.76it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.66it/s] 40%|████      | 45/112 [00:16<00:23,  2.80it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.74it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.76it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.63it/s] 41%|████      | 46/112 [00:17<00:23,  2.77it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.74it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.76it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.63it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.76it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.75it/s] 50%|█████     | 56/112 [00:20<00:20,  2.76it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.64it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.78it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.75it/s] 51%|█████     | 57/112 [00:21<00:19,  2.76it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.64it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.77it/s] 36%|███▌      | 40/112 [00:14<00:26,  2.74it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.75it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.78it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.65it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.75it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.75it/s] 46%|████▌     | 51/112 [00:19<00:21,  2.79it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.66it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.74it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.75it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.80it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.66it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.74it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.75it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.80it/s] 21%|██        | 23/112 [00:09<00:33,  2.66it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.74it/s] 55%|█████▌    | 62/112 [00:22<00:18,  2.75it/s] 48%|████▊     | 54/112 [00:20<00:20,  2.79it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.67it/s] 40%|████      | 45/112 [00:16<00:24,  2.74it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.72it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.80it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.67it/s] 41%|████      | 46/112 [00:17<00:24,  2.74it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.74it/s] 50%|█████     | 56/112 [00:20<00:20,  2.80it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.66it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.75it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.75it/s] 51%|█████     | 57/112 [00:21<00:19,  2.78it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.66it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.74it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.76it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.79it/s] 25%|██▌       | 28/112 [00:10<00:31,  2.66it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.74it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.76it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.78it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.66it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.75it/s] 61%|██████    | 68/112 [00:25<00:15,  2.76it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.79it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.66it/s] 46%|████▌     | 51/112 [00:18<00:22,  2.75it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.78it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.79it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.66it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.75it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.79it/s] 55%|█████▌    | 62/112 [00:23<00:17,  2.80it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.75it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.67it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.78it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.81it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.74it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.67it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.78it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.74it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.75it/s] 30%|███       | 34/112 [00:13<00:29,  2.64it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.78it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.75it/s] 50%|█████     | 56/112 [00:20<00:20,  2.75it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.78it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.63it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.76it/s] 51%|█████     | 57/112 [00:21<00:20,  2.75it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.78it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.63it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.77it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.75it/s] 68%|██████▊   | 76/112 [00:28<00:12,  2.79it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.65it/s] 61%|██████    | 68/112 [00:25<00:15,  2.78it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.74it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.76it/s] 34%|███▍      | 38/112 [00:14<00:28,  2.62it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.76it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.73it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.75it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.64it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.78it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.74it/s] 71%|███████   | 79/112 [00:29<00:11,  2.77it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.79it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.65it/s] 55%|█████▌    | 62/112 [00:22<00:18,  2.74it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.77it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.80it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.65it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.75it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.78it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.80it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.66it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.76it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.79it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.81it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.65it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.74it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.77it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.80it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.64it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.74it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.77it/s] 68%|██████▊   | 76/112 [00:28<00:12,  2.80it/s] 40%|████      | 45/112 [00:17<00:25,  2.65it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.76it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.78it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.82it/s] 41%|████      | 46/112 [00:17<00:24,  2.67it/s] 61%|██████    | 68/112 [00:25<00:16,  2.74it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.75it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.81it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.66it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.73it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.75it/s] 71%|███████   | 79/112 [00:29<00:11,  2.80it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.67it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.75it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.78it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.81it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.68it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.74it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.76it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.81it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.69it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.75it/s] 80%|████████  | 90/112 [00:33<00:07,  2.77it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.81it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.68it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.76it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.78it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.79it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.66it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.78it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.75it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.78it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.67it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.78it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.76it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.78it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.66it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.79it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.76it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.79it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.67it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.77it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.74it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.80it/s] 50%|█████     | 56/112 [00:21<00:20,  2.69it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.77it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.76it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.80it/s] 51%|█████     | 57/112 [00:21<00:20,  2.68it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.78it/s] 71%|███████   | 79/112 [00:29<00:11,  2.76it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.80it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.67it/s] 88%|████████▊ | 98/112 [00:35<00:05,  2.78it/s] 80%|████████  | 90/112 [00:33<00:07,  2.80it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.75it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.69it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.78it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.81it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.75it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.69it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.81it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.77it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.74it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.81it/s] 90%|█████████ | 101/112 [00:37<00:03,  2.77it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.68it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.75it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.82it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.78it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.69it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.75it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.80it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.78it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.75it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.68it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.80it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.79it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.76it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.69it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.81it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.79it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.73it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.70it/s] 88%|████████▊ | 98/112 [00:35<00:05,  2.78it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.78it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.72it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.67it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.79it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.78it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.74it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.69it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.79it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.79it/s] 80%|████████  | 90/112 [00:33<00:07,  2.76it/s] 61%|██████    | 68/112 [00:25<00:16,  2.69it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.79it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.75it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.75it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.68it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.76it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.76it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.76it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.66it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.77it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.77it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.76it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.67it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.76it/s]100%|██████████| 112/112 [00:41<00:00,  2.77it/s]100%|██████████| 112/112 [00:41<00:00,  2.73it/s]
 84%|████████▍ | 94/112 [00:34<00:06,  2.76it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.67it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.78it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.75it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.69it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.76it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.71it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.69it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.78it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.68it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.69it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.76it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.66it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.65it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.72it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.67it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.66it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.74it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.70it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.68it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.77it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.69it/s] 71%|███████   | 79/112 [00:30<00:12,  2.69it/s]100%|██████████| 112/112 [00:40<00:00,  2.78it/s]100%|██████████| 112/112 [00:40<00:00,  2.73it/s]
 91%|█████████ | 102/112 [00:37<00:03,  2.71it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.70it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.72it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.65it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.73it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.61it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.74it/s] 74%|███████▍  | 83/112 [00:31<00:11,  2.60it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.75it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.63it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.75it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.62it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.76it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.64it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.75it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.64it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.75it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.65it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.76it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.67it/s]100%|██████████| 112/112 [00:41<00:00,  2.76it/s]100%|██████████| 112/112 [00:41<00:00,  2.72it/s]
 80%|████████  | 90/112 [00:34<00:08,  2.66it/s]W0402 10:15:08.602000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:08.602000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:08.602000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:08.602000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:08.602000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:15:08.602000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:08.602000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:08.645000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:08.646000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:08.646000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:08.646000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:08.646000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:08.824000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:08.824000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:08.824000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:08.824000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:08.824000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 91/112 [00:34<00:07,  2.68it/s]W0402 10:15:09.145000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:09.146000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:15:09.146000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:09.146000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:09.146000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:09.146000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:09.146000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:09.180000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:09.180000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:09.180000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:09.180000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:09.180000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:09.251000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:09.251000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:09.251000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:09.251000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:09.251000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 82%|████████▏ | 92/112 [00:34<00:07,  2.69it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.70it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.69it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.69it/s]W0402 10:15:10.564000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 86%|████████▌ | 96/112 [00:36<00:05,  2.70it/s]W0402 10:15:11.020000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.020000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.020000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.020000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.020000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.020000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.021000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.021000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.021000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.021000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.021000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.021000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.021000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.021000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.050000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.050000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.050000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.050000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.050000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.061000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.061000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.061000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.061000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.062000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 87%|████████▋ | 97/112 [00:36<00:05,  2.70it/s]W0402 10:15:11.225000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.225000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.225000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.225000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.225000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.417000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.417000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.417000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.417000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.417000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 98/112 [00:37<00:05,  2.70it/s]W0402 10:15:11.525000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.525000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.525000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.525000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.525000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.525000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.525000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.557000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.557000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.557000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.557000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.557000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.623000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.623000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.623000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.623000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.623000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 99/112 [00:37<00:04,  2.69it/s]W0402 10:15:11.934000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:11.939000 139903392634688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 89%|████████▉ | 100/112 [00:37<00:04,  2.70it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.69it/s]W0402 10:15:12.867000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 102/112 [00:38<00:03,  2.70it/s]W0402 10:15:13.298000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:13.298000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:13.299000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:15:13.299000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:13.299000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:13.299000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:13.299000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:13.328000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:13.328000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:13.328000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:13.328000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:13.328000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 92%|█████████▏| 103/112 [00:39<00:03,  2.70it/s]W0402 10:15:13.691000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:13.691000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:13.692000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:13.692000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:13.692000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 93%|█████████▎| 104/112 [00:39<00:03,  2.66it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.68it/s]W0402 10:15:14.207000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:14.213000 139920602404672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 95%|█████████▍| 106/112 [00:40<00:02,  2.66it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.68it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.69it/s]W0402 10:15:15.292000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.292000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.292000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.292000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.292000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.293000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.293000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.335000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.335000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.335000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.335000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.335000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.515000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.515000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.516000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.516000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.516000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 109/112 [00:41<00:01,  2.69it/s]W0402 10:15:15.836000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.836000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.836000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.836000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.836000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.837000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.837000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.869000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.870000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.870000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.870000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.870000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.941000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.941000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.941000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.941000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:15.941000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 98%|█████████▊| 110/112 [00:41<00:00,  2.68it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.64it/s]100%|██████████| 112/112 [00:42<00:00,  2.61it/s]100%|██████████| 112/112 [00:42<00:00,  2.64it/s]
W0402 10:15:17.282000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:17.739000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:17.739000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:17.739000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:17.740000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:17.740000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:17.740000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:15:17.740000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:17.771000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:17.771000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:17.772000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:17.772000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:17.772000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:18.156000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:18.156000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:18.156000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:18.157000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:18.157000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:18.702000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:18.708000 139657380673344 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0402 10:15:19.091921 1618546 finetune.py:45] layer 8_down initial loss 0.002411837223917246
W0402 10:15:19.092133 1618546 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:15:19.633602 1618546 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

8_down proxy err 0.0037052910774946213 tr(WHW.T) 7.259359359741211
I0402 10:15:21.200361 1618708 finetune.py:45] layer 9_down initial loss 0.0030097300186753273
W0402 10:15:21.200638 1618708 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:15:21.923765 1618708 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

9_down proxy err 0.0036925524473190308 tr(WHW.T) 8.00308609008789
W0402 10:15:23.980000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:23.981000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:23.981000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:23.981000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:23.981000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:15:23.981000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:23.981000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.027000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.027000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.027000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.027000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.027000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.194000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.194000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.194000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.194000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.194000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.490000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.491000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.491000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.491000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.491000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.491000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.491000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.524000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.524000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.524000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.524000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.524000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.590000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.590000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.590000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.590000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:24.590000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
I0402 10:15:25.599858 1619583 finetune.py:45] layer 11_down initial loss 0.002552246442064643
W0402 10:15:25.600017 1619583 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 10:15:25.737123 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 12 in 1.41953444480896s
W0402 10:15:25.851000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:26.077718 1619583 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

I0402 10:15:26.214894 1606370 quantize_finetune_llama.py:159] layer 13 gpu 1
W0402 10:15:26.324000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:26.324000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:26.325000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:26.325000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:26.325000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:26.325000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:15:26.325000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:26.357000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:26.357000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:26.357000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:26.357000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:26.357000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
11_down proxy err 0.0035770509857684374 tr(WHW.T) 9.07177734375
W0402 10:15:26.720000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:26.720000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:26.720000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:26.720000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:26.720000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:27.255000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:27.264000 139793460844352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0402 10:15:28.264476 1623883 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:15:28.265293 1623883 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:15:28.266015 1623883 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:15:28.466468 1623883 config.py:58] PyTorch version 2.4.0 available.
I0402 10:15:30.533241 1623883 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 10:15:30.873483 1623883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:50,  1.64s/it]  6%|▋         | 2/32 [00:01<00:25,  1.16it/s]  9%|▉         | 3/32 [00:02<00:17,  1.63it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.01it/s]I0402 10:15:34.633489 1619072 finetune.py:45] layer 10_down initial loss 0.0019438915187492967
W0402 10:15:34.633784 1619072 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:02<00:11,  2.31it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.53it/s]W0402 10:15:35.122371 1619072 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 22%|██▏       | 7/32 [00:03<00:09,  2.69it/s]10_down proxy err 0.003691180841997266 tr(WHW.T) 8.421324729919434
 25%|██▌       | 8/32 [00:03<00:08,  2.80it/s] 28%|██▊       | 9/32 [00:04<00:07,  2.88it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.95it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.99it/s] 38%|███▊      | 12/32 [00:05<00:06,  3.00it/s] 41%|████      | 13/32 [00:05<00:06,  3.01it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.99it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.01it/s] 50%|█████     | 16/32 [00:06<00:05,  3.03it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.06it/s]I0402 10:15:38.705354 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 13 in 1.157111644744873s
 56%|█████▋    | 18/32 [00:07<00:04,  2.95it/s]I0402 10:15:39.235739 1606370 quantize_finetune_llama.py:159] layer 14 gpu 2
 59%|█████▉    | 19/32 [00:07<00:04,  2.92it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.91it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.91it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.93it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.97it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.99it/s]I0402 10:15:41.163833 1624525 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:15:41.163934 1624525 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:15:41.163992 1624525 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:15:41.252324 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 14 in 1.5843939781188965s
I0402 10:15:41.357638 1624525 config.py:58] PyTorch version 2.4.0 available.
 78%|███████▊  | 25/32 [00:09<00:02,  2.99it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.94it/s]I0402 10:15:41.788363 1606370 quantize_finetune_llama.py:159] layer 15 gpu 3
 84%|████████▍ | 27/32 [00:10<00:01,  2.91it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.90it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.90it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.95it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.95it/s]I0402 10:15:43.519722 1624525 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 10:15:43.627561 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 15 in 1.4027550220489502s
100%|██████████| 32/32 [00:11<00:00,  2.92it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
I0402 10:15:43.772896 1624706 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:15:43.773030 1624706 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:15:43.773090 1624706 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:15:43.979681 1624706 config.py:58] PyTorch version 2.4.0 available.
W0402 10:15:44.006918 1624525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 10:15:44.108172 1606370 quantize_finetune_llama.py:159] layer 16 gpu 0
  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:15:46.399896 1625139 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:15:46.400125 1625139 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:15:46.400196 1625139 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:15:46.454803 1624706 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 10:15:46.610343 1625139 config.py:58] PyTorch version 2.4.0 available.
W0402 10:15:46.839157 1624706 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 10:15:46.873000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:46.874000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:46.874000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:46.874000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:46.874000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:15:46.874000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:46.874000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:46.900000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:46.900000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:46.900000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:46.900000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:46.900000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:55,  1.77s/it]W0402 10:15:47.179000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:47.179000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:47.180000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:47.180000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:47.180000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:02<00:28,  1.07it/s]  9%|▉         | 3/32 [00:02<00:19,  1.50it/s]W0402 10:15:47.795000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:47.795000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:47.795000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:47.796000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:47.796000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:15:47.796000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:47.796000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:47.814000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:47.814000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:47.814000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:47.814000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:47.814000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
  0%|          | 0/32 [00:00<?, ?it/s]W0402 10:15:48.015000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:48.015000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:15:48.015000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:48.015000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:48.015000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:02<00:15,  1.86it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.13it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.33it/s]W0402 10:15:49.165000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:15:49.165000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:15:49.165000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:49.165000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:49.165000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:49.165000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:49.165000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
I0402 10:15:49.173585 1625139 data_utils.py:336] using 256 training seqs, 128 validation seqs
 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s]W0402 10:15:49.184000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:49.184000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:49.184000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:49.184000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:49.184000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:04<00:09,  2.60it/s]W0402 10:15:49.637851 1625139 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:54,  1.76s/it]W0402 10:15:49.813000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:15:49.813000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:15:49.813000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:15:49.814000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:15:49.814000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:04<00:08,  2.68it/s]  6%|▋         | 2/32 [00:02<00:27,  1.08it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s]  9%|▉         | 3/32 [00:02<00:19,  1.52it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 34%|███▍      | 11/32 [00:05<00:07,  2.81it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.87it/s]  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.85it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.16it/s] 41%|████      | 13/32 [00:05<00:06,  2.86it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.35it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.85it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.83it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.59it/s] 50%|█████     | 16/32 [00:06<00:05,  2.80it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.66it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.81it/s]  3%|▎         | 1/32 [00:01<00:58,  1.88s/it] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.82it/s]  6%|▋         | 2/32 [00:02<00:29,  1.02it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.75it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.82it/s]  9%|▉         | 3/32 [00:02<00:20,  1.45it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.77it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.83it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.80it/s] 41%|████      | 13/32 [00:05<00:06,  2.79it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.83it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.08it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.81it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.83it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.81it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.84it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 50%|█████     | 16/32 [00:07<00:05,  2.82it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.64it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.84it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.83it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.76it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.85it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.83it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.83it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.85it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.83it/s]I0402 10:15:56.089851 1623883 finetune.py:45] layer 12_v initial loss 0.0028046655934304
W0402 10:15:56.090271 1623883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:05<00:07,  2.88it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.89it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.95it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.88it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.90it/s] 41%|████      | 13/32 [00:05<00:06,  2.97it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.92it/s]W0402 10:15:57.025189 1623883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:09<00:03,  2.92it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.96it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.91it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.90it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.98it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.91it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.94it/s] 50%|█████     | 16/32 [00:06<00:05,  3.00it/s]100%|██████████| 32/32 [00:12<00:00,  2.95it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
12_v proxy err 0.0027501918375492096 tr(WHW.T) 68.45219421386719
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:07<00:04,  3.01it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.94it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.02it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.91it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.03it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.91it/s] 62%|██████▎   | 20/32 [00:08<00:03,  3.03it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.92it/s]  3%|▎         | 1/32 [00:01<00:39,  1.27s/it] 66%|██████▌   | 21/32 [00:08<00:03,  3.01it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.95it/s]  6%|▋         | 2/32 [00:01<00:21,  1.37it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.99it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.97it/s]  9%|▉         | 3/32 [00:01<00:16,  1.79it/s] 72%|███████▏  | 23/32 [00:09<00:03,  3.00it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.97it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.08it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.01it/s]100%|██████████| 32/32 [00:12<00:00,  2.95it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
W0402 10:16:00.499000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:00.499000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:00.499000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:00.499000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:00.499000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:00.499000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:00.499000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:00.524000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:00.524000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:00.524000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:00.525000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:00.525000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.01it/s]W0402 10:16:00.811000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:00.811000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:00.811000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:00.811000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:00.811000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:10<00:01,  3.00it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.41it/s]W0402 10:16:01.415000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:01.415000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:01.415000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:01.415000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:01.415000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:01.415000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:01.415000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  3.01it/s]W0402 10:16:01.433000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:01.433000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:01.433000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:01.434000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:01.434000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s]W0402 10:16:01.637000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:01.637000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:01.637000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:01.637000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:01.637000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  3.02it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.56it/s] 91%|█████████ | 29/32 [00:11<00:00,  3.02it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.62it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.02it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s]W0402 10:16:02.732000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:02.732000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:02.732000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:02.732000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:02.732000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:02.733000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:02.733000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:02.750000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:02.750000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:02.750000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:02.750000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:02.750000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  3.00it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.99it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
W0402 10:16:03.150000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.150000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.151000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.151000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.151000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.151000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.151000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.178000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.178000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.178000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.178000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.178000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:05<00:07,  2.77it/s]W0402 10:16:03.342000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.342000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.342000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.342000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.342000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.456000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.456000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.456000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.456000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:03.456000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.75it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.76it/s]W0402 10:16:04.066000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:04.066000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:04.066000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:04.066000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:04.066000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:04.066000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:04.066000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 10:16:04.084000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:04.085000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:04.085000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:04.085000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:04.085000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:04.290000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:04.290000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:04.290000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:04.290000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:04.290000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 15/32 [00:06<00:06,  2.73it/s] 50%|█████     | 16/32 [00:06<00:05,  2.77it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.77it/s]W0402 10:16:05.434000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.434000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.434000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.435000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.435000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.435000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.435000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.453000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.453000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.454000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.454000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.454000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:07<00:05,  2.75it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s]W0402 10:16:05.930000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.930000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.930000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.930000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.931000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.931000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.931000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.958000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.958000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.958000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.958000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:05.958000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.093000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.093000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.093000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.093000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.093000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:08<00:04,  2.73it/s]W0402 10:16:06.238000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.238000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.238000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.238000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.238000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s]W0402 10:16:06.830000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.831000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.831000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.831000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.831000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.831000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.831000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 10:16:06.848000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.848000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.848000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.848000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:06.848000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s]W0402 10:16:07.041000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:07.041000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:07.041000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:07.041000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:07.041000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:09<00:03,  2.70it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.65it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.59it/s]W0402 10:16:08.139000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:08.139000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:08.139000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:08.139000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:08.139000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:08.139000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:08.140000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:08.157000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:08.157000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:08.157000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:08.157000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:08.157000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s]W0402 10:16:08.748000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:08.748000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:08.748000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:08.748000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:08.748000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0402 10:16:09.491207 1624525 finetune.py:45] layer 13_v initial loss 0.003967637196183205
W0402 10:16:09.491600 1624525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]W0402 10:16:10.759149 1624525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
13_v proxy err 0.00287695974111557 tr(WHW.T) 71.04966735839844
  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:16:12.159067 1624706 finetune.py:45] layer 14_v initial loss 0.003110888646915555
W0402 10:16:12.159312 1624706 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:16:13.313669 1624706 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:41,  1.35s/it]  6%|▋         | 2/32 [00:01<00:23,  1.28it/s]  9%|▉         | 3/32 [00:02<00:17,  1.67it/s]14_v proxy err 0.0026320805773139 tr(WHW.T) 74.802978515625
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.94it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.13it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.25it/s]  3%|▎         | 1/32 [00:01<00:40,  1.31s/it] 22%|██▏       | 7/32 [00:03<00:10,  2.35it/s]I0402 10:16:15.676209 1625139 finetune.py:45] layer 15_v initial loss 0.00394464423879981
W0402 10:16:15.676414 1625139 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:01<00:22,  1.32it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.47it/s]  9%|▉         | 3/32 [00:02<00:16,  1.72it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.54it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.01it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.60it/s]W0402 10:16:16.794131 1625139 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:02<00:12,  2.20it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.61it/s]W0402 10:16:17.439000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.440000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.440000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.440000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.440000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.441000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.441000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.468000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.469000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.469000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.469000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.469000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:03<00:11,  2.35it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.65it/s]W0402 10:16:17.623000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.623000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.623000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.623000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.623000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.845000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.846000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.846000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.846000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.846000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.846000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.846000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 41%|████      | 13/32 [00:05<00:07,  2.67it/s]W0402 10:16:17.866000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.867000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.867000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.867000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.867000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
15_v proxy err 0.0034280496183782816 tr(WHW.T) 67.86465454101562
  0%|          | 0/32 [00:00<?, ?it/s]W0402 10:16:17.927000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.927000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.927000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.927000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:17.928000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:09,  2.54it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.67it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.58it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s]W0402 10:16:18.918000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s]  3%|▎         | 1/32 [00:01<00:39,  1.26s/it]W0402 10:16:19.216000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:19.216000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:19.216000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:19.216000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:19.217000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:19.217000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:19.217000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:19.236000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:19.236000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:19.236000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:19.236000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:19.237000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:07<00:05,  2.73it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.65it/s]W0402 10:16:19.472000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:19.473000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:19.473000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:19.473000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:19.473000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:22,  1.36it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.69it/s]  9%|▉         | 3/32 [00:01<00:16,  1.76it/s]W0402 10:16:19.922000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 19/32 [00:08<00:04,  2.73it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.04it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.72it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.69it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.25it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.70it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.40it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.73it/s] 50%|█████     | 16/32 [00:06<00:05,  2.73it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.74it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.72it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.56it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.75it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.73it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.59it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.76it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.74it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.77it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.75it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.75it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.80it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.77it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.80it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.60it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.76it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.80it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.76it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.59it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.74it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.74it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.61it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.74it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.61it/s]I0402 10:16:26.797599 1623883 finetune.py:45] layer 12_q initial loss 0.0028047196101397276
W0402 10:16:26.798127 1623883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
 69%|██████▉   | 22/32 [00:09<00:03,  2.64it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.67it/s]W0402 10:16:27.992413 1623883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 78%|███████▊  | 25/32 [00:10<00:02,  2.64it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.70it/s]12_q proxy err 0.0003943651099689305 tr(WHW.T) 6419.8271484375
  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.71it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.73it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.71it/s]W0402 10:16:30.194000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.195000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.195000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.195000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.195000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.195000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.195000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:37,  1.21s/it]W0402 10:16:30.221000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.222000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.222000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.222000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.222000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.379000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.379000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.379000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.379000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.379000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:12<00:00,  2.72it/s]  6%|▋         | 2/32 [00:01<00:21,  1.42it/s]W0402 10:16:30.591000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.591000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.592000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.592000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.592000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.592000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.592000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.610000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.611000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.611000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.611000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.611000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.671000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.671000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.671000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.671000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:30.671000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
  9%|▉         | 3/32 [00:01<00:15,  1.82it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.13it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s]W0402 10:16:31.702000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:02<00:10,  2.49it/s]W0402 10:16:32.002000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.002000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.002000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.002000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.002000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.002000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.002000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.023000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.023000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.023000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.024000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.024000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.258000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.258000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.258000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.258000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.258000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s]W0402 10:16:32.584000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.584000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.584000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.584000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.584000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.584000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.584000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.614000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.614000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.614000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.615000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.615000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s]W0402 10:16:32.706000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.772000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.772000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.773000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.773000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.773000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.982000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.983000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.983000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.983000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.983000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.983000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:32.983000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:33.003000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:33.003000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:33.003000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:33.003000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:33.003000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:04<00:08,  2.70it/s]W0402 10:16:33.066000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:33.066000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:33.066000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:33.067000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:33.067000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s]W0402 10:16:34.054000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:05<00:07,  2.80it/s]W0402 10:16:34.358000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:34.358000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:34.358000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:34.359000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:34.359000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:34.359000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:34.359000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:34.381000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:34.381000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:34.381000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:34.381000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:34.381000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.80it/s]W0402 10:16:34.635000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:34.635000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:34.635000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:34.635000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:34.635000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.79it/s]W0402 10:16:35.098000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 15/32 [00:06<00:06,  2.79it/s] 50%|█████     | 16/32 [00:06<00:05,  2.80it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.84it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s]W0402 10:16:36.698000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:36.699000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:36.699000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:36.699000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:36.699000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:36.699000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:36.699000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:36.727000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:36.727000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:36.727000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:36.727000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:36.727000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:36.884000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:36.885000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:36.885000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:36.885000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:36.885000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:07<00:04,  2.86it/s]W0402 10:16:37.098000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:37.098000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:37.098000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:37.098000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:37.098000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:37.099000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:37.099000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:37.118000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:37.118000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:37.118000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:37.118000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:37.118000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:37.179000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:37.179000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:37.179000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:37.179000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:37.179000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:03,  2.87it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.89it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.88it/s]W0402 10:16:38.171000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:09<00:02,  2.88it/s]W0402 10:16:38.471000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:38.471000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:16:38.472000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:38.472000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:16:38.472000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:38.472000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:38.472000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:16:38.491000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:38.491000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:38.491000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:38.491000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:38.491000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:09<00:02,  2.89it/s]I0402 10:16:38.705575 1624525 finetune.py:45] layer 13_q initial loss 0.003967685624957085
W0402 10:16:38.705850 1624525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:16:38.727000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:16:38.727000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:16:38.727000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:16:38.727000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:16:38.727000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:09<00:02,  2.91it/s]W0402 10:16:39.160000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  2.91it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s]W0402 10:16:39.802738 1624525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:11<00:01,  2.87it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.88it/s]13_q proxy err 0.0006219465867616236 tr(WHW.T) 5299.77734375
  0%|          | 0/32 [00:00<?, ?it/s]100%|██████████| 32/32 [00:12<00:00,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
I0402 10:16:41.347621 1624706 finetune.py:45] layer 14_q initial loss 0.003111176658421755
W0402 10:16:41.347829 1624706 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:39,  1.26s/it]W0402 10:16:42.353394 1624706 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:01<00:22,  1.34it/s]  9%|▉         | 3/32 [00:02<00:16,  1.73it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.00it/s]14_q proxy err 0.0005586791667155921 tr(WHW.T) 5552.1181640625
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.20it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.34it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.43it/s]  3%|▎         | 1/32 [00:01<00:38,  1.24s/it] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s]  6%|▋         | 2/32 [00:01<00:22,  1.36it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.55it/s]  9%|▉         | 3/32 [00:01<00:16,  1.76it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.58it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.04it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.61it/s]I0402 10:16:46.060685 1625139 finetune.py:45] layer 15_q initial loss 0.003943047020584345
W0402 10:16:46.060913 1625139 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:02<00:12,  2.23it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.67it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.41it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.71it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s]W0402 10:16:47.244995 1625139 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 15/32 [00:06<00:06,  2.73it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.65it/s] 50%|█████     | 16/32 [00:06<00:05,  2.73it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s]I0402 10:16:47.949271 1623883 finetune.py:45] layer 12_k initial loss 0.0028041864279657602
W0402 10:16:47.949651 1623883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 53%|█████▎    | 17/32 [00:07<00:05,  2.74it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.73it/s]15_q proxy err 0.0005636364803649485 tr(WHW.T) 6709.19140625
  0%|          | 0/32 [00:00<?, ?it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.76it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.77it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.76it/s] 41%|████      | 13/32 [00:05<00:06,  2.76it/s]W0402 10:16:49.044555 1623883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 62%|██████▎   | 20/32 [00:08<00:04,  2.74it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.77it/s]  3%|▎         | 1/32 [00:01<00:38,  1.25s/it] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.78it/s]  6%|▋         | 2/32 [00:01<00:21,  1.38it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s]12_k proxy err 0.00035362131893634796 tr(WHW.T) 4340.4931640625
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:01<00:16,  1.79it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.77it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.80it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.08it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.79it/s]  3%|▎         | 1/32 [00:00<00:23,  1.31it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.76it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.79it/s]  6%|▋         | 2/32 [00:01<00:15,  1.89it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.44it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.76it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.79it/s]  9%|▉         | 3/32 [00:01<00:13,  2.21it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.40it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.63it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.77it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.52it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.78it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.80it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.61it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.78it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.80it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.66it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.75it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.79it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
 81%|████████▏ | 26/32 [00:10<00:02,  2.79it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.72it/s] 41%|████      | 13/32 [00:05<00:06,  2.76it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.78it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.76it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.78it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.74it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.77it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.77it/s] 41%|████      | 13/32 [00:05<00:06,  2.78it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.79it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.78it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.80it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
 47%|████▋     | 15/32 [00:05<00:06,  2.83it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 50%|█████     | 16/32 [00:06<00:05,  2.83it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.83it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.84it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.84it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.87it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.83it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.85it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.83it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.87it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.88it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.83it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.87it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.87it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.80it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.86it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s]I0402 10:16:59.818290 1624525 finetune.py:45] layer 13_k initial loss 0.003966686315834522
W0402 10:16:59.818660 1624525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:09<00:02,  2.81it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.88it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.88it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
 88%|████████▊ | 28/32 [00:10<00:01,  2.82it/s]W0402 10:17:00.862058 1624525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:10<00:01,  2.82it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.83it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
13_k proxy err 0.0004646867164410651 tr(WHW.T) 4513.9775390625
  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:17:02.333122 1624706 finetune.py:45] layer 14_k initial loss 0.003109978511929512
W0402 10:17:02.333446 1624706 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:00<00:25,  1.24it/s]  6%|▋         | 2/32 [00:01<00:16,  1.79it/s]W0402 10:17:03.400863 1624706 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.27it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s]14_k proxy err 0.00040995964081957936 tr(WHW.T) 4950.548828125
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.49it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s]  3%|▎         | 1/32 [00:00<00:24,  1.27it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s]  6%|▋         | 2/32 [00:01<00:16,  1.84it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s]  9%|▉         | 3/32 [00:01<00:13,  2.15it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.33it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.63it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.65it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.67it/s]I0402 10:17:07.881496 1625139 finetune.py:45] layer 15_k initial loss 0.003948091994971037
W0402 10:17:07.881786 1625139 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.66it/s] 50%|█████     | 16/32 [00:06<00:05,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.74it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.73it/s]W0402 10:17:09.022624 1625139 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 10:17:09.111781 1623883 finetune.py:45] layer 12_o initial loss 0.0027379593811929226
W0402 10:17:09.112127 1623883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▋    | 18/32 [00:07<00:05,  2.75it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 41%|████      | 13/32 [00:05<00:06,  2.75it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s]W0402 10:17:10.073904 1623883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

15_k proxy err 0.0004341987951193005 tr(WHW.T) 4509.1435546875
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.75it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s] 50%|█████     | 16/32 [00:06<00:05,  2.75it/s]  3%|▎         | 1/32 [00:00<00:23,  1.29it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s]  6%|▋         | 2/32 [00:01<00:15,  1.89it/s]12_o proxy err 0.003195872064679861 tr(WHW.T) 5.624481201171875
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.76it/s]  9%|▉         | 3/32 [00:01<00:13,  2.20it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.75it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.41it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.73it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.75it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.53it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.75it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.61it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.74it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.65it/s]  3%|▎         | 1/32 [00:01<00:57,  1.86s/it] 91%|█████████ | 29/32 [00:11<00:01,  2.73it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.75it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.74it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.71it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.75it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
 81%|████████▏ | 26/32 [00:09<00:02,  2.75it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.72it/s]  6%|▋         | 2/32 [00:03<00:47,  1.60s/it] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.69it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.69it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.78it/s]  9%|▉         | 3/32 [00:04<00:43,  1.51s/it] 97%|█████████▋| 31/32 [00:11<00:00,  2.72it/s] 50%|█████     | 16/32 [00:06<00:05,  2.79it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 56%|█████▋    | 18/32 [00:06<00:04,  2.81it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.82it/s] 12%|█▎        | 4/32 [00:06<00:40,  1.46s/it] 62%|██████▎   | 20/32 [00:07<00:04,  2.82it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.83it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.82it/s] 16%|█▌        | 5/32 [00:07<00:38,  1.43s/it] 75%|███████▌  | 24/32 [00:09<00:02,  2.83it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.82it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.80it/s] 19%|█▉        | 6/32 [00:08<00:36,  1.41s/it] 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.80it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.81it/s]I0402 10:17:20.984151 1624525 finetune.py:45] layer 13_o initial loss 0.003996381536126137
W0402 10:17:20.984323 1624525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 22%|██▏       | 7/32 [00:10<00:35,  1.41s/it] 97%|█████████▋| 31/32 [00:11<00:00,  2.77it/s]W0402 10:17:21.947673 1624525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:11<00:00,  2.78it/s]100%|██████████| 32/32 [00:11<00:00,  2.69it/s]
 25%|██▌       | 8/32 [00:11<00:33,  1.41s/it]13_o proxy err 0.0031979039777070284 tr(WHW.T) 6.745936870574951
  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:17:23.464665 1624706 finetune.py:45] layer 14_o initial loss 0.0030760038644075394
W0402 10:17:23.464917 1624706 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:13<00:32,  1.41s/it]W0402 10:17:24.449882 1624706 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<01:00,  1.96s/it]14_o proxy err 0.00333999446593225 tr(WHW.T) 6.85344123840332
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:14<00:31,  1.41s/it]  6%|▋         | 2/32 [00:03<00:50,  1.67s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.42s/it]  3%|▎         | 1/32 [00:01<00:59,  1.92s/it]  9%|▉         | 3/32 [00:04<00:45,  1.57s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it]I0402 10:17:28.950513 1625139 finetune.py:45] layer 15_o initial loss 0.003970208577811718
W0402 10:17:28.950879 1625139 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:03<00:49,  1.64s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it] 41%|████      | 13/32 [00:18<00:26,  1.41s/it]W0402 10:17:30.027942 1625139 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:04<00:45,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.50s/it]15_o proxy err 0.0035661067813634872 tr(WHW.T) 6.766373634338379
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:20<00:25,  1.41s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.51s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.40s/it]  3%|▎         | 1/32 [00:01<00:59,  1.91s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.48s/it] 50%|█████     | 16/32 [00:22<00:22,  1.40s/it]  6%|▋         | 2/32 [00:03<00:49,  1.64s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it]  9%|▉         | 3/32 [00:04<00:44,  1.55s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.50s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.46s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.46s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.40s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.46s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.45s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.45s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.40s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.45s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.40s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.45s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.45s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.45s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.40s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.40s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.45s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.45s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.40s/it] 50%|█████     | 16/32 [00:23<00:23,  1.45s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.43s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.45s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.40s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.44s/it] 50%|█████     | 16/32 [00:23<00:23,  1.45s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.40s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.45s/it] 41%|████      | 13/32 [00:19<00:27,  1.44s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.45s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.40s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.45s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.44s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.45s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.40s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.44s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.45s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.41s/it] 66%|██████▌   | 21/32 [00:31<00:15,  1.45s/it] 50%|█████     | 16/32 [00:23<00:22,  1.44s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.41s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.45s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.44s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.44s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]100%|██████████| 32/32 [00:45<00:00,  1.42s/it]
 72%|███████▏  | 23/32 [00:33<00:13,  1.45s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.45s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.44s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.45s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.46s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.44s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.45s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.46s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.44s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.44s/it] 66%|██████▌   | 21/32 [00:30<00:16,  1.47s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.44s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.44s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.48s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.43s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.44s/it]I0402 10:18:04.980324 1623883 finetune.py:45] layer 12_up initial loss 0.0027030538767576218
W0402 10:18:04.980538 1623883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 72%|███████▏  | 23/32 [00:33<00:13,  1.49s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.43s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.45s/it]W0402 10:18:05.915689 1623883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:35<00:11,  1.48s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.43s/it]12_up proxy err 0.0026552113704383373 tr(WHW.T) 855.3870849609375
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:44<00:02,  1.45s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.46s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.44s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.45s/it]  3%|▎         | 1/32 [00:01<00:56,  1.82s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.46s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]100%|██████████| 32/32 [00:46<00:00,  1.47s/it]
  6%|▋         | 2/32 [00:03<00:47,  1.58s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.45s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.45s/it]  9%|▉         | 3/32 [00:04<00:43,  1.51s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.45s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.44s/it] 16%|█▌        | 5/32 [00:07<00:38,  1.44s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.42s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.43s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.42s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.42s/it]I0402 10:18:17.815102 1624525 finetune.py:45] layer 13_up initial loss 0.003940637689083815
W0402 10:18:17.815397 1624525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:46<00:00,  1.42s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
W0402 10:18:18.689809 1624525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it]13_up proxy err 0.002616739831864834 tr(WHW.T) 916.3327026367188
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it]I0402 10:18:20.293070 1624706 finetune.py:45] layer 14_up initial loss 0.00304940203204751
W0402 10:18:20.293385 1624706 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:18:21.234312 1624706 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it]  3%|▎         | 1/32 [00:01<00:58,  1.89s/it]14_up proxy err 0.002814213978126645 tr(WHW.T) 932.0479125976562
  0%|          | 0/32 [00:00<?, ?it/s] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it]  6%|▋         | 2/32 [00:03<00:49,  1.64s/it]  3%|▎         | 1/32 [00:01<00:57,  1.85s/it]  9%|▉         | 3/32 [00:04<00:44,  1.54s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.48s/it]I0402 10:18:25.415036 1625139 finetune.py:45] layer 15_up initial loss 0.0039248536340892315
W0402 10:18:25.415367 1625139 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:03<00:48,  1.61s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.51s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it]W0402 10:18:26.281965 1625139 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:04<00:44,  1.53s/it]15_up proxy err 0.0028272175695747137 tr(WHW.T) 988.9022827148438
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.50s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.47s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.45s/it]  3%|▎         | 1/32 [00:01<00:57,  1.86s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it] 50%|█████     | 16/32 [00:23<00:23,  1.45s/it]  6%|▋         | 2/32 [00:03<00:48,  1.63s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.47s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.44s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.46s/it]  9%|▉         | 3/32 [00:04<00:44,  1.55s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.43s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.46s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.43s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.45s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.44s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.45s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it] 31%|███▏      | 10/32 [00:14<00:32,  1.45s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.43s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.48s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.45s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.43s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.45s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.43s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.45s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.42s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it] 31%|███▏      | 10/32 [00:14<00:32,  1.46s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it] 78%|███████▊  | 25/32 [00:36<00:09,  1.43s/it] 50%|█████     | 16/32 [00:23<00:23,  1.46s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.45s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.45s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 50%|█████     | 16/32 [00:23<00:23,  1.45s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.42s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.45s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.45s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.42s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.45s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.45s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.45s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.42s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.45s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.45s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.42s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.45s/it] 50%|█████     | 16/32 [00:23<00:23,  1.45s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.42s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.45s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.45s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.42s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 72%|███████▏  | 23/32 [00:33<00:13,  1.45s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.44s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.45s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.44s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.45s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.47s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.44s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.44s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.43s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.44s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.50s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.43s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.43s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.51s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.42s/it]I0402 10:19:01.866317 1623883 finetune.py:45] layer 12_gate initial loss 0.0026765691582113504
W0402 10:19:01.866539 1623883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:42<00:04,  1.44s/it]W0402 10:19:02.634573 1623883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:35<00:11,  1.50s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.43s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.44s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.50s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.43s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.44s/it]12_gate proxy err 0.0010140122612938285 tr(WHW.T) 3184.862548828125
  0%|          | 0/112 [00:00<?, ?it/s] 81%|████████▏ | 26/32 [00:38<00:08,  1.48s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.43s/it]  1%|          | 1/112 [00:00<01:29,  1.24it/s]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
  2%|▏         | 2/112 [00:01<00:59,  1.85it/s]  3%|▎         | 3/112 [00:01<00:49,  2.19it/s]  4%|▎         | 4/112 [00:01<00:45,  2.36it/s] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.44s/it]  4%|▍         | 5/112 [00:02<00:42,  2.50it/s]  5%|▌         | 6/112 [00:02<00:40,  2.60it/s]  6%|▋         | 7/112 [00:02<00:39,  2.64it/s]  7%|▋         | 8/112 [00:03<00:38,  2.69it/s] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
  8%|▊         | 9/112 [00:03<00:38,  2.70it/s]  9%|▉         | 10/112 [00:04<00:37,  2.73it/s] 10%|▉         | 11/112 [00:04<00:36,  2.77it/s] 11%|█         | 12/112 [00:04<00:35,  2.80it/s] 91%|█████████ | 29/32 [00:42<00:04,  1.45s/it] 12%|█▏        | 13/112 [00:05<00:35,  2.80it/s] 12%|█▎        | 14/112 [00:05<00:34,  2.80it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.80it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.81it/s] 94%|█████████▍| 30/32 [00:44<00:02,  1.45s/it] 15%|█▌        | 17/112 [00:06<00:34,  2.78it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.79it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.81it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.83it/s] 97%|█████████▋| 31/32 [00:45<00:01,  1.44s/it] 19%|█▉        | 21/112 [00:07<00:32,  2.82it/s] 20%|█▉        | 22/112 [00:08<00:31,  2.83it/s] 21%|██        | 23/112 [00:08<00:31,  2.82it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.82it/s]I0402 10:19:14.566639 1624525 finetune.py:45] layer 13_gate initial loss 0.0038968396838754416
W0402 10:19:14.567121 1624525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:47<00:00,  1.43s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]
 22%|██▏       | 25/112 [00:09<00:30,  2.83it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.79it/s]W0402 10:19:15.344297 1624525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 24%|██▍       | 27/112 [00:10<00:30,  2.74it/s] 25%|██▌       | 28/112 [00:10<00:31,  2.71it/s] 26%|██▌       | 29/112 [00:10<00:30,  2.74it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.77it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.76it/s]I0402 10:19:17.139687 1624706 finetune.py:45] layer 14_gate initial loss 0.0030198502354323864
W0402 10:19:17.139892 1624706 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 29%|██▊       | 32/112 [00:11<00:28,  2.78it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.73it/s]W0402 10:19:17.887233 1624706 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 30%|███       | 34/112 [00:12<00:28,  2.71it/s]13_gate proxy err 0.0009546806686557829 tr(WHW.T) 3562.817138671875
  0%|          | 0/112 [00:00<?, ?it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.69it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.68it/s]  1%|          | 1/112 [00:00<01:35,  1.17it/s] 33%|███▎      | 37/112 [00:13<00:28,  2.66it/s]  2%|▏         | 2/112 [00:01<01:03,  1.74it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.66it/s]  3%|▎         | 3/112 [00:01<00:52,  2.10it/s] 35%|███▍      | 39/112 [00:14<00:27,  2.65it/s]  4%|▎         | 4/112 [00:01<00:46,  2.32it/s] 36%|███▌      | 40/112 [00:14<00:27,  2.65it/s]  4%|▍         | 5/112 [00:02<00:43,  2.45it/s]14_gate proxy err 0.0009280734811909497 tr(WHW.T) 4249.7998046875
  0%|          | 0/112 [00:00<?, ?it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.65it/s]  5%|▌         | 6/112 [00:02<00:41,  2.55it/s] 38%|███▊      | 42/112 [00:15<00:26,  2.66it/s]  6%|▋         | 7/112 [00:03<00:40,  2.60it/s]  1%|          | 1/112 [00:00<01:30,  1.22it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.66it/s]  7%|▋         | 8/112 [00:03<00:39,  2.65it/s]  2%|▏         | 2/112 [00:01<01:00,  1.82it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.66it/s]  8%|▊         | 9/112 [00:03<00:38,  2.69it/s]  3%|▎         | 3/112 [00:01<00:50,  2.15it/s]I0402 10:19:22.291545 1625139 finetune.py:45] layer 15_gate initial loss 0.0039002576377242804
W0402 10:19:22.291909 1625139 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 40%|████      | 45/112 [00:16<00:25,  2.65it/s]  9%|▉         | 10/112 [00:04<00:37,  2.70it/s]  4%|▎         | 4/112 [00:01<00:46,  2.34it/s] 41%|████      | 46/112 [00:17<00:24,  2.68it/s] 10%|▉         | 11/112 [00:04<00:37,  2.71it/s]  4%|▍         | 5/112 [00:02<00:43,  2.48it/s]W0402 10:19:23.009540 1625139 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 42%|████▏     | 47/112 [00:17<00:23,  2.73it/s] 11%|█         | 12/112 [00:04<00:36,  2.72it/s]  5%|▌         | 6/112 [00:02<00:41,  2.55it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.71it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.70it/s]  6%|▋         | 7/112 [00:03<00:40,  2.59it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.72it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.71it/s]  7%|▋         | 8/112 [00:03<00:39,  2.64it/s] 45%|████▍     | 50/112 [00:18<00:23,  2.69it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.72it/s]  8%|▊         | 9/112 [00:03<00:38,  2.67it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.67it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.72it/s]  9%|▉         | 10/112 [00:04<00:37,  2.70it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.70it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.73it/s] 10%|▉         | 11/112 [00:04<00:37,  2.72it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.73it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.74it/s] 11%|█         | 12/112 [00:04<00:36,  2.72it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.75it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.73it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.73it/s]15_gate proxy err 0.0008845042320899665 tr(WHW.T) 5081.4052734375
  0%|          | 0/112 [00:00<?, ?it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.76it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.74it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.74it/s] 50%|█████     | 56/112 [00:20<00:20,  2.78it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.73it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.73it/s] 51%|█████     | 57/112 [00:21<00:19,  2.79it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.73it/s]  1%|          | 1/112 [00:00<01:30,  1.23it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.74it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.80it/s]  2%|▏         | 2/112 [00:01<01:00,  1.83it/s] 21%|██        | 23/112 [00:08<00:32,  2.74it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.73it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.80it/s]  3%|▎         | 3/112 [00:01<00:50,  2.16it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.74it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.73it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.80it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.74it/s]  4%|▎         | 4/112 [00:01<00:46,  2.34it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.74it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.80it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.74it/s]  4%|▍         | 5/112 [00:02<00:43,  2.47it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.74it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.80it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.74it/s]  5%|▌         | 6/112 [00:02<00:41,  2.56it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.74it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.80it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.74it/s]  6%|▋         | 7/112 [00:03<00:40,  2.61it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.74it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.77it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.74it/s]  7%|▋         | 8/112 [00:03<00:39,  2.62it/s] 21%|██        | 23/112 [00:08<00:32,  2.73it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.75it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.74it/s]  8%|▊         | 9/112 [00:03<00:38,  2.65it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.73it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.77it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.74it/s]  9%|▉         | 10/112 [00:04<00:37,  2.69it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.75it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.79it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.73it/s] 10%|▉         | 11/112 [00:04<00:37,  2.71it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.74it/s] 61%|██████    | 68/112 [00:25<00:15,  2.79it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.73it/s] 11%|█         | 12/112 [00:04<00:36,  2.71it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.74it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.79it/s] 30%|███       | 34/112 [00:12<00:28,  2.75it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.73it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.74it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.78it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.73it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.72it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.73it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.78it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.73it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.73it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.73it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.76it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.73it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.75it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.72it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.76it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.76it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.73it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.74it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.77it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.76it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.73it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.73it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.78it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.76it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.73it/s] 30%|███       | 34/112 [00:12<00:28,  2.72it/s] 68%|██████▊   | 76/112 [00:28<00:12,  2.78it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.76it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.73it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.73it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.79it/s] 19%|█▉        | 21/112 [00:08<00:32,  2.77it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.73it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.74it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.80it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.77it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.74it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.74it/s] 71%|███████   | 79/112 [00:29<00:11,  2.80it/s] 21%|██        | 23/112 [00:08<00:32,  2.75it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.74it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.74it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.79it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.76it/s] 40%|████      | 45/112 [00:16<00:24,  2.74it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.75it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.80it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.76it/s] 41%|████      | 46/112 [00:17<00:24,  2.74it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.73it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.79it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.73it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.73it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.74it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.77it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.74it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.73it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.73it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.77it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.73it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.73it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.74it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.78it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.74it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.73it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.73it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.80it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.76it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.72it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.79it/s] 40%|████      | 45/112 [00:16<00:24,  2.72it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.75it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.72it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.79it/s] 41%|████      | 46/112 [00:17<00:24,  2.72it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.76it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.72it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.79it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.73it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.75it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.72it/s] 80%|████████  | 90/112 [00:33<00:07,  2.79it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.74it/s] 30%|███       | 34/112 [00:12<00:28,  2.75it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.73it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.80it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.74it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.76it/s] 50%|█████     | 56/112 [00:20<00:20,  2.74it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.81it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.74it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.76it/s] 51%|█████     | 57/112 [00:21<00:20,  2.71it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.79it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.72it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.75it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.71it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.78it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.73it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.76it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.72it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.79it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.72it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.75it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.72it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.79it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.72it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.76it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.73it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.80it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.74it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.76it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.73it/s] 88%|████████▊ | 98/112 [00:35<00:04,  2.80it/s] 50%|█████     | 56/112 [00:20<00:20,  2.74it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.74it/s] 56%|█████▋    | 63/112 [00:23<00:18,  2.71it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.78it/s] 51%|█████     | 57/112 [00:21<00:20,  2.72it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.75it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.80it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.71it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.72it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.74it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.77it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.72it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.74it/s] 40%|████      | 45/112 [00:16<00:24,  2.72it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.77it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.71it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.73it/s] 41%|████      | 46/112 [00:17<00:24,  2.74it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.77it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.72it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.73it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.72it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.77it/s] 61%|██████    | 68/112 [00:25<00:16,  2.72it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.73it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.74it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.78it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.71it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.72it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.75it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.79it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.72it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.73it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.75it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.79it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.74it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.74it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.75it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.80it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.72it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.73it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.75it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.79it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.72it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.73it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.75it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.79it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.72it/s] 61%|██████    | 68/112 [00:25<00:16,  2.73it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.76it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.79it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.72it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.73it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.77it/s]100%|██████████| 112/112 [00:40<00:00,  2.80it/s]100%|██████████| 112/112 [00:40<00:00,  2.74it/s]
 68%|██████▊   | 76/112 [00:28<00:13,  2.72it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.74it/s] 50%|█████     | 56/112 [00:20<00:20,  2.76it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.73it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.74it/s] 51%|█████     | 57/112 [00:21<00:20,  2.70it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.74it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.74it/s] 52%|█████▏    | 58/112 [00:21<00:20,  2.66it/s] 71%|███████   | 79/112 [00:29<00:12,  2.74it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.75it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.66it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.73it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.74it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.69it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.73it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.74it/s] 54%|█████▍    | 61/112 [00:22<00:19,  2.64it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.74it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.75it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.67it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.74it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.75it/s] 56%|█████▋    | 63/112 [00:23<00:18,  2.67it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.74it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.75it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.69it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.74it/s] 71%|███████   | 79/112 [00:29<00:11,  2.75it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.72it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.73it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.74it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.72it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.73it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.74it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.73it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.74it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.74it/s] 61%|██████    | 68/112 [00:25<00:16,  2.74it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.73it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.73it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.74it/s] 80%|████████  | 90/112 [00:33<00:08,  2.73it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.73it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.73it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.73it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.73it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.75it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.74it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.73it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.74it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.75it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.74it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.75it/s]W0402 10:19:53.107000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.107000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.108000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.108000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.108000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.108000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.108000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 94/112 [00:34<00:06,  2.75it/s]W0402 10:19:53.148000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.149000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.149000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.149000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.149000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 79%|███████▊  | 88/112 [00:32<00:08,  2.74it/s]W0402 10:19:53.313000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.313000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.313000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.313000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.314000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 74/112 [00:27<00:13,  2.76it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.74it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.74it/s]W0402 10:19:53.609000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.609000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.610000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.610000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.610000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.610000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.610000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.642000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.642000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.642000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.642000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.642000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.708000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.708000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.708000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.708000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:19:53.708000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 67%|██████▋   | 75/112 [00:27<00:13,  2.76it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.74it/s] 80%|████████  | 90/112 [00:33<00:08,  2.73it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.75it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.74it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.73it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.75it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.72it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.72it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.75it/s]W0402 10:19:54.953000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 99/112 [00:36<00:04,  2.72it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.73it/s] 71%|███████   | 79/112 [00:29<00:11,  2.76it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.72it/s]W0402 10:19:55.389000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:19:55.390000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:19:55.390000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:19:55.390000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:19:55.390000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:19:55.390000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:19:55.390000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 94/112 [00:34<00:06,  2.72it/s]W0402 10:19:55.420000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:19:55.420000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:19:55.420000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:19:55.420000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:19:55.420000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 71%|███████▏  | 80/112 [00:29<00:11,  2.74it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.72it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.73it/s]W0402 10:19:55.781000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:19:55.781000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:19:55.781000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:19:55.781000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:19:55.781000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 81/112 [00:30<00:11,  2.71it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.72it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.69it/s]W0402 10:19:56.302000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:19:56.307000 139765453952832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 73%|███████▎  | 82/112 [00:30<00:11,  2.73it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.72it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.69it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.73it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.68it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.70it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.75it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.69it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.71it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.74it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.71it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.72it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.75it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.72it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.73it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.73it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.72it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.74it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.70it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.73it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.75it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.68it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.75it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.76it/s] 80%|████████  | 90/112 [00:33<00:08,  2.67it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.76it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.77it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.66it/s]100%|██████████| 112/112 [00:41<00:00,  2.77it/s]100%|██████████| 112/112 [00:41<00:00,  2.70it/s]
 95%|█████████▍| 106/112 [00:39<00:02,  2.77it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.65it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.75it/s] 83%|████████▎ | 93/112 [00:34<00:07,  2.64it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.72it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.63it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.69it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.62it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.72it/s] 86%|████████▌ | 96/112 [00:35<00:06,  2.62it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.72it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.61it/s]100%|██████████| 112/112 [00:41<00:00,  2.71it/s]100%|██████████| 112/112 [00:41<00:00,  2.70it/s]
 88%|████████▊ | 98/112 [00:36<00:05,  2.61it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.61it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.61it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.61it/s]I0402 10:20:03.639244 1623883 finetune.py:45] layer 12_down initial loss 0.0026565254665911198
W0402 10:20:03.639604 1623883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 102/112 [00:37<00:03,  2.64it/s]W0402 10:20:04.157141 1623883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 92%|█████████▏| 103/112 [00:38<00:03,  2.68it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.67it/s]12_down proxy err 0.003452864708378911 tr(WHW.T) 10.028185844421387
 94%|█████████▍| 105/112 [00:39<00:02,  2.71it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.73it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.73it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.74it/s]W0402 10:20:06.194000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.194000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.194000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.194000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.194000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.194000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.195000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.235000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.235000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.235000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.235000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.235000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.408000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.408000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.408000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.408000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.408000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 109/112 [00:40<00:01,  2.74it/s]W0402 10:20:06.708000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.708000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.708000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.708000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.708000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.708000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.708000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.736000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.736000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.736000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.736000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.737000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 98%|█████████▊| 110/112 [00:40<00:00,  2.75it/s]W0402 10:20:06.803000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.803000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.803000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.804000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:06.804000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 99%|█████████▉| 111/112 [00:41<00:00,  2.76it/s]100%|██████████| 112/112 [00:41<00:00,  2.77it/s]100%|██████████| 112/112 [00:41<00:00,  2.69it/s]
W0402 10:20:08.049000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.448000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.449000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.449000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.449000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.449000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.449000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.449000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.472000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.472000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.472000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.472000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.472000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.472000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.472000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.490000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.491000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.491000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.491000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.491000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.501000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.501000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.501000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.502000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.502000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.658000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.659000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.659000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.659000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.659000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.858000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.858000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.858000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.858000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.858000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.960000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.960000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.960000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.960000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.961000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.961000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.961000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.990000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.990000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.990000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.990000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:08.990000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:09.058000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:09.058000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:09.058000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:09.059000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:09.059000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:09.366000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:09.371000 139813793421120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:10.319000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:10.757000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:10.757000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:10.757000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:10.757000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:10.757000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:10.757000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:10.757000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:10.788000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:10.788000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:10.788000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:10.789000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:10.789000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:11.185000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:11.185000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:11.185000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:11.185000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:11.185000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:11.732000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:11.737000 139857631434560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.040000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.040000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.040000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.040000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.040000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.041000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.041000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.080000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.080000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.080000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.080000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.080000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.245000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.245000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.245000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.245000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.246000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.544000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.544000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.545000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.545000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.545000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.545000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.545000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.576000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.576000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.576000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.576000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.576000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.643000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.643000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.643000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.643000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:14.643000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:15.894000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
I0402 10:20:16.305688 1624525 finetune.py:45] layer 13_down initial loss 0.0038785801734775305
W0402 10:20:16.306005 1624525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:20:16.331000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:16.331000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:16.331000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:16.331000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:16.332000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:16.332000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:16.332000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:16.361000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:16.361000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:16.361000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:16.361000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:16.361000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:16.716000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:16.717000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:16.717000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:16.717000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:16.717000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:16.835520 1624525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0402 10:20:17.225000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:17.230000 140358634833728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
13_down proxy err 0.0035360227338969707 tr(WHW.T) 11.704399108886719
I0402 10:20:18.897890 1624706 finetune.py:45] layer 14_down initial loss 0.0029921282548457384
W0402 10:20:18.898106 1624706 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:20:19.712400 1624706 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

14_down proxy err 0.0036742014344781637 tr(WHW.T) 13.352069854736328
I0402 10:20:21.221321 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 16 in 1.5623359680175781s
I0402 10:20:21.699114 1606370 quantize_finetune_llama.py:159] layer 17 gpu 1
I0402 10:20:23.486200 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 17 in 1.2562692165374756s
I0402 10:20:23.680171 1629356 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:20:23.680290 1629356 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:20:23.680348 1629356 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:20:23.843389 1625139 finetune.py:45] layer 15_down initial loss 0.003876088885590434
W0402 10:20:23.843628 1625139 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 10:20:23.872825 1629356 config.py:58] PyTorch version 2.4.0 available.
I0402 10:20:23.969397 1606370 quantize_finetune_llama.py:159] layer 18 gpu 2
W0402 10:20:24.334521 1625139 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

15_down proxy err 0.0036816962528973818 tr(WHW.T) 16.894805908203125
I0402 10:20:25.933553 1629527 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:20:25.933760 1629527 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:20:25.933881 1629527 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:20:26.061689 1629356 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 10:20:26.136274 1629527 config.py:58] PyTorch version 2.4.0 available.
W0402 10:20:26.410244 1629356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:20:28.132311 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 18 in 1.3794338703155518s
I0402 10:20:28.344810 1629527 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 10:20:28.625705 1606370 quantize_finetune_llama.py:159] layer 19 gpu 3
W0402 10:20:28.735502 1629527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:57,  1.85s/it]  6%|▋         | 2/32 [00:02<00:28,  1.04it/s]  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:02<00:19,  1.50it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.88it/s]I0402 10:20:30.265773 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 19 in 1.2077000141143799s
 16%|█▌        | 5/32 [00:03<00:12,  2.16it/s]I0402 10:20:30.616024 1630335 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:20:30.616178 1630335 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:20:30.616253 1630335 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:20:30.777303 1606370 quantize_finetune_llama.py:159] layer 20 gpu 0
I0402 10:20:30.829448 1630335 config.py:58] PyTorch version 2.4.0 available.
 19%|█▉        | 6/32 [00:03<00:11,  2.36it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s]  3%|▎         | 1/32 [00:01<00:58,  1.90s/it] 25%|██▌       | 8/32 [00:04<00:09,  2.55it/s]  6%|▋         | 2/32 [00:02<00:29,  1.01it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.65it/s]  9%|▉         | 3/32 [00:02<00:20,  1.45it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.76it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.81it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.83it/s]I0402 10:20:32.805807 1630589 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:20:32.805925 1630589 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:20:32.805988 1630589 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:20:32.941518 1630335 data_utils.py:336] using 256 training seqs, 128 validation seqs
 16%|█▌        | 5/32 [00:03<00:12,  2.08it/s]I0402 10:20:33.007761 1630589 config.py:58] PyTorch version 2.4.0 available.
 38%|███▊      | 12/32 [00:05<00:06,  2.86it/s]W0402 10:20:33.268304 1630335 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:03<00:11,  2.30it/s] 41%|████      | 13/32 [00:05<00:06,  2.90it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.91it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.92it/s]  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.70it/s] 50%|█████     | 16/32 [00:06<00:05,  2.98it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.99it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.75it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.00it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.80it/s]I0402 10:20:35.143494 1630589 data_utils.py:336] using 256 training seqs, 128 validation seqs
 59%|█████▉    | 19/32 [00:07<00:04,  2.98it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.81it/s]W0402 10:20:35.450822 1630589 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 62%|██████▎   | 20/32 [00:08<00:04,  2.97it/s] 41%|████      | 13/32 [00:06<00:06,  2.83it/s]  3%|▎         | 1/32 [00:01<00:55,  1.80s/it] 66%|██████▌   | 21/32 [00:08<00:03,  2.99it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.88it/s]  6%|▋         | 2/32 [00:02<00:28,  1.06it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.01it/s]  0%|          | 0/32 [00:00<?, ?it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.90it/s]  9%|▉         | 3/32 [00:02<00:19,  1.50it/s] 72%|███████▏  | 23/32 [00:09<00:02,  3.04it/s] 50%|█████     | 16/32 [00:07<00:05,  2.93it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.85it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.04it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.92it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.13it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.06it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.94it/s] 81%|████████▏ | 26/32 [00:10<00:01,  3.08it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.35it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.96it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.09it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.97it/s]  3%|▎         | 1/32 [00:01<00:54,  1.76s/it] 88%|████████▊ | 28/32 [00:10<00:01,  3.09it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.99it/s]  6%|▋         | 2/32 [00:02<00:27,  1.08it/s] 91%|█████████ | 29/32 [00:11<00:00,  3.10it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.74it/s] 69%|██████▉   | 22/32 [00:09<00:03,  3.01it/s]  9%|▉         | 3/32 [00:02<00:19,  1.52it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.11it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.80it/s] 72%|███████▏  | 23/32 [00:09<00:02,  3.02it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.88it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.11it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.85it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.02it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.17it/s]100%|██████████| 32/32 [00:12<00:00,  3.11it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
 38%|███▊      | 12/32 [00:05<00:06,  2.89it/s] 78%|███████▊  | 25/32 [00:10<00:02,  3.03it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.38it/s] 41%|████      | 13/32 [00:05<00:06,  2.87it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.97it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.89it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.97it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.61it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.87it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.95it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.70it/s] 50%|█████     | 16/32 [00:06<00:05,  2.90it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.97it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.92it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.00it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.82it/s] 97%|█████████▋| 31/32 [00:12<00:00,  3.02it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.94it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  3.01it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
 59%|█████▉    | 19/32 [00:07<00:04,  2.94it/s]W0402 10:20:42.223000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:42.223000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:42.223000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:42.223000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:42.224000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:42.224000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:42.224000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:42.249000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:42.249000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:42.249000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:42.249000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:42.249000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.89it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.92it/s]W0402 10:20:42.549000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:42.549000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:42.550000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:42.550000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:42.550000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:06<00:06,  2.85it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.91it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.86it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.89it/s]W0402 10:20:43.171000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:43.171000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:43.171000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:43.171000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:43.171000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:43.171000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:43.172000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:43.190000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:43.190000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:43.190000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:43.190000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:43.190000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:05,  2.86it/s]W0402 10:20:43.384000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:43.384000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:43.384000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:43.384000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:43.384000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:09<00:03,  2.90it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.89it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.93it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.91it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.94it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.92it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.94it/s]W0402 10:20:44.488000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.488000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.488000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.488000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.489000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.489000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.489000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.506000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.506000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.506000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.506000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.506000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:08<00:04,  2.93it/s]W0402 10:20:44.766000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.766000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.766000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.766000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.767000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.767000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.767000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.792000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.792000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.792000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.792000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:44.792000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  2.95it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.93it/s]W0402 10:20:45.073000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.073000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.073000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.073000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.073000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.102000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.102000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.102000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.102000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.102000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:11<00:01,  2.95it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.94it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.95it/s]W0402 10:20:45.660000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.660000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.660000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.660000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.660000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.660000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.660000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.677000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.677000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.677000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.677000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.677000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:09<00:03,  2.94it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 94%|█████████▍| 30/32 [00:11<00:00,  2.96it/s]W0402 10:20:45.869000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.869000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.869000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.869000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:45.869000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:09<00:02,  2.95it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.97it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.95it/s]100%|██████████| 32/32 [00:12<00:00,  2.94it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
 81%|████████▏ | 26/32 [00:10<00:02,  2.91it/s]W0402 10:20:47.024000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:47.024000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:47.024000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:47.024000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:47.025000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:47.025000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:47.025000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:47.043000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:47.043000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:47.043000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:47.043000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:47.043000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  2.88it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.85it/s]W0402 10:20:47.683000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:47.683000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:47.683000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:47.683000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:47.683000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.83it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.83it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 97%|█████████▋| 31/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
W0402 10:20:49.259000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:49.260000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:49.260000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:49.260000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:49.260000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:49.260000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:49.260000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:49.287000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:49.287000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:49.287000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:49.287000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:49.287000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:49.586000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:49.587000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:49.587000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:49.587000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:49.587000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:50.219000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:50.219000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:50.219000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:50.219000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:50.220000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:50.220000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:50.220000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:50.238000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:50.238000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:50.238000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:50.238000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:50.238000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:50.444000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:50.444000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:50.444000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:50.444000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:50.444000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
I0402 10:20:51.111250 1629356 finetune.py:45] layer 16_v initial loss 0.004161989316344261
W0402 10:20:51.111662 1629356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:20:51.565000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.565000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.565000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.566000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.566000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.566000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.566000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.583000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.583000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.583000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.583000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.583000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.723000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.723000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.723000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.723000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.723000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.723000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.723000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.751000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.751000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.751000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.751000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:51.751000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.053000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.053000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.053000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.053000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.053000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.171748 1629356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 10:20:52.187000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.187000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.187000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.187000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.187000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.684000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.685000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.685000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.685000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.685000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.685000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.685000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.703000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.703000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.703000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.703000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.703000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.911000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.911000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.911000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.911000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:52.911000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
16_v proxy err 0.0027892389334738255 tr(WHW.T) 68.60269165039062
  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:20:53.805637 1629527 finetune.py:45] layer 17_v initial loss 0.0052413418889045715
W0402 10:20:53.806006 1629527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:20:54.087000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:20:54.087000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:54.087000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:54.088000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:54.088000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:54.088000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:20:54.088000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:54.105000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:54.105000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:54.105000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:54.105000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:54.105000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:40,  1.30s/it]W0402 10:20:54.772000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:20:54.772000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:20:54.772000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:20:54.772000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:20:54.773000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:20:54.814693 1629527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:01<00:22,  1.32it/s]  9%|▉         | 3/32 [00:02<00:16,  1.71it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 12%|█▎        | 4/32 [00:02<00:14,  2.00it/s]17_v proxy err 0.0036693287547677755 tr(WHW.T) 69.1786880493164
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.20it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.35it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s]  3%|▎         | 1/32 [00:01<00:41,  1.33s/it] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s]  6%|▋         | 2/32 [00:01<00:23,  1.29it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.56it/s]  9%|▉         | 3/32 [00:02<00:17,  1.67it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.62it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.94it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.64it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.12it/s]I0402 10:20:58.760378 1630335 finetune.py:45] layer 18_v initial loss 0.004396330565214157
W0402 10:20:58.760544 1630335 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.26it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.67it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.35it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.41it/s] 50%|█████     | 16/32 [00:06<00:06,  2.67it/s]W0402 10:21:00.194318 1630335 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:04<00:09,  2.47it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.67it/s]I0402 10:21:00.610828 1630589 finetune.py:45] layer 19_v initial loss 0.006597564090043306
W0402 10:21:00.613116 1630589 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.72it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.57it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s]18_v proxy err 0.00304727116599679 tr(WHW.T) 73.61924743652344
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.76it/s] 41%|████      | 13/32 [00:05<00:07,  2.63it/s]W0402 10:21:01.959015 1630589 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.64it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.77it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s]  3%|▎         | 1/32 [00:01<00:39,  1.26s/it] 72%|███████▏  | 23/32 [00:09<00:03,  2.79it/s] 50%|█████     | 16/32 [00:07<00:05,  2.67it/s]  6%|▋         | 2/32 [00:01<00:22,  1.34it/s]19_v proxy err 0.002952983370050788 tr(WHW.T) 87.24554443359375
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.79it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.69it/s]  9%|▉         | 3/32 [00:02<00:16,  1.75it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.80it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.03it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.78it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.70it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.22it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.80it/s]  3%|▎         | 1/32 [00:01<00:40,  1.30s/it] 62%|██████▎   | 20/32 [00:08<00:04,  2.71it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.81it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.37it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s]  6%|▋         | 2/32 [00:01<00:22,  1.32it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.81it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.71it/s]  9%|▉         | 3/32 [00:02<00:16,  1.72it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.00it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.82it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.61it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.71it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.19it/s]100%|██████████| 32/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
 31%|███▏      | 10/32 [00:04<00:08,  2.67it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.71it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.33it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.70it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.40it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.72it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.68it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.48it/s] 41%|████      | 13/32 [00:05<00:06,  2.74it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.66it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.73it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.73it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.74it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.70it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.74it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
 56%|█████▋    | 18/32 [00:07<00:05,  2.75it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.63it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.72it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s] 50%|█████     | 16/32 [00:06<00:06,  2.61it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.72it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.63it/s]W0402 10:21:11.295000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.295000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.295000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.296000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.296000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.296000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.296000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.323000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.323000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.323000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.323000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.323000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:10<00:02,  2.74it/s]W0402 10:21:11.479000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.479000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.479000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.480000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.480000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s]W0402 10:21:11.688000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.688000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.688000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.688000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.688000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.688000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.688000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.707000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.707000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.707000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.707000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.707000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.769000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.769000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.769000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.769000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:11.769000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:10<00:02,  2.76it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.66it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.66it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.77it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.67it/s]W0402 10:21:12.748000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.77it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s]W0402 10:21:13.046000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:21:13.046000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:13.046000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:13.046000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:13.046000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:13.046000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:13.047000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:13.068000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:13.068000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:13.068000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:13.068000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:13.068000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.77it/s]W0402 10:21:13.307000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:13.307000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:13.307000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:13.307000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:13.307000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:10<00:02,  2.62it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.77it/s]W0402 10:21:13.740000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:10<00:02,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
 84%|████████▍ | 27/32 [00:11<00:01,  2.62it/s]W0402 10:21:14.268000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.269000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.269000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.269000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.269000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.269000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.269000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.299000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.299000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.299000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.299000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.299000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.465000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.466000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.466000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.466000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.466000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s]W0402 10:21:14.692000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.692000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.692000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.692000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.692000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.693000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.693000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.714000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.715000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.715000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.715000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.715000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.779000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.779000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.779000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.779000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:14.779000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.65it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.66it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]W0402 10:21:15.822000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.46it/s]
W0402 10:21:16.141000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:16.141000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:16.141000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:16.141000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:16.141000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:16.141000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:21:16.141000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:16.164000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:16.164000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:16.164000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:16.164000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:16.164000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:16.416000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:16.417000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:16.417000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:16.417000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:16.417000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:16.884000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.601000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.601000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.601000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.601000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.601000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.601000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.601000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.629000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.629000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.629000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.629000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.629000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.786000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.786000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.786000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.787000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.787000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.996000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.996000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.996000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.996000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.996000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.996000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:19.997000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:20.015000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:20.015000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:20.016000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:20.016000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:20.016000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
I0402 10:21:20.023616 1629356 finetune.py:45] layer 16_q initial loss 0.004163822159171104
W0402 10:21:20.023878 1629356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:21:20.077000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:20.077000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:20.077000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:20.077000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:20.077000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.024970 1629356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 10:21:21.070000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.370000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.370000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.370000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.370000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.371000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.371000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.371000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.392000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.392000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.392000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.392000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.393000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.632000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.632000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.632000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.632000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.632000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.669000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.669000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.669000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.669000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.669000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.669000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.669000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.697000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.697000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.698000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.698000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.698000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.855000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.855000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.855000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.855000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:21.855000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:22.062000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:22.067000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:22.067000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:22.068000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:22.068000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:22.068000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:22.068000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:21:22.068000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:22.087000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:22.087000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:22.087000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:22.087000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:22.088000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
16_q proxy err 0.0005390479927882552 tr(WHW.T) 6124.5419921875
  0%|          | 0/32 [00:00<?, ?it/s]W0402 10:21:22.153000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:22.153000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:22.153000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:22.153000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:22.153000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:23.182000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0402 10:21:23.194272 1629527 finetune.py:45] layer 17_q initial loss 0.005239454563707113
W0402 10:21:23.194568 1629527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:38,  1.24s/it]W0402 10:21:23.483000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:23.483000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:23.483000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:23.484000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:23.484000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:21:23.484000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:21:23.484000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:21:23.504000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:23.504000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:23.504000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:23.504000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:23.504000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:21,  1.38it/s]W0402 10:21:23.743000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:21:23.743000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:21:23.743000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:23.743000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:21:23.743000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:01<00:16,  1.80it/s]W0402 10:21:24.204000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:21:24.250473 1629527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:02<00:13,  2.08it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.47it/s]17_q proxy err 0.0005485940491780639 tr(WHW.T) 6716.6806640625
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.75it/s]  3%|▎         | 1/32 [00:01<00:37,  1.20s/it] 31%|███▏      | 10/32 [00:04<00:07,  2.78it/s]  6%|▋         | 2/32 [00:01<00:21,  1.42it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.81it/s]  9%|▉         | 3/32 [00:01<00:15,  1.85it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.83it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.14it/s] 41%|████      | 13/32 [00:05<00:06,  2.84it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.86it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.87it/s]I0402 10:21:28.495391 1630335 finetune.py:45] layer 18_q initial loss 0.00439695967361331
W0402 10:21:28.495578 1630335 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:03<00:09,  2.60it/s] 50%|█████     | 16/32 [00:06<00:05,  2.86it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.67it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.87it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.87it/s]W0402 10:21:29.561559 1630335 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.86it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.79it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.85it/s]18_q proxy err 0.0006733454647473991 tr(WHW.T) 5733.501953125
  0%|          | 0/32 [00:00<?, ?it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.85it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.81it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.86it/s]I0402 10:21:31.221064 1630589 finetune.py:45] layer 19_q initial loss 0.006597243715077639
W0402 10:21:31.221434 1630589 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 47%|████▋     | 15/32 [00:06<00:06,  2.81it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.87it/s] 50%|█████     | 16/32 [00:06<00:05,  2.79it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s]  3%|▎         | 1/32 [00:01<00:38,  1.25s/it] 53%|█████▎    | 17/32 [00:06<00:05,  2.79it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.85it/s]  6%|▋         | 2/32 [00:01<00:21,  1.37it/s]W0402 10:21:32.306869 1630589 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 56%|█████▋    | 18/32 [00:07<00:05,  2.80it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.85it/s]  9%|▉         | 3/32 [00:01<00:16,  1.77it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.80it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.06it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.86it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.79it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s]19_q proxy err 0.0006465492770075798 tr(WHW.T) 6149.1044921875
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.44it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.84it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s]100%|██████████| 32/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
 72%|███████▏  | 23/32 [00:09<00:03,  2.79it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.63it/s]  3%|▎         | 1/32 [00:01<00:38,  1.23s/it] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.66it/s]  6%|▋         | 2/32 [00:01<00:21,  1.38it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s]  9%|▉         | 3/32 [00:01<00:16,  1.79it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.76it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.73it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.06it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.75it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.27it/s] 41%|████      | 13/32 [00:05<00:06,  2.76it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.40it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.73it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.78it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.71it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.80it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.80it/s] 28%|██▊       | 9/32 [00:04<00:10,  2.09it/s]100%|██████████| 32/32 [00:12<00:00,  2.17it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
 53%|█████▎    | 17/32 [00:07<00:06,  2.21it/s] 31%|███▏      | 10/32 [00:04<00:09,  2.24it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.35it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.38it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.48it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.47it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.56it/s] 41%|████      | 13/32 [00:05<00:07,  2.54it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.63it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.68it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.73it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 50%|█████     | 16/32 [00:07<00:05,  2.68it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.78it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.70it/s]I0402 10:21:40.963678 1629356 finetune.py:45] layer 16_k initial loss 0.004163689911365509
W0402 10:21:40.964046 1629356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:10<00:02,  2.79it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.71it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.80it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.72it/s]W0402 10:21:41.983505 1629356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:11<00:01,  2.81it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.73it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.78it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.71it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.77it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s]16_k proxy err 0.0003830502973869443 tr(WHW.T) 4878.88427734375
  0%|          | 0/32 [00:00<?, ?it/s]100%|██████████| 32/32 [00:12<00:00,  2.77it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.67it/s]  3%|▎         | 1/32 [00:00<00:24,  1.26it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s]  6%|▋         | 2/32 [00:01<00:16,  1.84it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.68it/s]I0402 10:21:44.517563 1629527 finetune.py:45] layer 17_k initial loss 0.005244616884738207
W0402 10:21:44.517827 1629527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:01<00:13,  2.15it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.70it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.37it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.69it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.51it/s]W0402 10:21:45.518986 1629527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:12<00:00,  2.66it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.64it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.65it/s]100%|██████████| 32/32 [00:13<00:00,  2.62it/s]100%|██████████| 32/32 [00:13<00:00,  2.46it/s]
 25%|██▌       | 8/32 [00:03<00:08,  2.70it/s]17_k proxy err 0.0004886124515905976 tr(WHW.T) 4246.078125
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.74it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s]  3%|▎         | 1/32 [00:00<00:23,  1.31it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.80it/s]  6%|▋         | 2/32 [00:01<00:15,  1.90it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.80it/s]  9%|▉         | 3/32 [00:01<00:13,  2.22it/s] 41%|████      | 13/32 [00:05<00:06,  2.81it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.40it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.81it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.52it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.82it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.59it/s] 50%|█████     | 16/32 [00:06<00:05,  2.84it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.65it/s]I0402 10:21:49.610640 1630335 finetune.py:45] layer 18_k initial loss 0.004397298209369183
W0402 10:21:49.610826 1630335 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.85it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.70it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s]W0402 10:21:50.601570 1630335 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.73it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.84it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.75it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.84it/s]18_k proxy err 0.0005278856260702014 tr(WHW.T) 4452.47412109375
  0%|          | 0/32 [00:00<?, ?it/s] 41%|████      | 13/32 [00:05<00:06,  2.75it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.84it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.83it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.73it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.83it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.73it/s]  3%|▎         | 1/32 [00:00<00:24,  1.26it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.82it/s] 50%|█████     | 16/32 [00:06<00:05,  2.74it/s]  6%|▋         | 2/32 [00:01<00:16,  1.81it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.82it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.74it/s]  9%|▉         | 3/32 [00:01<00:13,  2.11it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.83it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.75it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.28it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.81it/s]I0402 10:21:53.871562 1630589 finetune.py:45] layer 19_k initial loss 0.006597649306058884
W0402 10:21:53.871942 1630589 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.74it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s]W0402 10:21:55.021560 1630589 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.72it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.70it/s]19_k proxy err 0.0005608235951513052 tr(WHW.T) 3978.64404296875
  0%|          | 0/32 [00:00<?, ?it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.69it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.66it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.71it/s]  3%|▎         | 1/32 [00:00<00:24,  1.26it/s] 41%|████      | 13/32 [00:05<00:07,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.68it/s]  6%|▋         | 2/32 [00:01<00:16,  1.86it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.73it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.65it/s]  9%|▉         | 3/32 [00:01<00:13,  2.18it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.63it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.36it/s] 50%|█████     | 16/32 [00:06<00:05,  2.74it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.48it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.61it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
 22%|██▏       | 7/32 [00:02<00:09,  2.61it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.66it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.67it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.76it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.70it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.77it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.71it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s] 41%|████      | 13/32 [00:05<00:06,  2.74it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.78it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s]I0402 10:22:01.814773 1629356 finetune.py:45] layer 16_o initial loss 0.004263401497155428
W0402 10:22:01.815201 1629356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.72it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.76it/s]W0402 10:22:02.787838 1629356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 17/32 [00:06<00:05,  2.72it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 62%|██████▎   | 20/32 [00:07<00:04,  2.69it/s]16_o proxy err 0.0029711690731346607 tr(WHW.T) 7.369767189025879
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.64it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s]I0402 10:22:05.713824 1629527 finetune.py:45] layer 17_o initial loss 0.005292717833071947
W0402 10:22:05.714137 1629527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 78%|███████▊  | 25/32 [00:09<00:02,  2.67it/s]  3%|▎         | 1/32 [00:01<01:00,  1.96s/it] 81%|████████▏ | 26/32 [00:10<00:02,  2.65it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s]W0402 10:22:06.649000 1629527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 88%|████████▊ | 28/32 [00:10<00:01,  2.59it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.57it/s]  6%|▋         | 2/32 [00:03<00:49,  1.65s/it] 94%|█████████▍| 30/32 [00:11<00:00,  2.56it/s]17_o proxy err 0.0030884696170687675 tr(WHW.T) 6.400020122528076
  0%|          | 0/32 [00:00<?, ?it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.56it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
  9%|▉         | 3/32 [00:04<00:44,  1.54s/it]  3%|▎         | 1/32 [00:01<00:59,  1.91s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.48s/it]I0402 10:22:10.548134 1630335 finetune.py:45] layer 18_o initial loss 0.00442621111869812
W0402 10:22:10.548379 1630335 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:03<00:49,  1.64s/it]W0402 10:22:11.505743 1630335 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:07<00:39,  1.45s/it]  9%|▉         | 3/32 [00:04<00:45,  1.55s/it]18_o proxy err 0.0029783097561448812 tr(WHW.T) 4.730301856994629
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:08<00:37,  1.43s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.42s/it]  3%|▎         | 1/32 [00:02<01:02,  2.03s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.50s/it] 25%|██▌       | 8/32 [00:11<00:33,  1.41s/it]  6%|▋         | 2/32 [00:03<00:52,  1.74s/it]I0402 10:22:16.271750 1630589 finetune.py:45] layer 19_o initial loss 0.006487781181931496
W0402 10:22:16.271972 1630589 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.41s/it]W0402 10:22:17.249879 1630589 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.48s/it]19_o proxy err 0.0031555157620459795 tr(WHW.T) 3.939014434814453
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:14<00:31,  1.41s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.48s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.41s/it]  3%|▎         | 1/32 [00:01<01:00,  1.95s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.52s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.48s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it]  6%|▋         | 2/32 [00:03<00:50,  1.67s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.50s/it] 41%|████      | 13/32 [00:18<00:26,  1.41s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.48s/it]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.48s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.48s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.52s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.50s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.46s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.45s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.46s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it] 50%|█████     | 16/32 [00:23<00:23,  1.48s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.48s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.45s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.48s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.41s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.48s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.45s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.48s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 50%|█████     | 16/32 [00:23<00:23,  1.46s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.41s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.48s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.45s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.41s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.45s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.40s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.48s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.45s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.40s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.41s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.48s/it] 66%|██████▌   | 21/32 [00:31<00:15,  1.45s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.41s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.48s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.45s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.41s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.48s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.45s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.41s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.44s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.41s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.47s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.48s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.44s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]
 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.50s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.45s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.48s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.50s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.45s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.45s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.51s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.45s/it]100%|██████████| 32/32 [00:47<00:00,  1.53s/it]100%|██████████| 32/32 [00:47<00:00,  1.50s/it]
 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.45s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.48s/it]I0402 10:22:57.626949 1629356 finetune.py:45] layer 16_up initial loss 0.004242491442710161
W0402 10:22:57.627275 1629356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 31/32 [00:45<00:01,  1.45s/it]W0402 10:22:58.518069 1629356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it]16_up proxy err 0.003115118946880102 tr(WHW.T) 981.8323974609375
  0%|          | 0/32 [00:00<?, ?it/s]100%|██████████| 32/32 [00:47<00:00,  1.44s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]
 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it]  3%|▎         | 1/32 [00:01<00:59,  1.92s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it]  6%|▋         | 2/32 [00:03<00:49,  1.67s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.49s/it]I0402 10:23:03.839331 1629527 finetune.py:45] layer 17_up initial loss 0.005259065423160791
W0402 10:23:03.839546 1629527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:04<00:45,  1.56s/it]W0402 10:23:04.701699 1629527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:46<00:01,  1.51s/it]17_up proxy err 0.0030529124196618795 tr(WHW.T) 1061.939208984375
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:06<00:42,  1.50s/it]100%|██████████| 32/32 [00:47<00:00,  1.52s/it]100%|██████████| 32/32 [00:47<00:00,  1.50s/it]
 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it]I0402 10:23:07.546335 1630335 finetune.py:45] layer 18_up initial loss 0.004388532601296902
W0402 10:23:07.546699 1630335 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:57,  1.84s/it]W0402 10:23:08.393762 1630335 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:09<00:37,  1.45s/it]  6%|▋         | 2/32 [00:03<00:48,  1.61s/it]18_up proxy err 0.0033242199569940567 tr(WHW.T) 1055.73193359375
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:10<00:35,  1.43s/it]  9%|▉         | 3/32 [00:04<00:44,  1.53s/it]  3%|▎         | 1/32 [00:01<01:00,  1.94s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.42s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.50s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.41s/it]  6%|▋         | 2/32 [00:03<00:50,  1.70s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.48s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.41s/it]  9%|▉         | 3/32 [00:04<00:46,  1.62s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.47s/it]I0402 10:23:15.234207 1630589 finetune.py:45] layer 19_up initial loss 0.006435264367610216
W0402 10:23:15.234571 1630589 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:16<00:29,  1.41s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.57s/it]W0402 10:23:16.144716 1630589 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it]19_up proxy err 0.0034955423325300217 tr(WHW.T) 1067.8548583984375
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it] 25%|██▌       | 8/32 [00:11<00:35,  1.46s/it] 41%|████      | 13/32 [00:18<00:26,  1.41s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.50s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it]  3%|▎         | 1/32 [00:01<00:58,  1.88s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.41s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.48s/it] 31%|███▏      | 10/32 [00:14<00:32,  1.46s/it]  6%|▋         | 2/32 [00:03<00:49,  1.64s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.41s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it]  9%|▉         | 3/32 [00:04<00:45,  1.57s/it] 50%|█████     | 16/32 [00:23<00:22,  1.41s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.55s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.46s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.46s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 50%|█████     | 16/32 [00:23<00:23,  1.46s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.50s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.41s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.46s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.51s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.41s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.50s/it] 50%|█████     | 16/32 [00:23<00:23,  1.45s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.46s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.41s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.45s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.46s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.41s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.44s/it] 66%|██████▌   | 21/32 [00:30<00:16,  1.46s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.45s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.46s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.42s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.50s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.44s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.46s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.41s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.50s/it] 66%|██████▌   | 21/32 [00:31<00:15,  1.44s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.46s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.41s/it] 50%|█████     | 16/32 [00:24<00:23,  1.50s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.44s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.46s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.41s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.45s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.46s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.41s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.45s/it]100%|██████████| 32/32 [00:45<00:00,  1.40s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]
 84%|████████▍ | 27/32 [00:39<00:07,  1.45s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.49s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.45s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.44s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.48s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.44s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.49s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.44s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.50s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.44s/it]100%|██████████| 32/32 [00:47<00:00,  1.51s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
I0402 10:23:53.565700 1629356 finetune.py:45] layer 16_gate initial loss 0.004204174038022757
W0402 10:23:53.566025 1629356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 94%|█████████▍| 30/32 [00:44<00:02,  1.44s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it]W0402 10:23:54.329529 1629356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:45<00:01,  1.43s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.47s/it]
 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it]16_gate proxy err 0.0010900095803663135 tr(WHW.T) 4852.61962890625
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:35,  1.16it/s] 84%|████████▍ | 27/32 [00:40<00:07,  1.51s/it]  2%|▏         | 2/112 [00:01<01:03,  1.73it/s]  3%|▎         | 3/112 [00:01<00:53,  2.05it/s]  4%|▎         | 4/112 [00:01<00:47,  2.26it/s]  4%|▍         | 5/112 [00:02<00:44,  2.38it/s] 88%|████████▊ | 28/32 [00:42<00:06,  1.50s/it]  5%|▌         | 6/112 [00:02<00:43,  2.46it/s]  6%|▋         | 7/112 [00:03<00:41,  2.51it/s]  7%|▋         | 8/112 [00:03<00:40,  2.55it/s]  8%|▊         | 9/112 [00:03<00:39,  2.58it/s] 91%|█████████ | 29/32 [00:43<00:04,  1.52s/it]I0402 10:24:01.330370 1629527 finetune.py:45] layer 17_gate initial loss 0.005190703552216291
W0402 10:24:01.330593 1629527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 10/112 [00:04<00:38,  2.62it/s] 10%|▉         | 11/112 [00:04<00:38,  2.65it/s]W0402 10:24:02.060350 1629527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 11%|█         | 12/112 [00:04<00:37,  2.69it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.69it/s] 94%|█████████▍| 30/32 [00:45<00:03,  1.53s/it] 12%|█▎        | 14/112 [00:05<00:36,  2.69it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.67it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.68it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.71it/s] 97%|█████████▋| 31/32 [00:46<00:01,  1.53s/it] 16%|█▌        | 18/112 [00:07<00:34,  2.74it/s]I0402 10:24:04.503654 1630335 finetune.py:45] layer 18_gate initial loss 0.004364617168903351
W0402 10:24:04.504060 1630335 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

17_gate proxy err 0.001100526424124837 tr(WHW.T) 5241.20556640625
  0%|          | 0/112 [00:00<?, ?it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.77it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.77it/s]W0402 10:24:05.243030 1630335 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 21/112 [00:08<00:32,  2.76it/s]  1%|          | 1/112 [00:00<01:30,  1.23it/s]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
 20%|█▉        | 22/112 [00:08<00:32,  2.77it/s]  2%|▏         | 2/112 [00:01<01:00,  1.82it/s] 21%|██        | 23/112 [00:08<00:32,  2.77it/s]  3%|▎         | 3/112 [00:01<00:50,  2.16it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.79it/s]  4%|▎         | 4/112 [00:01<00:45,  2.35it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.79it/s]  4%|▍         | 5/112 [00:02<00:43,  2.48it/s] 23%|██▎       | 26/112 [00:10<00:30,  2.78it/s]  5%|▌         | 6/112 [00:02<00:41,  2.55it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.78it/s]  6%|▋         | 7/112 [00:03<00:40,  2.61it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.79it/s]  7%|▋         | 8/112 [00:03<00:39,  2.66it/s]18_gate proxy err 0.0013595829950645566 tr(WHW.T) 4660.9404296875
  0%|          | 0/112 [00:00<?, ?it/s] 26%|██▌       | 29/112 [00:11<00:29,  2.79it/s]  8%|▊         | 9/112 [00:03<00:38,  2.69it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.81it/s]  9%|▉         | 10/112 [00:04<00:37,  2.71it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.82it/s]  1%|          | 1/112 [00:00<01:33,  1.18it/s] 10%|▉         | 11/112 [00:04<00:37,  2.72it/s] 29%|██▊       | 32/112 [00:12<00:28,  2.83it/s]  2%|▏         | 2/112 [00:01<01:03,  1.72it/s] 11%|█         | 12/112 [00:04<00:36,  2.73it/s] 29%|██▉       | 33/112 [00:12<00:27,  2.83it/s]  3%|▎         | 3/112 [00:01<00:53,  2.02it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.73it/s] 30%|███       | 34/112 [00:12<00:27,  2.83it/s]  4%|▎         | 4/112 [00:02<00:48,  2.21it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.74it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.83it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.74it/s]  4%|▍         | 5/112 [00:02<00:46,  2.32it/s] 32%|███▏      | 36/112 [00:13<00:26,  2.84it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.75it/s]  5%|▌         | 6/112 [00:02<00:44,  2.40it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.83it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.75it/s]  6%|▋         | 7/112 [00:03<00:42,  2.45it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.84it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.75it/s]  7%|▋         | 8/112 [00:03<00:41,  2.48it/s] 35%|███▍      | 39/112 [00:14<00:25,  2.84it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.74it/s]  8%|▊         | 9/112 [00:03<00:41,  2.50it/s] 36%|███▌      | 40/112 [00:15<00:25,  2.84it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.75it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.84it/s]  9%|▉         | 10/112 [00:04<00:40,  2.52it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.75it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.84it/s] 10%|▉         | 11/112 [00:04<00:39,  2.54it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.76it/s] 38%|███▊      | 43/112 [00:16<00:24,  2.83it/s] 11%|█         | 12/112 [00:05<00:39,  2.54it/s] 21%|██        | 23/112 [00:08<00:32,  2.75it/s] 39%|███▉      | 44/112 [00:16<00:23,  2.84it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.55it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.76it/s] 40%|████      | 45/112 [00:16<00:23,  2.85it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.55it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.76it/s] 41%|████      | 46/112 [00:17<00:23,  2.85it/s]I0402 10:24:14.478190 1630589 finetune.py:45] layer 19_gate initial loss 0.0064043449237942696
W0402 10:24:14.478500 1630589 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 13%|█▎        | 15/112 [00:06<00:38,  2.55it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.75it/s] 42%|████▏     | 47/112 [00:17<00:22,  2.85it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.58it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.75it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.84it/s]W0402 10:24:15.260900 1630589 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 15%|█▌        | 17/112 [00:07<00:36,  2.62it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.75it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.83it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.61it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.73it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.81it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.61it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.74it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.82it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.74it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.59it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.82it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.73it/s] 47%|████▋     | 53/112 [00:19<00:20,  2.82it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.57it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.74it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.82it/s] 20%|█▉        | 22/112 [00:09<00:34,  2.58it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.81it/s] 30%|███       | 34/112 [00:12<00:28,  2.73it/s] 21%|██        | 23/112 [00:09<00:34,  2.60it/s] 50%|█████     | 56/112 [00:20<00:19,  2.82it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.73it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.63it/s] 51%|█████     | 57/112 [00:21<00:19,  2.82it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.74it/s]19_gate proxy err 0.0015008387854322791 tr(WHW.T) 4580.61328125
  0%|          | 0/112 [00:00<?, ?it/s] 22%|██▏       | 25/112 [00:10<00:32,  2.66it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.82it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.74it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.67it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.81it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.72it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.68it/s]  1%|          | 1/112 [00:00<01:30,  1.23it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.82it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.73it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.69it/s]  2%|▏         | 2/112 [00:01<01:00,  1.81it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.81it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.72it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.66it/s]  3%|▎         | 3/112 [00:01<00:51,  2.11it/s] 55%|█████▌    | 62/112 [00:22<00:18,  2.78it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.72it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.68it/s]  4%|▎         | 4/112 [00:01<00:46,  2.32it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.78it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.72it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.69it/s]  4%|▍         | 5/112 [00:02<00:43,  2.45it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.78it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.72it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.69it/s]  5%|▌         | 6/112 [00:02<00:41,  2.54it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.79it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.72it/s] 29%|██▉       | 33/112 [00:13<00:29,  2.70it/s]  6%|▋         | 7/112 [00:03<00:40,  2.60it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.80it/s] 40%|████      | 45/112 [00:16<00:24,  2.72it/s] 30%|███       | 34/112 [00:13<00:28,  2.71it/s]  7%|▋         | 8/112 [00:03<00:39,  2.63it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.78it/s] 41%|████      | 46/112 [00:17<00:24,  2.71it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.70it/s]  8%|▊         | 9/112 [00:03<00:38,  2.65it/s] 61%|██████    | 68/112 [00:24<00:15,  2.78it/s] 42%|████▏     | 47/112 [00:17<00:24,  2.70it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.70it/s]  9%|▉         | 10/112 [00:04<00:38,  2.67it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.79it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.72it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.70it/s] 10%|▉         | 11/112 [00:04<00:37,  2.68it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.79it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.71it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.67it/s] 11%|█         | 12/112 [00:04<00:37,  2.66it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.79it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.72it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.68it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.67it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.80it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.72it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.64it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.65it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.80it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.72it/s] 37%|███▋      | 41/112 [00:16<00:26,  2.67it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.79it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.66it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.73it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.68it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.79it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.68it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.73it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.68it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.79it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.69it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.73it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.79it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.69it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.69it/s] 50%|█████     | 56/112 [00:20<00:20,  2.73it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.79it/s] 40%|████      | 45/112 [00:17<00:24,  2.70it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.69it/s] 51%|█████     | 57/112 [00:21<00:20,  2.71it/s] 71%|███████   | 79/112 [00:28<00:11,  2.77it/s] 41%|████      | 46/112 [00:17<00:24,  2.69it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.69it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.71it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.78it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.69it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.69it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.71it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.78it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.67it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.68it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.71it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.78it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.68it/s] 21%|██        | 23/112 [00:08<00:33,  2.69it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.72it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.80it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.68it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.70it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.73it/s] 75%|███████▌  | 84/112 [00:30<00:09,  2.81it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.68it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.68it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.73it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.79it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.69it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.69it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.73it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.80it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.69it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.70it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.74it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.80it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.68it/s] 25%|██▌       | 28/112 [00:10<00:31,  2.70it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.74it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.80it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.69it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.70it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.75it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.80it/s] 50%|█████     | 56/112 [00:21<00:20,  2.68it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.69it/s] 61%|██████    | 68/112 [00:25<00:16,  2.75it/s] 80%|████████  | 90/112 [00:32<00:07,  2.80it/s] 51%|█████     | 57/112 [00:22<00:20,  2.63it/s] 28%|██▊       | 31/112 [00:11<00:30,  2.67it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.73it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.78it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.62it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.64it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.73it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.79it/s] 53%|█████▎    | 59/112 [00:22<00:20,  2.64it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.66it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.74it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.80it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.67it/s] 30%|███       | 34/112 [00:13<00:29,  2.68it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.74it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.80it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.69it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.67it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.74it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.79it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.70it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.68it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.74it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.79it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.70it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.68it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.73it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.78it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.70it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.69it/s] 88%|████████▊ | 98/112 [00:35<00:05,  2.79it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.73it/s] 35%|███▍      | 39/112 [00:14<00:27,  2.69it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.69it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.79it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.73it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.69it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.68it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.79it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.74it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.79it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.68it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.67it/s] 71%|███████   | 79/112 [00:29<00:12,  2.72it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.78it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.68it/s] 61%|██████    | 68/112 [00:26<00:16,  2.69it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.71it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.78it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.70it/s] 62%|██████▏   | 69/112 [00:26<00:15,  2.70it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.71it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.79it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.70it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.70it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.72it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.75it/s] 40%|████      | 45/112 [00:17<00:24,  2.70it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.70it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.72it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.77it/s] 41%|████      | 46/112 [00:17<00:24,  2.71it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.71it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.72it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.79it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.71it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.71it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.73it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.80it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.71it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.71it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.72it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.78it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.70it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.69it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.72it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.78it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.66it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.70it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.66it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.78it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.68it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.70it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.68it/s]100%|██████████| 112/112 [00:40<00:00,  2.78it/s]100%|██████████| 112/112 [00:40<00:00,  2.75it/s]
 46%|████▋     | 52/112 [00:19<00:22,  2.69it/s] 80%|████████  | 90/112 [00:33<00:08,  2.71it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.68it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.70it/s] 71%|███████   | 79/112 [00:30<00:12,  2.70it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.70it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.71it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.71it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.67it/s] 49%|████▉     | 55/112 [00:20<00:21,  2.71it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.72it/s] 83%|████████▎ | 93/112 [00:34<00:07,  2.65it/s] 50%|█████     | 56/112 [00:21<00:20,  2.71it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.72it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.67it/s] 51%|█████     | 57/112 [00:21<00:20,  2.73it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.72it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.66it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.72it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.70it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.67it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.71it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.69it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.69it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.72it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.69it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.71it/s] 54%|█████▍    | 61/112 [00:23<00:18,  2.72it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.70it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.72it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.73it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.71it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.73it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.72it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.70it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.72it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.72it/s] 80%|████████  | 90/112 [00:34<00:08,  2.71it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.74it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.72it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.71it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.73it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.72it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.70it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.73it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.72it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.69it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.73it/s] 61%|██████    | 68/112 [00:25<00:16,  2.67it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.72it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.66it/s]W0402 10:24:44.384000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.384000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.384000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.384000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.384000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.384000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.385000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
 62%|██████▏   | 69/112 [00:26<00:16,  2.67it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.73it/s]W0402 10:24:44.425000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.425000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.425000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.425000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.425000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 85%|████████▍ | 95/112 [00:36<00:06,  2.65it/s]W0402 10:24:44.589000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.589000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.589000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.589000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.589000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 70/112 [00:26<00:15,  2.69it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.73it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.67it/s]W0402 10:24:44.890000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.890000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.890000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.890000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.890000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.890000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.890000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.919000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.919000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.919000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.919000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.919000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.985000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.985000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.985000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.985000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:44.985000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 63%|██████▎   | 71/112 [00:26<00:15,  2.69it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.73it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.67it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.71it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.73it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.68it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.72it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.73it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.69it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.72it/s]W0402 10:24:46.227000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
100%|██████████| 112/112 [00:41<00:00,  2.74it/s]100%|██████████| 112/112 [00:41<00:00,  2.70it/s]
 89%|████████▉ | 100/112 [00:38<00:04,  2.70it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.73it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.68it/s]W0402 10:24:46.668000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:24:46.669000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:46.669000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:46.669000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:46.669000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:46.669000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:24:46.669000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:46.700000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:46.700000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:46.700000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:46.700000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:46.700000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 68%|██████▊   | 76/112 [00:28<00:13,  2.73it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.69it/s]W0402 10:24:47.080000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:47.080000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:47.080000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:47.080000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:47.080000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 77/112 [00:28<00:12,  2.72it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.69it/s]W0402 10:24:47.620000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:24:47.626000 140012024231744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 70%|██████▉   | 78/112 [00:29<00:12,  2.71it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.68it/s] 71%|███████   | 79/112 [00:29<00:12,  2.68it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.65it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.69it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.67it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.70it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.69it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.71it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.69it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.69it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.71it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.72it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.72it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.73it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.72it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.70it/s]100%|██████████| 112/112 [00:42<00:00,  2.68it/s]100%|██████████| 112/112 [00:42<00:00,  2.63it/s]
 78%|███████▊  | 87/112 [00:32<00:09,  2.70it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.66it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.63it/s] 80%|████████  | 90/112 [00:33<00:08,  2.62it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.66it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.65it/s] 83%|████████▎ | 93/112 [00:34<00:07,  2.67it/s]W0402 10:24:53.394000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.395000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.395000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.395000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.395000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.395000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.395000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.439000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.439000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.439000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.439000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.439000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.618000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.618000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.618000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.619000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.619000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 94/112 [00:35<00:06,  2.68it/s]W0402 10:24:53.944000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.944000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.944000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.944000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.945000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.945000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.945000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.975000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.975000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.976000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.976000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:53.976000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:54.046000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:54.046000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:54.046000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:54.047000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:54.047000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 85%|████████▍ | 95/112 [00:35<00:06,  2.68it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.69it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.69it/s]I0402 10:24:54.818642 1629356 finetune.py:45] layer 16_down initial loss 0.004175162874162197
W0402 10:24:54.818978 1629356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 98/112 [00:36<00:05,  2.70it/s]W0402 10:24:55.321765 1629356 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0402 10:24:55.348000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 99/112 [00:37<00:04,  2.72it/s]16_down proxy err 0.0037507559172809124 tr(WHW.T) 17.929277420043945
W0402 10:24:55.785000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:55.785000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:55.785000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:55.785000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:24:55.785000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:55.785000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:24:55.786000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:55.814000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:55.814000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:24:55.815000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:55.815000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:55.815000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 89%|████████▉ | 100/112 [00:37<00:04,  2.72it/s]W0402 10:24:56.174000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:56.175000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:24:56.175000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:56.175000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:56.175000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 90%|█████████ | 101/112 [00:37<00:04,  2.71it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.72it/s]W0402 10:24:56.691000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:24:56.696000 139977957832512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 92%|█████████▏| 103/112 [00:38<00:03,  2.73it/s]W0402 10:24:57.325000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.326000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.326000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.326000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.326000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.326000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.326000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 93%|█████████▎| 104/112 [00:39<00:02,  2.69it/s]W0402 10:24:57.371000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.371000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.371000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.371000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.371000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.550000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.551000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.551000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.551000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.551000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 105/112 [00:39<00:02,  2.68it/s]W0402 10:24:57.855000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.855000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.855000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.855000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.855000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.855000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.856000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.884000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.884000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.884000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.884000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.884000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.950000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.950000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.950000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.951000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:57.951000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 95%|█████████▍| 106/112 [00:39<00:02,  2.69it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.70it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.71it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.72it/s]W0402 10:24:59.213000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 98%|█████████▊| 110/112 [00:41<00:00,  2.73it/s]W0402 10:24:59.641000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:59.641000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:24:59.641000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:24:59.641000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:59.641000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:24:59.641000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:24:59.641000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:59.669000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:24:59.669000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:24:59.669000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:24:59.669000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:24:59.669000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 99%|█████████▉| 111/112 [00:41<00:00,  2.74it/s]W0402 10:25:00.026000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:00.026000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:00.026000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:00.026000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:00.027000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
100%|██████████| 112/112 [00:41<00:00,  2.73it/s]100%|██████████| 112/112 [00:41<00:00,  2.67it/s]
W0402 10:25:00.543000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:00.548000 140024462231360 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0402 10:25:03.711241 1629527 finetune.py:45] layer 17_down initial loss 0.005158393643796444
W0402 10:25:03.711405 1629527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:25:04.214977 1629527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

17_down proxy err 0.0038223015144467354 tr(WHW.T) 21.307649612426758
W0402 10:25:07.586000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:07.587000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:07.587000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:07.587000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:07.587000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:07.587000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:25:07.587000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
I0402 10:25:07.592759 1630335 finetune.py:45] layer 18_down initial loss 0.0043398477137088776
W0402 10:25:07.593019 1630335 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:25:07.628000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:07.628000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:07.628000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:07.628000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:07.628000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:07.801000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:07.801000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:07.801000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:07.801000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:07.801000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
I0402 10:25:07.965380 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 20 in 1.4979932308197021s
W0402 10:25:08.109000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:08.109000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:08.109000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:08.110000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:08.110000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:08.110000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:25:08.110000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:08.143000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:08.143000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:08.143000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:08.143000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:08.143000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:08.157064 1630335 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0402 10:25:08.214000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:08.214000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:08.214000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:08.215000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:08.215000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
I0402 10:25:08.481875 1606370 quantize_finetune_llama.py:159] layer 21 gpu 1
18_down proxy err 0.003911902662366629 tr(WHW.T) 21.0947265625
W0402 10:25:09.526000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:09.958000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:09.958000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:09.958000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:09.959000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:09.959000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:25:09.959000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:09.959000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:09.988000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:09.988000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:09.988000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:09.989000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:09.989000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:10.358000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:10.358000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:10.359000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:10.359000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:10.359000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
I0402 10:25:10.433416 1634714 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:25:10.433588 1634714 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:25:10.433662 1634714 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:25:10.644630 1634714 config.py:58] PyTorch version 2.4.0 available.
W0402 10:25:10.891000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:10.896000 140089416447808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0402 10:25:12.138997 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 21 in 1.532456398010254s
I0402 10:25:12.573716 1606370 quantize_finetune_llama.py:159] layer 22 gpu 2
I0402 10:25:12.874080 1634714 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 10:25:13.282945 1634714 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:25:14.501649 1634982 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:25:14.501754 1634982 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:25:14.501813 1634982 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:25:14.686392 1634982 config.py:58] PyTorch version 2.4.0 available.
  3%|▎         | 1/32 [00:01<00:50,  1.62s/it]  6%|▋         | 2/32 [00:01<00:25,  1.17it/s]  9%|▉         | 3/32 [00:02<00:17,  1.65it/s]I0402 10:25:16.747961 1634982 data_utils.py:336] using 256 training seqs, 128 validation seqs
 12%|█▎        | 4/32 [00:02<00:13,  2.02it/s]W0402 10:25:17.144247 1634982 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.51it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.71it/s]  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:25:18.056011 1630589 finetune.py:45] layer 19_down initial loss 0.006346802227199078
W0402 10:25:18.056188 1630589 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:08,  2.83it/s] 28%|██▊       | 9/32 [00:04<00:07,  2.92it/s]W0402 10:25:18.515321 1630589 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 31%|███▏      | 10/32 [00:04<00:07,  3.00it/s]19_down proxy err 0.003922083880752325 tr(WHW.T) 21.985565185546875
 34%|███▍      | 11/32 [00:04<00:06,  3.05it/s] 38%|███▊      | 12/32 [00:05<00:06,  3.05it/s] 41%|████      | 13/32 [00:05<00:06,  3.08it/s]  3%|▎         | 1/32 [00:01<00:59,  1.91s/it] 44%|████▍     | 14/32 [00:05<00:05,  3.12it/s]  6%|▋         | 2/32 [00:02<00:29,  1.02it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.11it/s]  9%|▉         | 3/32 [00:02<00:20,  1.45it/s] 50%|█████     | 16/32 [00:06<00:05,  3.11it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.79it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.07it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.09it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.11it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.33it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.15it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.17it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s]I0402 10:25:22.034922 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 22 in 1.1193063259124756s
 66%|██████▌   | 21/32 [00:08<00:03,  3.12it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.61it/s]I0402 10:25:22.542201 1606370 quantize_finetune_llama.py:159] layer 23 gpu 3
 69%|██████▉   | 22/32 [00:08<00:03,  3.08it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.67it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.08it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.74it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.07it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.08it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.84it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.12it/s] 41%|████      | 13/32 [00:06<00:06,  2.90it/s]I0402 10:25:24.122034 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 23 in 1.1555485725402832s
 84%|████████▍ | 27/32 [00:09<00:01,  3.12it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.87it/s]I0402 10:25:24.545337 1635867 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:25:24.545621 1635867 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:25:24.545751 1635867 utils.py:162] NumExpr defaulting to 16 threads.
 88%|████████▊ | 28/32 [00:10<00:01,  3.05it/s]I0402 10:25:24.634642 1606370 quantize_finetune_llama.py:159] layer 24 gpu 0
 47%|████▋     | 15/32 [00:06<00:06,  2.76it/s]I0402 10:25:24.852523 1635867 config.py:58] PyTorch version 2.4.0 available.
 91%|█████████ | 29/32 [00:10<00:00,  3.01it/s] 50%|█████     | 16/32 [00:07<00:05,  2.80it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.03it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.81it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.02it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.83it/s]100%|██████████| 32/32 [00:11<00:00,  3.04it/s]100%|██████████| 32/32 [00:11<00:00,  2.75it/s]
 59%|█████▉    | 19/32 [00:08<00:04,  2.87it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.85it/s]I0402 10:25:26.700678 1636029 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:25:26.700809 1636029 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:25:26.700874 1636029 utils.py:162] NumExpr defaulting to 16 threads.
 66%|██████▌   | 21/32 [00:08<00:03,  2.85it/s]I0402 10:25:26.901315 1636029 config.py:58] PyTorch version 2.4.0 available.
 69%|██████▉   | 22/32 [00:09<00:03,  2.73it/s]I0402 10:25:27.397735 1635867 data_utils.py:336] using 256 training seqs, 128 validation seqs
 72%|███████▏  | 23/32 [00:09<00:03,  2.78it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s]W0402 10:25:27.997252 1635867 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 78%|███████▊  | 25/32 [00:10<00:02,  2.86it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.92it/s]W0402 10:25:28.671000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:25:28.671000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:28.671000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:28.671000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:28.671000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:28.671000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:28.671000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:28.696000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:28.697000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:28.697000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:28.697000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:28.697000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  2.92it/s]W0402 10:25:28.989000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:28.989000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:28.989000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:28.989000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:28.989000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
I0402 10:25:29.054162 1636029 data_utils.py:336] using 256 training seqs, 128 validation seqs
 88%|████████▊ | 28/32 [00:11<00:01,  2.90it/s]W0402 10:25:29.496756 1636029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 10:25:29.615000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:29.615000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:25:29.615000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:29.615000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:29.615000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:29.615000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:29.615000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:29.633000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:29.633000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:29.634000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:29.634000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:29.634000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.89it/s]  0%|          | 0/32 [00:00<?, ?it/s]W0402 10:25:29.837000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:29.837000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:29.837000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:29.838000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:29.838000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.92it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.92it/s]  0%|          | 0/32 [00:00<?, ?it/s]100%|██████████| 32/32 [00:12<00:00,  2.93it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
W0402 10:25:30.967000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:30.967000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:30.967000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:30.968000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:30.968000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:30.968000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:25:30.968000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:30.986000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:30.986000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:30.987000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:30.987000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:30.987000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:57,  1.86s/it]W0402 10:25:31.623000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:31.624000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:31.624000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:31.624000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:31.624000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:02<00:29,  1.03it/s]  3%|▎         | 1/32 [00:01<00:55,  1.78s/it]  9%|▉         | 3/32 [00:02<00:19,  1.47it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
  6%|▋         | 2/32 [00:02<00:27,  1.08it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.84it/s]  9%|▉         | 3/32 [00:02<00:18,  1.54it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.13it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.89it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.35it/s]W0402 10:25:33.319000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:25:33.319000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:33.319000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:33.320000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:33.320000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:33.320000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:33.320000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:33.350000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:33.350000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:33.350000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:33.350000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:33.350000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:03<00:12,  2.16it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s]W0402 10:25:33.649000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:33.649000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:33.649000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:33.649000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:33.649000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:03<00:10,  2.37it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.64it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s]W0402 10:25:34.262000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:34.262000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:34.262000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:34.262000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:34.262000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:34.262000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:34.262000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:04<00:08,  2.76it/s]W0402 10:25:34.281000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:34.281000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:34.281000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:34.281000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:34.281000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:34.483000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:34.483000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:34.483000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:34.483000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:34.484000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:04<00:08,  2.67it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.83it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.77it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.85it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.82it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.90it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.85it/s] 41%|████      | 13/32 [00:05<00:06,  2.92it/s]W0402 10:25:35.627000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:35.628000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:35.628000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:25:35.628000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:35.628000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:35.628000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:35.628000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:35.646000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:35.646000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:35.646000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:35.646000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:35.646000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:05<00:06,  2.88it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.94it/s] 41%|████      | 13/32 [00:05<00:06,  2.92it/s]W0402 10:25:36.257000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:36.257000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:36.257000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:36.258000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:36.258000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 15/32 [00:06<00:05,  2.95it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.92it/s] 50%|█████     | 16/32 [00:06<00:05,  2.93it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.92it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.93it/s] 50%|█████     | 16/32 [00:06<00:05,  2.92it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 56%|█████▋    | 18/32 [00:07<00:04,  2.92it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.91it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.91it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.92it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.93it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.91it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.92it/s]I0402 10:25:38.521804 1634714 finetune.py:45] layer 20_v initial loss 0.006846503354609013
W0402 10:25:38.522373 1634714 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 20/32 [00:08<00:04,  2.90it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.94it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.89it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.95it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.91it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.95it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.91it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.95it/s]W0402 10:25:39.893758 1634714 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:09<00:02,  2.90it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.94it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.88it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.95it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.86it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.96it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.97it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s]20_v proxy err 0.003212905488908291 tr(WHW.T) 90.61302185058594
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.98it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.85it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.98it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.99it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
 94%|█████████▍| 30/32 [00:11<00:00,  2.84it/s]  3%|▎         | 1/32 [00:01<00:40,  1.32s/it] 97%|█████████▋| 31/32 [00:12<00:00,  2.83it/s]  6%|▋         | 2/32 [00:01<00:22,  1.32it/s]100%|██████████| 32/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
I0402 10:25:42.956071 1634982 finetune.py:45] layer 21_v initial loss 0.009475772269070148
W0402 10:25:42.957093 1634982 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:02<00:16,  1.73it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.03it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.25it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.43it/s]W0402 10:25:44.431410 1634982 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s]W0402 10:25:44.964000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:44.964000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:44.964000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:44.965000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:25:44.965000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:44.965000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:44.965000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:44.990000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:44.990000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:44.990000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:44.991000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:44.991000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:04<00:08,  2.72it/s]W0402 10:25:45.277000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.277000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.277000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.277000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.277000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
21_v proxy err 0.003180884523317218 tr(WHW.T) 95.41011047363281
  0%|          | 0/32 [00:00<?, ?it/s]W0402 10:25:45.576000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.576000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.577000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.577000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.577000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.577000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.577000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.602000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.603000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.603000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.603000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.603000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s]W0402 10:25:45.860000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.860000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.860000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.861000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.861000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.861000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.861000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.878000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.878000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.878000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.878000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.878000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.885000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.885000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.885000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.885000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:45.885000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s]W0402 10:25:46.068000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.068000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.068000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.069000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.069000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:05<00:07,  2.85it/s]W0402 10:25:46.475000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.476000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.476000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.476000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.476000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.476000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.476000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.493000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.493000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.493000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.493000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.493000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.88it/s]W0402 10:25:46.685000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.685000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.685000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.685000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:46.686000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:39,  1.27s/it] 44%|████▍     | 14/32 [00:05<00:06,  2.87it/s]  6%|▋         | 2/32 [00:01<00:22,  1.35it/s]W0402 10:25:47.164000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.164000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.164000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.164000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.164000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.164000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.164000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.181000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.181000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.181000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.181000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.181000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 15/32 [00:06<00:05,  2.89it/s]  9%|▉         | 3/32 [00:02<00:16,  1.75it/s] 50%|█████     | 16/32 [00:06<00:05,  2.91it/s]W0402 10:25:47.773000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.773000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.774000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.774000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.774000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.800000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.801000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.801000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.801000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.801000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.801000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.801000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.818000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.818000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.819000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.819000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:47.819000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:02<00:13,  2.04it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.90it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.24it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.91it/s]W0402 10:25:48.421000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:48.421000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:48.421000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:48.421000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:48.421000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 19%|█▉        | 6/32 [00:03<00:10,  2.39it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.91it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.88it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.85it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.66it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.80it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.70it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.79it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.66it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s] 41%|████      | 13/32 [00:05<00:07,  2.71it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.76it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.74it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.77it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.75it/s] 50%|█████     | 16/32 [00:06<00:05,  2.77it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.73it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.79it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.79it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
 62%|██████▎   | 20/32 [00:08<00:04,  2.79it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.75it/s]I0402 10:25:54.054239 1635867 finetune.py:45] layer 22_v initial loss 0.006984907202422619
W0402 10:25:54.054571 1635867 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 69%|██████▉   | 22/32 [00:08<00:03,  2.76it/s]I0402 10:25:54.438881 1636029 finetune.py:45] layer 23_v initial loss 0.010550259612500668
W0402 10:25:54.439265 1636029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 72%|███████▏  | 23/32 [00:09<00:03,  2.77it/s]W0402 10:25:55.032127 1635867 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:09<00:02,  2.78it/s]W0402 10:25:55.424993 1636029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.75it/s]22_v proxy err 0.0034664738923311234 tr(WHW.T) 101.29380798339844
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s]23_v proxy err 0.0036339566577225924 tr(WHW.T) 112.7859115600586
  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.77it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.76it/s]  3%|▎         | 1/32 [00:01<00:39,  1.26s/it] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s]  3%|▎         | 1/32 [00:01<00:39,  1.26s/it]  6%|▋         | 2/32 [00:01<00:22,  1.34it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.67it/s]  6%|▋         | 2/32 [00:01<00:22,  1.36it/s]  9%|▉         | 3/32 [00:02<00:16,  1.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
  9%|▉         | 3/32 [00:01<00:16,  1.76it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.05it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.02it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.26it/s]W0402 10:25:59.046000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.046000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.046000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.046000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.047000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.047000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.047000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.075000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.075000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.076000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.076000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.076000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:12,  2.22it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.40it/s]W0402 10:25:59.230000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.230000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.230000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.230000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.230000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.445000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.445000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.445000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.445000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.445000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.445000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.445000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.464000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.464000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.464000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.464000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.464000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.36it/s]W0402 10:25:59.526000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.526000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.527000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.527000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:25:59.527000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.65it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s]W0402 10:26:00.536000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.61it/s]W0402 10:26:00.830000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:00.830000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:26:00.830000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:00.830000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:00.830000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:00.830000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:26:00.830000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:00.852000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:00.852000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:00.852000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:26:00.852000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:00.852000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.71it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s]W0402 10:26:01.085000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:01.085000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:01.085000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:26:01.085000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:01.085000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:05<00:07,  2.73it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.70it/s]W0402 10:26:01.516000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.75it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.69it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s] 41%|████      | 13/32 [00:05<00:07,  2.71it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.76it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.72it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.74it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.78it/s] 50%|█████     | 16/32 [00:06<00:05,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.78it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.70it/s]W0402 10:26:03.781000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:26:03.782000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:03.782000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:03.782000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:03.782000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:03.782000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:26:03.782000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:03.809000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:03.809000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:03.809000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:03.809000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:03.809000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.65it/s]W0402 10:26:03.967000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:03.967000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:03.967000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:03.968000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:03.968000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:04.184000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:04.185000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:04.185000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:26:04.185000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:04.185000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:26:04.185000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:04.185000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:08<00:04,  2.74it/s]W0402 10:26:04.204000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:04.204000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:04.204000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:04.204000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:04.204000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:04.264000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:04.265000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:04.265000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:04.265000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:04.265000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.73it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.63it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.63it/s]W0402 10:26:05.242000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:09<00:03,  2.76it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.64it/s]W0402 10:26:05.533000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:05.533000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:05.533000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:26:05.533000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:05.533000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:26:05.534000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:05.534000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:05.555000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:05.555000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:26:05.555000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:05.555000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:05.555000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:09<00:02,  2.74it/s]W0402 10:26:05.793000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:05.793000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:26:05.794000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:05.794000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:05.794000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.73it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.63it/s]W0402 10:26:06.219000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:10<00:02,  2.76it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.63it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.78it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.78it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.79it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.62it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.79it/s]I0402 10:26:07.833726 1634714 finetune.py:45] layer 20_q initial loss 0.006842446979135275
W0402 10:26:07.834115 1634714 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.66it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
W0402 10:26:08.817649 1634714 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
20_q proxy err 0.0007009063265286386 tr(WHW.T) 5690.95703125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:37,  1.23s/it]  6%|▋         | 2/32 [00:01<00:21,  1.42it/s]  9%|▉         | 3/32 [00:01<00:15,  1.86it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.16it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.57it/s]I0402 10:26:13.144961 1634982 finetune.py:45] layer 21_q initial loss 0.00947860348969698
W0402 10:26:13.145173 1634982 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:03<00:09,  2.68it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.78it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.82it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.86it/s]W0402 10:26:14.263536 1634982 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 10:26:14.529000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.529000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.529000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.530000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.530000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.530000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.530000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.559000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.559000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.559000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.559000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.559000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.89it/s]W0402 10:26:14.676000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.676000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.676000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.676000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.676000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.677000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.677000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.705000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.705000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.705000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.705000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.705000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.713000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.713000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.713000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.713000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.713000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.864000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.864000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.864000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.864000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.864000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:04<00:06,  2.92it/s]W0402 10:26:14.930000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.930000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.930000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.930000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.930000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.930000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.930000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.952000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.952000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.952000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.952000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:14.952000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.018000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.018000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.019000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.019000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.019000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.075000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.075000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.075000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.075000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.075000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.075000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.075000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.096000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.096000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.097000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.097000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.097000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.159000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.159000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.159000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.159000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:15.159000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.94it/s]21_q proxy err 0.0005579140270128846 tr(WHW.T) 6793.6064453125
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.94it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.93it/s]W0402 10:26:16.004000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.162000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:05,  2.94it/s]W0402 10:26:16.304000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.304000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.304000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.304000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.304000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.304000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.305000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.324000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.324000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.324000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.324000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.324000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.461000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.461000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.461000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.461000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.462000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.462000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.462000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.483000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.483000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.483000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.483000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.483000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:37,  1.20s/it]W0402 10:26:16.557000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.557000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.557000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.557000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.557000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:06<00:05,  2.94it/s]W0402 10:26:16.723000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.723000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.723000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.723000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:26:16.723000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:21,  1.42it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.95it/s]W0402 10:26:16.982000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:26:17.164000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:01<00:15,  1.83it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.92it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.09it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.92it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.93it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.45it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.94it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.94it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.89it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.86it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.69it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.84it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.83it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.74it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.82it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.76it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.81it/s] 41%|████      | 13/32 [00:05<00:06,  2.78it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.79it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.80it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.81it/s] 50%|█████     | 16/32 [00:06<00:05,  2.80it/s]100%|██████████| 32/32 [00:11<00:00,  2.80it/s]100%|██████████| 32/32 [00:11<00:00,  2.69it/s]
 53%|█████▎    | 17/32 [00:06<00:05,  2.80it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.81it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.82it/s]I0402 10:26:23.066624 1636029 finetune.py:45] layer 23_q initial loss 0.010549299418926239
W0402 10:26:23.066797 1636029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 10:26:23.181390 1635867 finetune.py:45] layer 22_q initial loss 0.006987752392888069
W0402 10:26:23.181649 1635867 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 20/32 [00:07<00:04,  2.82it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.84it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s]W0402 10:26:24.019948 1636029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 10:26:24.132216 1635867 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:09<00:03,  2.79it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.82it/s]23_q proxy err 0.0006203138618730009 tr(WHW.T) 6408.4541015625
  0%|          | 0/32 [00:00<?, ?it/s]22_q proxy err 0.0006298749358393252 tr(WHW.T) 5947.34326171875
  0%|          | 0/32 [00:00<?, ?it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.82it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.83it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.83it/s]  3%|▎         | 1/32 [00:01<00:36,  1.19s/it]  3%|▎         | 1/32 [00:01<00:39,  1.26s/it] 91%|█████████ | 29/32 [00:11<00:01,  2.81it/s]  6%|▋         | 2/32 [00:01<00:21,  1.42it/s]  6%|▋         | 2/32 [00:01<00:22,  1.35it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s]  9%|▉         | 3/32 [00:01<00:15,  1.82it/s]  9%|▉         | 3/32 [00:02<00:16,  1.73it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.12it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.02it/s]100%|██████████| 32/32 [00:12<00:00,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.20it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.43it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.34it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s]I0402 10:26:28.566575 1634714 finetune.py:45] layer 20_k initial loss 0.0068400707095861435
W0402 10:26:28.566990 1634714 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.54it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.65it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.59it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s]W0402 10:26:29.547991 1634714 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.62it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.72it/s]20_k proxy err 0.0005282059428282082 tr(WHW.T) 4224.9970703125
  0%|          | 0/32 [00:00<?, ?it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s] 41%|████      | 13/32 [00:05<00:06,  2.75it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.62it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.77it/s]  3%|▎         | 1/32 [00:00<00:22,  1.36it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.62it/s]  6%|▋         | 2/32 [00:01<00:15,  1.98it/s] 50%|█████     | 16/32 [00:06<00:05,  2.82it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s]  9%|▉         | 3/32 [00:01<00:12,  2.31it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.82it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.62it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.51it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.83it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.65it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.73it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.84it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.80it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.85it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.83it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.85it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.85it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.84it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.86it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.62it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.84it/s]I0402 10:26:34.624217 1634982 finetune.py:45] layer 21_k initial loss 0.009478738531470299
W0402 10:26:34.624513 1634982 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:04<00:07,  2.89it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.85it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.91it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.67it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.85it/s] 41%|████      | 13/32 [00:04<00:06,  2.89it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.72it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s]W0402 10:26:35.698489 1634982 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:05<00:06,  2.88it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.71it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.82it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.87it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.72it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.82it/s] 50%|█████     | 16/32 [00:05<00:05,  2.87it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.75it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s]21_k proxy err 0.0004971115267835557 tr(WHW.T) 4413.919921875
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.88it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.76it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.82it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.87it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
 59%|█████▉    | 19/32 [00:06<00:04,  2.87it/s]  3%|▎         | 1/32 [00:00<00:24,  1.29it/s]100%|██████████| 32/32 [00:12<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
 62%|██████▎   | 20/32 [00:07<00:04,  2.83it/s]  6%|▋         | 2/32 [00:01<00:15,  1.88it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.79it/s]  9%|▉         | 3/32 [00:01<00:13,  2.19it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.40it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.78it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.51it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.80it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.59it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.65it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.76it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.67it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.75it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.68it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.74it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.42it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.38it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.50it/s] 38%|███▊      | 12/32 [00:04<00:08,  2.49it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.57it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s]100%|██████████| 32/32 [00:11<00:00,  2.61it/s]100%|██████████| 32/32 [00:11<00:00,  2.70it/s]
 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.67it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.71it/s]I0402 10:26:43.719246 1636029 finetune.py:45] layer 23_k initial loss 0.010552659630775452
W0402 10:26:43.719524 1636029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▋    | 18/32 [00:07<00:05,  2.71it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.71it/s]I0402 10:26:44.553484 1635867 finetune.py:45] layer 22_k initial loss 0.006988828536123037
W0402 10:26:44.553667 1635867 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 20/32 [00:07<00:04,  2.73it/s]W0402 10:26:44.783043 1636029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 66%|██████▌   | 21/32 [00:08<00:03,  2.75it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.78it/s]W0402 10:26:45.872645 1635867 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

23_k proxy err 0.0005453874473460019 tr(WHW.T) 4211.31982421875
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s]  3%|▎         | 1/32 [00:00<00:24,  1.24it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.79it/s]22_k proxy err 0.0005094791995361447 tr(WHW.T) 4309.5947265625
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:01<00:16,  1.80it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s]  9%|▉         | 3/32 [00:01<00:13,  2.09it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.77it/s]  3%|▎         | 1/32 [00:00<00:23,  1.30it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.27it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.77it/s]  6%|▋         | 2/32 [00:01<00:15,  1.88it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.77it/s]  9%|▉         | 3/32 [00:01<00:13,  2.20it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.77it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.40it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s]100%|██████████| 32/32 [00:12<00:00,  2.77it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 16%|█▌        | 5/32 [00:02<00:10,  2.51it/s]I0402 10:26:49.349193 1634714 finetune.py:45] layer 20_o initial loss 0.006749706342816353
W0402 10:26:49.349467 1634714 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:09,  2.56it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.54it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.56it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s]W0402 10:26:50.293064 1634714 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 41%|████      | 13/32 [00:05<00:07,  2.70it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.62it/s]20_o proxy err 0.0033059953711926937 tr(WHW.T) 4.07815408706665
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.72it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.74it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 50%|█████     | 16/32 [00:06<00:05,  2.73it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.59it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.73it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.73it/s]  3%|▎         | 1/32 [00:01<00:57,  1.85s/it] 50%|█████     | 16/32 [00:06<00:06,  2.59it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.75it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.74it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.58it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s]  6%|▋         | 2/32 [00:03<00:47,  1.57s/it] 72%|███████▏  | 23/32 [00:08<00:03,  2.74it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.59it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.77it/s]  9%|▉         | 3/32 [00:04<00:42,  1.48s/it] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s]I0402 10:26:56.170739 1634982 finetune.py:45] layer 21_o initial loss 0.009470018558204174
W0402 10:26:56.171036 1634982 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.63it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.63it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.78it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s]W0402 10:26:57.222978 1634982 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:11<00:00,  2.78it/s] 12%|█▎        | 4/32 [00:05<00:40,  1.44s/it] 84%|████████▍ | 27/32 [00:10<00:01,  2.65it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.77it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s]21_o proxy err 0.0023845720570534468 tr(WHW.T) 7.5875244140625
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.71it/s] 16%|█▌        | 5/32 [00:07<00:38,  1.42s/it] 97%|█████████▋| 31/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
 19%|█▉        | 6/32 [00:08<00:36,  1.42s/it]  3%|▎         | 1/32 [00:01<00:58,  1.89s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.43s/it]  6%|▋         | 2/32 [00:03<00:48,  1.62s/it]  9%|▉         | 3/32 [00:04<00:44,  1.53s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it]I0402 10:27:04.525998 1636029 finetune.py:45] layer 23_o initial loss 0.010602405294775963
W0402 10:27:04.526183 1636029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it]W0402 10:27:05.466300 1636029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it]I0402 10:27:06.349684 1635867 finetune.py:45] layer 22_o initial loss 0.006973530165851116
W0402 10:27:06.349888 1635867 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

23_o proxy err 0.003025813726708293 tr(WHW.T) 6.165833473205566
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:09<00:37,  1.45s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.43s/it]W0402 10:27:07.534812 1635867 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:59,  1.92s/it]22_o proxy err 0.003558560274541378 tr(WHW.T) 5.007936477661133
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:10<00:36,  1.44s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it]  6%|▋         | 2/32 [00:03<00:48,  1.63s/it] 41%|████      | 13/32 [00:18<00:26,  1.41s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it]  3%|▎         | 1/32 [00:01<00:58,  1.88s/it]  9%|▉         | 3/32 [00:04<00:44,  1.53s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.40s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.45s/it]  6%|▋         | 2/32 [00:03<00:48,  1.63s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.40s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it]  9%|▉         | 3/32 [00:04<00:44,  1.54s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it] 50%|█████     | 16/32 [00:22<00:22,  1.39s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.44s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.50s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.45s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.39s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.44s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.44s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.38s/it] 41%|████      | 13/32 [00:19<00:27,  1.44s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.46s/it] 59%|█████▉    | 19/32 [00:27<00:17,  1.38s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.44s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.38s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.44s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.38s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 50%|█████     | 16/32 [00:23<00:23,  1.44s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 69%|██████▉   | 22/32 [00:31<00:13,  1.37s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.42s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.43s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.37s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.44s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.44s/it] 75%|███████▌  | 24/32 [00:33<00:10,  1.37s/it] 41%|████      | 13/32 [00:18<00:26,  1.42s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.44s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.37s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.44s/it] 41%|████      | 13/32 [00:19<00:27,  1.44s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.38s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.44s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.44s/it] 84%|████████▍ | 27/32 [00:38<00:06,  1.38s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.44s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.43s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.38s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.44s/it] 50%|█████     | 16/32 [00:23<00:23,  1.44s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.38s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.42s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.44s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.44s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.38s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.44s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.37s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.43s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.44s/it]100%|██████████| 32/32 [00:44<00:00,  1.37s/it]100%|██████████| 32/32 [00:44<00:00,  1.40s/it]
 59%|█████▉    | 19/32 [00:27<00:18,  1.43s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.44s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.43s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.43s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.44s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.43s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.44s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.43s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.43s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.45s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.43s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.42s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.46s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.43s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.43s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.47s/it]100%|██████████| 32/32 [00:46<00:00,  1.43s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]
I0402 10:27:44.856009 1634714 finetune.py:45] layer 20_up initial loss 0.006690915673971176
W0402 10:27:44.856374 1634714 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 78%|███████▊  | 25/32 [00:36<00:10,  1.43s/it]W0402 10:27:45.682105 1634714 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:39<00:07,  1.47s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.46s/it]20_up proxy err 0.003575265174731612 tr(WHW.T) 1127.510498046875
  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:40<00:05,  1.46s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.45s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.44s/it]  3%|▎         | 1/32 [00:01<00:54,  1.77s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.47s/it]  6%|▋         | 2/32 [00:03<00:45,  1.52s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.42s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.49s/it]  9%|▉         | 3/32 [00:04<00:42,  1.45s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.41s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.49s/it] 12%|█▎        | 4/32 [00:05<00:39,  1.42s/it]100%|██████████| 32/32 [00:46<00:00,  1.41s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
I0402 10:27:53.390316 1634982 finetune.py:45] layer 21_up initial loss 0.009380290284752846
W0402 10:27:53.390735 1634982 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:07<00:38,  1.42s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.52s/it]W0402 10:27:54.401355 1634982 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:08<00:36,  1.41s/it]21_up proxy err 0.0034522509668022394 tr(WHW.T) 1237.08935546875
  0%|          | 0/32 [00:00<?, ?it/s]100%|██████████| 32/32 [00:46<00:00,  1.51s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
 22%|██▏       | 7/32 [00:10<00:35,  1.43s/it]  3%|▎         | 1/32 [00:01<00:56,  1.83s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it]  6%|▋         | 2/32 [00:03<00:48,  1.62s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it]I0402 10:28:00.292885 1636029 finetune.py:45] layer 23_up initial loss 0.01053682155907154
W0402 10:28:00.293189 1636029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:04<00:45,  1.56s/it]W0402 10:28:01.135905 1636029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.53s/it]23_up proxy err 0.0036921838764101267 tr(WHW.T) 1298.444580078125
  0%|          | 0/32 [00:00<?, ?it/s] 34%|███▍      | 11/32 [00:15<00:29,  1.42s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.52s/it]I0402 10:28:03.613370 1635867 finetune.py:45] layer 22_up initial loss 0.006937162484973669
W0402 10:28:03.613826 1635867 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it]  3%|▎         | 1/32 [00:01<00:57,  1.84s/it]W0402 10:28:04.771476 1635867 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it] 41%|████      | 13/32 [00:18<00:26,  1.40s/it]  6%|▋         | 2/32 [00:03<00:47,  1.59s/it]22_up proxy err 0.003608441213145852 tr(WHW.T) 1249.8564453125
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it] 44%|████▍     | 14/32 [00:19<00:24,  1.39s/it]  9%|▉         | 3/32 [00:04<00:43,  1.52s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.48s/it]  3%|▎         | 1/32 [00:01<00:57,  1.85s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.39s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.48s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it]  6%|▋         | 2/32 [00:03<00:48,  1.61s/it] 50%|█████     | 16/32 [00:22<00:22,  1.38s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.45s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.46s/it]  9%|▉         | 3/32 [00:04<00:44,  1.54s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.38s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.38s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.43s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.48s/it] 59%|█████▉    | 19/32 [00:26<00:17,  1.38s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.46s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.37s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.42s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.37s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.45s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 69%|██████▉   | 22/32 [00:30<00:13,  1.37s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.45s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.42s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.37s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.45s/it] 50%|█████     | 16/32 [00:23<00:23,  1.45s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it] 75%|███████▌  | 24/32 [00:33<00:10,  1.37s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.45s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.45s/it] 41%|████      | 13/32 [00:18<00:26,  1.42s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.36s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.44s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.44s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.37s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.44s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.45s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 84%|████████▍ | 27/32 [00:37<00:06,  1.37s/it] 41%|████      | 13/32 [00:19<00:27,  1.44s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.37s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.44s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.45s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.37s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.44s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.45s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.42s/it] 94%|█████████▍| 30/32 [00:41<00:02,  1.37s/it] 50%|█████     | 16/32 [00:23<00:23,  1.44s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.45s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.37s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.44s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.44s/it] 62%|██████▎   | 20/32 [00:28<00:17,  1.42s/it]100%|██████████| 32/32 [00:44<00:00,  1.37s/it]100%|██████████| 32/32 [00:44<00:00,  1.39s/it]
 56%|█████▋    | 18/32 [00:26<00:20,  1.44s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.45s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.43s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.43s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.44s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.44s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.44s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.44s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.45s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.43s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.43s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.47s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.43s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.43s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.48s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.43s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.43s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.48s/it]I0402 10:28:40.166938 1634714 finetune.py:45] layer 20_gate initial loss 0.006610523909330368
W0402 10:28:40.167294 1634714 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 75%|███████▌  | 24/32 [00:34<00:11,  1.43s/it]W0402 10:28:40.877329 1634714 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:45<00:01,  1.43s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.47s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.43s/it]100%|██████████| 32/32 [00:46<00:00,  1.43s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
 88%|████████▊ | 28/32 [00:40<00:05,  1.48s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.46s/it]20_gate proxy err 0.0016209937166422606 tr(WHW.T) 4549.56396484375
  0%|          | 0/112 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:42<00:04,  1.46s/it]  1%|          | 1/112 [00:00<01:28,  1.26it/s]  2%|▏         | 2/112 [00:01<00:57,  1.90it/s] 84%|████████▍ | 27/32 [00:39<00:07,  1.48s/it]  3%|▎         | 3/112 [00:01<00:48,  2.26it/s] 94%|█████████▍| 30/32 [00:43<00:02,  1.45s/it]  4%|▎         | 4/112 [00:01<00:43,  2.46it/s]  4%|▍         | 5/112 [00:02<00:40,  2.62it/s]  5%|▌         | 6/112 [00:02<00:38,  2.73it/s] 88%|████████▊ | 28/32 [00:40<00:05,  1.49s/it]  6%|▋         | 7/112 [00:02<00:37,  2.80it/s] 97%|█████████▋| 31/32 [00:44<00:01,  1.44s/it]  7%|▋         | 8/112 [00:03<00:36,  2.83it/s]  8%|▊         | 9/112 [00:03<00:35,  2.87it/s]  9%|▉         | 10/112 [00:03<00:35,  2.87it/s] 10%|▉         | 11/112 [00:04<00:34,  2.90it/s] 91%|█████████ | 29/32 [00:42<00:04,  1.48s/it]100%|██████████| 32/32 [00:46<00:00,  1.42s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]
 11%|█         | 12/112 [00:04<00:34,  2.92it/s] 12%|█▏        | 13/112 [00:04<00:34,  2.91it/s] 12%|█▎        | 14/112 [00:05<00:34,  2.85it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.82it/s] 94%|█████████▍| 30/32 [00:43<00:02,  1.50s/it] 14%|█▍        | 16/112 [00:05<00:33,  2.83it/s] 15%|█▌        | 17/112 [00:06<00:33,  2.86it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.83it/s]I0402 10:28:50.789932 1634982 finetune.py:45] layer 21_gate initial loss 0.009295367635786533
W0402 10:28:50.790131 1634982 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 17%|█▋        | 19/112 [00:07<00:32,  2.85it/s] 97%|█████████▋| 31/32 [00:45<00:01,  1.49s/it] 18%|█▊        | 20/112 [00:07<00:32,  2.82it/s]W0402 10:28:51.528056 1634982 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 21/112 [00:07<00:32,  2.79it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.76it/s] 21%|██        | 23/112 [00:08<00:32,  2.75it/s] 21%|██▏       | 24/112 [00:08<00:32,  2.74it/s]100%|██████████| 32/32 [00:46<00:00,  1.51s/it]100%|██████████| 32/32 [00:46<00:00,  1.47s/it]
 22%|██▏       | 25/112 [00:09<00:31,  2.73it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.73it/s] 24%|██▍       | 27/112 [00:09<00:31,  2.72it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.73it/s]21_gate proxy err 0.0015721276868134737 tr(WHW.T) 5024.1337890625
  0%|          | 0/112 [00:00<?, ?it/s] 26%|██▌       | 29/112 [00:10<00:30,  2.73it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.74it/s]  1%|          | 1/112 [00:00<01:30,  1.23it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.74it/s]  2%|▏         | 2/112 [00:01<01:00,  1.81it/s] 29%|██▊       | 32/112 [00:11<00:29,  2.72it/s]I0402 10:28:56.088146 1636029 finetune.py:45] layer 23_gate initial loss 0.010444847866892815
W0402 10:28:56.088491 1636029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 3/112 [00:01<00:50,  2.17it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.74it/s]  4%|▎         | 4/112 [00:01<00:46,  2.34it/s] 30%|███       | 34/112 [00:12<00:28,  2.77it/s]W0402 10:28:56.783012 1636029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 35/112 [00:12<00:27,  2.81it/s]  4%|▍         | 5/112 [00:02<00:44,  2.42it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.81it/s]  5%|▌         | 6/112 [00:02<00:42,  2.49it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.83it/s]  6%|▋         | 7/112 [00:03<00:41,  2.51it/s] 34%|███▍      | 38/112 [00:13<00:26,  2.80it/s]  7%|▋         | 8/112 [00:03<00:40,  2.55it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.78it/s]  8%|▊         | 9/112 [00:03<00:40,  2.56it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.81it/s]  9%|▉         | 10/112 [00:04<00:39,  2.59it/s] 37%|███▋      | 41/112 [00:14<00:25,  2.83it/s] 10%|▉         | 11/112 [00:04<00:38,  2.60it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.83it/s]23_gate proxy err 0.001833983464166522 tr(WHW.T) 4776.345703125
  0%|          | 0/112 [00:00<?, ?it/s] 11%|█         | 12/112 [00:04<00:38,  2.60it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.86it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.62it/s] 39%|███▉      | 44/112 [00:16<00:23,  2.86it/s]  1%|          | 1/112 [00:00<01:30,  1.23it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.61it/s] 40%|████      | 45/112 [00:16<00:23,  2.87it/s]  2%|▏         | 2/112 [00:01<01:00,  1.83it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.61it/s] 41%|████      | 46/112 [00:16<00:22,  2.88it/s]  3%|▎         | 3/112 [00:01<00:50,  2.16it/s]I0402 10:29:01.047407 1635867 finetune.py:45] layer 22_gate initial loss 0.006890583783388138
W0402 10:29:01.048159 1635867 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 42%|████▏     | 47/112 [00:17<00:22,  2.89it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.61it/s]  4%|▎         | 4/112 [00:01<00:45,  2.36it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.88it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.64it/s]  4%|▍         | 5/112 [00:02<00:42,  2.49it/s] 44%|████▍     | 49/112 [00:17<00:21,  2.89it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.66it/s]W0402 10:29:01.847090 1635867 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  5%|▌         | 6/112 [00:02<00:41,  2.57it/s] 45%|████▍     | 50/112 [00:18<00:21,  2.87it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.67it/s]  6%|▋         | 7/112 [00:02<00:40,  2.62it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.87it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.67it/s]  7%|▋         | 8/112 [00:03<00:39,  2.66it/s] 46%|████▋     | 52/112 [00:18<00:20,  2.88it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.65it/s]  8%|▊         | 9/112 [00:03<00:38,  2.70it/s] 47%|████▋     | 53/112 [00:19<00:20,  2.89it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.63it/s]  9%|▉         | 10/112 [00:04<00:37,  2.70it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.88it/s] 21%|██        | 23/112 [00:09<00:33,  2.63it/s] 10%|▉         | 11/112 [00:04<00:37,  2.72it/s] 49%|████▉     | 55/112 [00:19<00:19,  2.89it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.66it/s] 11%|█         | 12/112 [00:04<00:36,  2.73it/s] 50%|█████     | 56/112 [00:20<00:19,  2.89it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.69it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.73it/s] 51%|█████     | 57/112 [00:20<00:19,  2.88it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.68it/s]22_gate proxy err 0.0016935429302975535 tr(WHW.T) 4890.9677734375
  0%|          | 0/112 [00:00<?, ?it/s] 52%|█████▏    | 58/112 [00:20<00:18,  2.89it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.75it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.69it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.89it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.77it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.71it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.87it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.75it/s]  1%|          | 1/112 [00:00<01:30,  1.22it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.72it/s] 54%|█████▍    | 61/112 [00:21<00:17,  2.88it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.76it/s]  2%|▏         | 2/112 [00:01<01:00,  1.82it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.88it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.74it/s] 16%|█▌        | 18/112 [00:06<00:34,  2.76it/s]  3%|▎         | 3/112 [00:01<00:50,  2.14it/s] 56%|█████▋    | 63/112 [00:22<00:16,  2.89it/s] 28%|██▊       | 31/112 [00:12<00:29,  2.74it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.76it/s]  4%|▎         | 4/112 [00:01<00:45,  2.36it/s] 57%|█████▋    | 64/112 [00:22<00:16,  2.90it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.75it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.76it/s]  4%|▍         | 5/112 [00:02<00:42,  2.50it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.90it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.76it/s] 19%|█▉        | 21/112 [00:08<00:32,  2.76it/s]  5%|▌         | 6/112 [00:02<00:40,  2.59it/s] 59%|█████▉    | 66/112 [00:23<00:15,  2.89it/s] 30%|███       | 34/112 [00:13<00:28,  2.75it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.77it/s]  6%|▋         | 7/112 [00:02<00:39,  2.66it/s] 60%|█████▉    | 67/112 [00:23<00:15,  2.89it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.76it/s] 21%|██        | 23/112 [00:08<00:32,  2.76it/s]  7%|▋         | 8/112 [00:03<00:38,  2.70it/s] 61%|██████    | 68/112 [00:24<00:15,  2.88it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.76it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.76it/s]  8%|▊         | 9/112 [00:03<00:37,  2.72it/s] 62%|██████▏   | 69/112 [00:24<00:14,  2.88it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.76it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.77it/s]  9%|▉         | 10/112 [00:04<00:37,  2.73it/s] 62%|██████▎   | 70/112 [00:25<00:14,  2.89it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.77it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.78it/s] 10%|▉         | 11/112 [00:04<00:36,  2.75it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.89it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.76it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.77it/s] 11%|█         | 12/112 [00:04<00:36,  2.74it/s] 64%|██████▍   | 72/112 [00:25<00:13,  2.86it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.76it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.77it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.74it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.87it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.76it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.77it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.75it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.87it/s] 38%|███▊      | 42/112 [00:16<00:25,  2.76it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.77it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.75it/s] 67%|██████▋   | 75/112 [00:26<00:12,  2.88it/s] 38%|███▊      | 43/112 [00:16<00:24,  2.77it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.77it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.77it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.88it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.76it/s] 29%|██▊       | 32/112 [00:12<00:28,  2.77it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.75it/s] 69%|██████▉   | 77/112 [00:27<00:12,  2.88it/s] 40%|████      | 45/112 [00:17<00:24,  2.74it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.77it/s] 16%|█▌        | 18/112 [00:06<00:34,  2.75it/s] 70%|██████▉   | 78/112 [00:27<00:11,  2.88it/s] 41%|████      | 46/112 [00:17<00:24,  2.74it/s] 30%|███       | 34/112 [00:12<00:28,  2.76it/s] 71%|███████   | 79/112 [00:28<00:11,  2.87it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.75it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.74it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.76it/s] 71%|███████▏  | 80/112 [00:28<00:11,  2.88it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.76it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.74it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.77it/s] 72%|███████▏  | 81/112 [00:28<00:10,  2.89it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.74it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.72it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.90it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.76it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.76it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.74it/s] 74%|███████▍  | 83/112 [00:29<00:09,  2.91it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.76it/s] 21%|██        | 23/112 [00:08<00:32,  2.76it/s] 75%|███████▌  | 84/112 [00:29<00:09,  2.92it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.77it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.77it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.77it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.91it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.76it/s] 36%|███▌      | 40/112 [00:14<00:26,  2.77it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.78it/s] 77%|███████▋  | 86/112 [00:30<00:08,  2.91it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.77it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.77it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.77it/s] 78%|███████▊  | 87/112 [00:30<00:08,  2.91it/s] 48%|████▊     | 54/112 [00:20<00:20,  2.77it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.77it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.78it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.91it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.77it/s] 38%|███▊      | 43/112 [00:16<00:24,  2.78it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.77it/s] 79%|███████▉  | 89/112 [00:31<00:07,  2.90it/s] 50%|█████     | 56/112 [00:21<00:20,  2.75it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.77it/s] 26%|██▌       | 29/112 [00:10<00:30,  2.77it/s] 80%|████████  | 90/112 [00:31<00:07,  2.90it/s] 51%|█████     | 57/112 [00:21<00:19,  2.76it/s] 40%|████      | 45/112 [00:16<00:24,  2.77it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.77it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.90it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.74it/s] 41%|████      | 46/112 [00:17<00:23,  2.76it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.76it/s] 82%|████████▏ | 92/112 [00:32<00:06,  2.89it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.75it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.77it/s] 29%|██▊       | 32/112 [00:12<00:28,  2.77it/s] 83%|████████▎ | 93/112 [00:32<00:06,  2.90it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.76it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.77it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.77it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.91it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.76it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.77it/s] 30%|███       | 34/112 [00:12<00:28,  2.78it/s] 85%|████████▍ | 95/112 [00:33<00:05,  2.92it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.77it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.72it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.78it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.92it/s] 46%|████▌     | 51/112 [00:18<00:22,  2.76it/s] 56%|█████▋    | 63/112 [00:23<00:18,  2.69it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.74it/s] 87%|████████▋ | 97/112 [00:34<00:05,  2.91it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.76it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.71it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.74it/s] 88%|████████▊ | 98/112 [00:34<00:04,  2.90it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.77it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.71it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.91it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.74it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.77it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.72it/s] 89%|████████▉ | 100/112 [00:35<00:04,  2.92it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.75it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.76it/s] 90%|█████████ | 101/112 [00:35<00:03,  2.92it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.73it/s] 36%|███▌      | 40/112 [00:14<00:26,  2.75it/s] 50%|█████     | 56/112 [00:20<00:20,  2.77it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.92it/s] 61%|██████    | 68/112 [00:25<00:16,  2.73it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.74it/s] 92%|█████████▏| 103/112 [00:36<00:03,  2.89it/s] 51%|█████     | 57/112 [00:21<00:20,  2.75it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.73it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.74it/s] 93%|█████████▎| 104/112 [00:36<00:02,  2.90it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.75it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.74it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.76it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.92it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.76it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.75it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.75it/s] 95%|█████████▍| 106/112 [00:37<00:02,  2.92it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.76it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.76it/s] 40%|████      | 45/112 [00:16<00:24,  2.76it/s] 96%|█████████▌| 107/112 [00:37<00:01,  2.91it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.77it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.76it/s] 41%|████      | 46/112 [00:17<00:23,  2.77it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.90it/s] 55%|█████▌    | 62/112 [00:22<00:18,  2.76it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.76it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.76it/s] 97%|█████████▋| 109/112 [00:38<00:01,  2.88it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.75it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.74it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.75it/s] 98%|█████████▊| 110/112 [00:38<00:00,  2.88it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.75it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.76it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.77it/s] 99%|█████████▉| 111/112 [00:39<00:00,  2.89it/s] 58%|█████▊    | 65/112 [00:23<00:17,  2.73it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.74it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.71it/s]100%|██████████| 112/112 [00:39<00:00,  2.90it/s]100%|██████████| 112/112 [00:39<00:00,  2.83it/s]
 59%|█████▉    | 66/112 [00:24<00:16,  2.75it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.74it/s] 46%|████▌     | 51/112 [00:18<00:22,  2.73it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.72it/s] 71%|███████   | 79/112 [00:29<00:11,  2.77it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.75it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.77it/s] 61%|██████    | 68/112 [00:25<00:16,  2.69it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.76it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.76it/s] 62%|██████▏   | 69/112 [00:25<00:16,  2.67it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.75it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.74it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.67it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.75it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.75it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.68it/s] 50%|█████     | 56/112 [00:20<00:20,  2.76it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.74it/s] 51%|█████     | 57/112 [00:21<00:20,  2.75it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.68it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.76it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.76it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.70it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.77it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.76it/s] 66%|██████▌   | 74/112 [00:27<00:14,  2.71it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.77it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.76it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.73it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.77it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.77it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.75it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.76it/s] 55%|█████▌    | 62/112 [00:22<00:18,  2.76it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.76it/s] 80%|████████  | 90/112 [00:33<00:07,  2.77it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.78it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.76it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.77it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.77it/s] 71%|███████   | 79/112 [00:29<00:12,  2.75it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.75it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.76it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.74it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.76it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.77it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.75it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.74it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.76it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.76it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.74it/s] 61%|██████    | 68/112 [00:25<00:15,  2.76it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.77it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.74it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.77it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.77it/s]W0402 10:29:30.382000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.382000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.382000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.382000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.382000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.383000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.383000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.426000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.426000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.426000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.426000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.426000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.597000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.597000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.597000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.597000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.597000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 87%|████████▋ | 97/112 [00:36<00:05,  2.73it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.76it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.76it/s]W0402 10:29:30.898000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.898000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.898000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.898000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.898000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.898000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.898000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.931000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.931000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.931000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.931000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.932000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 98/112 [00:36<00:05,  2.74it/s]W0402 10:29:30.997000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.998000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.998000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.998000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:30.998000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 63%|██████▎   | 71/112 [00:26<00:14,  2.77it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.76it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.75it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.77it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.77it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.70it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.76it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.73it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.73it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.75it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.76it/s]W0402 10:29:32.253000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 102/112 [00:37<00:03,  2.74it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.76it/s] 80%|████████  | 90/112 [00:33<00:07,  2.76it/s]W0402 10:29:32.690000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:32.690000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:32.690000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:32.691000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:32.691000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:32.691000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:29:32.691000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:32.718000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:32.718000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:32.718000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:32.718000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:32.718000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 68%|██████▊   | 76/112 [00:27<00:13,  2.74it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.76it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.72it/s]W0402 10:29:33.080000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:33.080000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:33.080000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:33.080000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:33.080000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 77/112 [00:28<00:12,  2.76it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.74it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.76it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.77it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.75it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.77it/s]W0402 10:29:33.590000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:33.595000 139628849293120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 71%|███████   | 79/112 [00:29<00:11,  2.75it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.75it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.68it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.77it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.75it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.71it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.78it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.75it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.72it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.79it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.76it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.73it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.78it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.75it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.75it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.78it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.75it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.71it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.78it/s]100%|██████████| 112/112 [00:41<00:00,  2.76it/s]100%|██████████| 112/112 [00:41<00:00,  2.70it/s]
 89%|████████▉ | 100/112 [00:36<00:04,  2.68it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.76it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.67it/s] 78%|███████▊  | 87/112 [00:31<00:09,  2.71it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.65it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.68it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.65it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.65it/s] 93%|█████████▎| 104/112 [00:38<00:03,  2.65it/s] 80%|████████  | 90/112 [00:33<00:08,  2.68it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.62it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.68it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.64it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.68it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.63it/s] 83%|████████▎ | 93/112 [00:34<00:07,  2.71it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.63it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.74it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.63it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.70it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.63it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.74it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.63it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.76it/s]100%|██████████| 112/112 [00:41<00:00,  2.64it/s]100%|██████████| 112/112 [00:41<00:00,  2.71it/s]
I0402 10:29:40.760610 1634714 finetune.py:45] layer 20_down initial loss 0.0065666805021464825
W0402 10:29:40.760965 1634714 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 98/112 [00:36<00:05,  2.75it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.76it/s]W0402 10:29:41.338429 1634714 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 89%|████████▉ | 100/112 [00:36<00:04,  2.78it/s]20_down proxy err 0.003885681740939617 tr(WHW.T) 23.935514450073242
 90%|█████████ | 101/112 [00:37<00:03,  2.78it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.78it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.78it/s]W0402 10:29:42.865000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:42.865000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:42.865000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:42.865000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:42.865000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:42.865000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:29:42.865000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:42.902000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:42.902000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:42.902000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:42.902000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:42.902000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 93%|█████████▎| 104/112 [00:38<00:02,  2.78it/s]W0402 10:29:43.070000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.070000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.070000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.070000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.070000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.374000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.374000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.374000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.374000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.374000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.374000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.374000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 105/112 [00:38<00:02,  2.77it/s]W0402 10:29:43.403000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.403000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.403000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.403000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.403000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.468000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.469000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.469000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.469000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:43.469000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 95%|█████████▍| 106/112 [00:38<00:02,  2.78it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.79it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.75it/s]W0402 10:29:44.713000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 109/112 [00:40<00:01,  2.74it/s]W0402 10:29:45.163000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:45.164000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:45.164000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:45.164000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:45.164000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:29:45.164000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:45.164000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:45.196000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:45.196000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:45.196000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:45.196000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:45.196000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 98%|█████████▊| 110/112 [00:40<00:00,  2.71it/s]W0402 10:29:45.548000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:45.548000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:45.548000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:45.549000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:45.549000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 99%|█████████▉| 111/112 [00:40<00:00,  2.75it/s]100%|██████████| 112/112 [00:41<00:00,  2.76it/s]100%|██████████| 112/112 [00:41<00:00,  2.73it/s]
W0402 10:29:46.056000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:46.062000 140112327755584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.157000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.157000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.157000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.157000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.157000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.157000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.158000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.200000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.200000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.200000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.200000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.200000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.371000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.371000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.371000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.372000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.372000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.672000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.672000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.673000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.673000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.673000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.673000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.673000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.702000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.702000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.702000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.702000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.702000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.769000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.769000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.769000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.769000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:47.769000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:49.033000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:49.472000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:49.472000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:49.473000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:29:49.473000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:49.473000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:49.473000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:49.473000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:49.503000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:49.503000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:49.503000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:49.503000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:49.503000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:49.860000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:49.860000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:49.860000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:49.860000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:49.860000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:50.371000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:50.376000 139982538057536 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.370000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.370000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.370000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.371000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.371000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.371000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.371000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.417000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.417000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.417000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.417000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.417000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.594000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.594000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.594000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.594000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.594000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
I0402 10:29:53.614831 1634982 finetune.py:45] layer 21_down initial loss 0.009235338307917118
W0402 10:29:53.615330 1634982 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:29:53.902000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.902000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.902000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.902000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.903000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.903000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.903000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.939000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.939000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.939000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.939000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:53.939000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:54.006000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:54.006000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:54.006000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:54.006000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:54.007000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:54.195263 1634982 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

21_down proxy err 0.0036672959104180336 tr(WHW.T) 29.132450103759766
W0402 10:29:55.269000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:55.703000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:29:55.703000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:55.703000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:55.703000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:29:55.704000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:55.704000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:55.704000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:55.732000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:55.732000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:55.732000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:55.732000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:55.732000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:56.098000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:29:56.099000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:56.099000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:29:56.099000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:29:56.099000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:29:56.607000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:29:56.612000 140399062792000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0402 10:29:57.203339 1636029 finetune.py:45] layer 23_down initial loss 0.010375458747148514
W0402 10:29:57.203546 1636029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:29:57.660359 1636029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

I0402 10:29:58.031211 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 24 in 1.2429132461547852s
23_down proxy err 0.003722668159753084 tr(WHW.T) 32.75361251831055
I0402 10:29:58.514284 1606370 quantize_finetune_llama.py:159] layer 25 gpu 1
I0402 10:30:00.489796 1640130 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:30:00.489899 1640130 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:30:00.489955 1640130 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:30:00.673369 1640130 config.py:58] PyTorch version 2.4.0 available.
I0402 10:30:02.743963 1640130 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 10:30:03.054059 1640130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 10:30:03.477100 1635867 finetune.py:45] layer 22_down initial loss 0.006844792515039444
W0402 10:30:03.477398 1635867 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:30:03.988018 1635867 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

  0%|          | 0/32 [00:00<?, ?it/s]22_down proxy err 0.003693019039928913 tr(WHW.T) 30.777223587036133
  3%|▎         | 1/32 [00:01<00:50,  1.63s/it]  6%|▋         | 2/32 [00:01<00:25,  1.16it/s]  9%|▉         | 3/32 [00:02<00:17,  1.61it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.98it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.52it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.69it/s]I0402 10:30:07.800944 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 25 in 1.4269616603851318s
 25%|██▌       | 8/32 [00:03<00:08,  2.80it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.82it/s]I0402 10:30:08.311002 1606370 quantize_finetune_llama.py:159] layer 26 gpu 2
 31%|███▏      | 10/32 [00:04<00:07,  2.86it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.90it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.94it/s] 41%|████      | 13/32 [00:05<00:06,  3.01it/s]I0402 10:30:09.837958 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 26 in 1.1186747550964355s
 44%|████▍     | 14/32 [00:05<00:05,  3.03it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.98it/s]I0402 10:30:10.321449 1640731 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:30:10.321565 1640731 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:30:10.321635 1640731 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:30:10.328902 1606370 quantize_finetune_llama.py:159] layer 27 gpu 3
I0402 10:30:10.519714 1640731 config.py:58] PyTorch version 2.4.0 available.
 50%|█████     | 16/32 [00:06<00:05,  2.96it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.85it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.87it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.92it/s]I0402 10:30:11.906778 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 27 in 1.161210536956787s
 62%|██████▎   | 20/32 [00:07<00:04,  2.92it/s]I0402 10:30:12.322359 1640877 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:30:12.322547 1640877 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:30:12.322618 1640877 utils.py:162] NumExpr defaulting to 16 threads.
 66%|██████▌   | 21/32 [00:08<00:03,  2.93it/s]I0402 10:30:12.374861 1606370 quantize_finetune_llama.py:159] layer 28 gpu 0
I0402 10:30:12.595031 1640877 config.py:58] PyTorch version 2.4.0 available.
 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s]I0402 10:30:12.837843 1640731 data_utils.py:336] using 256 training seqs, 128 validation seqs
 72%|███████▏  | 23/32 [00:09<00:03,  2.82it/s]W0402 10:30:13.226667 1640731 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:09<00:02,  2.82it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.87it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.94it/s]  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.00it/s]I0402 10:30:14.430365 1641132 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:30:14.430533 1641132 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:30:14.430597 1641132 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:30:14.683788 1641132 config.py:58] PyTorch version 2.4.0 available.
 88%|████████▊ | 28/32 [00:10<00:01,  2.99it/s]I0402 10:30:15.058449 1640877 data_utils.py:336] using 256 training seqs, 128 validation seqs
 91%|█████████ | 29/32 [00:11<00:01,  2.99it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.99it/s]W0402 10:30:15.477382 1640877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:11<00:00,  2.98it/s]  3%|▎         | 1/32 [00:01<00:56,  1.83s/it]100%|██████████| 32/32 [00:12<00:00,  3.01it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
  6%|▋         | 2/32 [00:02<00:28,  1.05it/s]  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:02<00:19,  1.47it/s]I0402 10:30:16.901139 1641132 data_utils.py:336] using 256 training seqs, 128 validation seqs
 12%|█▎        | 4/32 [00:02<00:15,  1.79it/s]W0402 10:30:17.219367 1641132 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:03<00:13,  2.05it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.27it/s]  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.60it/s]  3%|▎         | 1/32 [00:01<00:56,  1.82s/it]W0402 10:30:18.803000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:18.803000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:30:18.804000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:18.804000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:18.804000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:18.804000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:18.804000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:18.830000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:18.830000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:18.830000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:18.830000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:18.830000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:04<00:08,  2.68it/s]  6%|▋         | 2/32 [00:02<00:28,  1.04it/s]W0402 10:30:19.119000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.119000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.119000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.119000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.119000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s]  9%|▉         | 3/32 [00:02<00:19,  1.46it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.77it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.80it/s]W0402 10:30:19.725000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.725000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.725000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.725000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.725000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.725000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.725000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.742000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.743000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.743000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.743000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.743000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:05<00:07,  2.82it/s]  3%|▎         | 1/32 [00:01<00:55,  1.78s/it] 16%|█▌        | 5/32 [00:03<00:12,  2.08it/s]W0402 10:30:19.935000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.935000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.935000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.935000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:19.935000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:06<00:06,  2.86it/s]  6%|▋         | 2/32 [00:02<00:27,  1.08it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.31it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.89it/s]  9%|▉         | 3/32 [00:02<00:18,  1.53it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.91it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.89it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.60it/s]W0402 10:30:21.048000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:21.048000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:21.048000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:21.048000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:21.048000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:21.048000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:30:21.048000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:21.066000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:21.066000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:21.066000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:21.066000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:21.066000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:07<00:05,  2.93it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.18it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.68it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.93it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.39it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s]W0402 10:30:21.672000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:21.672000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:21.672000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:21.672000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:21.672000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:07<00:04,  2.94it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.76it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.93it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.65it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.78it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.95it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.76it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 41%|████      | 13/32 [00:06<00:06,  2.81it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.96it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.83it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.83it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.99it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.90it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.86it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.97it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.91it/s] 50%|█████     | 16/32 [00:07<00:05,  2.85it/s] 41%|████      | 13/32 [00:05<00:06,  2.91it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.93it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.85it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.90it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.92it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.84it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.89it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.91it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.83it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.89it/s] 50%|█████     | 16/32 [00:06<00:05,  2.87it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.82it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.89it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.87it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.82it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.89it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.87it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.82it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.90it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.87it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.82it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.90it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.87it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
 66%|██████▌   | 21/32 [00:08<00:03,  2.87it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.82it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.85it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.81it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.86it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.80it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.80it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.80it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.82it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.79it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.83it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.80it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s]I0402 10:30:28.892770 1640130 finetune.py:45] layer 24_v initial loss 0.011960020288825035
W0402 10:30:28.894227 1640130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 31/32 [00:12<00:00,  2.80it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
W0402 10:30:29.504000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:29.505000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:29.505000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:29.505000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:29.505000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:29.505000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:29.505000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:30:29.532000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:29.532000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:29.532000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:29.532000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:29.532000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.88it/s]W0402 10:30:29.826000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:29.826000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:29.826000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:29.826000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:29.826000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.89it/s]W0402 10:30:30.439000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:30.439000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:30.439000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:30.439000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:30.439000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:30.439000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:30.439000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:30:30.457000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:30.457000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:30.457000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:30.457000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:30.457000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:30.526256 1640130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:12<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
W0402 10:30:30.658000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:30.659000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:30.659000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:30.659000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:30.659000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
24_v proxy err 0.0035998483654111624 tr(WHW.T) 136.39785766601562
  0%|          | 0/32 [00:00<?, ?it/s]W0402 10:30:31.801000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:31.801000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:31.801000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:31.801000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:31.801000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:31.801000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:30:31.801000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:31.818000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:31.818000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:31.818000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:31.818000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:31.819000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.223000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.223000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.223000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.223000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.223000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.223000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.223000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.251000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.251000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.251000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.251000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.251000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.423000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.423000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.423000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.423000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.423000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.539000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.539000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.539000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.539000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:32.539000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:40,  1.30s/it]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 10:30:33.139000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.139000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.139000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.139000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.139000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.139000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.139000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.157000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.157000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.157000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.157000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.157000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.243000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.243000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.243000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.243000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.243000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.243000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.243000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.270000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.270000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.270000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.270000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.270000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:22,  1.33it/s]W0402 10:30:33.355000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.355000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.355000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.355000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.355000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.554000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.554000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.554000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.554000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:33.554000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:02<00:16,  1.76it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.07it/s]W0402 10:30:34.201000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.201000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.201000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.201000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.202000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.202000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.202000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.220000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.220000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.220000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.220000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.220000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s]W0402 10:30:34.449000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.449000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.450000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.450000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.450000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.510000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.510000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.510000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.510000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.510000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.510000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.511000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.529000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.529000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.529000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.529000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:34.529000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:03<00:10,  2.46it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s]W0402 10:30:35.171000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:35.171000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:35.171000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:35.171000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:35.171000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s]W0402 10:30:35.716000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:30:35.716000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:35.716000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:35.716000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:35.716000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:35.716000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:35.716000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:35.735000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:35.735000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:35.735000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:35.735000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:35.735000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:04<00:08,  2.72it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 31%|███▏      | 10/32 [00:04<00:07,  2.76it/s]W0402 10:30:36.379000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:36.379000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:36.379000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:36.379000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:36.379000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.79it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.81it/s] 41%|████      | 13/32 [00:05<00:06,  2.83it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 44%|████▍     | 14/32 [00:05<00:06,  2.86it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.87it/s] 50%|█████     | 16/32 [00:06<00:05,  2.88it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.86it/s]I0402 10:30:38.894055 1640731 finetune.py:45] layer 25_v initial loss 0.013528839685022831
W0402 10:30:38.894489 1640731 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▋    | 18/32 [00:07<00:04,  2.88it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.88it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.88it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.86it/s]W0402 10:30:40.165607 1640731 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:08<00:03,  2.84it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.83it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.83it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s]25_v proxy err 0.0031582436058670282 tr(WHW.T) 164.04367065429688
  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:30:41.705885 1640877 finetune.py:45] layer 26_v initial loss 0.018130669370293617
W0402 10:30:41.706202 1640877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:10<00:02,  2.83it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.81it/s]W0402 10:30:42.725938 1640877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:41,  1.34s/it] 91%|█████████ | 29/32 [00:11<00:01,  2.82it/s]I0402 10:30:42.879717 1641132 finetune.py:45] layer 27_v initial loss 0.020041638985276222
W0402 10:30:42.880760 1641132 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:01<00:23,  1.30it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s]  9%|▉         | 3/32 [00:02<00:17,  1.70it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s]26_v proxy err 0.004185671918094158 tr(WHW.T) 123.81900024414062
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.01it/s]100%|██████████| 32/32 [00:12<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
W0402 10:30:44.225937 1641132 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:02<00:12,  2.20it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.33it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s]  3%|▎         | 1/32 [00:01<00:40,  1.30s/it]27_v proxy err 0.0033272842410951853 tr(WHW.T) 203.18115234375
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.54it/s]  6%|▋         | 2/32 [00:01<00:22,  1.32it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.57it/s]  9%|▉         | 3/32 [00:02<00:17,  1.70it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.97it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.64it/s]  3%|▎         | 1/32 [00:01<00:39,  1.26s/it] 16%|█▌        | 5/32 [00:02<00:12,  2.17it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.67it/s]  6%|▋         | 2/32 [00:01<00:21,  1.37it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s]  9%|▉         | 3/32 [00:01<00:16,  1.78it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.41it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.69it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.07it/s] 25%|██▌       | 8/32 [00:04<00:11,  2.06it/s] 47%|████▋     | 15/32 [00:06<00:07,  2.22it/s] 16%|█▌        | 5/32 [00:02<00:14,  1.86it/s] 28%|██▊       | 9/32 [00:04<00:10,  2.20it/s] 19%|█▉        | 6/32 [00:03<00:12,  2.11it/s] 50%|█████     | 16/32 [00:07<00:06,  2.34it/s] 31%|███▏      | 10/32 [00:04<00:09,  2.30it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.29it/s] 53%|█████▎    | 17/32 [00:07<00:06,  2.45it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.40it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.43it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.54it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.48it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.54it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.53it/s] 41%|████      | 13/32 [00:06<00:07,  2.51it/s]W0402 10:30:49.881000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:49.881000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:49.881000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:49.882000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:49.882000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:30:49.882000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:49.882000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:49.911000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:49.911000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:49.911000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:49.911000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:49.911000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s]W0402 10:30:50.066000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.066000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.066000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.066000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.066000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:06<00:07,  2.55it/s]W0402 10:30:50.279000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.279000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.279000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.279000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.280000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.280000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.280000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.300000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.300000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.300000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.300000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.300000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:05<00:07,  2.66it/s]W0402 10:30:50.361000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.361000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.361000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.361000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:50.361000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:09<00:04,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.70it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.65it/s] 50%|█████     | 16/32 [00:07<00:06,  2.59it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.67it/s]W0402 10:30:51.352000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:07<00:05,  2.60it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.75it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.69it/s]W0402 10:30:51.653000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:51.653000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:51.654000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:51.654000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:30:51.654000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:51.654000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:30:51.654000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:30:51.676000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:51.676000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:51.676000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:51.676000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:51.676000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:08<00:05,  2.61it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.74it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.70it/s]W0402 10:30:51.929000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:30:51.929000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:30:51.929000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:30:51.929000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:30:51.929000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 19/32 [00:08<00:04,  2.61it/s] 50%|█████     | 16/32 [00:06<00:05,  2.73it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s]W0402 10:30:52.367000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:07<00:05,  2.76it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.61it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.70it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.78it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.59it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.71it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.79it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.80it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.75it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.82it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.74it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.79it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.64it/s]100%|██████████| 32/32 [00:13<00:00,  2.74it/s]100%|██████████| 32/32 [00:13<00:00,  2.46it/s]
 72%|███████▏  | 23/32 [00:09<00:03,  2.81it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.66it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.78it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.63it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.78it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.64it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.80it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.62it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.75it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.63it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.75it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.75it/s]100%|██████████| 32/32 [00:13<00:00,  2.65it/s]100%|██████████| 32/32 [00:13<00:00,  2.40it/s]
 94%|█████████▍| 30/32 [00:11<00:00,  2.75it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
I0402 10:30:59.270599 1640130 finetune.py:45] layer 24_q initial loss 0.011963608674705029
W0402 10:30:59.270819 1640130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:31:00.196000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.197000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.197000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.197000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.197000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.197000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.197000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.226000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.226000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.226000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.226000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.226000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.381000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.381000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.381000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.381000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.381000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.590000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.590000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.590000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.590000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.590000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.590000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.591000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.611000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.611000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.611000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.611000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.611000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.614767 1640130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 10:31:00.672000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.672000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.672000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.672000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:00.672000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
24_q proxy err 0.0005869825254194438 tr(WHW.T) 6549.0634765625
  0%|          | 0/32 [00:00<?, ?it/s]W0402 10:31:01.653000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:01.949000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:01.949000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:01.949000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:01.949000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:01.950000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:31:01.950000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:01.950000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:01.973000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:01.973000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:01.973000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:01.973000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:01.973000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.216000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.216000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.216000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.216000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.216000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.647000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.750000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.750000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.750000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.750000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.750000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.750000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.750000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.780000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.780000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.780000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.780000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.780000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:37,  1.20s/it]W0402 10:31:02.942000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.943000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.943000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.943000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:02.943000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.160000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.160000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.160000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.160000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.160000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.160000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.160000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:20,  1.43it/s]W0402 10:31:03.181000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.182000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.182000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.182000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.182000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.243000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.243000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.243000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.243000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.244000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.462000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.462000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.462000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.462000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.462000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.462000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.462000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.490000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.490000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.490000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.490000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.490000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:01<00:15,  1.85it/s]W0402 10:31:03.642000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.642000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.642000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.642000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.643000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.847000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.847000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.847000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.847000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.847000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.848000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.848000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:02<00:12,  2.16it/s]W0402 10:31:03.868000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.868000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.868000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.869000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.869000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.929000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.929000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.929000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.929000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:03.929000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s]W0402 10:31:04.249000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s]W0402 10:31:04.566000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:04.566000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:04.566000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:04.566000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:31:04.566000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:04.566000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:04.566000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:04.589000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:04.589000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:04.590000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:04.590000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:04.590000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:04.840000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:04.840000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:04.840000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:04.840000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:04.841000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.62it/s]W0402 10:31:04.912000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:05.213000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:31:05.213000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:05.213000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:31:05.213000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:05.213000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:05.213000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:05.214000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:31:05.236000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:05.236000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:05.236000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:05.236000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:05.236000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s]W0402 10:31:05.312000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:05.481000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:31:05.481000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:31:05.481000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:31:05.481000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:31:05.481000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s]W0402 10:31:05.909000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.85it/s] 41%|████      | 13/32 [00:05<00:06,  2.87it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.89it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.89it/s] 50%|█████     | 16/32 [00:06<00:05,  2.89it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.86it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.86it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s]I0402 10:31:09.215288 1640731 finetune.py:45] layer 25_q initial loss 0.013531675562262535
W0402 10:31:09.215618 1640731 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 20/32 [00:07<00:04,  2.83it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.83it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.84it/s]W0402 10:31:10.371847 1640731 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:08<00:03,  2.84it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.85it/s]25_q proxy err 0.0005040658288635314 tr(WHW.T) 7679.1796875
  0%|          | 0/32 [00:00<?, ?it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.84it/s]I0402 10:31:11.877877 1640877 finetune.py:45] layer 26_q initial loss 0.018134543672204018
W0402 10:31:11.878196 1640877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 84%|████████▍ | 27/32 [00:10<00:01,  2.83it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.82it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.82it/s]  3%|▎         | 1/32 [00:01<00:38,  1.23s/it]I0402 10:31:12.715989 1641132 finetune.py:45] layer 27_q initial loss 0.020041653886437416
W0402 10:31:12.716260 1641132 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:31:12.880613 1640877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:11<00:00,  2.83it/s]  6%|▋         | 2/32 [00:01<00:21,  1.39it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.84it/s]  9%|▉         | 3/32 [00:01<00:16,  1.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
 12%|█▎        | 4/32 [00:02<00:13,  2.09it/s]26_q proxy err 0.0005868171574547887 tr(WHW.T) 6098.626953125
  0%|          | 0/32 [00:00<?, ?it/s]W0402 10:31:14.077411 1641132 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s]27_q proxy err 0.0006090352544561028 tr(WHW.T) 6387.7978515625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:40,  1.31s/it] 25%|██▌       | 8/32 [00:03<00:09,  2.63it/s]  6%|▋         | 2/32 [00:01<00:22,  1.31it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s]  9%|▉         | 3/32 [00:02<00:16,  1.72it/s]  3%|▎         | 1/32 [00:01<00:37,  1.21s/it] 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.02it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.77it/s]  6%|▋         | 2/32 [00:01<00:21,  1.40it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.21it/s] 41%|████      | 13/32 [00:05<00:06,  2.81it/s]  9%|▉         | 3/32 [00:01<00:16,  1.80it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.37it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.83it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.09it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.84it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.56it/s] 50%|█████     | 16/32 [00:06<00:05,  2.84it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.43it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.61it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.82it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.62it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.71it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.85it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 41%|████      | 13/32 [00:05<00:06,  2.74it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.84it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.68it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.74it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.84it/s]I0402 10:31:20.215007 1640130 finetune.py:45] layer 24_k initial loss 0.011961985379457474
W0402 10:31:20.215341 1640130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:05<00:07,  2.70it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.74it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.82it/s] 41%|████      | 13/32 [00:05<00:06,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.73it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.82it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.73it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.77it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.73it/s]W0402 10:31:21.517749 1640130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 81%|████████▏ | 26/32 [00:10<00:02,  2.80it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.80it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.78it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.80it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.80it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.73it/s]24_k proxy err 0.0005021215183660388 tr(WHW.T) 4140.12353515625
  0%|          | 0/32 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.80it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.81it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.79it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.82it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s]  3%|▎         | 1/32 [00:00<00:23,  1.30it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.82it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s]  6%|▋         | 2/32 [00:01<00:15,  1.91it/s]100%|██████████| 32/32 [00:12<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.70it/s]  9%|▉         | 3/32 [00:01<00:12,  2.24it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.84it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.44it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.83it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.68it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.57it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.86it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.66it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.65it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.83it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.66it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.82it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.73it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.66it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.85it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.75it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.64it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.83it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.79it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.87it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
 41%|████      | 13/32 [00:05<00:06,  2.82it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.79it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.80it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.76it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.75it/s] 66%|██████▌   | 21/32 [00:07<00:04,  2.73it/s]I0402 10:31:30.560322 1640731 finetune.py:45] layer 25_k initial loss 0.0135298827663064
W0402 10:31:30.560595 1640731 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 69%|██████▉   | 22/32 [00:08<00:03,  2.72it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.72it/s]W0402 10:31:31.620931 1640731 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.71it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.71it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.71it/s]25_k proxy err 0.0005014975322410464 tr(WHW.T) 4239.72509765625
  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.71it/s]I0402 10:31:33.551929 1640877 finetune.py:45] layer 26_k initial loss 0.018127357587218285
W0402 10:31:33.552173 1640877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:00<00:24,  1.24it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s]  6%|▋         | 2/32 [00:01<00:16,  1.83it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.71it/s]  9%|▉         | 3/32 [00:01<00:13,  2.14it/s]I0402 10:31:34.360959 1641132 finetune.py:45] layer 27_k initial loss 0.020048076286911964
W0402 10:31:34.361245 1641132 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:31:34.566715 1640877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:11<00:00,  2.72it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
 12%|█▎        | 4/32 [00:01<00:12,  2.33it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s]26_k proxy err 0.0004725902108475566 tr(WHW.T) 4395.3486328125
  0%|          | 0/32 [00:00<?, ?it/s]W0402 10:31:35.782651 1641132 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:03<00:09,  2.60it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s]  3%|▎         | 1/32 [00:00<00:24,  1.26it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.66it/s]  6%|▋         | 2/32 [00:01<00:16,  1.82it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s]  9%|▉         | 3/32 [00:01<00:13,  2.13it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.71it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.32it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.73it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s]27_k proxy err 0.0005441594403237104 tr(WHW.T) 4200.8603515625
  0%|          | 0/32 [00:00<?, ?it/s] 41%|████      | 13/32 [00:05<00:06,  2.74it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.74it/s]  3%|▎         | 1/32 [00:00<00:25,  1.20it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 50%|█████     | 16/32 [00:06<00:05,  2.75it/s]  6%|▋         | 2/32 [00:01<00:17,  1.76it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s]  9%|▉         | 3/32 [00:01<00:14,  2.07it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.76it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.25it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.77it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.66it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.45it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.77it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.50it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.67it/s]I0402 10:31:41.408007 1640130 finetune.py:45] layer 24_o initial loss 0.011902961879968643
W0402 10:31:41.408224 1640130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.76it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.67it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s]W0402 10:31:42.381127 1640130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 17/32 [00:06<00:05,  2.64it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.63it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.64it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.65it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.72it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.63it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.72it/s]24_o proxy err 0.0032355075236409903 tr(WHW.T) 6.529373645782471
  0%|          | 0/32 [00:00<?, ?it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.63it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.72it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.62it/s] 50%|█████     | 16/32 [00:06<00:05,  2.73it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.70it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 75%|███████▌  | 24/32 [00:09<00:03,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s]  3%|▎         | 1/32 [00:01<00:58,  1.89s/it] 78%|███████▊  | 25/32 [00:09<00:02,  2.57it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.73it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.55it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.75it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.57it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.57it/s]  6%|▋         | 2/32 [00:03<00:48,  1.61s/it] 72%|███████▏  | 23/32 [00:08<00:03,  2.76it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.56it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.78it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.58it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.56it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
  9%|▉         | 3/32 [00:04<00:43,  1.51s/it] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.79it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s] 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it] 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it]I0402 10:31:52.261882 1640731 finetune.py:45] layer 25_o initial loss 0.01354396715760231
W0402 10:31:52.262107 1640731 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:08<00:37,  1.45s/it]W0402 10:31:53.312144 1640731 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it]25_o proxy err 0.00260526267811656 tr(WHW.T) 8.825087547302246
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:11<00:35,  1.47s/it]I0402 10:31:55.533002 1640877 finetune.py:45] layer 26_o initial loss 0.018266554921865463
W0402 10:31:55.533250 1640877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:31:56.460819 1640877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<01:00,  1.94s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.48s/it]26_o proxy err 0.001974776852875948 tr(WHW.T) 16.217060089111328
  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:31:57.896260 1641132 finetune.py:45] layer 27_o initial loss 0.020031027495861053
W0402 10:31:57.896580 1641132 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:03<00:49,  1.65s/it] 31%|███▏      | 10/32 [00:14<00:32,  1.48s/it]W0402 10:31:58.888609 1641132 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:04<00:45,  1.55s/it]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.48s/it]27_o proxy err 0.002340466482564807 tr(WHW.T) 16.17061424255371
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it]  6%|▋         | 2/32 [00:03<00:50,  1.67s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.47s/it]  3%|▎         | 1/32 [00:01<00:58,  1.89s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it]  6%|▋         | 2/32 [00:03<00:48,  1.61s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.45s/it]  9%|▉         | 3/32 [00:04<00:44,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.52s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.45s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it] 50%|█████     | 16/32 [00:23<00:23,  1.45s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.44s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.46s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.50s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.44s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.43s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.44s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.44s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.42s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.44s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.48s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.45s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.44s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.42s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.45s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.43s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it] 50%|█████     | 16/32 [00:23<00:23,  1.45s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.44s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 41%|████      | 13/32 [00:18<00:27,  1.42s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.45s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.44s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.45s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.44s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.46s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.44s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.44s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.45s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.45s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.43s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.49s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.45s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.44s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.45s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.45s/it] 62%|██████▎   | 20/32 [00:28<00:17,  1.43s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.45s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.45s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.45s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.45s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.45s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.48s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.44s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.46s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.44s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.47s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.46s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.43s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.48s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.46s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.43s/it]I0402 10:32:38.547535 1640130 finetune.py:45] layer 24_up initial loss 0.011771461926400661
W0402 10:32:38.547926 1640130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 84%|████████▍ | 27/32 [00:39<00:07,  1.48s/it]W0402 10:32:39.414503 1640130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 88%|████████▊ | 28/32 [00:41<00:05,  1.46s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.43s/it]24_up proxy err 0.003817108692601323 tr(WHW.T) 1342.69384765625
  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:40<00:05,  1.46s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
 91%|█████████ | 29/32 [00:42<00:04,  1.45s/it]  3%|▎         | 1/32 [00:01<00:55,  1.80s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.49s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.44s/it]  6%|▋         | 2/32 [00:03<00:46,  1.56s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.43s/it]  9%|▉         | 3/32 [00:04<00:42,  1.48s/it]100%|██████████| 32/32 [00:47<00:00,  1.51s/it]100%|██████████| 32/32 [00:47<00:00,  1.50s/it]
100%|██████████| 32/32 [00:46<00:00,  1.42s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]
 12%|█▎        | 4/32 [00:05<00:40,  1.45s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it]I0402 10:32:49.930994 1640731 finetune.py:45] layer 25_up initial loss 0.013456958346068859
W0402 10:32:49.931194 1640731 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:32:50.855661 1640731 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.45s/it]25_up proxy err 0.003832846647128463 tr(WHW.T) 1428.771728515625
  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:32:53.787204 1640877 finetune.py:45] layer 26_up initial loss 0.018207639455795288
W0402 10:32:53.787390 1640877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it]  3%|▎         | 1/32 [00:01<01:00,  1.94s/it]W0402 10:32:54.591929 1640877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 10:32:55.016681 1641132 finetune.py:45] layer 27_up initial loss 0.019834274426102638
W0402 10:32:55.016993 1641132 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:14<00:32,  1.46s/it]26_up proxy err 0.0037639604415744543 tr(WHW.T) 1584.181640625
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:03<00:50,  1.67s/it]W0402 10:32:55.903230 1641132 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:16<00:30,  1.44s/it]27_up proxy err 0.00350798387080431 tr(WHW.T) 1872.387451171875
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:04<00:45,  1.58s/it]  3%|▎         | 1/32 [00:01<00:58,  1.90s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it]  3%|▎         | 1/32 [00:01<00:56,  1.83s/it]  6%|▋         | 2/32 [00:03<00:49,  1.67s/it] 41%|████      | 13/32 [00:18<00:27,  1.43s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it]  6%|▋         | 2/32 [00:03<00:48,  1.60s/it]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it]  9%|▉         | 3/32 [00:04<00:44,  1.52s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.41s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 50%|█████     | 16/32 [00:23<00:22,  1.41s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.50s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.45s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.40s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.50s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.48s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.40s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.40s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.50s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.40s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.44s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.41s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.44s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.40s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it] 41%|████      | 13/32 [00:18<00:27,  1.43s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.40s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.50s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.43s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.41s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.50s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.44s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.41s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it] 50%|█████     | 16/32 [00:24<00:24,  1.50s/it] 50%|█████     | 16/32 [00:23<00:23,  1.44s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.41s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.50s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.43s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.40s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.50s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.43s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.40s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.50s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.43s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.40s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.43s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.50s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]
 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.45s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.48s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.45s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.46s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.46s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.48s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.48s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.46s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.49s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.45s/it]I0402 10:33:34.650214 1640130 finetune.py:45] layer 24_gate initial loss 0.01164515782147646
W0402 10:33:34.650570 1640130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:39<00:08,  1.47s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.49s/it]W0402 10:33:35.480798 1640130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:43<00:04,  1.46s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.48s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.45s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.48s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.48s/it]24_gate proxy err 0.0019682729616761208 tr(WHW.T) 4752.3671875
  0%|          | 0/112 [00:00<?, ?it/s] 97%|█████████▋| 31/32 [00:46<00:01,  1.45s/it]  1%|          | 1/112 [00:00<01:31,  1.21it/s] 91%|█████████ | 29/32 [00:42<00:04,  1.47s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it]  2%|▏         | 2/112 [00:01<01:00,  1.82it/s]  3%|▎         | 3/112 [00:01<00:50,  2.18it/s]100%|██████████| 32/32 [00:47<00:00,  1.45s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
  4%|▎         | 4/112 [00:01<00:45,  2.39it/s]  4%|▍         | 5/112 [00:02<00:42,  2.52it/s] 94%|█████████▍| 30/32 [00:43<00:02,  1.45s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.50s/it]  5%|▌         | 6/112 [00:02<00:40,  2.61it/s]  6%|▋         | 7/112 [00:02<00:39,  2.65it/s]  7%|▋         | 8/112 [00:03<00:38,  2.70it/s]  8%|▊         | 9/112 [00:03<00:44,  2.31it/s] 97%|█████████▋| 31/32 [00:45<00:01,  1.51s/it]  9%|▉         | 10/112 [00:04<00:42,  2.42it/s] 97%|█████████▋| 31/32 [00:46<00:01,  1.57s/it] 10%|▉         | 11/112 [00:04<00:39,  2.54it/s] 11%|█         | 12/112 [00:04<00:38,  2.61it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.67it/s]100%|██████████| 32/32 [00:46<00:00,  1.48s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
 12%|█▎        | 14/112 [00:05<00:35,  2.73it/s]100%|██████████| 32/32 [00:48<00:00,  1.55s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
 13%|█▎        | 15/112 [00:06<00:35,  2.72it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.70it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.67it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.66it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.71it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.73it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.74it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.77it/s] 21%|██        | 23/112 [00:08<00:32,  2.74it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.72it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.70it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.69it/s]I0402 10:33:48.796149 1640731 finetune.py:45] layer 25_gate initial loss 0.013320928439497948
W0402 10:33:48.796420 1640731 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 24%|██▍       | 27/112 [00:10<00:31,  2.68it/s] 25%|██▌       | 28/112 [00:10<00:31,  2.68it/s]W0402 10:33:49.628852 1640731 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 26%|██▌       | 29/112 [00:11<00:31,  2.68it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.67it/s] 28%|██▊       | 31/112 [00:11<00:30,  2.67it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.67it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.66it/s] 30%|███       | 34/112 [00:13<00:29,  2.66it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.66it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.66it/s]I0402 10:33:52.473832 1641132 finetune.py:45] layer 27_gate initial loss 0.01963830552995205
W0402 10:33:52.474083 1641132 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 10:33:52.584000 1640877 finetune.py:45] layer 26_gate initial loss 0.01804278790950775
W0402 10:33:52.584332 1640877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

25_gate proxy err 0.001960090594366193 tr(WHW.T) 5091.62109375
  0%|          | 0/112 [00:00<?, ?it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.68it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.72it/s]W0402 10:33:53.271448 1641132 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 10:33:53.284946 1640877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 35%|███▍      | 39/112 [00:14<00:26,  2.75it/s]  1%|          | 1/112 [00:00<01:31,  1.21it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.73it/s]  2%|▏         | 2/112 [00:01<01:01,  1.78it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.74it/s]  3%|▎         | 3/112 [00:01<00:51,  2.10it/s] 38%|███▊      | 42/112 [00:16<00:25,  2.71it/s]  4%|▎         | 4/112 [00:01<00:47,  2.27it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.68it/s]  4%|▍         | 5/112 [00:02<00:44,  2.39it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.68it/s]  5%|▌         | 6/112 [00:02<00:42,  2.49it/s] 40%|████      | 45/112 [00:17<00:24,  2.72it/s]  6%|▋         | 7/112 [00:03<00:40,  2.58it/s] 41%|████      | 46/112 [00:17<00:24,  2.73it/s]  7%|▋         | 8/112 [00:03<00:39,  2.63it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.76it/s]26_gate proxy err 0.0018071618396788836 tr(WHW.T) 5988.38623046875
  0%|          | 0/112 [00:00<?, ?it/s]27_gate proxy err 0.0016311632934957743 tr(WHW.T) 7248.01416015625
  0%|          | 0/112 [00:00<?, ?it/s]  8%|▊         | 9/112 [00:03<00:38,  2.67it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.78it/s]  9%|▉         | 10/112 [00:04<00:37,  2.69it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.78it/s]  1%|          | 1/112 [00:00<01:32,  1.20it/s] 10%|▉         | 11/112 [00:04<00:37,  2.70it/s]  1%|          | 1/112 [00:00<01:29,  1.24it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.80it/s] 11%|█         | 12/112 [00:04<00:36,  2.70it/s]  2%|▏         | 2/112 [00:01<01:02,  1.75it/s]  2%|▏         | 2/112 [00:01<01:00,  1.83it/s] 46%|████▌     | 51/112 [00:19<00:21,  2.79it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.73it/s]  3%|▎         | 3/112 [00:01<00:50,  2.17it/s]  3%|▎         | 3/112 [00:01<00:52,  2.07it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.80it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.73it/s]  4%|▎         | 4/112 [00:01<00:45,  2.37it/s]  4%|▎         | 4/112 [00:01<00:47,  2.26it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.78it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.73it/s]  4%|▍         | 5/112 [00:02<00:43,  2.47it/s]  4%|▍         | 5/112 [00:02<00:45,  2.37it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.75it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.73it/s]  5%|▌         | 6/112 [00:02<00:41,  2.54it/s]  5%|▌         | 6/112 [00:02<00:43,  2.46it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.75it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.73it/s]  6%|▋         | 7/112 [00:02<00:40,  2.61it/s]  6%|▋         | 7/112 [00:03<00:41,  2.52it/s] 50%|█████     | 56/112 [00:21<00:20,  2.75it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.73it/s]  7%|▋         | 8/112 [00:03<00:39,  2.66it/s]  7%|▋         | 8/112 [00:03<00:40,  2.55it/s] 51%|█████     | 57/112 [00:21<00:19,  2.77it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.73it/s]  8%|▊         | 9/112 [00:03<00:38,  2.71it/s]  8%|▊         | 9/112 [00:03<00:39,  2.58it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.79it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.73it/s]  9%|▉         | 10/112 [00:04<00:37,  2.74it/s]  9%|▉         | 10/112 [00:04<00:39,  2.60it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.78it/s] 10%|▉         | 11/112 [00:04<00:36,  2.76it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.74it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.80it/s] 10%|▉         | 11/112 [00:04<00:38,  2.60it/s] 11%|█         | 12/112 [00:04<00:35,  2.78it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.73it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.82it/s] 11%|█         | 12/112 [00:05<00:38,  2.62it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.78it/s] 21%|██        | 23/112 [00:08<00:32,  2.73it/s] 55%|█████▌    | 62/112 [00:23<00:17,  2.81it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.62it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.78it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.73it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.80it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.61it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.77it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.73it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.79it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.62it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.77it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.73it/s] 58%|█████▊    | 65/112 [00:24<00:16,  2.79it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.63it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.77it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.73it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.81it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.63it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.78it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.73it/s] 60%|█████▉    | 67/112 [00:25<00:15,  2.81it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.63it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.77it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.72it/s] 61%|██████    | 68/112 [00:25<00:15,  2.81it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.63it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.77it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.73it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.81it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.63it/s] 19%|█▉        | 21/112 [00:08<00:32,  2.79it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.73it/s] 62%|██████▎   | 70/112 [00:26<00:14,  2.81it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.63it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.79it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.73it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.81it/s] 21%|██        | 23/112 [00:08<00:31,  2.79it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.64it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.73it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.80it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.76it/s] 21%|██        | 23/112 [00:09<00:33,  2.64it/s] 30%|███       | 34/112 [00:12<00:28,  2.73it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.77it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.75it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.64it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.73it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.79it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.78it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.64it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.82it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.73it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.77it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.64it/s] 68%|██████▊   | 76/112 [00:28<00:12,  2.81it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.73it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.79it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.82it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.63it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.72it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.79it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.82it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.63it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.72it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.78it/s] 71%|███████   | 79/112 [00:29<00:11,  2.82it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.73it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.63it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.78it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.83it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.73it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.63it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.79it/s] 72%|███████▏  | 81/112 [00:30<00:10,  2.83it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.73it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.64it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.78it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.81it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.72it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.63it/s] 30%|███       | 34/112 [00:12<00:28,  2.77it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.80it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.73it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.64it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.78it/s] 75%|███████▌  | 84/112 [00:31<00:09,  2.80it/s] 40%|████      | 45/112 [00:16<00:24,  2.73it/s] 30%|███       | 34/112 [00:13<00:29,  2.64it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.78it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.80it/s] 41%|████      | 46/112 [00:17<00:24,  2.74it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.64it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.78it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.81it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.74it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.64it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.79it/s] 78%|███████▊  | 87/112 [00:32<00:08,  2.81it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.74it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.65it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.78it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.81it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.74it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.65it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.78it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.82it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.74it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.65it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.76it/s] 80%|████████  | 90/112 [00:33<00:07,  2.81it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.74it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.64it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.76it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.82it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.73it/s] 37%|███▋      | 41/112 [00:16<00:26,  2.64it/s] 38%|███▊      | 43/112 [00:15<00:25,  2.72it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.76it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.74it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.64it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.74it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.69it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.74it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.77it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.63it/s] 40%|████      | 45/112 [00:16<00:24,  2.71it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.74it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.77it/s] 41%|████      | 46/112 [00:17<00:24,  2.73it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.64it/s] 50%|█████     | 56/112 [00:20<00:20,  2.74it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.78it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.74it/s] 40%|████      | 45/112 [00:17<00:25,  2.64it/s] 51%|█████     | 57/112 [00:21<00:20,  2.74it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.80it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.74it/s] 41%|████      | 46/112 [00:17<00:25,  2.63it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.74it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.79it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.74it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.64it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.74it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.80it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.75it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.64it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.74it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.81it/s] 46%|████▌     | 51/112 [00:18<00:22,  2.76it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.64it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.74it/s] 90%|█████████ | 101/112 [00:37<00:03,  2.80it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.75it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.64it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.74it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.76it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.75it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.63it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.74it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.76it/s] 48%|████▊     | 54/112 [00:19<00:21,  2.74it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.63it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.74it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.77it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.74it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.64it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.74it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.79it/s] 50%|█████     | 56/112 [00:20<00:20,  2.75it/s] 48%|████▊     | 54/112 [00:20<00:22,  2.63it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.73it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.79it/s] 51%|█████     | 57/112 [00:21<00:19,  2.76it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.74it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.64it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.80it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.76it/s] 61%|██████    | 68/112 [00:25<00:16,  2.74it/s] 50%|█████     | 56/112 [00:21<00:21,  2.64it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.82it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.76it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.74it/s] 51%|█████     | 57/112 [00:22<00:20,  2.64it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.82it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.77it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.74it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.64it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.79it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.73it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.74it/s] 53%|█████▎    | 59/112 [00:22<00:20,  2.61it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.76it/s] 55%|█████▌    | 62/112 [00:22<00:18,  2.72it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.74it/s]100%|██████████| 112/112 [00:41<00:00,  2.76it/s]100%|██████████| 112/112 [00:41<00:00,  2.72it/s]
 54%|█████▎    | 60/112 [00:23<00:19,  2.62it/s] 56%|█████▋    | 63/112 [00:23<00:18,  2.72it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.74it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.63it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.71it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.75it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.64it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.67it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.74it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.62it/s] 59%|█████▉    | 66/112 [00:24<00:17,  2.66it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.74it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.63it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.66it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.74it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.63it/s] 61%|██████    | 68/112 [00:25<00:16,  2.68it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.74it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.63it/s] 62%|██████▏   | 69/112 [00:25<00:16,  2.68it/s] 71%|███████   | 79/112 [00:29<00:12,  2.73it/s] 60%|█████▉    | 67/112 [00:25<00:17,  2.63it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.69it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.73it/s] 61%|██████    | 68/112 [00:26<00:16,  2.63it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.70it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.73it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.64it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.70it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.73it/s] 62%|██████▎   | 70/112 [00:27<00:15,  2.64it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.72it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.73it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.64it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.73it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.70it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.75it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.63it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.71it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.76it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.64it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.72it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.75it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.64it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.73it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.76it/s] 67%|██████▋   | 75/112 [00:28<00:14,  2.64it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.74it/s] 71%|███████   | 79/112 [00:29<00:12,  2.74it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.63it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.75it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.73it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.63it/s] 80%|████████  | 90/112 [00:33<00:08,  2.73it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.73it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.63it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.73it/s]W0402 10:34:26.471000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:26.472000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:26.472000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:26.472000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:26.472000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:34:26.472000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:26.472000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:26.516000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:26.516000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:26.516000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:26.516000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:26.516000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 73%|███████▎  | 82/112 [00:30<00:11,  2.71it/s]W0402 10:34:26.702000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:26.702000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:26.702000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:26.702000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:26.702000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 71%|███████   | 79/112 [00:30<00:12,  2.64it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.74it/s]W0402 10:34:27.001000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:27.001000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:27.001000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:34:27.001000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:27.001000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:27.001000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:27.001000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:27.030000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:27.030000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:27.031000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:27.031000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:27.031000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 74%|███████▍  | 83/112 [00:30<00:10,  2.73it/s]W0402 10:34:27.096000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:27.096000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:27.096000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:27.096000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:27.096000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 71%|███████▏  | 80/112 [00:30<00:12,  2.62it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.72it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.75it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.63it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.73it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.75it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.74it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.64it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.76it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.72it/s] 74%|███████▍  | 83/112 [00:31<00:11,  2.63it/s]W0402 10:34:28.354000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 87/112 [00:32<00:08,  2.78it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.73it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.64it/s]W0402 10:34:28.793000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:28.794000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:28.794000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:28.794000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:28.794000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:28.794000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:28.794000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:34:28.824000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:28.825000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:28.825000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:28.825000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:28.825000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 79%|███████▊  | 88/112 [00:32<00:08,  2.77it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.74it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.63it/s]W0402 10:34:29.180000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:29.180000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:29.180000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:29.181000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:29.181000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 79%|███████▉  | 89/112 [00:32<00:08,  2.74it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.73it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.64it/s] 80%|████████  | 90/112 [00:33<00:08,  2.73it/s]W0402 10:34:29.697000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:29.702000 139834199549760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 89%|████████▉ | 100/112 [00:37<00:04,  2.73it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.64it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.74it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.71it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.62it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.75it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.72it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.62it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.73it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.71it/s] 80%|████████  | 90/112 [00:34<00:08,  2.62it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.74it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.72it/s] 81%|████████▏ | 91/112 [00:35<00:07,  2.63it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.76it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.73it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.64it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.71it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.76it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.66it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.68it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.78it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.66it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.67it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.79it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.68it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.66it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.80it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.68it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.65it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.78it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.68it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.64it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.80it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.69it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.64it/s]100%|██████████| 112/112 [00:41<00:00,  2.80it/s]100%|██████████| 112/112 [00:41<00:00,  2.70it/s]
 88%|████████▊ | 99/112 [00:37<00:04,  2.69it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.64it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.65it/s] 93%|█████████▎| 104/112 [00:38<00:03,  2.65it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.62it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.65it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.57it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.64it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.62it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.64it/s] 93%|█████████▎| 104/112 [00:39<00:03,  2.62it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.64it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.63it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.63it/s]I0402 10:34:36.895378 1640130 finetune.py:45] layer 24_down initial loss 0.011546645313501358
W0402 10:34:36.895773 1640130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 95%|█████████▍| 106/112 [00:40<00:02,  2.65it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.65it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.65it/s]W0402 10:34:37.426527 1640130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 99%|█████████▉| 111/112 [00:41<00:00,  2.69it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.64it/s]100%|██████████| 112/112 [00:41<00:00,  2.71it/s]100%|██████████| 112/112 [00:41<00:00,  2.71it/s]
24_down proxy err 0.0037315632216632366 tr(WHW.T) 34.95817565917969
 97%|█████████▋| 109/112 [00:41<00:01,  2.64it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.65it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.65it/s]100%|██████████| 112/112 [00:42<00:00,  2.65it/s]100%|██████████| 112/112 [00:42<00:00,  2.61it/s]
W0402 10:34:40.793000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:40.793000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:40.793000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:34:40.793000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:40.793000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:40.793000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:40.793000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:40.835000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:40.835000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:40.836000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:40.836000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:40.836000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.009000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.009000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.009000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.009000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.009000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.309000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.309000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.309000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.310000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.310000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.310000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.310000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.340000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.341000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.341000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.341000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.341000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.406000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.406000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.406000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.406000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:41.407000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:42.662000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.091000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.091000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.091000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.091000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.091000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.092000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.092000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.122000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.122000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.122000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.122000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.122000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.477000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.477000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.477000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.477000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.477000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.980000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:43.985000 139922417600320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:44.723000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:44.724000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:44.724000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:44.724000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:44.724000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:44.724000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:44.724000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:34:44.766000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:44.766000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:44.767000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:44.767000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:44.767000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:44.931000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:44.931000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:44.931000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:44.931000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:44.931000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:45.226000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:34:45.226000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:45.226000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:45.227000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:45.227000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:45.227000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:45.227000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:45.257000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:45.257000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:45.257000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:45.257000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:45.257000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:45.322000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:45.322000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:45.322000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:45.322000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:45.323000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.223000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.223000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.223000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.224000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.224000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.224000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.224000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.266000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.266000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.266000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.266000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.267000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.442000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.442000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.442000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.442000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.443000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.588000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.766000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.766000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.766000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.767000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.767000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.767000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.767000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.801000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.802000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.802000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.802000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.802000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.873000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.873000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.873000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.873000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:46.873000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.037000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.037000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.037000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.037000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.037000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.037000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.038000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.068000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.068000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.068000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.068000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.068000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.430000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.430000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.430000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.430000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.431000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.941000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:47.946000 140702340966208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:48.226000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:48.684000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:48.684000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:34:48.684000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:48.684000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:34:48.685000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:48.685000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:48.685000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:48.714000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:48.714000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:48.714000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:48.714000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:48.714000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:49.101000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:34:49.101000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:34:49.101000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:49.101000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:34:49.101000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:34:49.649000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:34:49.654000 140345636300608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0402 10:34:51.022924 1640731 finetune.py:45] layer 25_down initial loss 0.013242529705166817
W0402 10:34:51.023083 1640731 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:34:51.486764 1640731 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

25_down proxy err 0.003762332024052739 tr(WHW.T) 37.92062759399414
I0402 10:34:54.923855 1641132 finetune.py:45] layer 27_down initial loss 0.019465744495391846
W0402 10:34:54.924063 1641132 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 10:34:55.323751 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 28 in 1.3837270736694336s
W0402 10:34:55.404384 1641132 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

I0402 10:34:55.744587 1606370 quantize_finetune_llama.py:159] layer 29 gpu 1
27_down proxy err 0.0032020029611885548 tr(WHW.T) 61.202301025390625
I0402 10:34:56.675069 1640877 finetune.py:45] layer 26_down initial loss 0.017916850745677948
W0402 10:34:56.675332 1640877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:34:57.179700 1640877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

26_down proxy err 0.0038105961866676807 tr(WHW.T) 43.263397216796875
I0402 10:34:57.771636 1645621 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:34:57.771802 1645621 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:34:57.771942 1645621 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:34:58.032259 1645621 config.py:58] PyTorch version 2.4.0 available.
I0402 10:35:00.262694 1645621 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 10:35:00.758094 1645621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 10:35:00.903803 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 29 in 1.1901359558105469s
I0402 10:35:01.335993 1606370 quantize_finetune_llama.py:159] layer 30 gpu 2
  0%|          | 0/32 [00:00<?, ?it/s]I0402 10:35:02.892167 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 30 in 1.1695594787597656s
I0402 10:35:03.270097 1646100 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:35:03.270223 1646100 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:35:03.270284 1646100 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:35:03.398084 1606370 quantize_finetune_llama.py:159] layer 31 gpu 3
I0402 10:35:03.484132 1646100 config.py:58] PyTorch version 2.4.0 available.
  3%|▎         | 1/32 [00:01<00:56,  1.82s/it]  6%|▋         | 2/32 [00:02<00:28,  1.06it/s]  9%|▉         | 3/32 [00:02<00:19,  1.52it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.89it/s]I0402 10:35:05.116235 1606370 quantize_finetune_llama.py:190] computed original embedding for layer 31 in 1.3066117763519287s
I0402 10:35:05.384241 1646340 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:35:05.384382 1646340 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:35:05.384442 1646340 utils.py:162] NumExpr defaulting to 16 threads.
 16%|█▌        | 5/32 [00:03<00:12,  2.10it/s]I0402 10:35:05.632065 1646100 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 10:35:05.637958 1646340 config.py:58] PyTorch version 2.4.0 available.
 19%|█▉        | 6/32 [00:03<00:11,  2.30it/s]W0402 10:35:05.999861 1646100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.62it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.73it/s]  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.84it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.92it/s]I0402 10:35:07.637257 1646704 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:35:07.637371 1646704 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:35:07.637441 1646704 utils.py:162] NumExpr defaulting to 16 threads.
 38%|███▊      | 12/32 [00:05<00:06,  2.96it/s]I0402 10:35:07.838999 1646704 config.py:58] PyTorch version 2.4.0 available.
I0402 10:35:07.936249 1646340 data_utils.py:336] using 256 training seqs, 128 validation seqs
 41%|████      | 13/32 [00:05<00:06,  2.96it/s]W0402 10:35:08.292660 1646340 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:06<00:06,  2.99it/s]  3%|▎         | 1/32 [00:01<00:56,  1.81s/it] 47%|████▋     | 15/32 [00:06<00:05,  2.97it/s] 50%|█████     | 16/32 [00:06<00:05,  3.01it/s]  6%|▋         | 2/32 [00:02<00:28,  1.06it/s]  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:07<00:04,  3.05it/s]  9%|▉         | 3/32 [00:02<00:19,  1.49it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.07it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.83it/s]I0402 10:35:09.957871 1646704 data_utils.py:336] using 256 training seqs, 128 validation seqs
 59%|█████▉    | 19/32 [00:07<00:04,  3.02it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.09it/s]W0402 10:35:10.298685 1646704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 62%|██████▎   | 20/32 [00:08<00:03,  3.06it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.28it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.06it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.07it/s]  3%|▎         | 1/32 [00:01<00:56,  1.81s/it]  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:02,  3.09it/s]  6%|▋         | 2/32 [00:02<00:28,  1.06it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.69it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.09it/s]  9%|▉         | 3/32 [00:02<00:19,  1.49it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.09it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.86it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.78it/s] 81%|████████▏ | 26/32 [00:10<00:01,  3.10it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.15it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.81it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.09it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.36it/s] 41%|████      | 13/32 [00:05<00:06,  2.83it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.09it/s]  3%|▎         | 1/32 [00:01<00:55,  1.79s/it] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.87it/s] 91%|█████████ | 29/32 [00:11<00:00,  3.10it/s]  6%|▋         | 2/32 [00:02<00:28,  1.07it/s] 25%|██▌       | 8/32 [00:04<00:08,  2.68it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.90it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.10it/s]  9%|▉         | 3/32 [00:02<00:19,  1.52it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.78it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.10it/s] 50%|█████     | 16/32 [00:06<00:05,  2.91it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.89it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  3.08it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
 53%|█████▎    | 17/32 [00:07<00:05,  2.93it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.18it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.91it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.91it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.39it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.90it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.89it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 41%|████      | 13/32 [00:05<00:06,  2.92it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.64it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.85it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.90it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.72it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.86it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.93it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.81it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.89it/s] 50%|█████     | 16/32 [00:06<00:05,  2.97it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.87it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.92it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.99it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.90it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.94it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.00it/s] 41%|████      | 13/32 [00:05<00:06,  2.93it/s]W0402 10:35:17.049000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.049000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.049000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.049000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.049000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.049000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.049000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:10<00:02,  2.94it/s]W0402 10:35:17.074000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.074000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.074000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.075000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.075000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 19/32 [00:07<00:04,  3.00it/s]W0402 10:35:17.356000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.356000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.356000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.356000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.356000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:06<00:06,  2.96it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.95it/s] 62%|██████▎   | 20/32 [00:08<00:03,  3.01it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.98it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.96it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.02it/s]W0402 10:35:17.949000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.949000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.949000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.949000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.949000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.949000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.949000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.966000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.967000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.967000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.967000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:17.967000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:05,  2.99it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.95it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.02it/s]W0402 10:35:18.159000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:18.159000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:18.160000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:18.160000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:18.160000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:07<00:04,  3.00it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.93it/s] 72%|███████▏  | 23/32 [00:09<00:02,  3.03it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.01it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.94it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.04it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.02it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.04it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.94it/s]W0402 10:35:19.275000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:19.275000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:19.275000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:19.276000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:19.276000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:19.276000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:19.276000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:19.294000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:19.294000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:19.294000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:19.294000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:19.294000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:08<00:03,  3.01it/s] 81%|████████▏ | 26/32 [00:10<00:01,  3.03it/s]100%|██████████| 32/32 [00:12<00:00,  2.93it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
 66%|██████▌   | 21/32 [00:08<00:03,  3.03it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.05it/s]W0402 10:35:19.883000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:19.883000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:19.883000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:19.884000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:19.884000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  3.00it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.00it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.99it/s] 91%|█████████ | 29/32 [00:11<00:01,  3.00it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 75%|███████▌  | 24/32 [00:09<00:02,  3.01it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.01it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.97it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.98it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.95it/s]100%|██████████| 32/32 [00:12<00:00,  2.96it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 84%|████████▍ | 27/32 [00:10<00:01,  2.90it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.88it/s]W0402 10:35:22.776000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:22.777000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:22.777000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:22.777000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:22.777000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:22.777000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:22.777000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s]W0402 10:35:22.804000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:22.804000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:22.804000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:22.804000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:22.805000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.107000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.107000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.107000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.107000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.107000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  2.88it/s]100%|██████████| 32/32 [00:12<00:00,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
W0402 10:35:23.733000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.733000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.733000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.733000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.733000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.733000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.733000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.751000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.751000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.751000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.751000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.752000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.958000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.958000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.958000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.958000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:23.958000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:24.294000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:24.294000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:24.294000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:24.294000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:24.294000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:24.295000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:24.295000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:24.322000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:24.322000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:24.322000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:24.322000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:24.322000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:24.620000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:24.620000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:24.621000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:24.621000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:24.621000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.129000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.129000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.129000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.130000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.130000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.130000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.130000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.148000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.148000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.148000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.148000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.148000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.248000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.248000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.248000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.249000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.249000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.249000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.249000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.267000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.267000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.267000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.267000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.267000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.471000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.471000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.472000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.472000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.472000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.792000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.792000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.792000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.792000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:25.793000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.259000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.259000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.259000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.259000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.259000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.259000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.260000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.287000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.287000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.287000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.287000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.287000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 10:35:26.583000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.583000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.583000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.583000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.583000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.634000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.634000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.635000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.635000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.635000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.635000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.635000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.653000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.653000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.653000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.653000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:26.653000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
I0402 10:35:26.916856 1645621 finetune.py:45] layer 28_v initial loss 0.025985581800341606
W0402 10:35:26.917253 1645621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:35:27.198000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.198000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.198000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.198000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.198000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.198000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.198000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.216000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.216000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.216000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.216000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.216000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.284000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.284000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.284000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.284000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.284000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.422000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.422000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.422000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.422000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.422000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:27.937213 1645621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 10:35:28.591000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:28.591000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:28.592000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:28.592000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:28.592000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:28.592000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:28.592000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:28.610000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:28.610000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:28.610000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:28.610000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:28.610000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
28_v proxy err 0.004000305663794279 tr(WHW.T) 175.40756225585938
  0%|          | 0/32 [00:00<?, ?it/s]W0402 10:35:29.249000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:29.249000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:29.249000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:29.250000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:29.250000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
  3%|▎         | 1/32 [00:01<00:38,  1.25s/it]  6%|▋         | 2/32 [00:01<00:21,  1.39it/s]  9%|▉         | 3/32 [00:01<00:15,  1.83it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.12it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s]I0402 10:35:31.714588 1646100 finetune.py:45] layer 29_v initial loss 0.0340813547372818
W0402 10:35:31.715580 1646100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.62it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s]W0402 10:35:32.825857 1646100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:04<00:08,  2.73it/s]I0402 10:35:33.302944 1646340 finetune.py:45] layer 30_v initial loss 0.056867606937885284
W0402 10:35:33.303144 1646340 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:04<00:07,  2.78it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s]29_v proxy err 0.003329054219648242 tr(WHW.T) 249.68582153320312
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.85it/s]W0402 10:35:34.300172 1646340 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 13/32 [00:05<00:06,  2.86it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.88it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.87it/s]  3%|▎         | 1/32 [00:01<00:41,  1.33s/it]30_v proxy err 0.0041311089880764484 tr(WHW.T) 252.81201171875
  0%|          | 0/32 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:06<00:05,  2.85it/s]  6%|▋         | 2/32 [00:01<00:23,  1.29it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.85it/s]I0402 10:35:35.845970 1646704 finetune.py:45] layer 31_v initial loss 0.02939249388873577
W0402 10:35:35.848052 1646704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:02<00:17,  1.67it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.85it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.97it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.83it/s]  3%|▎         | 1/32 [00:01<00:40,  1.32s/it] 16%|█▌        | 5/32 [00:02<00:12,  2.16it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.83it/s]  6%|▋         | 2/32 [00:01<00:22,  1.32it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.83it/s]  9%|▉         | 3/32 [00:02<00:16,  1.73it/s]W0402 10:35:37.377436 1646704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:03<00:10,  2.42it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.80it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.00it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.80it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.23it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.56it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.38it/s]31_v proxy err 0.0016904684016481042 tr(WHW.T) 365.68487548828125
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.81it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.64it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.65it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.61it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.78it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s]  3%|▎         | 1/32 [00:01<00:40,  1.30s/it] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.80it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.69it/s]  6%|▋         | 2/32 [00:01<00:22,  1.33it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.69it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s]  9%|▉         | 3/32 [00:02<00:16,  1.73it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.72it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s] 50%|█████     | 16/32 [00:06<00:05,  2.70it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.02it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 53%|█████▎    | 17/32 [00:07<00:05,  2.70it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.22it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.39it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.63it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.50it/s] 50%|█████     | 16/32 [00:06<00:05,  2.70it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.66it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.71it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.63it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.69it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.68it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.74it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.67it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 41%|████      | 13/32 [00:05<00:07,  2.70it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.71it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.72it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.72it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.78it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.74it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.73it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.78it/s] 50%|█████     | 16/32 [00:06<00:05,  2.74it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.73it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.79it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.73it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.73it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.78it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.73it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.71it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.77it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
 91%|█████████ | 29/32 [00:11<00:01,  2.77it/s]W0402 10:35:46.896000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:46.897000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:46.897000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:46.897000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:46.897000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:46.897000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:46.897000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:46.926000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:46.926000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:46.926000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:46.926000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:46.927000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.083000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.083000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.083000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.083000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.083000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s]W0402 10:35:47.297000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.297000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.297000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.297000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.298000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.298000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.298000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.318000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.319000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.319000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.319000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.319000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.380000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.380000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.380000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.380000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:47.380000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  2.72it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.71it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
 75%|███████▌  | 24/32 [00:09<00:02,  2.78it/s]W0402 10:35:48.375000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:10<00:02,  2.79it/s]W0402 10:35:48.672000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:48.672000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:48.672000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:48.672000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:48.672000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:48.672000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:48.672000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:48.693000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:48.693000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:48.693000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:48.693000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:48.694000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:10<00:02,  2.78it/s]W0402 10:35:48.936000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:48.937000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:48.937000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:48.937000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:48.937000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  2.78it/s]W0402 10:35:49.370000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:11<00:01,  2.76it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.73it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
W0402 10:35:52.249000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.249000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.249000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.250000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.250000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.250000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.250000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.278000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.278000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.278000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.278000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.279000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.439000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.439000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.440000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.440000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.440000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.648000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.649000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.649000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.649000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.649000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.649000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.649000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.672000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.672000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.672000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.672000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.672000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.736000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.736000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.736000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.736000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:52.736000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.485000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.485000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.485000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.485000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.486000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.486000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.486000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.517000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.517000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.517000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.517000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.518000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.679000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.679000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.679000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.679000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.679000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.772000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.902000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.902000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.902000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.902000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.903000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.903000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.903000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.925000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.925000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.925000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.925000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.925000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.990000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.990000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.990000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.990000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:53.990000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:54.089000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:54.089000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:54.089000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:54.089000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:54.089000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:54.089000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:54.089000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:54.110000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:54.110000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:54.110000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:54.110000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:54.110000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:54.359000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:54.359000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:54.359000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:54.359000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:54.359000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:54.810000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:55.035000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:55.350000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:55.350000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:55.350000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:55.350000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:55.350000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:55.350000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:55.351000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:55.374000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:55.374000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:55.374000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:55.374000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:55.375000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
I0402 10:35:55.557230 1645621 finetune.py:45] layer 28_q initial loss 0.02599037066102028
W0402 10:35:55.557609 1645621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:35:55.622000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:55.622000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:55.622000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:55.622000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:55.622000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:56.079000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:56.543336 1645621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 10:35:57.241000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.242000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.242000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.242000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.242000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.242000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.242000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.271000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.271000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.272000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.272000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.272000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.430000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.430000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.430000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.430000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.431000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
28_q proxy err 0.0005544026498682797 tr(WHW.T) 6755.15625
  0%|          | 0/32 [00:00<?, ?it/s]W0402 10:35:57.639000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.639000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.639000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.639000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.639000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.639000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.640000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.658000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.658000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.658000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.658000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.659000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.718000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.718000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.719000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.719000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:57.719000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:29,  1.05it/s]W0402 10:35:58.714000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:17,  1.68it/s]W0402 10:35:59.012000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:35:59.012000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:59.012000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:35:59.013000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:59.013000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:35:59.013000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:59.013000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:59.034000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:59.034000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:59.034000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:59.034000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:59.035000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:01<00:14,  2.07it/s]W0402 10:35:59.282000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:35:59.282000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:35:59.282000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:35:59.282000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:35:59.282000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:01<00:12,  2.32it/s]W0402 10:35:59.721000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:10,  2.50it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.63it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.68it/s]I0402 10:36:00.759753 1646100 finetune.py:45] layer 29_q initial loss 0.03408277407288551
W0402 10:36:00.759980 1646100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.80it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s]W0402 10:36:01.747199 1646100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:04<00:07,  2.78it/s]I0402 10:36:02.267398 1646340 finetune.py:45] layer 30_q initial loss 0.0568835623562336
W0402 10:36:02.267563 1646340 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:04<00:07,  2.77it/s] 41%|████      | 13/32 [00:05<00:06,  2.76it/s]29_q proxy err 0.0006654488970525563 tr(WHW.T) 6063.78369140625
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s]W0402 10:36:03.171008 1646340 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 15/32 [00:05<00:06,  2.74it/s] 50%|█████     | 16/32 [00:06<00:05,  2.73it/s]  3%|▎         | 1/32 [00:01<00:37,  1.21s/it]30_q proxy err 0.00045596438576467335 tr(WHW.T) 7045.0302734375
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.73it/s]  6%|▋         | 2/32 [00:01<00:21,  1.41it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.73it/s]  9%|▉         | 3/32 [00:01<00:15,  1.81it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.11it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.73it/s]  3%|▎         | 1/32 [00:01<00:38,  1.23s/it] 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.73it/s]  6%|▋         | 2/32 [00:01<00:21,  1.39it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.43it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s]  9%|▉         | 3/32 [00:01<00:15,  1.82it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s]I0402 10:36:06.211245 1646704 finetune.py:45] layer 31_q initial loss 0.029386963695287704
W0402 10:36:06.211647 1646704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 72%|███████▏  | 23/32 [00:08<00:03,  2.74it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.13it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.66it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.79it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s]W0402 10:36:07.277353 1646704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.77it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.68it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.78it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.71it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.82it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.69it/s]31_q proxy err 0.00030957278795540333 tr(WHW.T) 9335.720703125
  0%|          | 0/32 [00:00<?, ?it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.82it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.84it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.76it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.74it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.85it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.78it/s] 50%|█████     | 16/32 [00:06<00:05,  2.75it/s]  3%|▎         | 1/32 [00:01<00:37,  1.22s/it]100%|██████████| 32/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
 41%|████      | 13/32 [00:05<00:06,  2.79it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.75it/s]  6%|▋         | 2/32 [00:01<00:21,  1.40it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.79it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.75it/s]  9%|▉         | 3/32 [00:01<00:16,  1.78it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.80it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.07it/s] 50%|█████     | 16/32 [00:06<00:05,  2.80it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.75it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.26it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.81it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.41it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.81it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.81it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.75it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.56it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.81it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.82it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.58it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.82it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.77it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.82it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.77it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.82it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.80it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.61it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.80it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.74it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.82it/s] 50%|█████     | 16/32 [00:06<00:06,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
 91%|█████████ | 29/32 [00:11<00:01,  2.84it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.61it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.61it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.81it/s]I0402 10:36:16.207563 1645621 finetune.py:45] layer 28_k initial loss 0.0259816013276577
W0402 10:36:16.207834 1645621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
 62%|██████▎   | 20/32 [00:08<00:04,  2.65it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s]W0402 10:36:17.245697 1645621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:09<00:03,  2.70it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s]28_k proxy err 0.00045641776523552835 tr(WHW.T) 4372.6787109375
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.77it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.79it/s]  3%|▎         | 1/32 [00:00<00:23,  1.34it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s]  6%|▋         | 2/32 [00:01<00:15,  1.97it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.80it/s]  9%|▉         | 3/32 [00:01<00:12,  2.29it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.80it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.49it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.81it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
 22%|██▏       | 7/32 [00:02<00:09,  2.75it/s]I0402 10:36:21.342372 1646100 finetune.py:45] layer 29_k initial loss 0.034073930233716965
W0402 10:36:21.342588 1646100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:08,  2.76it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.79it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.79it/s]W0402 10:36:22.350765 1646100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:04<00:07,  2.80it/s]I0402 10:36:22.704909 1646340 finetune.py:45] layer 30_k initial loss 0.056873247027397156
W0402 10:36:22.705104 1646340 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:04<00:07,  2.81it/s] 41%|████      | 13/32 [00:04<00:06,  2.77it/s]29_k proxy err 0.0004950347938574851 tr(WHW.T) 4804.9951171875
  0%|          | 0/32 [00:00<?, ?it/s]W0402 10:36:23.623791 1646340 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.72it/s]  3%|▎         | 1/32 [00:00<00:24,  1.28it/s] 50%|█████     | 16/32 [00:06<00:05,  2.70it/s]  6%|▋         | 2/32 [00:01<00:16,  1.86it/s]30_k proxy err 0.00039110484067350626 tr(WHW.T) 4109.9091796875
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.70it/s]  9%|▉         | 3/32 [00:01<00:13,  2.18it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.69it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.37it/s]  3%|▎         | 1/32 [00:00<00:23,  1.33it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.48it/s]  6%|▋         | 2/32 [00:01<00:15,  1.92it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.69it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.57it/s]  9%|▉         | 3/32 [00:01<00:12,  2.25it/s] 66%|██████▌   | 21/32 [00:07<00:04,  2.68it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.63it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.42it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.66it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.55it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.67it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.68it/s]I0402 10:36:27.229265 1646704 finetune.py:45] layer 31_k initial loss 0.029381580650806427
W0402 10:36:27.229639 1646704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:02<00:09,  2.62it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.69it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.65it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.71it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.68it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.67it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.72it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.68it/s]W0402 10:36:28.297630 1646704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:03<00:08,  2.70it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.72it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.74it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.72it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.78it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.71it/s]31_k proxy err 0.0003418322012294084 tr(WHW.T) 4138.115234375
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.79it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s] 41%|████      | 13/32 [00:05<00:06,  2.75it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.81it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.71it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s]  3%|▎         | 1/32 [00:00<00:23,  1.31it/s]100%|██████████| 32/32 [00:11<00:00,  2.82it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.73it/s]  6%|▋         | 2/32 [00:01<00:16,  1.86it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s] 50%|█████     | 16/32 [00:06<00:05,  2.74it/s]  9%|▉         | 3/32 [00:01<00:13,  2.13it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.72it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.74it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.33it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.73it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.72it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.73it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.72it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.73it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.72it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.59it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.74it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.75it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.74it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.60it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.75it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.59it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.74it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.75it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.60it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.77it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.74it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.77it/s]100%|██████████| 32/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.60it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s]I0402 10:36:37.091331 1645621 finetune.py:45] layer 28_o initial loss 0.026044130325317383
W0402 10:36:37.091525 1645621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s]W0402 10:36:38.026870 1645621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:08<00:03,  2.68it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s]28_o proxy err 0.0022664403077214956 tr(WHW.T) 24.542434692382812
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.75it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.74it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.77it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.76it/s]  3%|▎         | 1/32 [00:01<00:57,  1.84s/it] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
I0402 10:36:42.117935 1646100 finetune.py:45] layer 29_o initial loss 0.03417186066508293
W0402 10:36:42.118267 1646100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:03<00:47,  1.58s/it]W0402 10:36:43.134352 1646100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 10:36:43.337001 1646340 finetune.py:45] layer 30_o initial loss 0.05715274065732956
W0402 10:36:43.337176 1646340 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:04<00:43,  1.50s/it]W0402 10:36:44.222392 1646340 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

29_o proxy err 0.0014064442366361618 tr(WHW.T) 36.68419647216797
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it]30_o proxy err 0.001267700339667499 tr(WHW.T) 84.08126831054688
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.96s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.48s/it]  3%|▎         | 1/32 [00:01<00:58,  1.88s/it]  6%|▋         | 2/32 [00:03<00:49,  1.66s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it]I0402 10:36:48.562037 1646704 finetune.py:45] layer 31_o initial loss 0.02965984120965004
W0402 10:36:48.562437 1646704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:03<00:48,  1.61s/it]  9%|▉         | 3/32 [00:04<00:45,  1.56s/it]W0402 10:36:49.529542 1646704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it]  9%|▉         | 3/32 [00:04<00:44,  1.53s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it]31_o proxy err 0.0008353256271220744 tr(WHW.T) 182.20681762695312
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.50s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it]  3%|▎         | 1/32 [00:01<00:59,  1.90s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.41s/it]  6%|▋         | 2/32 [00:03<00:49,  1.64s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.46s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.48s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.41s/it]  9%|▉         | 3/32 [00:04<00:45,  1.55s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.44s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.40s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it] 41%|████      | 13/32 [00:18<00:26,  1.40s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.40s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.47s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.39s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.43s/it] 50%|█████     | 16/32 [00:23<00:22,  1.40s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 25%|██▌       | 8/32 [00:12<00:34,  1.46s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.40s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.45s/it] 41%|████      | 13/32 [00:18<00:27,  1.43s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.40s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.45s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.40s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.44s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.40s/it] 50%|█████     | 16/32 [00:23<00:23,  1.46s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.45s/it] 50%|█████     | 16/32 [00:23<00:22,  1.43s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.40s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 69%|██████▉   | 22/32 [00:31<00:13,  1.40s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.45s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.45s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.42s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.40s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.45s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.45s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.39s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.46s/it] 62%|██████▎   | 20/32 [00:28<00:17,  1.42s/it] 50%|█████     | 16/32 [00:23<00:23,  1.45s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.40s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.46s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.43s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.45s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.40s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.46s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.43s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.45s/it] 84%|████████▍ | 27/32 [00:38<00:06,  1.40s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.46s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.43s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.45s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.40s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.46s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.42s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.40s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.46s/it] 78%|███████▊  | 25/32 [00:36<00:09,  1.43s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.45s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.40s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.46s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.43s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.45s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.40s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.46s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.43s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.45s/it]100%|██████████| 32/32 [00:45<00:00,  1.40s/it]100%|██████████| 32/32 [00:45<00:00,  1.42s/it]
 88%|████████▊ | 28/32 [00:41<00:05,  1.46s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.43s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.46s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.46s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.43s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.46s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.46s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.43s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.48s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.43s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.46s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.49s/it]100%|██████████| 32/32 [00:46<00:00,  1.42s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
100%|██████████| 32/32 [00:47<00:00,  1.46s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]
 88%|████████▊ | 28/32 [00:41<00:06,  1.50s/it]I0402 10:37:32.581018 1645621 finetune.py:45] layer 28_up initial loss 0.025815872475504875
W0402 10:37:32.581225 1645621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:37:33.414702 1645621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:42<00:04,  1.49s/it]28_up proxy err 0.0029460277874022722 tr(WHW.T) 2478.37158203125
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:44<00:02,  1.48s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.46s/it]  3%|▎         | 1/32 [00:01<00:55,  1.79s/it]  6%|▋         | 2/32 [00:03<00:46,  1.55s/it]100%|██████████| 32/32 [00:47<00:00,  1.45s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]
I0402 10:37:39.040373 1646100 finetune.py:45] layer 29_up initial loss 0.03399825096130371
W0402 10:37:39.040550 1646100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 10:37:39.136983 1646340 finetune.py:45] layer 30_up initial loss 0.056574635207653046
W0402 10:37:39.137145 1646340 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:04<00:43,  1.51s/it]W0402 10:37:39.897715 1646100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 10:37:39.931038 1646340 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it]29_up proxy err 0.00240855454467237 tr(WHW.T) 3248.7060546875
  0%|          | 0/32 [00:00<?, ?it/s]30_up proxy err 0.0014828556450083852 tr(WHW.T) 5513.9267578125
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it]  3%|▎         | 1/32 [00:01<00:57,  1.84s/it]  3%|▎         | 1/32 [00:01<00:57,  1.87s/it] 19%|█▉        | 6/32 [00:08<00:38,  1.48s/it]  6%|▋         | 2/32 [00:03<00:48,  1.61s/it]  6%|▋         | 2/32 [00:03<00:49,  1.64s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.48s/it]I0402 10:37:45.733901 1646704 finetune.py:45] layer 31_up initial loss 0.029242757707834244
W0402 10:37:45.734182 1646704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:04<00:44,  1.53s/it]  9%|▉         | 3/32 [00:04<00:45,  1.56s/it] 25%|██▌       | 8/32 [00:11<00:35,  1.46s/it]W0402 10:37:46.593674 1646704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it]31_up proxy err 0.0005855243653059006 tr(WHW.T) 12277.7177734375
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:13<00:33,  1.45s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.50s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it]  3%|▎         | 1/32 [00:01<00:57,  1.85s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.46s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.43s/it]  6%|▋         | 2/32 [00:03<00:48,  1.60s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.48s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it]  9%|▉         | 3/32 [00:04<00:44,  1.53s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 41%|████      | 13/32 [00:18<00:26,  1.42s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.48s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.46s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.44s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.44s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.47s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.45s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it] 41%|████      | 13/32 [00:19<00:27,  1.47s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.42s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.45s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.44s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.47s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.45s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.44s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.47s/it] 62%|██████▎   | 20/32 [00:28<00:17,  1.42s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.44s/it] 50%|█████     | 16/32 [00:23<00:23,  1.44s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 50%|█████     | 16/32 [00:23<00:23,  1.47s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.44s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.44s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 41%|████      | 13/32 [00:19<00:27,  1.44s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.45s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.41s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.48s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.44s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.45s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.41s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.44s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.41s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.48s/it] 50%|█████     | 16/32 [00:23<00:23,  1.44s/it] 66%|██████▌   | 21/32 [00:30<00:16,  1.46s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.41s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.44s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.45s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.42s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.48s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.44s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.45s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.42s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.45s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.45s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.42s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.48s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.44s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.44s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.42s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.48s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.45s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.44s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.42s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.45s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.44s/it]100%|██████████| 32/32 [00:45<00:00,  1.42s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]
 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.45s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.44s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.48s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.46s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.45s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.47s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.45s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.49s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.45s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.50s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]
100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
I0402 10:38:28.593281 1645621 finetune.py:45] layer 28_gate initial loss 0.02568991668522358
W0402 10:38:28.593479 1645621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 28/32 [00:41<00:05,  1.50s/it]W0402 10:38:29.313455 1645621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:42<00:04,  1.49s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.48s/it]28_gate proxy err 0.001435952726751566 tr(WHW.T) 8688.7451171875
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:29,  1.24it/s] 97%|█████████▋| 31/32 [00:45<00:01,  1.46s/it]  2%|▏         | 2/112 [00:01<00:58,  1.88it/s]  3%|▎         | 3/112 [00:01<00:49,  2.21it/s]  4%|▎         | 4/112 [00:01<00:43,  2.46it/s]  4%|▍         | 5/112 [00:02<00:40,  2.61it/s]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
  5%|▌         | 6/112 [00:02<00:39,  2.71it/s]  6%|▋         | 7/112 [00:02<00:38,  2.76it/s]I0402 10:38:35.315734 1646340 finetune.py:45] layer 30_gate initial loss 0.05618428811430931
W0402 10:38:35.315940 1646340 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  7%|▋         | 8/112 [00:03<00:37,  2.74it/s]  8%|▊         | 9/112 [00:03<00:37,  2.74it/s]W0402 10:38:36.089125 1646340 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 10/112 [00:03<00:37,  2.74it/s]I0402 10:38:36.145730 1646100 finetune.py:45] layer 29_gate initial loss 0.03380665183067322
W0402 10:38:36.146084 1646100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 10%|▉         | 11/112 [00:04<00:36,  2.79it/s] 11%|█         | 12/112 [00:04<00:35,  2.79it/s]W0402 10:38:36.863986 1646100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▏        | 13/112 [00:05<00:35,  2.79it/s] 12%|█▎        | 14/112 [00:05<00:34,  2.83it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.79it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.77it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.75it/s]30_gate proxy err 0.0010159522062167525 tr(WHW.T) 13194.71484375
  0%|          | 0/112 [00:00<?, ?it/s] 16%|█▌        | 18/112 [00:06<00:34,  2.74it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.74it/s]29_gate proxy err 0.0012978585436940193 tr(WHW.T) 9685.263671875
  0%|          | 0/112 [00:00<?, ?it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.74it/s]  1%|          | 1/112 [00:00<01:31,  1.22it/s]  2%|▏         | 2/112 [00:01<00:59,  1.84it/s] 19%|█▉        | 21/112 [00:07<00:33,  2.73it/s]  3%|▎         | 3/112 [00:01<00:50,  2.17it/s]  1%|          | 1/112 [00:00<01:31,  1.21it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.73it/s]  4%|▎         | 4/112 [00:01<00:45,  2.36it/s]  2%|▏         | 2/112 [00:01<01:01,  1.78it/s] 21%|██        | 23/112 [00:08<00:32,  2.72it/s]  4%|▍         | 5/112 [00:02<00:42,  2.51it/s]  3%|▎         | 3/112 [00:01<00:51,  2.12it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.72it/s]  5%|▌         | 6/112 [00:02<00:40,  2.61it/s]  4%|▎         | 4/112 [00:01<00:46,  2.33it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.72it/s]  6%|▋         | 7/112 [00:02<00:39,  2.67it/s]  4%|▍         | 5/112 [00:02<00:43,  2.46it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.71it/s]  7%|▋         | 8/112 [00:03<00:38,  2.71it/s]  5%|▌         | 6/112 [00:02<00:41,  2.53it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.72it/s]  8%|▊         | 9/112 [00:03<00:37,  2.72it/s]  6%|▋         | 7/112 [00:03<00:40,  2.58it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.71it/s]I0402 10:38:42.806987 1646704 finetune.py:45] layer 31_gate initial loss 0.02899027243256569
W0402 10:38:42.807201 1646704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 10/112 [00:04<00:37,  2.75it/s] 26%|██▌       | 29/112 [00:10<00:30,  2.75it/s]  7%|▋         | 8/112 [00:03<00:39,  2.62it/s] 10%|▉         | 11/112 [00:04<00:36,  2.76it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.78it/s]  8%|▊         | 9/112 [00:03<00:39,  2.64it/s]W0402 10:38:43.542542 1646704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 11%|█         | 12/112 [00:04<00:36,  2.77it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.80it/s]  9%|▉         | 10/112 [00:04<00:38,  2.67it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.75it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.77it/s] 10%|▉         | 11/112 [00:04<00:37,  2.68it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.76it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.78it/s] 11%|█         | 12/112 [00:04<00:37,  2.69it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.76it/s] 30%|███       | 34/112 [00:12<00:28,  2.76it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.69it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.77it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.75it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.70it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.77it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.78it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.71it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.77it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.77it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.70it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.77it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.81it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.70it/s]31_gate proxy err 0.00044288369826972485 tr(WHW.T) 25696.34765625
  0%|          | 0/112 [00:00<?, ?it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.74it/s] 35%|███▍      | 39/112 [00:14<00:25,  2.82it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.69it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.75it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.84it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.68it/s]  1%|          | 1/112 [00:00<01:31,  1.21it/s] 37%|███▋      | 41/112 [00:15<00:24,  2.85it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.74it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.68it/s]  2%|▏         | 2/112 [00:01<01:01,  1.78it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.82it/s] 21%|██        | 23/112 [00:08<00:32,  2.74it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.68it/s]  3%|▎         | 3/112 [00:01<00:51,  2.11it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.84it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.73it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.68it/s]  4%|▎         | 4/112 [00:01<00:46,  2.32it/s] 39%|███▉      | 44/112 [00:16<00:23,  2.85it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.74it/s] 21%|██        | 23/112 [00:08<00:33,  2.69it/s] 40%|████      | 45/112 [00:16<00:23,  2.85it/s]  4%|▍         | 5/112 [00:02<00:43,  2.44it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.74it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.69it/s] 41%|████      | 46/112 [00:16<00:23,  2.84it/s]  5%|▌         | 6/112 [00:02<00:42,  2.52it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.73it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.68it/s] 42%|████▏     | 47/112 [00:17<00:22,  2.84it/s]  6%|▋         | 7/112 [00:03<00:41,  2.56it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.74it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.69it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.84it/s]  7%|▋         | 8/112 [00:03<00:39,  2.61it/s] 26%|██▌       | 29/112 [00:10<00:30,  2.75it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.69it/s] 44%|████▍     | 49/112 [00:17<00:22,  2.85it/s]  8%|▊         | 9/112 [00:03<00:38,  2.65it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.76it/s] 25%|██▌       | 28/112 [00:10<00:31,  2.70it/s] 45%|████▍     | 50/112 [00:18<00:21,  2.85it/s]  9%|▉         | 10/112 [00:04<00:38,  2.66it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.76it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.86it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.70it/s] 10%|▉         | 11/112 [00:04<00:37,  2.67it/s] 29%|██▊       | 32/112 [00:12<00:28,  2.76it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.85it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.71it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.77it/s] 11%|█         | 12/112 [00:04<00:37,  2.69it/s] 47%|████▋     | 53/112 [00:19<00:20,  2.85it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.71it/s] 30%|███       | 34/112 [00:12<00:28,  2.77it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.69it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.84it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.71it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.77it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.70it/s] 49%|████▉     | 55/112 [00:20<00:19,  2.85it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.71it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.77it/s] 13%|█▎        | 15/112 [00:06<00:35,  2.71it/s] 50%|█████     | 56/112 [00:20<00:19,  2.81it/s] 30%|███       | 34/112 [00:13<00:28,  2.70it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.77it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.69it/s] 51%|█████     | 57/112 [00:20<00:19,  2.82it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.70it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.77it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.71it/s] 52%|█████▏    | 58/112 [00:21<00:18,  2.84it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.71it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.77it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.72it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.84it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.71it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.77it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.73it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.85it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.71it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.77it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.73it/s] 54%|█████▍    | 61/112 [00:22<00:17,  2.84it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.71it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.77it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.70it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.81it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.70it/s] 38%|███▊      | 43/112 [00:16<00:24,  2.77it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.71it/s] 56%|█████▋    | 63/112 [00:22<00:17,  2.83it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.71it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.77it/s] 21%|██        | 23/112 [00:08<00:32,  2.72it/s] 57%|█████▋    | 64/112 [00:23<00:16,  2.83it/s] 38%|███▊      | 42/112 [00:16<00:25,  2.71it/s] 40%|████      | 45/112 [00:16<00:24,  2.77it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.69it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.80it/s] 41%|████      | 46/112 [00:17<00:24,  2.74it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.68it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.70it/s] 59%|█████▉    | 66/112 [00:23<00:16,  2.82it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.75it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.69it/s] 60%|█████▉    | 67/112 [00:24<00:15,  2.82it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.70it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.76it/s] 40%|████      | 45/112 [00:17<00:24,  2.69it/s] 61%|██████    | 68/112 [00:24<00:15,  2.83it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.70it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.76it/s] 41%|████      | 46/112 [00:17<00:24,  2.69it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.85it/s] 25%|██▌       | 28/112 [00:10<00:31,  2.70it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.77it/s] 42%|████▏     | 47/112 [00:17<00:24,  2.70it/s] 62%|██████▎   | 70/112 [00:25<00:14,  2.85it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.71it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.77it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.70it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.83it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.69it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.76it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.84it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.69it/s] 28%|██▊       | 31/112 [00:11<00:30,  2.70it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.77it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.84it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.71it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.71it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.76it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.85it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.69it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.72it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.77it/s] 67%|██████▋   | 75/112 [00:27<00:12,  2.87it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.70it/s] 30%|███       | 34/112 [00:13<00:28,  2.73it/s] 50%|█████     | 56/112 [00:20<00:20,  2.77it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.87it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.69it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.70it/s] 69%|██████▉   | 77/112 [00:27<00:12,  2.84it/s] 51%|█████     | 57/112 [00:21<00:19,  2.75it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.69it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.71it/s] 70%|██████▉   | 78/112 [00:28<00:11,  2.85it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.76it/s] 49%|████▉     | 55/112 [00:20<00:21,  2.69it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.73it/s] 71%|███████   | 79/112 [00:28<00:11,  2.86it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.76it/s] 50%|█████     | 56/112 [00:21<00:20,  2.70it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.72it/s] 71%|███████▏  | 80/112 [00:28<00:11,  2.87it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.76it/s] 51%|█████     | 57/112 [00:21<00:20,  2.69it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.72it/s] 72%|███████▏  | 81/112 [00:29<00:10,  2.86it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.77it/s] 52%|█████▏    | 58/112 [00:21<00:20,  2.70it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.70it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.84it/s] 55%|█████▌    | 62/112 [00:22<00:18,  2.77it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.71it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.71it/s] 74%|███████▍  | 83/112 [00:29<00:10,  2.85it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.75it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.68it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.72it/s] 75%|███████▌  | 84/112 [00:30<00:09,  2.85it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.73it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.65it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.69it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.82it/s] 58%|█████▊    | 65/112 [00:23<00:17,  2.73it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.65it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.71it/s] 77%|███████▋  | 86/112 [00:30<00:09,  2.83it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.73it/s] 56%|█████▋    | 63/112 [00:23<00:18,  2.67it/s] 40%|████      | 45/112 [00:17<00:24,  2.70it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.83it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.73it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.65it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.82it/s] 41%|████      | 46/112 [00:17<00:24,  2.70it/s] 61%|██████    | 68/112 [00:25<00:16,  2.71it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.83it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.65it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.71it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.71it/s] 80%|████████  | 90/112 [00:32<00:07,  2.84it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.72it/s] 59%|█████▉    | 66/112 [00:24<00:17,  2.66it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.67it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.82it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.70it/s] 60%|█████▉    | 67/112 [00:25<00:17,  2.64it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.69it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.83it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.72it/s] 61%|██████    | 68/112 [00:25<00:16,  2.66it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.69it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.83it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.72it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.64it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.70it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.83it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.72it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.64it/s] 66%|██████▌   | 74/112 [00:27<00:14,  2.68it/s] 85%|████████▍ | 95/112 [00:34<00:05,  2.84it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.73it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.62it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.67it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.82it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.71it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.64it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.69it/s] 87%|████████▋ | 97/112 [00:34<00:05,  2.82it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.72it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.64it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.70it/s] 88%|████████▊ | 98/112 [00:35<00:04,  2.83it/s] 50%|█████     | 56/112 [00:21<00:20,  2.73it/s] 66%|██████▌   | 74/112 [00:27<00:14,  2.65it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.72it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.83it/s] 51%|█████     | 57/112 [00:21<00:20,  2.73it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.68it/s] 71%|███████   | 79/112 [00:29<00:12,  2.73it/s] 89%|████████▉ | 100/112 [00:35<00:04,  2.83it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.73it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.68it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.82it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.73it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.72it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.68it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.82it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.73it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.72it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.66it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.79it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.71it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.70it/s] 71%|███████   | 79/112 [00:29<00:12,  2.63it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.79it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.68it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.71it/s] 71%|███████▏  | 80/112 [00:30<00:12,  2.63it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.80it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.68it/s] 56%|█████▋    | 63/112 [00:23<00:18,  2.71it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.80it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.64it/s] 76%|███████▌  | 85/112 [00:31<00:10,  2.69it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.71it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.81it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.65it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.71it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.72it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.82it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.66it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.71it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.73it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.83it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.68it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.73it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.73it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.84it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.68it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.72it/s] 61%|██████    | 68/112 [00:25<00:16,  2.71it/s] 99%|█████████▉| 111/112 [00:39<00:00,  2.82it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.69it/s] 80%|████████  | 90/112 [00:33<00:08,  2.73it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.72it/s]100%|██████████| 112/112 [00:40<00:00,  2.84it/s]100%|██████████| 112/112 [00:40<00:00,  2.79it/s]
 78%|███████▊  | 87/112 [00:32<00:09,  2.69it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.73it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.72it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.68it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.72it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.66it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.65it/s] 83%|████████▎ | 93/112 [00:34<00:07,  2.70it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.63it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.68it/s] 80%|████████  | 90/112 [00:34<00:08,  2.63it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.64it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.71it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.66it/s] 66%|██████▌   | 74/112 [00:27<00:14,  2.68it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.72it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.66it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.68it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.73it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.68it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.66it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.75it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.69it/s] 69%|██████▉   | 77/112 [00:28<00:13,  2.69it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.75it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.69it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.71it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.76it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.70it/s] 71%|███████   | 79/112 [00:29<00:12,  2.72it/s] 90%|█████████ | 101/112 [00:37<00:03,  2.76it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.69it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.72it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.75it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.69it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.74it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.76it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.69it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.75it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.76it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.70it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.75it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.77it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.68it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.75it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.77it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.69it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.76it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.76it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.70it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.77it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.77it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.71it/s]W0402 10:39:18.934000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:39:18.934000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:18.934000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:18.934000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:18.934000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:18.934000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:18.935000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 87/112 [00:32<00:09,  2.75it/s]W0402 10:39:18.975000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:18.976000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:18.976000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:18.976000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:18.976000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 109/112 [00:40<00:01,  2.77it/s]W0402 10:39:19.142000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.142000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.142000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.142000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.142000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 105/112 [00:39<00:02,  2.71it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.76it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.78it/s]W0402 10:39:19.440000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.440000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.441000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.441000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.441000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.441000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.441000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.471000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.471000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.471000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.471000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.472000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.537000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.537000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.538000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.538000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:19.538000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 95%|█████████▍| 106/112 [00:39<00:02,  2.73it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.77it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.79it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.73it/s] 80%|████████  | 90/112 [00:33<00:07,  2.77it/s]100%|██████████| 112/112 [00:41<00:00,  2.80it/s]100%|██████████| 112/112 [00:41<00:00,  2.72it/s]
 96%|█████████▋| 108/112 [00:40<00:01,  2.73it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.77it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.69it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.76it/s]W0402 10:39:20.798000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 98%|█████████▊| 110/112 [00:41<00:00,  2.65it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.76it/s]W0402 10:39:21.232000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:21.232000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:21.232000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:39:21.232000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:21.232000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:21.233000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:21.233000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:21.261000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:21.261000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:21.261000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:21.261000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:21.261000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 99%|█████████▉| 111/112 [00:41<00:00,  2.63it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.77it/s]W0402 10:39:21.618000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:21.618000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:21.618000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:21.619000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:21.619000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
100%|██████████| 112/112 [00:42<00:00,  2.66it/s]100%|██████████| 112/112 [00:42<00:00,  2.66it/s]
 85%|████████▍ | 95/112 [00:35<00:06,  2.77it/s]W0402 10:39:22.129000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:22.134000 140618788001600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
 86%|████████▌ | 96/112 [00:35<00:05,  2.78it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.76it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.74it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.76it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.77it/s] 90%|█████████ | 101/112 [00:37<00:03,  2.76it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.72it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.67it/s] 93%|█████████▎| 104/112 [00:38<00:03,  2.65it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.64it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.62it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.62it/s]W0402 10:39:26.689000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:26.689000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:26.689000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:26.690000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:26.690000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:26.690000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:26.690000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
 96%|█████████▋| 108/112 [00:40<00:01,  2.61it/s]W0402 10:39:26.732000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:26.732000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:26.732000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:26.732000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:26.733000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:26.902000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:26.902000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:26.902000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:26.902000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:26.902000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 109/112 [00:40<00:01,  2.61it/s]W0402 10:39:27.212000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:27.212000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:27.212000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:27.212000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:27.212000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:39:27.212000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:27.213000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:27.245000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:27.245000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:27.245000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:27.245000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:27.245000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:27.310000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:27.310000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:27.310000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:27.311000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:27.311000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 98%|█████████▊| 110/112 [00:41<00:00,  2.61it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.60it/s]W0402 10:39:28.204000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.204000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.205000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.205000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.205000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.205000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.205000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
100%|██████████| 112/112 [00:41<00:00,  2.60it/s]100%|██████████| 112/112 [00:41<00:00,  2.68it/s]
W0402 10:39:28.246000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.246000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.246000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.246000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.246000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.412000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.412000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.412000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.412000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.412000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.569000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.713000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.713000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.713000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.713000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.713000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.713000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.713000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.742000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.742000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.742000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.742000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.742000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.808000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.809000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.809000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.809000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:28.809000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:29.010000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:29.010000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:29.010000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:39:29.010000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:29.010000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:29.010000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:29.011000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
I0402 10:39:29.038511 1645621 finetune.py:45] layer 28_down initial loss 0.025491055101156235
W0402 10:39:29.038768 1645621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:39:29.039000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:29.039000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:29.039000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:29.039000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:29.039000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:29.412000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:29.412000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:29.412000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:29.412000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:29.412000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:29.554868 1645621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0402 10:39:29.938000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:29.943000 140014503548736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
28_down proxy err 0.0030659353360533714 tr(WHW.T) 82.2515869140625
W0402 10:39:30.079000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:30.526000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:30.526000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:30.526000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:30.526000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:39:30.526000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:30.526000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:30.527000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:30.556000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:30.556000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:30.556000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:30.556000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:30.556000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:30.913000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:30.914000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:30.914000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:30.914000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:30.914000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:31.426000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:31.431000 140612344657728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:34.953000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:34.953000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:34.953000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:34.953000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:34.953000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:34.953000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:34.953000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 10:39:34.994000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:34.994000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:34.994000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:34.994000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:34.994000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.158000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.159000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.159000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.159000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.159000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.459000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.459000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.459000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.460000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.460000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.460000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.460000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.488000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.488000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.488000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.488000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.488000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.554000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.554000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.554000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.554000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:35.554000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:36.810000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
I0402 10:39:37.009929 1646340 finetune.py:45] layer 30_down initial loss 0.05580408498644829
W0402 10:39:37.010294 1646340 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:39:37.250000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 10:39:37.250000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:37.250000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:37.250000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:37.250000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 10:39:37.250000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:37.250000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:37.278000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:37.278000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:37.278000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:37.278000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:37.278000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 10:39:37.534459 1646340 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0402 10:39:37.634000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 10:39:37.634000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 10:39:37.634000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:37.634000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 10:39:37.634000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
30_down proxy err 0.0013882939238101244 tr(WHW.T) 368.5620422363281
W0402 10:39:38.153000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 10:39:38.158000 139854024234816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps4 is not in var_ranges, defaulting to unknown range.
I0402 10:39:38.540128 1646100 finetune.py:45] layer 29_down initial loss 0.03354907035827637
W0402 10:39:38.540359 1646100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:39:39.046027 1646100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

29_down proxy err 0.002568997675552964 tr(WHW.T) 132.4029083251953
I0402 10:39:45.228312 1646704 finetune.py:45] layer 31_down initial loss 0.028907008469104767
W0402 10:39:45.228608 1646704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 10:39:45.732801 1646704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

31_down proxy err 0.0004261736467014998 tr(WHW.T) 2762.897216796875
I0402 10:39:57.986019 1651132 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 10:39:57.986158 1651132 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 10:39:57.986207 1651132 utils.py:162] NumExpr defaulting to 16 threads.
I0402 10:39:58.169256 1651132 config.py:58] PyTorch version 2.4.0 available.
W0402 10:40:00.068234 1651132 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0402 10:40:00.068812 1651132 hfize_llama.py:25] LlamaConfig {
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {
    "K": 4,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  4.32it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  4.80it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:01,  3.53it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:01<00:00,  3.25it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  3.29it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  3.41it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  3.53it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  3.54it/s]
Some weights of the model checkpoint at ../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B and are newly initialized: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.tp_rank', 'model.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  3.13it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  3.37it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:01,  3.59it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:01<00:00,  3.76it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  3.85it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  3.98it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  4.12it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  3.86it/s]
W0402 10:40:05.687269 1651132 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(f'{args.quantized_path}/{ii}_layernorm.pt',

W0402 10:40:05.688643 1651132 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',

W0402 10:40:05.713914 1651132 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_k.pt',

W0402 10:40:05.730869 1651132 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_v.pt',

W0402 10:40:05.745128 1651132 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_o.pt',

W0402 10:40:05.767804 1651132 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',

W0402 10:40:05.792029 1651132 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_gate.pt',

W0402 10:40:05.815964 1651132 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_down.pt',

I0402 10:40:05.842779 1651132 hfize_llama.py:113] loaded layer 0
I0402 10:40:05.982326 1651132 hfize_llama.py:113] loaded layer 1
I0402 10:40:06.108358 1651132 hfize_llama.py:113] loaded layer 2
I0402 10:40:06.241186 1651132 hfize_llama.py:113] loaded layer 3
I0402 10:40:06.396097 1651132 hfize_llama.py:113] loaded layer 4
I0402 10:40:06.519212 1651132 hfize_llama.py:113] loaded layer 5
I0402 10:40:06.668771 1651132 hfize_llama.py:113] loaded layer 6
I0402 10:40:06.784681 1651132 hfize_llama.py:113] loaded layer 7
I0402 10:40:06.882310 1651132 hfize_llama.py:113] loaded layer 8
I0402 10:40:07.024124 1651132 hfize_llama.py:113] loaded layer 9
I0402 10:40:07.164027 1651132 hfize_llama.py:113] loaded layer 10
I0402 10:40:07.267172 1651132 hfize_llama.py:113] loaded layer 11
I0402 10:40:07.427295 1651132 hfize_llama.py:113] loaded layer 12
I0402 10:40:07.544951 1651132 hfize_llama.py:113] loaded layer 13
I0402 10:40:07.675613 1651132 hfize_llama.py:113] loaded layer 14
I0402 10:40:07.841226 1651132 hfize_llama.py:113] loaded layer 15
I0402 10:40:07.961591 1651132 hfize_llama.py:113] loaded layer 16
I0402 10:40:08.079892 1651132 hfize_llama.py:113] loaded layer 17
I0402 10:40:08.217668 1651132 hfize_llama.py:113] loaded layer 18
I0402 10:40:08.347477 1651132 hfize_llama.py:113] loaded layer 19
I0402 10:40:08.444520 1651132 hfize_llama.py:113] loaded layer 20
I0402 10:40:08.566038 1651132 hfize_llama.py:113] loaded layer 21
I0402 10:40:08.720075 1651132 hfize_llama.py:113] loaded layer 22
I0402 10:40:08.864010 1651132 hfize_llama.py:113] loaded layer 23
I0402 10:40:09.024162 1651132 hfize_llama.py:113] loaded layer 24
I0402 10:40:09.163937 1651132 hfize_llama.py:113] loaded layer 25
I0402 10:40:09.275948 1651132 hfize_llama.py:113] loaded layer 26
I0402 10:40:09.400247 1651132 hfize_llama.py:113] loaded layer 27
I0402 10:40:09.517911 1651132 hfize_llama.py:113] loaded layer 28
I0402 10:40:09.628839 1651132 hfize_llama.py:113] loaded layer 29
I0402 10:40:09.779929 1651132 hfize_llama.py:113] loaded layer 30
I0402 10:40:09.956032 1651132 hfize_llama.py:113] loaded layer 31
I0402 10:40:09.956135 1651132 hfize_llama.py:115] saving model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.97it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]
I0402 10:40:25.135720 1651132 hfize_llama.py:122] successfully loaded hfized model
