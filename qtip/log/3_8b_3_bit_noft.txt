I0402 09:16:30.893847 1558010 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:16:30.893960 1558010 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:16:30.894002 1558010 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:16:31.066756 1558010 config.py:58] PyTorch version 2.4.0 available.
W0402 09:16:33.191159 1558010 warnings.py:110] /workspace/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.88it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.25it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.59it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.79it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.97it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.95it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  6.81it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.74it/s]
I0402 09:16:34.911447 1558010 quantize_finetune_llama.py:135] loaded model
I0402 09:16:55.257196 1558010 quantize_finetune_llama.py:139] loaded dataset and devset
I0402 09:17:01.652113 1558010 quantize_finetune_llama.py:159] layer 0 gpu 0
I0402 09:17:05.226465 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 0 in 3.4323031902313232s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0402 09:17:22.988235 1558010 quantize_finetune_llama.py:159] layer 1 gpu 1
I0402 09:17:25.037790 1559240 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:17:25.037939 1559240 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:17:25.037997 1559240 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:17:25.222333 1559240 config.py:58] PyTorch version 2.4.0 available.
I0402 09:17:26.393440 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 1 in 3.213843584060669s
I0402 09:17:26.895466 1558010 quantize_finetune_llama.py:159] layer 2 gpu 2
I0402 09:17:27.446200 1559240 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 09:17:27.891140 1559240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]I0402 09:17:28.981756 1559498 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:17:28.981894 1559498 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:17:28.981951 1559498 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:17:29.253411 1559498 config.py:58] PyTorch version 2.4.0 available.
I0402 09:17:29.854742 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 2 in 2.8100757598876953s
  3%|▎         | 1/32 [00:01<00:43,  1.42s/it]I0402 09:17:30.353304 1558010 quantize_finetune_llama.py:159] layer 3 gpu 3
  6%|▋         | 2/32 [00:01<00:23,  1.28it/s]  9%|▉         | 3/32 [00:02<00:16,  1.74it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.08it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s]I0402 09:17:31.626820 1559498 data_utils.py:336] using 256 training seqs, 128 validation seqs
 19%|█▉        | 6/32 [00:03<00:10,  2.55it/s]W0402 09:17:31.995562 1559498 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:03<00:09,  2.69it/s]I0402 09:17:32.399480 1560036 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:17:32.399621 1560036 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:17:32.399679 1560036 utils.py:162] NumExpr defaulting to 16 threads.
 25%|██▌       | 8/32 [00:03<00:08,  2.82it/s]I0402 09:17:32.716273 1560036 config.py:58] PyTorch version 2.4.0 available.
 28%|██▊       | 9/32 [00:04<00:07,  2.94it/s]  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.98it/s]I0402 09:17:33.285037 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 3 in 2.798203468322754s
 34%|███▍      | 11/32 [00:04<00:07,  2.97it/s]I0402 09:17:33.803642 1558010 quantize_finetune_llama.py:159] layer 4 gpu 0
 38%|███▊      | 12/32 [00:05<00:06,  2.97it/s] 41%|████      | 13/32 [00:05<00:06,  2.98it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.98it/s]  3%|▎         | 1/32 [00:01<00:50,  1.62s/it] 47%|████▋     | 15/32 [00:06<00:05,  3.00it/s]  6%|▋         | 2/32 [00:01<00:26,  1.15it/s]I0402 09:17:35.173944 1560036 data_utils.py:336] using 256 training seqs, 128 validation seqs
 50%|█████     | 16/32 [00:06<00:05,  2.99it/s]  9%|▉         | 3/32 [00:02<00:18,  1.59it/s]W0402 09:17:35.494633 1560036 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 17/32 [00:06<00:04,  3.02it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.96it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.01it/s]I0402 09:17:35.915892 1560549 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:17:35.916049 1560549 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:17:35.916109 1560549 utils.py:162] NumExpr defaulting to 16 threads.
 16%|█▌        | 5/32 [00:02<00:12,  2.23it/s]I0402 09:17:36.161924 1560549 config.py:58] PyTorch version 2.4.0 available.
 59%|█████▉    | 19/32 [00:07<00:04,  3.02it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.44it/s]  0%|          | 0/32 [00:00<?, ?it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.05it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.61it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.06it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.70it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.06it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.78it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.10it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.87it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.10it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.90it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.10it/s]  3%|▎         | 1/32 [00:01<00:53,  1.73s/it] 38%|███▊      | 12/32 [00:05<00:06,  2.92it/s]I0402 09:17:38.296312 1560549 data_utils.py:336] using 256 training seqs, 128 validation seqs
 81%|████████▏ | 26/32 [00:09<00:01,  3.07it/s]  6%|▋         | 2/32 [00:02<00:27,  1.10it/s] 41%|████      | 13/32 [00:05<00:06,  2.95it/s]W0402 09:17:38.772859 1560549 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:09<00:01,  3.09it/s]  9%|▉         | 3/32 [00:02<00:18,  1.57it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.96it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.06it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.93it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.97it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.07it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.24it/s] 50%|█████     | 16/32 [00:06<00:05,  2.98it/s]  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.11it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.49it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.02it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.13it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.67it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.02it/s]100%|██████████| 32/32 [00:11<00:00,  3.14it/s]100%|██████████| 32/32 [00:11<00:00,  2.77it/s]
 25%|██▌       | 8/32 [00:03<00:08,  2.81it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.03it/s] 28%|██▊       | 9/32 [00:04<00:07,  2.89it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.99it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.92it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.96it/s]  3%|▎         | 1/32 [00:01<00:51,  1.67s/it] 34%|███▍      | 11/32 [00:04<00:07,  2.94it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.97it/s]  6%|▋         | 2/32 [00:01<00:26,  1.14it/s] 38%|███▊      | 12/32 [00:05<00:06,  3.00it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.02it/s] 41%|████      | 13/32 [00:05<00:06,  3.06it/s]  9%|▉         | 3/32 [00:02<00:18,  1.61it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.05it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.09it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.98it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.04it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.10it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.27it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.06it/s] 50%|█████     | 16/32 [00:06<00:05,  3.12it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.50it/s]W0402 09:17:43.234000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.234000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.234000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.234000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.234000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.234000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.234000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  3.07it/s]W0402 09:17:43.259000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.259000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.259000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.259000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.259000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.275000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.275000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.276000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.276000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.276000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:06<00:04,  3.14it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.67it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.08it/s]W0402 09:17:43.585000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.585000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.585000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.585000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:43.585000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:07<00:04,  3.15it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.80it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.10it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.17it/s] 28%|██▊       | 9/32 [00:04<00:07,  2.90it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.10it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.18it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.96it/s]W0402 09:17:44.435000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:44.435000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:17:44.435000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:44.435000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:44.435000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:44.436000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:44.436000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:44.453000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:44.453000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:44.453000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:44.453000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:44.453000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  3.09it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.17it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.00it/s]W0402 09:17:44.684000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:44.684000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:44.684000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:44.685000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:44.685000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:11<00:00,  3.09it/s]100%|██████████| 32/32 [00:11<00:00,  2.69it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  3.16it/s] 38%|███▊      | 12/32 [00:05<00:06,  3.02it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.12it/s] 41%|████      | 13/32 [00:05<00:06,  2.99it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.10it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.99it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.07it/s]W0402 09:17:45.859000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:45.859000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:45.859000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:45.859000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:45.859000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:45.859000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:17:45.859000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:45.877000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:45.877000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:45.878000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:45.878000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:45.878000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 15/32 [00:06<00:05,  2.97it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.07it/s] 50%|█████     | 16/32 [00:06<00:05,  3.00it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.10it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.03it/s]W0402 09:17:46.757000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:46.757000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:46.757000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:46.757000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:46.757000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  3.13it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.06it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.12it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.04it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 94%|█████████▍| 30/32 [00:11<00:00,  3.12it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.03it/s]W0402 09:17:47.777000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:47.777000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:47.777000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:47.777000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:47.777000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:17:47.777000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:47.777000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  3.10it/s]W0402 09:17:47.803000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:47.803000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:47.803000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:47.803000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:47.803000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:47.820000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:47.820000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:47.820000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:47.820000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:47.820000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:03,  3.03it/s]100%|██████████| 32/32 [00:11<00:00,  3.08it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
W0402 09:17:48.149000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:48.150000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:48.150000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:48.150000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:48.150000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  3.01it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.97it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.95it/s]W0402 09:17:49.081000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:49.081000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:49.081000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:49.081000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:17:49.081000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:49.081000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:49.081000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:49.100000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:49.100000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:49.100000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:49.100000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:49.100000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:09<00:02,  2.92it/s]W0402 09:17:49.354000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:49.354000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:49.354000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:49.354000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:49.354000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:09<00:02,  2.91it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.91it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.91it/s]W0402 09:17:50.599000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:50.599000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:50.599000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:17:50.599000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:50.599000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:50.599000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:50.600000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:50.618000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:50.618000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:50.618000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:50.619000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:50.619000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:10<00:01,  2.90it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.90it/s]W0402 09:17:51.228000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.229000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.229000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.229000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.229000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.229000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.229000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.256000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.256000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.257000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.257000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.257000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.274000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.274000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.274000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.274000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.274000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  2.90it/s]W0402 09:17:51.586000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.586000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.586000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.586000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.586000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.609000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.609000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.609000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.609000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:51.609000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:11<00:00,  2.90it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0402 09:17:52.487626 1559240 finetune.py:45] layer 0_v initial loss 0.0001336175191681832
W0402 09:17:52.487962 1559240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:17:52.539000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:52.540000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:52.540000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:17:52.540000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:52.540000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:52.540000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:52.540000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:52.558000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:52.559000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:52.559000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:52.559000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:52.559000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:52.809000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:52.809000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:52.810000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:52.810000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:52.810000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:53.558532 1559240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:17:54.044000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.045000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.045000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.045000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.045000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.045000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.045000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.064000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.064000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.064000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.064000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.064000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
0_v proxy err 0.004657589364796877 tr(WHW.T) 1.3175290822982788
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:17:54.818000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.819000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.819000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.819000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.819000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.819000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.819000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.847000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.847000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.847000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.847000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.847000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.864000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.864000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.864000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.864000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:54.864000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:55.021000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:55.021000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:55.021000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:55.022000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:55.022000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:55.208000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:55.208000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:55.208000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:55.208000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:55.208000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:29,  1.04it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
  6%|▋         | 2/32 [00:01<00:18,  1.62it/s]W0402 09:17:56.151000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:56.151000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:17:56.151000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:56.152000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:56.152000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:56.152000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:56.152000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:56.170000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:56.170000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:56.171000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:56.171000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:56.171000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:01<00:14,  1.99it/s]W0402 09:17:56.433000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:56.433000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:56.433000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:56.433000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:56.434000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s]I0402 09:17:57.371740 1559498 finetune.py:45] layer 1_v initial loss 0.004754400812089443
W0402 09:17:57.371978 1559498 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s]W0402 09:17:57.692000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:57.692000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:57.692000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:17:57.692000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:17:57.692000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:57.692000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:57.693000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:17:57.711000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:57.711000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:57.711000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:57.711000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:57.711000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s]W0402 09:17:58.615894 1559498 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:17:58.679000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:17:58.679000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:17:58.679000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:17:58.680000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:17:58.680000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 38%|███▊      | 12/32 [00:05<00:07,  2.68it/s] 41%|████      | 13/32 [00:05<00:07,  2.71it/s]1_v proxy err 0.00461159273982048 tr(WHW.T) 5.699800491333008
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.72it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.73it/s]I0402 09:18:00.851760 1560036 finetune.py:45] layer 2_v initial loss 0.0007029824773781002
W0402 09:18:00.852178 1560036 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:31,  1.02s/it] 50%|█████     | 16/32 [00:06<00:05,  2.74it/s]  6%|▋         | 2/32 [00:01<00:19,  1.54it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.75it/s]  9%|▉         | 3/32 [00:01<00:15,  1.91it/s]W0402 09:18:01.865720 1560036 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.14it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.75it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.75it/s]2_v proxy err 0.0066235000267624855 tr(WHW.T) 26.839576721191406
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.40it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.73it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s]  3%|▎         | 1/32 [00:01<00:31,  1.01s/it] 75%|███████▌  | 24/32 [00:09<00:02,  2.73it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.55it/s]  6%|▋         | 2/32 [00:01<00:18,  1.58it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.73it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s]  9%|▉         | 3/32 [00:01<00:14,  1.95it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.74it/s]I0402 09:18:04.812472 1560549 finetune.py:45] layer 3_v initial loss 0.0013283725129440427
W0402 09:18:04.812662 1560549 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:04<00:08,  2.58it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.64it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.82it/s]W0402 09:18:05.839430 1560549 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:05<00:06,  2.70it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.60it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.66it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.82it/s] 50%|█████     | 16/32 [00:06<00:05,  2.73it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
3_v proxy err 0.008299178443849087 tr(WHW.T) 40.46714782714844
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.76it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.75it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s]  3%|▎         | 1/32 [00:00<00:29,  1.04it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.74it/s]  6%|▋         | 2/32 [00:01<00:18,  1.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s] 41%|████      | 13/32 [00:05<00:06,  2.74it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.78it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.28it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.82it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.78it/s] 50%|█████     | 16/32 [00:06<00:05,  2.83it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.56it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.79it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.62it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.84it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.79it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.86it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.75it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.86it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.78it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.87it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.79it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.87it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.78it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.79it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.88it/s] 41%|████      | 13/32 [00:05<00:06,  2.81it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.81it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.88it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
W0402 09:18:12.501000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.501000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.501000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.501000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.501000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.502000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.502000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.529000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.529000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.529000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.529000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.530000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.544000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.544000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.544000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.544000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.544000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.693000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.693000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.693000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.693000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.693000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:09<00:02,  2.86it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.80it/s]W0402 09:18:12.925000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.925000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.926000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.926000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.926000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.926000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.926000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.948000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.948000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.948000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.949000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:12.949000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:13.015000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:13.015000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:13.015000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:13.015000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:13.016000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s] 50%|█████     | 16/32 [00:06<00:05,  2.77it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.80it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.79it/s]W0402 09:18:13.880000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:07<00:05,  2.75it/s]W0402 09:18:14.183000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:14.183000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:14.183000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:18:14.183000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:14.184000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:14.184000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:14.184000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:14.204000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:14.205000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:14.205000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:14.205000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:14.205000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s]W0402 09:18:14.465000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:14.465000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:14.465000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:14.465000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:14.465000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s]W0402 09:18:14.713000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.76it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.74it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.77it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.73it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.67it/s]W0402 09:18:18.402000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.402000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.402000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.403000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.403000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.403000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.403000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.434000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.434000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.434000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.435000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.435000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.451000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.451000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.451000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.451000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.451000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.614000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.614000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.615000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.615000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.615000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  2.67it/s]W0402 09:18:18.851000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.852000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.852000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.852000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.852000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.852000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.852000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.873000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.873000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.873000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.873000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.873000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.941000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.941000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.941000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.941000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:18.942000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
W0402 09:18:19.855000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:20.175000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:20.175000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:20.175000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:18:20.175000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:20.175000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:20.175000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:20.176000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:20.196000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:20.196000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:20.197000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:20.197000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:20.197000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:20.464000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:20.464000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:20.464000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:20.464000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:20.464000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:20.731000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0402 09:18:20.788172 1559240 finetune.py:45] layer 0_q initial loss 0.0001334932167083025
W0402 09:18:20.788436 1559240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:18:21.108000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.108000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.108000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.109000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.109000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.109000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.109000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.141000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.141000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.141000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.141000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.142000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.158000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.158000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.158000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.158000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.158000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.320000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.320000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.320000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.320000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.320000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.542000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.542000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.542000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.542000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.542000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.543000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.543000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.564000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.564000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.564000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.564000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.564000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.629000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.629000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.629000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.629000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.629000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:21.816078 1559240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:18:22.498000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:22.832000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:18:22.832000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:22.832000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:22.833000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:22.833000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:22.833000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:22.833000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:22.856000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:22.856000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:22.856000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:22.856000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:22.856000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
0_q proxy err 9.617825708119199e-05 tr(WHW.T) 6234.0732421875
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:18:23.128000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:23.128000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:23.128000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:23.128000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:23.129000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:23.403000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:29,  1.06it/s]  6%|▋         | 2/32 [00:01<00:18,  1.66it/s]  9%|▉         | 3/32 [00:01<00:14,  2.03it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.27it/s]W0402 09:18:25.250000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.250000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.250000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.250000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.251000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.251000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.251000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.281000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.281000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.281000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.281000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.281000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.297000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.297000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.297000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.297000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.297000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s]W0402 09:18:25.461000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.461000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.461000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.461000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.461000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s]W0402 09:18:25.695000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.695000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.695000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.695000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.695000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.695000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.695000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.717000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.717000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.717000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.717000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.717000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.784000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.784000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.784000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.785000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:25.785000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.61it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.66it/s]W0402 09:18:26.669000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0402 09:18:26.682992 1559498 finetune.py:45] layer 1_q initial loss 0.004777723457664251
W0402 09:18:26.683173 1559498 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:03<00:08,  2.68it/s]W0402 09:18:26.987000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:26.987000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:18:26.987000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:26.988000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:18:26.988000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:26.988000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:26.988000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:18:27.010000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:27.010000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:27.010000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:27.010000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:27.010000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s]W0402 09:18:27.273000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:18:27.273000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:18:27.274000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:18:27.274000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:18:27.274000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.73it/s]W0402 09:18:27.542000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:04<00:07,  2.75it/s]W0402 09:18:27.895276 1559498 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 13/32 [00:05<00:06,  2.76it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.77it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.78it/s]1_q proxy err 7.959172944538295e-05 tr(WHW.T) 7568.30322265625
  0%|          | 0/32 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:06<00:05,  2.77it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s]I0402 09:18:29.707953 1560036 finetune.py:45] layer 2_q initial loss 0.0007036470924504101
W0402 09:18:29.708363 1560036 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▋    | 18/32 [00:07<00:05,  2.78it/s]  3%|▎         | 1/32 [00:00<00:29,  1.04it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s]  6%|▋         | 2/32 [00:01<00:18,  1.61it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s]W0402 09:18:30.887419 1560036 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:01<00:14,  1.95it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.16it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.31it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s]2_q proxy err 0.0007223940338008106 tr(WHW.T) 7137.59423828125
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.41it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.79it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.79it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.79it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.55it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s]  6%|▋         | 2/32 [00:01<00:17,  1.69it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s]  9%|▉         | 3/32 [00:01<00:14,  2.04it/s]I0402 09:18:33.736979 1560549 finetune.py:45] layer 3_q initial loss 0.00132827740162611
W0402 09:18:33.737244 1560549 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:11<00:01,  2.81it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.28it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.84it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.64it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s]W0402 09:18:34.908590 1560549 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
 22%|██▏       | 7/32 [00:03<00:09,  2.64it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.67it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s]3_q proxy err 0.0010416177101433277 tr(WHW.T) 6654.2763671875
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.71it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.78it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.73it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.80it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s]  3%|▎         | 1/32 [00:00<00:28,  1.10it/s] 41%|████      | 13/32 [00:05<00:06,  2.79it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.70it/s]  6%|▋         | 2/32 [00:01<00:17,  1.68it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.77it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.23it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.67it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.73it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.66it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.66it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.74it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.66it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.60it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.75it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.66it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s]I0402 09:18:41.115317 1559240 finetune.py:45] layer 0_k initial loss 0.0001334685366600752
W0402 09:18:41.115475 1559240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 75%|███████▌  | 24/32 [00:09<00:02,  2.74it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.64it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.72it/s]W0402 09:18:42.165488 1559240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.74it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.74it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.66it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.76it/s]0_k proxy err 6.18655321886763e-05 tr(WHW.T) 2167.80859375
  0%|          | 0/32 [00:00<?, ?it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.72it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
  3%|▎         | 1/32 [00:00<00:23,  1.29it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.70it/s]  6%|▋         | 2/32 [00:01<00:16,  1.86it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.70it/s]  9%|▉         | 3/32 [00:01<00:13,  2.17it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.35it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.67it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.67it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.66it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.65it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.68it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.63it/s]I0402 09:18:47.908987 1559498 finetune.py:45] layer 1_k initial loss 0.004818446934223175
W0402 09:18:47.909302 1559498 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:04<00:07,  2.69it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
 44%|████▍     | 14/32 [00:05<00:06,  2.70it/s]W0402 09:18:49.044740 1559498 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 15/32 [00:05<00:06,  2.71it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.71it/s]1_k proxy err 7.852500857552513e-05 tr(WHW.T) 3947.499267578125
  0%|          | 0/32 [00:00<?, ?it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.71it/s]I0402 09:18:50.532824 1560036 finetune.py:45] layer 2_k initial loss 0.0007038025069050491
W0402 09:18:50.533041 1560036 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s]  3%|▎         | 1/32 [00:00<00:24,  1.25it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.71it/s]  6%|▋         | 2/32 [00:01<00:16,  1.81it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s]W0402 09:18:51.637377 1560036 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:01<00:13,  2.11it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.72it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.29it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.72it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.73it/s]2_k proxy err 0.0007824172498658299 tr(WHW.T) 3891.80859375
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.73it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.73it/s]  3%|▎         | 1/32 [00:00<00:24,  1.27it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s]  6%|▋         | 2/32 [00:01<00:16,  1.84it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.73it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.57it/s]  9%|▉         | 3/32 [00:01<00:13,  2.15it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.74it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.34it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s]I0402 09:18:55.007477 1560549 finetune.py:45] layer 3_k initial loss 0.0013290871866047382
W0402 09:18:55.007665 1560549 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.74it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.64it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.56it/s]100%|██████████| 32/32 [00:12<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 41%|████      | 13/32 [00:05<00:07,  2.67it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.63it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.68it/s]W0402 09:18:56.016021 1560549 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.67it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.70it/s]3_k proxy err 0.0011093865614384413 tr(WHW.T) 3660.859619140625
  0%|          | 0/32 [00:00<?, ?it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.76it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.71it/s]  3%|▎         | 1/32 [00:00<00:23,  1.31it/s] 41%|████      | 13/32 [00:05<00:06,  2.75it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.69it/s]  6%|▋         | 2/32 [00:01<00:15,  1.88it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.73it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s]  9%|▉         | 3/32 [00:01<00:13,  2.17it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.72it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.34it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.46it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.71it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.71it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.62it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.70it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.61it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.70it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.67it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.61it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s]I0402 09:19:01.991643 1559240 finetune.py:45] layer 0_o initial loss 0.0001331292005488649
W0402 09:19:01.991856 1559240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
 81%|████████▏ | 26/32 [00:09<00:02,  2.76it/s]W0402 09:19:02.902964 1559240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.78it/s] 50%|█████     | 16/32 [00:06<00:05,  2.74it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.77it/s]0_o proxy err 0.0010142188984900713 tr(WHW.T) 0.23014843463897705
  0%|          | 0/32 [00:00<?, ?it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.75it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.81it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.70it/s]  3%|▎         | 1/32 [00:01<00:59,  1.92s/it] 75%|███████▌  | 24/32 [00:09<00:02,  2.69it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.69it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.69it/s]  6%|▋         | 2/32 [00:03<00:49,  1.66s/it] 84%|████████▍ | 27/32 [00:10<00:01,  2.69it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.66it/s]  9%|▉         | 3/32 [00:04<00:45,  1.58s/it] 97%|█████████▋| 31/32 [00:11<00:00,  2.67it/s]I0402 09:19:08.996689 1559498 finetune.py:45] layer 1_o initial loss 0.0040187835693359375
W0402 09:19:08.996907 1559498 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
W0402 09:19:10.052900 1559498 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it]1_o proxy err 0.003906527534127235 tr(WHW.T) 0.31429046392440796
  0%|          | 0/32 [00:00<?, ?it/s]I0402 09:19:11.376034 1560036 finetune.py:45] layer 2_o initial loss 0.0007159823435358703
W0402 09:19:11.376205 1560036 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:07<00:40,  1.51s/it]W0402 09:19:12.313926 1560036 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<01:01,  1.99s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it]2_o proxy err 0.0037935306318104267 tr(WHW.T) 0.5573468208312988
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:10<00:37,  1.48s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it]  3%|▎         | 1/32 [00:01<01:00,  1.96s/it]I0402 09:19:16.174705 1560549 finetune.py:45] layer 3_o initial loss 0.0013464785879477859
W0402 09:19:16.174892 1560549 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it]  9%|▉         | 3/32 [00:04<00:46,  1.61s/it]  6%|▋         | 2/32 [00:03<00:49,  1.67s/it]W0402 09:19:17.117058 1560549 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.55s/it]3_o proxy err 0.006054422818124294 tr(WHW.T) 0.9357947111129761
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:04<00:44,  1.55s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.50s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it]  3%|▎         | 1/32 [00:01<00:57,  1.85s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.41s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.47s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it]  6%|▋         | 2/32 [00:03<00:48,  1.61s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.44s/it]  9%|▉         | 3/32 [00:04<00:43,  1.51s/it] 41%|████      | 13/32 [00:19<00:26,  1.40s/it] 25%|██▌       | 8/32 [00:12<00:34,  1.45s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.43s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.40s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.45s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.45s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.39s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it] 50%|█████     | 16/32 [00:23<00:22,  1.39s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.44s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.44s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.40s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.43s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.42s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.39s/it] 41%|████      | 13/32 [00:19<00:27,  1.43s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.42s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.39s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.43s/it] 41%|████      | 13/32 [00:19<00:27,  1.43s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.41s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.38s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.43s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.41s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.38s/it] 50%|█████     | 16/32 [00:23<00:22,  1.43s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 69%|██████▉   | 22/32 [00:31<00:13,  1.39s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.43s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.38s/it] 41%|████      | 13/32 [00:18<00:26,  1.41s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.43s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.38s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.41s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.43s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.42s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.38s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.41s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.43s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.38s/it] 50%|█████     | 16/32 [00:23<00:22,  1.41s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 84%|████████▍ | 27/32 [00:38<00:06,  1.38s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.40s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.43s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.38s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.42s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.41s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.40s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.42s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.41s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.43s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.40s/it] 78%|███████▊  | 25/32 [00:36<00:09,  1.42s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.41s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.40s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.45s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.43s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.41s/it] 69%|██████▉   | 22/32 [00:31<00:13,  1.40s/it]100%|██████████| 32/32 [00:45<00:00,  1.45s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]
 84%|████████▍ | 27/32 [00:39<00:07,  1.43s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.41s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.45s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.43s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.43s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.46s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.44s/it] 78%|███████▊  | 25/32 [00:35<00:10,  1.44s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.47s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.45s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.45s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.49s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.45s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.46s/it]100%|██████████| 32/32 [00:46<00:00,  1.51s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
 97%|█████████▋| 31/32 [00:44<00:01,  1.47s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.47s/it]I0402 09:19:58.972176 1559240 finetune.py:45] layer 0_up initial loss 0.00013149996811989695
W0402 09:19:58.973801 1559240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:46<00:00,  1.47s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 91%|█████████ | 29/32 [00:41<00:04,  1.47s/it]W0402 09:20:00.368281 1559240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:43<00:02,  1.48s/it]0_up proxy err 0.00855313241481781 tr(WHW.T) 101.63944244384766
  0%|          | 0/32 [00:00<?, ?it/s] 97%|█████████▋| 31/32 [00:44<00:01,  1.48s/it]  3%|▎         | 1/32 [00:01<00:57,  1.84s/it]100%|██████████| 32/32 [00:46<00:00,  1.48s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
  6%|▋         | 2/32 [00:03<00:48,  1.61s/it]I0402 09:20:05.679681 1559498 finetune.py:45] layer 1_up initial loss 0.003933464176952839
W0402 09:20:05.680142 1559498 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:04<00:44,  1.53s/it]W0402 09:20:06.776464 1559498 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:20:07.399831 1560036 finetune.py:45] layer 2_up initial loss 0.0007094109314493835
W0402 09:20:07.400223 1560036 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

1_up proxy err 0.010105229914188385 tr(WHW.T) 159.80691528320312
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:06<00:41,  1.50s/it]W0402 09:20:08.371794 1560036 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:07<00:39,  1.48s/it]2_up proxy err 0.011786079034209251 tr(WHW.T) 225.97427368164062
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:58,  1.87s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.47s/it]  6%|▋         | 2/32 [00:03<00:48,  1.63s/it]  3%|▎         | 1/32 [00:01<00:56,  1.81s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it]  9%|▉         | 3/32 [00:04<00:44,  1.55s/it]  6%|▋         | 2/32 [00:03<00:47,  1.57s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.45s/it]I0402 09:20:13.943717 1560549 finetune.py:45] layer 3_up initial loss 0.0013263446744531393
W0402 09:20:13.944239 1560549 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 4/32 [00:06<00:42,  1.50s/it]  9%|▉         | 3/32 [00:04<00:43,  1.49s/it]W0402 09:20:15.205760 1560549 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:13<00:33,  1.45s/it] 12%|█▎        | 4/32 [00:06<00:40,  1.46s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it]3_up proxy err 0.011352851055562496 tr(WHW.T) 316.0293273925781
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 16%|█▌        | 5/32 [00:07<00:38,  1.43s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.47s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.42s/it]  3%|▎         | 1/32 [00:01<00:57,  1.85s/it] 19%|█▉        | 6/32 [00:08<00:36,  1.42s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it]  6%|▋         | 2/32 [00:03<00:48,  1.63s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.40s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.45s/it] 41%|████      | 13/32 [00:18<00:26,  1.40s/it] 25%|██▌       | 8/32 [00:11<00:33,  1.40s/it]  9%|▉         | 3/32 [00:04<00:45,  1.55s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.45s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.40s/it] 28%|██▊       | 9/32 [00:12<00:32,  1.40s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.40s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.39s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.44s/it] 50%|█████     | 16/32 [00:23<00:22,  1.39s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.39s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.44s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.38s/it] 38%|███▊      | 12/32 [00:17<00:27,  1.39s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.48s/it] 41%|████      | 13/32 [00:19<00:27,  1.44s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.39s/it] 41%|████      | 13/32 [00:18<00:26,  1.39s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.44s/it] 59%|█████▉    | 19/32 [00:27<00:17,  1.38s/it] 44%|████▍     | 14/32 [00:19<00:25,  1.39s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.44s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.38s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.39s/it] 50%|█████     | 16/32 [00:23<00:22,  1.44s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.37s/it] 50%|█████     | 16/32 [00:22<00:22,  1.39s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.43s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.47s/it] 69%|██████▉   | 22/32 [00:31<00:13,  1.37s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.39s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.44s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.47s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.38s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.39s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.44s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.38s/it] 59%|█████▉    | 19/32 [00:26<00:18,  1.39s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.44s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.38s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.39s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.44s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.38s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.38s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.44s/it] 84%|████████▍ | 27/32 [00:38<00:06,  1.37s/it] 50%|█████     | 16/32 [00:23<00:23,  1.46s/it] 69%|██████▉   | 22/32 [00:30<00:13,  1.38s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.38s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.45s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.39s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.37s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.44s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it] 75%|███████▌  | 24/32 [00:33<00:11,  1.39s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.37s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.45s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.46s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.39s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.37s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.44s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.46s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.38s/it]100%|██████████| 32/32 [00:45<00:00,  1.37s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]
 84%|████████▍ | 27/32 [00:39<00:07,  1.44s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.46s/it] 84%|████████▍ | 27/32 [00:37<00:07,  1.40s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.46s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.43s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.46s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.45s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.48s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.46s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.45s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.50s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.45s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.50s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
 81%|████████▏ | 26/32 [00:38<00:08,  1.45s/it]100%|██████████| 32/32 [00:45<00:00,  1.46s/it]100%|██████████| 32/32 [00:45<00:00,  1.42s/it]
I0402 09:20:55.634104 1559240 finetune.py:45] layer 0_gate initial loss 0.00012969058298040181
W0402 09:20:55.635301 1559240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 84%|████████▍ | 27/32 [00:39<00:07,  1.45s/it]W0402 09:20:56.711960 1559240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 88%|████████▊ | 28/32 [00:41<00:05,  1.45s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.44s/it]0_gate proxy err 0.005899767857044935 tr(WHW.T) 179.7012481689453
  0%|          | 0/112 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:44<00:02,  1.44s/it]  1%|          | 1/112 [00:00<01:39,  1.12it/s]  2%|▏         | 2/112 [00:01<01:05,  1.69it/s]  3%|▎         | 3/112 [00:01<00:53,  2.03it/s] 97%|█████████▋| 31/32 [00:45<00:01,  1.44s/it]  4%|▎         | 4/112 [00:02<00:48,  2.24it/s]I0402 09:21:02.400007 1559498 finetune.py:45] layer 1_gate initial loss 0.0036829146556556225
W0402 09:21:02.400254 1559498 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  4%|▍         | 5/112 [00:02<00:45,  2.36it/s]  5%|▌         | 6/112 [00:02<00:43,  2.45it/s]W0402 09:21:03.213051 1559498 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 7/112 [00:03<00:41,  2.52it/s]100%|██████████| 32/32 [00:47<00:00,  1.44s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]
  7%|▋         | 8/112 [00:03<00:40,  2.55it/s]I0402 09:21:03.671093 1560036 finetune.py:45] layer 2_gate initial loss 0.0007010636036284268
W0402 09:21:03.672325 1560036 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  8%|▊         | 9/112 [00:03<00:39,  2.59it/s]  9%|▉         | 10/112 [00:04<00:38,  2.64it/s]W0402 09:21:04.693898 1560036 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 10%|▉         | 11/112 [00:04<00:37,  2.66it/s] 11%|█         | 12/112 [00:05<00:37,  2.69it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.71it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.70it/s] 13%|█▎        | 15/112 [00:06<00:35,  2.70it/s]1_gate proxy err 0.0072186049073934555 tr(WHW.T) 270.8945617675781
  0%|          | 0/112 [00:00<?, ?it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.70it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.72it/s]  1%|          | 1/112 [00:00<01:37,  1.14it/s] 16%|█▌        | 18/112 [00:07<00:33,  2.78it/s]  2%|▏         | 2/112 [00:01<01:02,  1.75it/s] 17%|█▋        | 19/112 [00:07<00:32,  2.82it/s]  3%|▎         | 3/112 [00:01<00:52,  2.08it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.81it/s]  4%|▎         | 4/112 [00:01<00:47,  2.30it/s]2_gate proxy err 0.007657927926629782 tr(WHW.T) 449.34307861328125
  0%|          | 0/112 [00:00<?, ?it/s] 19%|█▉        | 21/112 [00:08<00:32,  2.84it/s]  4%|▍         | 5/112 [00:02<00:43,  2.44it/s] 20%|█▉        | 22/112 [00:08<00:31,  2.85it/s]  5%|▌         | 6/112 [00:02<00:41,  2.52it/s] 21%|██        | 23/112 [00:08<00:31,  2.83it/s]  1%|          | 1/112 [00:00<01:37,  1.14it/s]  6%|▋         | 7/112 [00:03<00:40,  2.61it/s] 21%|██▏       | 24/112 [00:09<00:30,  2.86it/s]  2%|▏         | 2/112 [00:01<01:04,  1.70it/s]  7%|▋         | 8/112 [00:03<00:39,  2.66it/s] 22%|██▏       | 25/112 [00:09<00:30,  2.87it/s]  3%|▎         | 3/112 [00:01<00:53,  2.02it/s]  8%|▊         | 9/112 [00:03<00:38,  2.68it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.84it/s]  4%|▎         | 4/112 [00:02<00:48,  2.22it/s]  9%|▉         | 10/112 [00:04<00:37,  2.72it/s] 24%|██▍       | 27/112 [00:10<00:29,  2.87it/s]  4%|▍         | 5/112 [00:02<00:45,  2.36it/s] 10%|▉         | 11/112 [00:04<00:37,  2.72it/s] 25%|██▌       | 28/112 [00:10<00:29,  2.87it/s]  5%|▌         | 6/112 [00:02<00:43,  2.45it/s] 11%|█         | 12/112 [00:04<00:36,  2.73it/s] 26%|██▌       | 29/112 [00:11<00:28,  2.89it/s]  6%|▋         | 7/112 [00:03<00:41,  2.51it/s] 27%|██▋       | 30/112 [00:11<00:28,  2.91it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.75it/s] 28%|██▊       | 31/112 [00:11<00:27,  2.91it/s]  7%|▋         | 8/112 [00:03<00:40,  2.56it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.74it/s] 29%|██▊       | 32/112 [00:12<00:27,  2.87it/s]  8%|▊         | 9/112 [00:03<00:39,  2.59it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.71it/s]I0402 09:21:12.304266 1560549 finetune.py:45] layer 3_gate initial loss 0.0013081764336675406
W0402 09:21:12.304539 1560549 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 29%|██▉       | 33/112 [00:12<00:27,  2.89it/s]  9%|▉         | 10/112 [00:04<00:38,  2.66it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.74it/s] 30%|███       | 34/112 [00:12<00:26,  2.90it/s] 10%|▉         | 11/112 [00:04<00:37,  2.70it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.75it/s]W0402 09:21:13.112148 1560549 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 35/112 [00:13<00:26,  2.92it/s] 11%|█         | 12/112 [00:04<00:36,  2.75it/s] 16%|█▌        | 18/112 [00:07<00:33,  2.77it/s] 32%|███▏      | 36/112 [00:13<00:25,  2.93it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.79it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.78it/s] 33%|███▎      | 37/112 [00:13<00:25,  2.92it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.76it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.77it/s] 34%|███▍      | 38/112 [00:14<00:25,  2.91it/s] 13%|█▎        | 15/112 [00:06<00:34,  2.79it/s] 19%|█▉        | 21/112 [00:08<00:32,  2.78it/s] 35%|███▍      | 39/112 [00:14<00:25,  2.89it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.76it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.75it/s] 36%|███▌      | 40/112 [00:14<00:24,  2.89it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.78it/s] 21%|██        | 23/112 [00:08<00:32,  2.76it/s] 37%|███▋      | 41/112 [00:15<00:24,  2.89it/s] 16%|█▌        | 18/112 [00:07<00:33,  2.78it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.75it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.88it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.78it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.75it/s] 38%|███▊      | 43/112 [00:15<00:23,  2.89it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.80it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.76it/s] 39%|███▉      | 44/112 [00:16<00:23,  2.90it/s]3_gate proxy err 0.006200061179697514 tr(WHW.T) 875.7509765625
  0%|          | 0/112 [00:00<?, ?it/s] 19%|█▉        | 21/112 [00:08<00:32,  2.82it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.77it/s] 40%|████      | 45/112 [00:16<00:23,  2.90it/s] 20%|█▉        | 22/112 [00:08<00:31,  2.83it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.77it/s] 41%|████      | 46/112 [00:16<00:22,  2.91it/s] 21%|██        | 23/112 [00:08<00:31,  2.83it/s] 26%|██▌       | 29/112 [00:11<00:29,  2.78it/s]  1%|          | 1/112 [00:00<01:32,  1.20it/s] 42%|████▏     | 47/112 [00:17<00:22,  2.89it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.84it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.78it/s]  2%|▏         | 2/112 [00:01<01:02,  1.77it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.90it/s] 22%|██▏       | 25/112 [00:09<00:30,  2.83it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.78it/s] 44%|████▍     | 49/112 [00:17<00:21,  2.90it/s]  3%|▎         | 3/112 [00:01<00:52,  2.08it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.84it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.75it/s] 45%|████▍     | 50/112 [00:18<00:21,  2.91it/s]  4%|▎         | 4/112 [00:01<00:47,  2.28it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.82it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.88it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.75it/s]  4%|▍         | 5/112 [00:02<00:44,  2.40it/s] 25%|██▌       | 28/112 [00:10<00:29,  2.82it/s] 46%|████▋     | 52/112 [00:18<00:20,  2.88it/s] 30%|███       | 34/112 [00:12<00:28,  2.74it/s]  5%|▌         | 6/112 [00:02<00:42,  2.48it/s] 26%|██▌       | 29/112 [00:11<00:29,  2.82it/s] 47%|████▋     | 53/112 [00:19<00:20,  2.88it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.76it/s]  6%|▋         | 7/112 [00:03<00:41,  2.54it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.82it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.88it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.76it/s]  7%|▋         | 8/112 [00:03<00:40,  2.58it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.83it/s] 49%|████▉     | 55/112 [00:19<00:19,  2.88it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.77it/s]  8%|▊         | 9/112 [00:03<00:39,  2.60it/s] 29%|██▊       | 32/112 [00:12<00:28,  2.84it/s] 50%|█████     | 56/112 [00:20<00:19,  2.90it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.79it/s]  9%|▉         | 10/112 [00:04<00:38,  2.63it/s] 29%|██▉       | 33/112 [00:12<00:27,  2.84it/s] 51%|█████     | 57/112 [00:20<00:18,  2.90it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.79it/s] 10%|▉         | 11/112 [00:04<00:38,  2.65it/s] 30%|███       | 34/112 [00:12<00:27,  2.85it/s] 52%|█████▏    | 58/112 [00:21<00:18,  2.89it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.76it/s] 11%|█         | 12/112 [00:04<00:37,  2.64it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.76it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.86it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.75it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.66it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.77it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.85it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.74it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.80it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.66it/s] 54%|█████▍    | 61/112 [00:22<00:17,  2.87it/s] 38%|███▊      | 43/112 [00:16<00:24,  2.76it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.83it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.89it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.66it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.75it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.79it/s] 56%|█████▋    | 63/112 [00:22<00:17,  2.88it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.65it/s] 40%|████      | 45/112 [00:16<00:24,  2.76it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.81it/s] 57%|█████▋    | 64/112 [00:23<00:16,  2.90it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.66it/s] 41%|████      | 46/112 [00:17<00:23,  2.77it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.82it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.90it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.66it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.76it/s] 59%|█████▉    | 66/112 [00:23<00:15,  2.88it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.81it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.65it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.77it/s] 60%|█████▉    | 67/112 [00:24<00:15,  2.89it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.83it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.66it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.78it/s] 61%|██████    | 68/112 [00:24<00:15,  2.90it/s] 39%|███▉      | 44/112 [00:16<00:23,  2.84it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.68it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.77it/s] 62%|██████▏   | 69/112 [00:24<00:14,  2.88it/s] 40%|████      | 45/112 [00:16<00:23,  2.82it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.64it/s] 46%|████▌     | 51/112 [00:18<00:22,  2.77it/s] 62%|██████▎   | 70/112 [00:25<00:14,  2.90it/s] 41%|████      | 46/112 [00:17<00:23,  2.83it/s] 21%|██        | 23/112 [00:09<00:33,  2.66it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.78it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.92it/s] 42%|████▏     | 47/112 [00:17<00:22,  2.84it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.66it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.78it/s] 64%|██████▍   | 72/112 [00:25<00:13,  2.91it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.84it/s] 48%|████▊     | 54/112 [00:20<00:20,  2.76it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.65it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.89it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.82it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.77it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.89it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.65it/s] 45%|████▍     | 50/112 [00:18<00:21,  2.83it/s] 67%|██████▋   | 75/112 [00:26<00:12,  2.89it/s] 50%|█████     | 56/112 [00:20<00:20,  2.78it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.66it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.84it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.90it/s] 51%|█████     | 57/112 [00:21<00:19,  2.80it/s] 25%|██▌       | 28/112 [00:10<00:31,  2.66it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.85it/s] 69%|██████▉   | 77/112 [00:27<00:12,  2.91it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.80it/s] 47%|████▋     | 53/112 [00:19<00:20,  2.85it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.67it/s] 70%|██████▉   | 78/112 [00:27<00:11,  2.86it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.75it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.80it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.67it/s] 71%|███████   | 79/112 [00:28<00:11,  2.85it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.74it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.81it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.67it/s] 71%|███████▏  | 80/112 [00:28<00:11,  2.87it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.75it/s] 50%|█████     | 56/112 [00:20<00:19,  2.82it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.67it/s] 72%|███████▏  | 81/112 [00:29<00:10,  2.88it/s] 55%|█████▌    | 62/112 [00:22<00:18,  2.77it/s] 51%|█████     | 57/112 [00:20<00:19,  2.84it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.68it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.90it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.78it/s] 52%|█████▏    | 58/112 [00:21<00:18,  2.85it/s] 30%|███       | 34/112 [00:13<00:29,  2.68it/s] 74%|███████▍  | 83/112 [00:29<00:10,  2.89it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.76it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.83it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.67it/s] 75%|███████▌  | 84/112 [00:30<00:09,  2.89it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.84it/s] 58%|█████▊    | 65/112 [00:24<00:16,  2.77it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.66it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.90it/s] 54%|█████▍    | 61/112 [00:22<00:17,  2.85it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.78it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.67it/s] 77%|███████▋  | 86/112 [00:30<00:08,  2.90it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.85it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.78it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.66it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.87it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.84it/s] 61%|██████    | 68/112 [00:25<00:15,  2.77it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.87it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.66it/s] 57%|█████▋    | 64/112 [00:23<00:16,  2.86it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.78it/s] 79%|███████▉  | 89/112 [00:31<00:07,  2.89it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.67it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.85it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.75it/s] 80%|████████  | 90/112 [00:32<00:07,  2.91it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.66it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.86it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.76it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.92it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.67it/s] 60%|█████▉    | 67/112 [00:24<00:15,  2.86it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.78it/s] 82%|████████▏ | 92/112 [00:32<00:06,  2.91it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.66it/s] 61%|██████    | 68/112 [00:24<00:15,  2.86it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.78it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.91it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.86it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.66it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.76it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.90it/s] 62%|██████▎   | 70/112 [00:25<00:14,  2.85it/s] 40%|████      | 45/112 [00:17<00:25,  2.66it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.77it/s] 85%|████████▍ | 95/112 [00:33<00:05,  2.90it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.86it/s] 41%|████      | 46/112 [00:17<00:24,  2.66it/s] 68%|██████▊   | 76/112 [00:28<00:12,  2.78it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.91it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.80it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.66it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.75it/s] 87%|████████▋ | 97/112 [00:34<00:05,  2.87it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.78it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.66it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.74it/s] 88%|████████▊ | 98/112 [00:34<00:04,  2.83it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.81it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.64it/s] 71%|███████   | 79/112 [00:29<00:11,  2.76it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.78it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.83it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.78it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.65it/s] 89%|████████▉ | 100/112 [00:35<00:04,  2.77it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.85it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.78it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.65it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.76it/s] 69%|██████▉   | 77/112 [00:27<00:12,  2.85it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.79it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.64it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.73it/s] 70%|██████▉   | 78/112 [00:28<00:11,  2.86it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.79it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.64it/s] 92%|█████████▏| 103/112 [00:36<00:03,  2.73it/s] 71%|███████   | 79/112 [00:28<00:11,  2.86it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.78it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.65it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.74it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.85it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.79it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.74it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.66it/s] 72%|███████▏  | 81/112 [00:29<00:10,  2.86it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.78it/s] 95%|█████████▍| 106/112 [00:37<00:02,  2.77it/s] 50%|█████     | 56/112 [00:21<00:20,  2.67it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.86it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.79it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.81it/s] 51%|█████     | 57/112 [00:21<00:20,  2.67it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.88it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.79it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.83it/s] 75%|███████▌  | 84/112 [00:30<00:09,  2.88it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.68it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.79it/s] 97%|█████████▋| 109/112 [00:38<00:01,  2.84it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.88it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.67it/s] 80%|████████  | 90/112 [00:33<00:07,  2.79it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.85it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.88it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.66it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.80it/s] 99%|█████████▉| 111/112 [00:39<00:00,  2.87it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.88it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.67it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.80it/s]100%|██████████| 112/112 [00:39<00:00,  2.87it/s]100%|██████████| 112/112 [00:39<00:00,  2.81it/s]
 79%|███████▊  | 88/112 [00:31<00:08,  2.87it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.67it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.80it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.86it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.66it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.73it/s] 80%|████████  | 90/112 [00:32<00:07,  2.80it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.66it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.69it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.76it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.66it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.68it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.76it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.63it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.70it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.78it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.65it/s] 88%|████████▊ | 98/112 [00:35<00:05,  2.69it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.77it/s] 61%|██████    | 68/112 [00:26<00:16,  2.66it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.73it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.80it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.67it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.75it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.82it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.67it/s] 90%|█████████ | 101/112 [00:37<00:03,  2.77it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.84it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.67it/s] 88%|████████▊ | 98/112 [00:35<00:04,  2.85it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.79it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.67it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.85it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.79it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.67it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.83it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.77it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.84it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.67it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.78it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.85it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.79it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.67it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.86it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.80it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.67it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.87it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.80it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.68it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.87it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.80it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.68it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.88it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.80it/s] 71%|███████   | 79/112 [00:30<00:12,  2.68it/s]W0402 09:21:46.779000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.780000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.780000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.780000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.780000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.780000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.780000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 96%|█████████▌| 107/112 [00:38<00:01,  2.89it/s]W0402 09:21:46.819000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.819000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.819000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.819000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.819000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.834000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.834000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.834000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.834000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.834000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 99%|█████████▉| 111/112 [00:40<00:00,  2.80it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.68it/s]W0402 09:21:46.994000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.994000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.994000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.994000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:46.994000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 96%|█████████▋| 108/112 [00:38<00:01,  2.90it/s]100%|██████████| 112/112 [00:40<00:00,  2.80it/s]100%|██████████| 112/112 [00:40<00:00,  2.73it/s]
W0402 09:21:47.302000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:47.303000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:47.303000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:47.303000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:21:47.303000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:47.303000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:47.303000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:47.337000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:47.337000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:47.337000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:47.337000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:47.337000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 81/112 [00:30<00:11,  2.68it/s]W0402 09:21:47.406000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:47.406000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:47.406000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:47.406000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:47.406000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 109/112 [00:39<00:01,  2.88it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.68it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.82it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.68it/s] 99%|█████████▉| 111/112 [00:39<00:00,  2.78it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.68it/s]W0402 09:21:48.592000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:48.597000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
100%|██████████| 112/112 [00:40<00:00,  2.77it/s]W0402 09:21:48.603000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:48.603000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
100%|██████████| 112/112 [00:40<00:00,  2.78it/s]
 76%|███████▌  | 85/112 [00:32<00:10,  2.68it/s]W0402 09:21:49.035000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.035000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.036000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.036000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.036000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.036000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.036000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.067000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.067000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.067000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.067000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.067000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 77%|███████▋  | 86/112 [00:32<00:09,  2.69it/s]W0402 09:21:49.424000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.424000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.424000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.424000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.425000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.425000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.425000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.425000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 87/112 [00:33<00:09,  2.68it/s]W0402 09:21:49.727000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.727000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.727000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.727000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:49.727000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 79%|███████▊  | 88/112 [00:33<00:08,  2.69it/s]W0402 09:21:50.074000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:50.079000 139707187226432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 79%|███████▉  | 89/112 [00:33<00:08,  2.68it/s] 80%|████████  | 90/112 [00:34<00:08,  2.70it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.71it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.71it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.71it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.72it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.72it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.73it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.72it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.71it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.71it/s]W0402 09:21:54.316000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.317000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.317000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.317000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.317000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.317000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.317000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.363000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.363000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.363000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.363000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.363000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 89%|████████▉ | 100/112 [00:37<00:04,  2.71it/s]W0402 09:21:54.379000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.379000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.380000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.380000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.380000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.555000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.555000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.556000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.556000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.556000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 90%|█████████ | 101/112 [00:38<00:04,  2.71it/s]W0402 09:21:54.893000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.893000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.894000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.894000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.894000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.894000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.894000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.926000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.926000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.926000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.927000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:54.927000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:55.003000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:55.003000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:55.003000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:55.003000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:55.003000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 102/112 [00:38<00:03,  2.68it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.70it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.70it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.70it/s]W0402 09:21:56.243000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.257000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.265000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.265000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 95%|█████████▍| 106/112 [00:40<00:02,  2.71it/s]W0402 09:21:56.610000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.611000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.611000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.611000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.611000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.611000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.611000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.654000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.654000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.654000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.654000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.654000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.670000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.670000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.670000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.670000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.671000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.738000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.738000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.739000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.739000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.739000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.739000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.739000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.772000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.772000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.772000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.772000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.772000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.846000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.846000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.846000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.847000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:56.847000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
I0402 09:21:56.957864 1559240 finetune.py:45] layer 0_down initial loss 0.00012887906632386148
W0402 09:21:56.958033 1559240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 96%|█████████▌| 107/112 [00:40<00:01,  2.70it/s]W0402 09:21:57.122000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.122000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.123000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.123000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.123000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.123000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.123000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.123000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.171000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.171000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.171000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.172000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.172000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.172000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.172000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.204000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.204000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.204000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.204000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.204000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.273000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.273000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.273000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.273000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.273000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 96%|█████████▋| 108/112 [00:40<00:01,  2.69it/s]W0402 09:21:57.416334 1559240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0402 09:21:57.419000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.419000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.419000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.419000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.419000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 109/112 [00:41<00:01,  2.69it/s]W0402 09:21:57.738000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:57.743000 140520797087552 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
0_down proxy err 0.006479789037257433 tr(WHW.T) 0.4814107120037079
 98%|█████████▊| 110/112 [00:41<00:00,  2.65it/s]W0402 09:21:58.422000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:58.436000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:58.444000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:58.444000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 99%|█████████▉| 111/112 [00:41<00:00,  2.66it/s]100%|██████████| 112/112 [00:42<00:00,  2.66it/s]100%|██████████| 112/112 [00:42<00:00,  2.64it/s]
W0402 09:21:58.868000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:21:58.868000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:58.868000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:58.868000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:58.868000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:58.868000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:58.868000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:58.898000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:58.898000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:58.898000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:58.899000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:58.899000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:59.238000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:21:59.238000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:59.238000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:59.238000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:21:59.238000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:59.238000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:59.239000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:59.239000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:59.536000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:21:59.536000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:21:59.536000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:59.536000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:21:59.536000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:21:59.880000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:21:59.887000 139804799858496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0402 09:22:04.623579 1559498 finetune.py:45] layer 1_down initial loss 0.00367918168194592
W0402 09:22:04.624102 1559498 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:22:05.185628 1559498 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

1_down proxy err 0.00019128478015772998 tr(WHW.T) 67.10469055175781
W0402 09:22:06.010000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.011000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.011000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.011000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.011000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.011000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.011000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.052000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.052000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.052000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.052000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.052000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.067000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.067000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.067000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.067000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.068000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.237000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.237000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.237000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.237000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.237000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.550000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.551000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.551000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.551000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.551000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.551000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.551000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.583000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.583000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.583000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.583000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.583000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.653000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.653000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.653000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.653000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:06.653000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
I0402 09:22:06.892703 1560036 finetune.py:45] layer 2_down initial loss 0.0006956632132641971
W0402 09:22:06.893326 1560036 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:22:07.707433 1560036 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0402 09:22:07.796000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:07.810000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:07.818000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:07.818000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.254000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.254000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.254000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.254000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.254000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.255000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.255000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.285000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.285000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.286000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.286000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.286000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
2_down proxy err 0.010919871740043163 tr(WHW.T) 1.2005820274353027
W0402 09:22:08.637000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.637000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.637000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.637000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.638000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.638000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.638000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.638000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.926000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.926000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.926000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.927000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:08.927000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
I0402 09:22:08.965703 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 4 in 1.184800624847412s
W0402 09:22:09.273000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:09.278000 139806182680384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0402 09:22:09.438215 1558010 quantize_finetune_llama.py:159] layer 5 gpu 1
I0402 09:22:11.409679 1564634 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:22:11.409837 1564634 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:22:11.409903 1564634 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:22:11.683352 1564634 config.py:58] PyTorch version 2.4.0 available.
I0402 09:22:12.170390 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 5 in 1.1710007190704346s
I0402 09:22:12.595965 1558010 quantize_finetune_llama.py:159] layer 6 gpu 2
I0402 09:22:13.945250 1564634 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 09:22:14.317129 1564634 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:22:14.659547 1564883 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:22:14.659700 1564883 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:22:14.659763 1564883 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:22:14.871845 1564883 config.py:58] PyTorch version 2.4.0 available.
  0%|          | 0/32 [00:00<?, ?it/s]I0402 09:22:16.519955 1560549 finetune.py:45] layer 3_down initial loss 0.0012908828211948276
W0402 09:22:16.520220 1560549 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:41,  1.35s/it]  6%|▋         | 2/32 [00:01<00:22,  1.35it/s]W0402 09:22:16.999813 1560549 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

  9%|▉         | 3/32 [00:01<00:15,  1.83it/s]I0402 09:22:17.290723 1564883 data_utils.py:336] using 256 training seqs, 128 validation seqs
3_down proxy err 0.012127477675676346 tr(WHW.T) 2.1166553497314453
 12%|█▎        | 4/32 [00:02<00:12,  2.20it/s]W0402 09:22:17.643422 1564883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:02<00:10,  2.47it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.68it/s] 22%|██▏       | 7/32 [00:03<00:08,  2.85it/s]  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.98it/s] 28%|██▊       | 9/32 [00:03<00:07,  3.06it/s] 31%|███▏      | 10/32 [00:04<00:07,  3.10it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.11it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.17it/s] 41%|████      | 13/32 [00:05<00:05,  3.18it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.22it/s]  3%|▎         | 1/32 [00:01<00:58,  1.90s/it]I0402 09:22:20.762986 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 6 in 1.1729638576507568s
 47%|████▋     | 15/32 [00:05<00:05,  3.13it/s]  6%|▋         | 2/32 [00:02<00:29,  1.01it/s]I0402 09:22:21.277955 1558010 quantize_finetune_llama.py:159] layer 7 gpu 3
 50%|█████     | 16/32 [00:06<00:05,  3.10it/s]  9%|▉         | 3/32 [00:02<00:20,  1.41it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.08it/s] 12%|█▎        | 4/32 [00:02<00:16,  1.75it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.09it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.00it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.07it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.23it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.13it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.44it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.18it/s]I0402 09:22:22.879317 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 7 in 1.1835405826568604s
 25%|██▌       | 8/32 [00:04<00:09,  2.45it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.14it/s]I0402 09:22:23.311386 1565763 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:22:23.311671 1565763 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:22:23.312078 1565763 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:22:23.398367 1558010 quantize_finetune_llama.py:159] layer 8 gpu 0
I0402 09:22:23.525513 1565763 config.py:58] PyTorch version 2.4.0 available.
 72%|███████▏  | 23/32 [00:08<00:02,  3.12it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.56it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.13it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.67it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.14it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.74it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.14it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.79it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.19it/s] 41%|████      | 13/32 [00:06<00:06,  2.87it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.22it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.94it/s]I0402 09:22:25.379384 1565925 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:22:25.379504 1565925 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:22:25.379567 1565925 utils.py:162] NumExpr defaulting to 16 threads.
 91%|█████████ | 29/32 [00:10<00:00,  3.18it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.95it/s]I0402 09:22:25.568328 1565925 config.py:58] PyTorch version 2.4.0 available.
 94%|█████████▍| 30/32 [00:10<00:00,  3.17it/s] 50%|█████     | 16/32 [00:07<00:05,  2.94it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.18it/s]I0402 09:22:26.093539 1565763 data_utils.py:336] using 256 training seqs, 128 validation seqs
 53%|█████▎    | 17/32 [00:07<00:05,  2.96it/s]100%|██████████| 32/32 [00:11<00:00,  3.17it/s]100%|██████████| 32/32 [00:11<00:00,  2.88it/s]
W0402 09:22:26.468301 1565763 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 56%|█████▋    | 18/32 [00:07<00:04,  2.94it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.92it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.91it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.89it/s]  0%|          | 0/32 [00:00<?, ?it/s]I0402 09:22:27.759392 1565925 data_utils.py:336] using 256 training seqs, 128 validation seqs
 69%|██████▉   | 22/32 [00:09<00:03,  2.87it/s]W0402 09:22:28.111399 1565925 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:09<00:03,  2.85it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.83it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.84it/s]  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:22:29.319000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.320000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.320000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.320000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.320000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.320000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.320000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:52,  1.70s/it]W0402 09:22:29.345000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.345000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.345000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.345000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.345000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.361000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.362000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.362000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.362000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.362000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:10<00:02,  2.86it/s]  6%|▋         | 2/32 [00:02<00:26,  1.12it/s]W0402 09:22:29.678000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.678000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.678000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.678000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:29.678000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s]  9%|▉         | 3/32 [00:02<00:18,  1.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.86it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.95it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.89it/s]W0402 09:22:30.539000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:30.539000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:30.540000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:30.540000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:30.540000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:30.540000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:30.540000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:22:30.557000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:30.557000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:30.557000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:30.557000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:30.557000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:48,  1.55s/it] 16%|█▌        | 5/32 [00:03<00:12,  2.24it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.90it/s]W0402 09:22:30.796000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:30.796000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:30.796000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:30.796000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:30.797000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:24,  1.20it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.45it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.91it/s]  9%|▉         | 3/32 [00:02<00:17,  1.67it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s]100%|██████████| 32/32 [00:12<00:00,  2.94it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
 12%|█▎        | 4/32 [00:02<00:13,  2.05it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.65it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s]W0402 09:22:31.946000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:31.946000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:31.947000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:22:31.947000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:31.947000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:31.947000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:31.947000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:31.965000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:31.965000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:31.965000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:31.965000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:31.965000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:04<00:08,  2.72it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.53it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.81it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.65it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.83it/s]W0402 09:22:32.870000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:32.871000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:32.871000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:32.871000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:32.871000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:08,  2.77it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.91it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.86it/s] 41%|████      | 13/32 [00:05<00:06,  2.95it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 31%|███▏      | 10/32 [00:04<00:07,  2.95it/s] 44%|████▍     | 14/32 [00:06<00:06,  3.00it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.01it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.03it/s] 38%|███▊      | 12/32 [00:05<00:06,  3.02it/s] 50%|█████     | 16/32 [00:06<00:05,  3.00it/s]W0402 09:22:34.500000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.500000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.501000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.501000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.501000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.501000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.501000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  3.00it/s]W0402 09:22:34.527000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.527000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.527000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.527000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.527000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.544000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.544000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.544000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.544000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.544000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:07<00:05,  2.96it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.97it/s]W0402 09:22:34.861000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.861000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.861000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.861000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:34.861000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:07<00:04,  2.94it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.96it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.93it/s] 50%|█████     | 16/32 [00:06<00:05,  2.95it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.92it/s]W0402 09:22:35.742000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:35.743000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:35.743000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:35.743000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:35.743000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:22:35.743000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:35.743000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:35.760000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:35.761000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:35.761000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:35.761000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:35.761000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:06<00:05,  2.93it/s]W0402 09:22:35.998000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:35.999000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:35.999000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:35.999000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:35.999000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:03,  2.91it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.93it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.90it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.92it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.90it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.92it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.91it/s]W0402 09:22:37.161000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:37.162000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:22:37.162000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:37.162000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:37.162000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:37.162000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:37.162000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:37.180000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:37.180000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:37.180000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:37.180000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:37.180000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:03,  2.91it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.88it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.92it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.86it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.92it/s]W0402 09:22:38.062000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:38.062000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:38.062000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:38.062000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:38.062000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  2.89it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.92it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.91it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.92it/s]I0402 09:22:38.688315 1564634 finetune.py:45] layer 4_v initial loss 0.0015335568459704518
W0402 09:22:38.688756 1564634 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 91%|█████████ | 29/32 [00:11<00:01,  2.92it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.97it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.90it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.98it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.90it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.02it/s]W0402 09:22:39.734386 1564634 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:12<00:00,  2.91it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
 91%|█████████ | 29/32 [00:10<00:01,  2.99it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.96it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.95it/s]4_v proxy err 0.008432278409600258 tr(WHW.T) 38.379119873046875
  0%|          | 0/32 [00:00<?, ?it/s]100%|██████████| 32/32 [00:11<00:00,  2.94it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
  3%|▎         | 1/32 [00:00<00:29,  1.05it/s]  6%|▋         | 2/32 [00:01<00:18,  1.65it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s]W0402 09:22:43.523000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.523000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.523000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.523000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.523000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.524000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.524000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s]W0402 09:22:43.551000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.551000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.551000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.551000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.551000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.567000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.567000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.568000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.568000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.568000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.60it/s]W0402 09:22:43.891000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.891000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.891000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.891000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:43.891000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.027000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.027000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.027000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.027000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.027000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.027000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.027000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.053000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.053000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.053000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.053000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.053000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.070000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.070000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.070000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.070000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.070000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s]W0402 09:22:44.389000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.389000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.389000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.389000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.389000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:03<00:08,  2.64it/s]W0402 09:22:44.775000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.776000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.776000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.776000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.776000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.776000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.776000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.793000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.793000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.794000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.794000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:44.794000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s]W0402 09:22:45.037000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.037000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.037000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.037000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.037000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.289000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.290000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.290000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.290000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.290000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.290000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.290000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
I0402 09:22:45.305487 1564883 finetune.py:45] layer 5_v initial loss 0.001163256587460637
W0402 09:22:45.307425 1564883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:22:45.307000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.307000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.308000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.308000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.308000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.70it/s]W0402 09:22:45.542000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.542000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.542000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.542000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:45.542000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:04<00:07,  2.71it/s] 41%|████      | 13/32 [00:05<00:06,  2.72it/s]W0402 09:22:46.203000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.204000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.204000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.204000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.204000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.204000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.204000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.221000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.221000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.222000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.222000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.222000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s]W0402 09:22:46.683000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.683000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.683000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.683000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.683000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.684000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.684000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.700000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.700000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.701000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.701000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:46.701000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 15/32 [00:06<00:06,  2.76it/s]W0402 09:22:46.888129 1564883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:22:47.121000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:47.121000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:47.121000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:47.121000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:47.121000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:05,  2.74it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.77it/s]W0402 09:22:47.589000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:47.590000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:47.590000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:47.590000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:47.590000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 56%|█████▋    | 18/32 [00:07<00:05,  2.78it/s]5_v proxy err 0.007989304140210152 tr(WHW.T) 37.700050354003906
  0%|          | 0/32 [00:00<?, ?it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.77it/s]  3%|▎         | 1/32 [00:00<00:30,  1.02it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.77it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s]  6%|▋         | 2/32 [00:01<00:19,  1.57it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.82it/s]  9%|▉         | 3/32 [00:01<00:15,  1.90it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.84it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.10it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.82it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.23it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.83it/s] 19%|█▉        | 6/32 [00:02<00:11,  2.33it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.85it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.40it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.83it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.45it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.85it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.47it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.86it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.85it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.48it/s]100%|██████████| 32/32 [00:12<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
 34%|███▍      | 11/32 [00:04<00:08,  2.50it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.50it/s]I0402 09:22:53.600909 1565925 finetune.py:45] layer 7_v initial loss 0.0020092418417334557
W0402 09:22:53.601081 1565925 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:05<00:07,  2.53it/s]I0402 09:22:53.915715 1565763 finetune.py:45] layer 6_v initial loss 0.0019361627055332065
W0402 09:22:53.916154 1565763 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 44%|████▍     | 14/32 [00:06<00:07,  2.57it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s]W0402 09:22:54.668554 1565925 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 50%|█████     | 16/32 [00:06<00:06,  2.59it/s]W0402 09:22:54.940447 1565763 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 17/32 [00:07<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.58it/s]7_v proxy err 0.006995062809437513 tr(WHW.T) 53.14668655395508
  0%|          | 0/32 [00:00<?, ?it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.55it/s]6_v proxy err 0.008387389592826366 tr(WHW.T) 42.837894439697266
  0%|          | 0/32 [00:00<?, ?it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.57it/s]  3%|▎         | 1/32 [00:00<00:28,  1.10it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.58it/s]  6%|▋         | 2/32 [00:01<00:17,  1.73it/s]  3%|▎         | 1/32 [00:00<00:28,  1.07it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.60it/s]  9%|▉         | 3/32 [00:01<00:13,  2.12it/s]  6%|▋         | 2/32 [00:01<00:18,  1.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.35it/s]  9%|▉         | 3/32 [00:01<00:14,  1.99it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.52it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.20it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.63it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.60it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.71it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s]W0402 09:22:58.928000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:58.929000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:58.929000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:58.929000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:58.929000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:58.929000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:58.929000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:22:58.957000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:58.957000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:58.957000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:58.957000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:58.957000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:58.972000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:58.972000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:58.972000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:58.972000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:58.972000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.59it/s]W0402 09:22:59.122000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.122000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.123000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.123000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.123000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s]W0402 09:22:59.345000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.345000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.345000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.345000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.346000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.346000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.346000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.366000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.366000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.366000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.366000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.366000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:03<00:08,  2.78it/s]W0402 09:22:59.428000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.428000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.429000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.429000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:22:59.429000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:11<00:01,  2.59it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.80it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.54it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.56it/s]W0402 09:23:00.251000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:08,  2.58it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s]W0402 09:23:00.552000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:00.552000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:00.552000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:23:00.552000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:00.552000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:00.552000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:00.553000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:00.574000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:00.574000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:00.574000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:00.574000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:00.575000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:12<00:00,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.60it/s] 41%|████      | 13/32 [00:05<00:06,  2.83it/s]W0402 09:23:00.821000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:00.821000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:00.821000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:00.821000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:00.821000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:13<00:00,  2.57it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
W0402 09:23:01.067000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.85it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.83it/s] 41%|████      | 13/32 [00:05<00:07,  2.63it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.81it/s] 50%|█████     | 16/32 [00:06<00:05,  2.67it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.81it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.67it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.69it/s] 66%|██████▌   | 21/32 [00:07<00:04,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.72it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.70it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.68it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.69it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.66it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.69it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.67it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.69it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.68it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.67it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.68it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.67it/s]I0402 09:23:07.304081 1564634 finetune.py:45] layer 4_q initial loss 0.001533233909867704
W0402 09:23:07.304667 1564634 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 31/32 [00:11<00:00,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s]W0402 09:23:07.596000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.597000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.597000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.597000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.597000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.597000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.597000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.627000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.627000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.627000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.627000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.627000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.642000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.642000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.643000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.643000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.643000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
W0402 09:23:07.796000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.796000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.797000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.797000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:07.797000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.66it/s]W0402 09:23:08.015000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:08.015000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:08.016000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:08.016000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:08.016000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:08.016000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:08.016000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:23:08.037000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:08.037000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:08.037000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:08.038000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:08.038000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:08.104000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:08.104000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:08.104000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:08.105000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:08.105000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]W0402 09:23:08.316770 1564634 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
W0402 09:23:08.976000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:09.278000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:09.278000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:23:09.278000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:09.278000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:09.278000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:09.278000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:09.279000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:09.298000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:09.298000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:09.299000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:09.299000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:09.299000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
4_q proxy err 0.0008326921961270273 tr(WHW.T) 6741.380859375
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:23:09.545000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:09.546000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:09.546000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:09.546000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:09.546000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:09.798000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:27,  1.15it/s]  6%|▋         | 2/32 [00:01<00:16,  1.81it/s]  9%|▉         | 3/32 [00:01<00:13,  2.22it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.47it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.65it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.74it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.77it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.78it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.80it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.80it/s]W0402 09:23:13.634000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.634000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.634000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.634000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.634000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.634000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.634000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.664000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.664000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.664000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.664000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.664000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.680000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.680000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.680000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.680000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.680000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.81it/s]W0402 09:23:13.843000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.843000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.843000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.843000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:13.843000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.078000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.078000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.078000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.078000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.078000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.078000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.078000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:04<00:07,  2.80it/s]W0402 09:23:14.098000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.098000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.099000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.099000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.099000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.166000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.166000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.166000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.166000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.166000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.80it/s]W0402 09:23:14.633000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.633000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.634000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.634000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.634000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.634000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.634000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.662000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.663000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.663000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.663000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.663000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.678000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.678000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.678000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.678000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.678000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.81it/s]W0402 09:23:14.837000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.837000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.837000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.837000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:14.837000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.054000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.054000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.054000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.054000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.055000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.055000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.055000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.068000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.076000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.076000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.076000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.077000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.077000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.141000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.141000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.141000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.141000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.141000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 15/32 [00:05<00:06,  2.80it/s]W0402 09:23:15.390000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.390000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.390000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.390000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.390000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.390000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.390000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.411000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.411000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.411000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.412000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.412000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:05,  2.80it/s]W0402 09:23:15.676000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.676000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.676000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.676000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.676000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:06<00:05,  2.81it/s]W0402 09:23:15.948000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:15.975000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:06<00:04,  2.81it/s]W0402 09:23:16.281000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:16.281000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:16.281000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:16.281000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:16.282000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:23:16.282000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:16.282000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:23:16.302000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:16.302000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:16.302000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:16.302000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:16.302000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:23:16.551000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:23:16.551000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:23:16.551000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:23:16.551000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:23:16.551000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 19/32 [00:07<00:04,  2.81it/s]W0402 09:23:16.800000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:07<00:04,  2.82it/s]I0402 09:23:17.250000 1564883 finetune.py:45] layer 5_q initial loss 0.001164934947155416
W0402 09:23:17.250436 1564883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 66%|██████▌   | 21/32 [00:07<00:03,  2.82it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.88it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.87it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.86it/s]W0402 09:23:18.438693 1564883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.83it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.82it/s]5_q proxy err 0.00115874782204628 tr(WHW.T) 6497.876953125
  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.82it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.81it/s]  3%|▎         | 1/32 [00:00<00:27,  1.12it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s]  6%|▋         | 2/32 [00:01<00:17,  1.71it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.81it/s]  9%|▉         | 3/32 [00:01<00:13,  2.07it/s]100%|██████████| 32/32 [00:11<00:00,  2.81it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
 12%|█▎        | 4/32 [00:01<00:12,  2.29it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s]I0402 09:23:22.065528 1565925 finetune.py:45] layer 7_q initial loss 0.002010750351473689
W0402 09:23:22.065714 1565925 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s]W0402 09:23:23.106967 1565925 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:03<00:08,  2.67it/s]I0402 09:23:23.567885 1565763 finetune.py:45] layer 6_q initial loss 0.0019355252152308822
W0402 09:23:23.568215 1565763 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.71it/s]7_q proxy err 0.0012621376663446426 tr(WHW.T) 6036.439453125
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.71it/s]W0402 09:23:24.590412 1565763 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 13/32 [00:05<00:07,  2.69it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.70it/s]  3%|▎         | 1/32 [00:00<00:29,  1.07it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.71it/s]6_q proxy err 0.0013073774753138423 tr(WHW.T) 6019.40673828125
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:01<00:18,  1.66it/s] 50%|█████     | 16/32 [00:06<00:05,  2.70it/s]  9%|▉         | 3/32 [00:01<00:14,  2.01it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.70it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.24it/s]  3%|▎         | 1/32 [00:00<00:27,  1.14it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.73it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s]  6%|▋         | 2/32 [00:01<00:17,  1.74it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.49it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.30it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.74it/s]I0402 09:23:27.780242 1564634 finetune.py:45] layer 4_k initial loss 0.0015332443872466683
W0402 09:23:27.780419 1564634 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.72it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.68it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.71it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.75it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s]W0402 09:23:28.739211 1564634 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.76it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.71it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.80it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.71it/s] 41%|████      | 13/32 [00:05<00:06,  2.83it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s]4_k proxy err 0.0008279532194137573 tr(WHW.T) 3941.66064453125
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.85it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.87it/s]  3%|▎         | 1/32 [00:00<00:22,  1.38it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.66it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.70it/s] 50%|█████     | 16/32 [00:06<00:05,  2.89it/s]  6%|▋         | 2/32 [00:01<00:14,  2.02it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.91it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s]  9%|▉         | 3/32 [00:01<00:12,  2.37it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.67it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.91it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.70it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.56it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.90it/s]100%|██████████| 32/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
 16%|█▌        | 5/32 [00:02<00:10,  2.68it/s] 50%|█████     | 16/32 [00:06<00:05,  2.67it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.91it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.73it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.85it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.75it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.84it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.69it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.80it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.84it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.79it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.84it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.69it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.84it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.87it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.70it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.88it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.86it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.83it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.83it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.72it/s] 41%|████      | 13/32 [00:04<00:06,  2.81it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.80it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.72it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.79it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.72it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.77it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.76it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.73it/s] 50%|█████     | 16/32 [00:05<00:05,  2.75it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.75it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
 88%|████████▊ | 28/32 [00:10<00:01,  2.73it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.76it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.74it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.75it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.75it/s] 66%|██████▌   | 21/32 [00:07<00:04,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.74it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.73it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.72it/s]I0402 09:23:39.516752 1564883 finetune.py:45] layer 5_k initial loss 0.0011654109694063663
W0402 09:23:39.516966 1564883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 84%|████████▍ | 27/32 [00:10<00:01,  2.72it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.71it/s]W0402 09:23:40.479710 1564883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:10<00:01,  2.71it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.71it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.71it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]100%|██████████| 32/32 [00:11<00:00,  2.70it/s]
5_k proxy err 0.0010760362492874265 tr(WHW.T) 4151.982421875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:23,  1.29it/s]I0402 09:23:42.753224 1565925 finetune.py:45] layer 7_k initial loss 0.002010282827541232
W0402 09:23:42.753434 1565925 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:01<00:16,  1.86it/s]  9%|▉         | 3/32 [00:01<00:13,  2.16it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.32it/s]W0402 09:23:43.925830 1565925 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s]I0402 09:23:44.815791 1565763 finetune.py:45] layer 6_k initial loss 0.0019336857367306948
W0402 09:23:44.816078 1565763 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s]7_k proxy err 0.0010415544966235757 tr(WHW.T) 4611.79638671875
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s]W0402 09:23:45.799067 1565763 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s]  3%|▎         | 1/32 [00:00<00:24,  1.27it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.62it/s]  6%|▋         | 2/32 [00:01<00:16,  1.82it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.64it/s]  9%|▉         | 3/32 [00:01<00:13,  2.09it/s]6_k proxy err 0.0010113363387063146 tr(WHW.T) 4418.953125
  0%|          | 0/32 [00:00<?, ?it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.29it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.62it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s]  3%|▎         | 1/32 [00:00<00:25,  1.21it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.63it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s]  6%|▋         | 2/32 [00:01<00:16,  1.79it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s]  9%|▉         | 3/32 [00:01<00:13,  2.11it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.67it/s]I0402 09:23:48.704001 1564634 finetune.py:45] layer 4_o initial loss 0.001531736459583044
W0402 09:23:48.704184 1564634 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.30it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.65it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.65it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.47it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.64it/s]W0402 09:23:49.650028 1564634 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:04<00:07,  2.70it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.72it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.56it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.64it/s] 41%|████      | 13/32 [00:05<00:06,  2.77it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s]4_o proxy err 0.007002510596066713 tr(WHW.T) 1.3690094947814941
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.79it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.58it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.63it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.80it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.55it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.60it/s] 50%|█████     | 16/32 [00:06<00:05,  2.81it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.80it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.79it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.82it/s]  3%|▎         | 1/32 [00:01<00:56,  1.82s/it] 44%|████▍     | 14/32 [00:05<00:06,  2.61it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.63it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.82it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.84it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]  6%|▋         | 2/32 [00:03<00:46,  1.56s/it] 72%|███████▏  | 23/32 [00:08<00:03,  2.84it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
 75%|███████▌  | 24/32 [00:09<00:02,  2.84it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.56it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.82it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.51it/s]  9%|▉         | 3/32 [00:04<00:42,  1.48s/it] 84%|████████▍ | 27/32 [00:10<00:01,  2.82it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.51it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.54it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.79it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.57it/s] 12%|█▎        | 4/32 [00:05<00:40,  1.43s/it] 97%|█████████▋| 31/32 [00:11<00:00,  2.82it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.59it/s]100%|██████████| 32/32 [00:11<00:00,  2.83it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
 81%|████████▏ | 26/32 [00:10<00:02,  2.58it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.56it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.55it/s] 16%|█▌        | 5/32 [00:07<00:38,  1.42s/it] 91%|█████████ | 29/32 [00:11<00:01,  2.56it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.55it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.54it/s] 19%|█▉        | 6/32 [00:08<00:36,  1.41s/it]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
 22%|██▏       | 7/32 [00:10<00:35,  1.43s/it]I0402 09:24:01.542817 1564883 finetune.py:45] layer 5_o initial loss 0.0011439323425292969
W0402 09:24:01.543079 1564883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it]W0402 09:24:02.538264 1564883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:24:03.566083 1565925 finetune.py:45] layer 7_o initial loss 0.001998117659240961
W0402 09:24:03.566359 1565925 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

5_o proxy err 0.006745117716491222 tr(WHW.T) 1.8119544982910156
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:13<00:33,  1.43s/it]W0402 09:24:04.575841 1565925 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it]  3%|▎         | 1/32 [00:02<01:03,  2.06s/it]7_o proxy err 0.008301188237965107 tr(WHW.T) 3.787720203399658
  0%|          | 0/32 [00:00<?, ?it/s]I0402 09:24:06.566261 1565763 finetune.py:45] layer 6_o initial loss 0.0018993563717231154
W0402 09:24:06.566595 1565763 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:15<00:29,  1.40s/it]  6%|▋         | 2/32 [00:03<00:52,  1.75s/it]W0402 09:24:07.538189 1565763 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:58,  1.88s/it] 38%|███▊      | 12/32 [00:17<00:27,  1.38s/it]6_o proxy err 0.008832059800624847 tr(WHW.T) 2.5981621742248535
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it]  6%|▋         | 2/32 [00:03<00:48,  1.61s/it] 41%|████      | 13/32 [00:18<00:26,  1.37s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it]  9%|▉         | 3/32 [00:04<00:43,  1.51s/it] 44%|████▍     | 14/32 [00:19<00:24,  1.37s/it]  3%|▎         | 1/32 [00:01<01:00,  1.96s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.37s/it]  6%|▋         | 2/32 [00:03<00:50,  1.69s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.44s/it] 50%|█████     | 16/32 [00:22<00:21,  1.36s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.43s/it] 53%|█████▎    | 17/32 [00:23<00:20,  1.37s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.42s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.37s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it] 59%|█████▉    | 19/32 [00:26<00:17,  1.36s/it] 25%|██▌       | 8/32 [00:11<00:33,  1.41s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.36s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.40s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.53s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 66%|██████▌   | 21/32 [00:29<00:14,  1.36s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.40s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it] 69%|██████▉   | 22/32 [00:30<00:13,  1.36s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.40s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.36s/it] 38%|███▊      | 12/32 [00:17<00:27,  1.40s/it] 41%|████      | 13/32 [00:20<00:28,  1.53s/it] 75%|███████▌  | 24/32 [00:33<00:10,  1.36s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 41%|████      | 13/32 [00:18<00:26,  1.40s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 78%|███████▊  | 25/32 [00:34<00:09,  1.36s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.40s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.36s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.53s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.40s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.52s/it] 84%|████████▍ | 27/32 [00:37<00:06,  1.37s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it] 50%|█████     | 16/32 [00:22<00:22,  1.40s/it] 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 88%|████████▊ | 28/32 [00:38<00:05,  1.36s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.40s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.36s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.40s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.52s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.50s/it] 94%|█████████▍| 30/32 [00:41<00:02,  1.37s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.39s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 50%|█████     | 16/32 [00:24<00:23,  1.50s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.37s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.39s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.49s/it]100%|██████████| 32/32 [00:44<00:00,  1.37s/it]100%|██████████| 32/32 [00:44<00:00,  1.39s/it]
 66%|██████▌   | 21/32 [00:29<00:15,  1.40s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.49s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.42s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.50s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.45s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.50s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.49s/it] 78%|███████▊  | 25/32 [00:35<00:10,  1.46s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.50s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it]I0402 09:24:42.860591 1564634 finetune.py:45] layer 4_up initial loss 0.0015125992940738797
W0402 09:24:42.860862 1564634 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:37<00:08,  1.46s/it]W0402 09:24:43.682132 1564634 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 81%|████████▏ | 26/32 [00:39<00:09,  1.50s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.45s/it]4_up proxy err 0.011066081933677197 tr(WHW.T) 400.241455078125
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.43s/it]  3%|▎         | 1/32 [00:01<00:55,  1.78s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.42s/it]  6%|▋         | 2/32 [00:03<00:46,  1.54s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.41s/it]  9%|▉         | 3/32 [00:04<00:42,  1.46s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.49s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.51s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.41s/it] 12%|█▎        | 4/32 [00:05<00:40,  1.43s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.49s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it]100%|██████████| 32/32 [00:45<00:00,  1.40s/it]100%|██████████| 32/32 [00:45<00:00,  1.42s/it]
 16%|█▌        | 5/32 [00:07<00:38,  1.42s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it]100%|██████████| 32/32 [00:49<00:00,  1.51s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]
 19%|█▉        | 6/32 [00:08<00:36,  1.42s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.51s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.42s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it]I0402 09:24:58.660410 1565925 finetune.py:45] layer 7_up initial loss 0.0019645290449261665
W0402 09:24:58.660679 1565925 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it]W0402 09:24:59.525779 1565925 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:25:00.315905 1564883 finetune.py:45] layer 5_up initial loss 0.001128581934608519
W0402 09:25:00.316096 1564883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

7_up proxy err 0.009514455683529377 tr(WHW.T) 651.590087890625
  0%|          | 0/32 [00:00<?, ?it/s] 34%|███▍      | 11/32 [00:15<00:29,  1.42s/it]W0402 09:25:01.232278 1564883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it]5_up proxy err 0.010801758617162704 tr(WHW.T) 497.4553527832031
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:55,  1.79s/it] 41%|████      | 13/32 [00:18<00:26,  1.40s/it]  6%|▋         | 2/32 [00:03<00:46,  1.54s/it]  3%|▎         | 1/32 [00:01<01:01,  1.97s/it] 44%|████▍     | 14/32 [00:19<00:24,  1.39s/it]  9%|▉         | 3/32 [00:04<00:42,  1.46s/it]I0402 09:25:05.257141 1565763 finetune.py:45] layer 6_up initial loss 0.0018717088969424367
W0402 09:25:05.257361 1565763 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:03<00:51,  1.71s/it]W0402 09:25:06.082301 1565763 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 15/32 [00:21<00:23,  1.38s/it] 12%|█▎        | 4/32 [00:05<00:40,  1.43s/it]6_up proxy err 0.010205878876149654 tr(WHW.T) 571.9508666992188
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:05<00:46,  1.62s/it] 50%|█████     | 16/32 [00:22<00:22,  1.38s/it] 16%|█▌        | 5/32 [00:07<00:38,  1.41s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.38s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it]  3%|▎         | 1/32 [00:01<00:59,  1.92s/it] 19%|█▉        | 6/32 [00:08<00:36,  1.41s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.38s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it] 22%|██▏       | 7/32 [00:10<00:34,  1.40s/it] 59%|█████▉    | 19/32 [00:26<00:17,  1.37s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it] 25%|██▌       | 8/32 [00:11<00:33,  1.39s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.37s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 28%|██▊       | 9/32 [00:12<00:31,  1.39s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.37s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.39s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it] 69%|██████▉   | 22/32 [00:30<00:13,  1.37s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.38s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.37s/it] 38%|███▊      | 12/32 [00:16<00:27,  1.38s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.52s/it] 75%|███████▌  | 24/32 [00:33<00:10,  1.37s/it] 41%|████      | 13/32 [00:18<00:26,  1.38s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.37s/it] 44%|████▍     | 14/32 [00:19<00:24,  1.38s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.37s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.38s/it] 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 84%|████████▍ | 27/32 [00:37<00:06,  1.37s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.50s/it] 50%|█████     | 16/32 [00:22<00:22,  1.38s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.50s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.37s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it] 53%|█████▎    | 17/32 [00:23<00:20,  1.38s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.37s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.50s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.50s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.38s/it] 94%|█████████▍| 30/32 [00:41<00:02,  1.37s/it] 50%|█████     | 16/32 [00:24<00:24,  1.50s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it] 59%|█████▉    | 19/32 [00:26<00:17,  1.38s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.37s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.50s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.38s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.50s/it]100%|██████████| 32/32 [00:44<00:00,  1.37s/it]100%|██████████| 32/32 [00:44<00:00,  1.40s/it]
 56%|█████▋    | 18/32 [00:27<00:21,  1.50s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.39s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.50s/it] 69%|██████▉   | 22/32 [00:30<00:13,  1.40s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.41s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.52s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 75%|███████▌  | 24/32 [00:33<00:11,  1.43s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.53s/it] 78%|███████▊  | 25/32 [00:35<00:10,  1.45s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it]I0402 09:25:37.314409 1564634 finetune.py:45] layer 4_gate initial loss 0.001489031477831304
W0402 09:25:37.314711 1564634 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:36<00:08,  1.45s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.53s/it]W0402 09:25:38.039211 1564634 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:38<00:07,  1.44s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.44s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it]4_gate proxy err 0.004982232116162777 tr(WHW.T) 1579.121337890625
  0%|          | 0/112 [00:00<?, ?it/s] 69%|██████▉   | 22/32 [00:33<00:15,  1.52s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.42s/it]  1%|          | 1/112 [00:00<01:27,  1.26it/s]  2%|▏         | 2/112 [00:01<00:58,  1.90it/s] 81%|████████▏ | 26/32 [00:39<00:09,  1.52s/it]  3%|▎         | 3/112 [00:01<00:48,  2.27it/s] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it]  4%|▎         | 4/112 [00:01<00:43,  2.48it/s] 94%|█████████▍| 30/32 [00:42<00:02,  1.41s/it]  4%|▍         | 5/112 [00:02<00:40,  2.63it/s]  5%|▌         | 6/112 [00:02<00:38,  2.73it/s]  6%|▋         | 7/112 [00:02<00:37,  2.78it/s] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.53s/it]  7%|▋         | 8/112 [00:03<00:36,  2.83it/s] 97%|█████████▋| 31/32 [00:43<00:01,  1.40s/it]  8%|▊         | 9/112 [00:03<00:35,  2.87it/s]  9%|▉         | 10/112 [00:03<00:35,  2.87it/s] 10%|▉         | 11/112 [00:04<00:34,  2.89it/s] 88%|████████▊ | 28/32 [00:42<00:06,  1.52s/it] 11%|█         | 12/112 [00:04<00:34,  2.91it/s] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it] 12%|█▏        | 13/112 [00:04<00:33,  2.92it/s]100%|██████████| 32/32 [00:45<00:00,  1.39s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]
 12%|█▎        | 14/112 [00:05<00:33,  2.92it/s] 13%|█▎        | 15/112 [00:05<00:33,  2.86it/s] 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it] 14%|█▍        | 16/112 [00:05<00:34,  2.82it/s] 81%|████████▏ | 26/32 [00:39<00:09,  1.52s/it] 15%|█▌        | 17/112 [00:06<00:33,  2.82it/s] 16%|█▌        | 18/112 [00:06<00:32,  2.86it/s] 17%|█▋        | 19/112 [00:07<00:32,  2.84it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.87it/s] 94%|█████████▍| 30/32 [00:45<00:03,  1.51s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it] 19%|█▉        | 21/112 [00:07<00:32,  2.84it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.81it/s] 21%|██        | 23/112 [00:08<00:31,  2.78it/s] 21%|██▏       | 24/112 [00:08<00:31,  2.77it/s] 97%|█████████▋| 31/32 [00:47<00:01,  1.50s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it] 22%|██▏       | 25/112 [00:09<00:31,  2.76it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.74it/s] 24%|██▍       | 27/112 [00:09<00:31,  2.74it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.74it/s]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]
 26%|██▌       | 29/112 [00:10<00:30,  2.74it/s] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it] 27%|██▋       | 30/112 [00:11<00:29,  2.73it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.73it/s] 29%|██▊       | 32/112 [00:11<00:29,  2.73it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.73it/s] 94%|█████████▍| 30/32 [00:45<00:03,  1.52s/it]I0402 09:25:53.186123 1565925 finetune.py:45] layer 7_gate initial loss 0.0019441122421994805
W0402 09:25:53.186321 1565925 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 30%|███       | 34/112 [00:12<00:28,  2.76it/s] 31%|███▏      | 35/112 [00:12<00:27,  2.79it/s]W0402 09:25:53.975230 1565925 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 32%|███▏      | 36/112 [00:13<00:26,  2.84it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.82it/s] 97%|█████████▋| 31/32 [00:47<00:01,  1.53s/it] 34%|███▍      | 38/112 [00:13<00:26,  2.84it/s] 35%|███▍      | 39/112 [00:14<00:25,  2.82it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.79it/s] 37%|███▋      | 41/112 [00:14<00:25,  2.81it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.84it/s]100%|██████████| 32/32 [00:48<00:00,  1.54s/it]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]
 38%|███▊      | 43/112 [00:15<00:24,  2.87it/s] 39%|███▉      | 44/112 [00:15<00:23,  2.86it/s]7_gate proxy err 0.00390829611569643 tr(WHW.T) 2636.09716796875
  0%|          | 0/112 [00:00<?, ?it/s] 40%|████      | 45/112 [00:16<00:23,  2.87it/s] 41%|████      | 46/112 [00:16<00:23,  2.87it/s]  1%|          | 1/112 [00:00<01:29,  1.24it/s] 42%|████▏     | 47/112 [00:17<00:22,  2.87it/s]  2%|▏         | 2/112 [00:01<00:59,  1.86it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.88it/s]  3%|▎         | 3/112 [00:01<00:49,  2.21it/s] 44%|████▍     | 49/112 [00:17<00:21,  2.91it/s]  4%|▎         | 4/112 [00:01<00:44,  2.43it/s] 45%|████▍     | 50/112 [00:18<00:21,  2.91it/s]I0402 09:25:58.991190 1564883 finetune.py:45] layer 5_gate initial loss 0.0011135587701573968
W0402 09:25:58.991417 1564883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  4%|▍         | 5/112 [00:02<00:41,  2.57it/s] 46%|████▌     | 51/112 [00:18<00:20,  2.91it/s]  5%|▌         | 6/112 [00:02<00:39,  2.66it/s] 46%|████▋     | 52/112 [00:18<00:20,  2.92it/s]  6%|▋         | 7/112 [00:02<00:38,  2.73it/s]W0402 09:25:59.824183 1564883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 53/112 [00:19<00:20,  2.92it/s]  7%|▋         | 8/112 [00:03<00:37,  2.78it/s] 48%|████▊     | 54/112 [00:19<00:19,  2.94it/s]  8%|▊         | 9/112 [00:03<00:36,  2.82it/s] 49%|████▉     | 55/112 [00:19<00:19,  2.94it/s]  9%|▉         | 10/112 [00:03<00:36,  2.82it/s] 50%|█████     | 56/112 [00:20<00:19,  2.93it/s] 10%|▉         | 11/112 [00:04<00:35,  2.82it/s] 51%|█████     | 57/112 [00:20<00:18,  2.93it/s] 11%|█         | 12/112 [00:04<00:35,  2.82it/s] 52%|█████▏    | 58/112 [00:20<00:18,  2.90it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.80it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.88it/s] 12%|█▎        | 14/112 [00:05<00:34,  2.82it/s] 54%|█████▎    | 60/112 [00:21<00:17,  2.91it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.83it/s] 54%|█████▍    | 61/112 [00:21<00:17,  2.93it/s]5_gate proxy err 0.004773376509547234 tr(WHW.T) 1974.3070068359375
  0%|          | 0/112 [00:00<?, ?it/s] 14%|█▍        | 16/112 [00:06<00:33,  2.84it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.93it/s] 15%|█▌        | 17/112 [00:06<00:33,  2.85it/s] 56%|█████▋    | 63/112 [00:22<00:16,  2.93it/s]  1%|          | 1/112 [00:00<01:36,  1.15it/s] 16%|█▌        | 18/112 [00:06<00:32,  2.86it/s] 57%|█████▋    | 64/112 [00:22<00:16,  2.94it/s] 17%|█▋        | 19/112 [00:07<00:32,  2.84it/s]  2%|▏         | 2/112 [00:01<01:04,  1.71it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.92it/s]I0402 09:26:04.178934 1565763 finetune.py:45] layer 6_gate initial loss 0.0018550434615463018
W0402 09:26:04.179191 1565763 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 18%|█▊        | 20/112 [00:07<00:32,  2.85it/s] 59%|█████▉    | 66/112 [00:23<00:15,  2.94it/s]  3%|▎         | 3/112 [00:01<00:53,  2.03it/s] 19%|█▉        | 21/112 [00:07<00:31,  2.85it/s] 60%|█████▉    | 67/112 [00:23<00:15,  2.94it/s]  4%|▎         | 4/112 [00:02<00:48,  2.22it/s]W0402 09:26:04.892379 1565763 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 20%|█▉        | 22/112 [00:08<00:31,  2.85it/s] 61%|██████    | 68/112 [00:24<00:15,  2.93it/s]  4%|▍         | 5/112 [00:02<00:45,  2.35it/s] 21%|██        | 23/112 [00:08<00:31,  2.87it/s] 62%|██████▏   | 69/112 [00:24<00:14,  2.94it/s]  5%|▌         | 6/112 [00:02<00:43,  2.41it/s] 21%|██▏       | 24/112 [00:08<00:30,  2.87it/s] 62%|██████▎   | 70/112 [00:24<00:14,  2.93it/s]  6%|▋         | 7/112 [00:03<00:42,  2.46it/s] 22%|██▏       | 25/112 [00:09<00:30,  2.86it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.92it/s]  7%|▋         | 8/112 [00:03<00:41,  2.48it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.86it/s] 64%|██████▍   | 72/112 [00:25<00:13,  2.93it/s]  8%|▊         | 9/112 [00:03<00:41,  2.49it/s] 24%|██▍       | 27/112 [00:09<00:29,  2.86it/s] 65%|██████▌   | 73/112 [00:25<00:13,  2.93it/s]  9%|▉         | 10/112 [00:04<00:40,  2.53it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.91it/s] 25%|██▌       | 28/112 [00:10<00:29,  2.84it/s] 67%|██████▋   | 75/112 [00:26<00:12,  2.92it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.84it/s] 10%|▉         | 11/112 [00:04<00:39,  2.58it/s] 68%|██████▊   | 76/112 [00:26<00:12,  2.92it/s] 27%|██▋       | 30/112 [00:10<00:28,  2.85it/s] 11%|█         | 12/112 [00:05<00:38,  2.58it/s]6_gate proxy err 0.004012777004390955 tr(WHW.T) 2580.454833984375
  0%|          | 0/112 [00:00<?, ?it/s] 69%|██████▉   | 77/112 [00:27<00:11,  2.93it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.86it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.61it/s] 70%|██████▉   | 78/112 [00:27<00:11,  2.91it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.83it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.62it/s] 71%|███████   | 79/112 [00:27<00:11,  2.90it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.81it/s]  1%|          | 1/112 [00:00<01:32,  1.20it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.63it/s] 71%|███████▏  | 80/112 [00:28<00:10,  2.91it/s] 30%|███       | 34/112 [00:12<00:27,  2.84it/s]  2%|▏         | 2/112 [00:01<01:02,  1.76it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.64it/s] 72%|███████▏  | 81/112 [00:28<00:10,  2.92it/s] 31%|███▏      | 35/112 [00:12<00:26,  2.86it/s]  3%|▎         | 3/112 [00:01<00:52,  2.07it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.64it/s] 73%|███████▎  | 82/112 [00:28<00:10,  2.92it/s] 32%|███▏      | 36/112 [00:13<00:26,  2.87it/s]  4%|▎         | 4/112 [00:01<00:48,  2.23it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.62it/s] 74%|███████▍  | 83/112 [00:29<00:09,  2.93it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.86it/s]  4%|▍         | 5/112 [00:02<00:45,  2.35it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.63it/s] 75%|███████▌  | 84/112 [00:29<00:09,  2.93it/s] 34%|███▍      | 38/112 [00:13<00:25,  2.85it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.93it/s]  5%|▌         | 6/112 [00:02<00:43,  2.43it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.63it/s] 35%|███▍      | 39/112 [00:14<00:25,  2.85it/s] 77%|███████▋  | 86/112 [00:30<00:08,  2.93it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.62it/s]  6%|▋         | 7/112 [00:03<00:42,  2.47it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.86it/s] 78%|███████▊  | 87/112 [00:30<00:08,  2.95it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.63it/s]  7%|▋         | 8/112 [00:03<00:41,  2.52it/s] 37%|███▋      | 41/112 [00:14<00:24,  2.87it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.95it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.86it/s] 21%|██        | 23/112 [00:09<00:33,  2.63it/s]  8%|▊         | 9/112 [00:03<00:40,  2.54it/s] 79%|███████▉  | 89/112 [00:31<00:07,  2.94it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.86it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.63it/s]  9%|▉         | 10/112 [00:04<00:39,  2.56it/s] 80%|████████  | 90/112 [00:31<00:07,  2.94it/s] 39%|███▉      | 44/112 [00:15<00:23,  2.86it/s] 22%|██▏       | 25/112 [00:10<00:32,  2.64it/s] 10%|▉         | 11/112 [00:04<00:39,  2.58it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.93it/s] 40%|████      | 45/112 [00:16<00:23,  2.85it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.64it/s] 11%|█         | 12/112 [00:05<00:38,  2.58it/s] 82%|████████▏ | 92/112 [00:32<00:06,  2.93it/s] 41%|████      | 46/112 [00:16<00:23,  2.86it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.65it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.60it/s] 83%|████████▎ | 93/112 [00:32<00:06,  2.93it/s] 42%|████▏     | 47/112 [00:16<00:22,  2.84it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.63it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.92it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.59it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.83it/s] 85%|████████▍ | 95/112 [00:33<00:05,  2.92it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.64it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.61it/s] 44%|████▍     | 49/112 [00:17<00:22,  2.85it/s] 86%|████████▌ | 96/112 [00:33<00:05,  2.94it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.65it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.62it/s] 45%|████▍     | 50/112 [00:17<00:21,  2.85it/s] 87%|████████▋ | 97/112 [00:34<00:05,  2.92it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.63it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.60it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.81it/s] 88%|████████▊ | 98/112 [00:34<00:04,  2.89it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.63it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.60it/s] 46%|████▋     | 52/112 [00:18<00:21,  2.82it/s] 88%|████████▊ | 99/112 [00:34<00:04,  2.89it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.63it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.59it/s] 47%|████▋     | 53/112 [00:19<00:20,  2.82it/s] 89%|████████▉ | 100/112 [00:35<00:04,  2.90it/s] 30%|███       | 34/112 [00:13<00:29,  2.61it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.82it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.58it/s] 90%|█████████ | 101/112 [00:35<00:03,  2.91it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.63it/s] 49%|████▉     | 55/112 [00:19<00:20,  2.84it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.60it/s] 91%|█████████ | 102/112 [00:35<00:03,  2.92it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.64it/s] 50%|█████     | 56/112 [00:20<00:19,  2.82it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.60it/s] 92%|█████████▏| 103/112 [00:36<00:03,  2.90it/s] 51%|█████     | 57/112 [00:20<00:19,  2.83it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.64it/s] 93%|█████████▎| 104/112 [00:36<00:02,  2.91it/s] 21%|██        | 23/112 [00:09<00:34,  2.60it/s] 52%|█████▏    | 58/112 [00:20<00:18,  2.84it/s] 94%|█████████▍| 105/112 [00:36<00:02,  2.91it/s] 34%|███▍      | 38/112 [00:14<00:28,  2.64it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.60it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.84it/s] 95%|█████████▍| 106/112 [00:37<00:02,  2.91it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.64it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.60it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.85it/s] 96%|█████████▌| 107/112 [00:37<00:01,  2.92it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.64it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.62it/s] 54%|█████▍    | 61/112 [00:21<00:17,  2.86it/s] 96%|█████████▋| 108/112 [00:37<00:01,  2.92it/s] 37%|███▋      | 41/112 [00:16<00:26,  2.64it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.63it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.86it/s] 97%|█████████▋| 109/112 [00:38<00:01,  2.92it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.62it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.60it/s] 56%|█████▋    | 63/112 [00:22<00:17,  2.84it/s] 98%|█████████▊| 110/112 [00:38<00:00,  2.91it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.62it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.61it/s] 57%|█████▋    | 64/112 [00:22<00:16,  2.85it/s] 99%|█████████▉| 111/112 [00:38<00:00,  2.92it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.62it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.60it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.84it/s]100%|██████████| 112/112 [00:39<00:00,  2.87it/s]100%|██████████| 112/112 [00:39<00:00,  2.85it/s]
 40%|████      | 45/112 [00:17<00:25,  2.61it/s] 59%|█████▉    | 66/112 [00:23<00:16,  2.84it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.60it/s] 41%|████      | 46/112 [00:18<00:25,  2.62it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.79it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.61it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.63it/s] 61%|██████    | 68/112 [00:24<00:15,  2.75it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.60it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.62it/s] 62%|██████▏   | 69/112 [00:24<00:15,  2.76it/s] 30%|███       | 34/112 [00:13<00:29,  2.60it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.63it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.76it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.61it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.63it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.75it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.61it/s] 64%|██████▍   | 72/112 [00:25<00:14,  2.78it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.62it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.61it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.79it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.63it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.62it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.80it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.63it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.60it/s] 67%|██████▋   | 75/112 [00:26<00:13,  2.82it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.64it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.61it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.83it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.64it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.61it/s] 69%|██████▉   | 77/112 [00:27<00:12,  2.84it/s] 50%|█████     | 56/112 [00:21<00:21,  2.63it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.60it/s] 70%|██████▉   | 78/112 [00:27<00:11,  2.85it/s] 51%|█████     | 57/112 [00:22<00:20,  2.64it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.60it/s] 71%|███████   | 79/112 [00:28<00:11,  2.86it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.64it/s] 71%|███████▏  | 80/112 [00:28<00:11,  2.86it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.58it/s] 53%|█████▎    | 59/112 [00:22<00:20,  2.62it/s] 72%|███████▏  | 81/112 [00:28<00:10,  2.86it/s] 40%|████      | 45/112 [00:17<00:25,  2.58it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.63it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.85it/s] 41%|████      | 46/112 [00:18<00:25,  2.59it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.63it/s] 74%|███████▍  | 83/112 [00:29<00:10,  2.82it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.58it/s]W0402 09:26:26.853000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:26.853000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:26.853000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:26.853000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:26.853000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:26.853000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:26.854000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
 55%|█████▌    | 62/112 [00:24<00:18,  2.64it/s] 75%|███████▌  | 84/112 [00:30<00:09,  2.83it/s]W0402 09:26:26.895000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:26.895000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:26.895000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:26.895000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:26.895000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:26.911000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:26.911000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:26.911000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:26.911000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:26.911000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 43%|████▎     | 48/112 [00:18<00:24,  2.60it/s]W0402 09:26:27.072000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.072000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.072000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.073000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.073000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 63/112 [00:24<00:18,  2.65it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.83it/s]W0402 09:26:27.386000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.386000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.386000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.386000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.386000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.386000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.387000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.419000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.419000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.419000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.419000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.419000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 49/112 [00:19<00:24,  2.60it/s]W0402 09:26:27.487000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.487000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.487000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.487000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:27.487000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 77%|███████▋  | 86/112 [00:30<00:09,  2.83it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.64it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.59it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.84it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.65it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.59it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.85it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.65it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.59it/s]W0402 09:26:28.625000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:28.630000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:28.636000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:28.636000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 79%|███████▉  | 89/112 [00:31<00:08,  2.81it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.64it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.59it/s] 80%|████████  | 90/112 [00:32<00:07,  2.82it/s]W0402 09:26:29.060000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.061000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.061000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.061000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.061000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.061000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.061000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.091000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.091000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.091000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.091000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.092000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 61%|██████    | 68/112 [00:26<00:16,  2.64it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.60it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.84it/s]W0402 09:26:29.420000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.420000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.420000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.420000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.420000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.420000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.420000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.420000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 62%|██████▏   | 69/112 [00:26<00:16,  2.65it/s]W0402 09:26:29.701000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.701000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.701000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.701000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:29.702000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 82%|████████▏ | 92/112 [00:32<00:07,  2.85it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.58it/s] 62%|██████▎   | 70/112 [00:27<00:15,  2.64it/s]W0402 09:26:30.031000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:30.036000 139922939422528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 83%|████████▎ | 93/112 [00:33<00:06,  2.83it/s] 50%|█████     | 56/112 [00:21<00:21,  2.59it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.63it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.83it/s] 51%|█████     | 57/112 [00:22<00:21,  2.59it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.64it/s] 85%|████████▍ | 95/112 [00:33<00:05,  2.85it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.58it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.64it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.82it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.59it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.64it/s] 87%|████████▋ | 97/112 [00:34<00:05,  2.83it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.59it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.64it/s] 88%|████████▊ | 98/112 [00:34<00:04,  2.81it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.60it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.66it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.77it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.62it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.68it/s] 89%|████████▉ | 100/112 [00:35<00:04,  2.73it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.63it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.67it/s] 90%|█████████ | 101/112 [00:36<00:04,  2.71it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.62it/s] 71%|███████   | 79/112 [00:30<00:12,  2.67it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.70it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.62it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.67it/s] 92%|█████████▏| 103/112 [00:36<00:03,  2.69it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.61it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.65it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.68it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.59it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.66it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.68it/s] 61%|██████    | 68/112 [00:26<00:16,  2.60it/s] 74%|███████▍  | 83/112 [00:32<00:10,  2.66it/s] 95%|█████████▍| 106/112 [00:37<00:02,  2.67it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.60it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.66it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.67it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.62it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.68it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.67it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.60it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.67it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.67it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.60it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.66it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.67it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.62it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.68it/s] 99%|█████████▉| 111/112 [00:39<00:00,  2.67it/s]I0402 09:26:36.850956 1564634 finetune.py:45] layer 4_down initial loss 0.0014708847738802433
W0402 09:26:36.851299 1564634 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 79%|███████▉  | 89/112 [00:34<00:08,  2.67it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.61it/s]100%|██████████| 112/112 [00:40<00:00,  2.70it/s]100%|██████████| 112/112 [00:40<00:00,  2.78it/s]
W0402 09:26:37.308264 1564634 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 80%|████████  | 90/112 [00:34<00:08,  2.67it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.61it/s]4_down proxy err 0.012543057091534138 tr(WHW.T) 3.4171693325042725
 81%|████████▏ | 91/112 [00:35<00:07,  2.64it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.61it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.65it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.61it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.66it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.61it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.66it/s] 71%|███████   | 79/112 [00:30<00:12,  2.61it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.63it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.60it/s] 86%|████████▌ | 96/112 [00:36<00:06,  2.65it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.61it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.65it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.61it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.64it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.60it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.65it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.61it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.64it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.59it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.65it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.60it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.64it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.60it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.64it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.60it/s] 93%|█████████▎| 104/112 [00:39<00:03,  2.65it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.61it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.66it/s] 80%|████████  | 90/112 [00:35<00:08,  2.60it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.66it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.62it/s]W0402 09:26:43.793000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:43.793000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:43.793000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:43.793000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:43.793000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:43.793000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:26:43.794000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 96%|█████████▌| 107/112 [00:41<00:01,  2.66it/s]W0402 09:26:43.832000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:43.832000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:43.833000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:43.833000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:43.833000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:43.847000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:43.847000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:43.848000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:43.848000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:43.848000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 82%|████████▏ | 92/112 [00:35<00:07,  2.62it/s]W0402 09:26:44.012000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.012000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.013000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.013000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.013000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 96%|█████████▋| 108/112 [00:41<00:01,  2.65it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.61it/s]W0402 09:26:44.324000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.324000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.324000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.324000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.324000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.324000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.324000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.356000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.356000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.356000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.356000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.356000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.424000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.424000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.424000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.424000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:44.424000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 109/112 [00:41<00:01,  2.66it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.61it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.66it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.61it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.64it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.60it/s]W0402 09:26:45.545000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:45.556000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:45.564000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:45.564000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
100%|██████████| 112/112 [00:42<00:00,  2.65it/s]100%|██████████| 112/112 [00:42<00:00,  2.61it/s]
 87%|████████▋ | 97/112 [00:37<00:05,  2.60it/s]W0402 09:26:45.998000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:45.998000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:26:45.998000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:45.998000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:45.998000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:45.998000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:45.998000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:46.026000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:46.026000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:46.026000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:46.026000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:46.026000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 98/112 [00:38<00:05,  2.58it/s]W0402 09:26:46.366000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:46.366000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:46.366000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:26:46.366000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:46.366000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:46.366000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:46.366000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:46.366000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:46.649000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:46.649000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:46.649000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:46.649000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:46.649000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 99/112 [00:38<00:05,  2.54it/s]W0402 09:26:46.965000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:46.970000 140068034762560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 89%|████████▉ | 100/112 [00:38<00:04,  2.53it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.54it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.54it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.55it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.56it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.59it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.61it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.63it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.63it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.61it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.62it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.63it/s]100%|██████████| 112/112 [00:43<00:00,  2.62it/s]100%|██████████| 112/112 [00:43<00:00,  2.57it/s]
W0402 09:26:52.454000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.455000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.455000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.455000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.455000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.455000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.455000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.498000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.498000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.498000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.498000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.498000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.514000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.514000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.514000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.514000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.514000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.687000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.687000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.687000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.687000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:52.687000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:53.024000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:53.024000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:53.024000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:53.025000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:53.025000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:53.025000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:53.025000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:26:53.060000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:53.060000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:53.060000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:53.060000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:53.060000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:53.130000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:53.130000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:53.130000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:53.130000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:53.130000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
I0402 09:26:53.550544 1565925 finetune.py:45] layer 7_down initial loss 0.0019175480119884014
W0402 09:26:53.550740 1565925 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:26:54.057279 1565925 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0402 09:26:54.299000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:54.313000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:54.321000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:54.321000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
7_down proxy err 0.012144211679697037 tr(WHW.T) 6.803934097290039
W0402 09:26:54.751000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:54.751000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:26:54.752000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:54.752000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:54.752000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:54.752000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:54.752000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:54.781000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:54.781000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:54.781000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:54.781000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:54.781000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:55.131000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:55.131000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:26:55.131000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:55.131000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:55.131000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:55.131000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:55.132000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:55.132000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:55.422000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:55.422000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:55.422000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:55.422000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:55.423000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:55.751000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:55.757000 140017194051392 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.740000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.740000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.740000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.740000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.740000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.741000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.741000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.786000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.786000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.786000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.786000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.786000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.802000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.802000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.802000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.802000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.802000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.976000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.976000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.976000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.976000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:58.976000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:59.312000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:59.312000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:59.312000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:59.312000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:59.312000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:59.313000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:26:59.313000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:26:59.353000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:59.353000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:59.353000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:59.353000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:59.353000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:26:59.427000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:26:59.427000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:26:59.427000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:26:59.427000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:26:59.427000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:00.643000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:00.656000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:00.664000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:00.664000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.119000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.119000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.119000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.119000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.119000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.120000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.120000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.152000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.152000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.152000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.152000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.152000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.506000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.506000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.506000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.506000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.506000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.506000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.506000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.506000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.810000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.810000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.810000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.810000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:01.810000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:02.151000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:02.156000 140213949163328 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0402 09:27:02.613927 1564883 finetune.py:45] layer 5_down initial loss 0.001104720402508974
W0402 09:27:02.614196 1564883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:27:03.107185 1564883 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

5_down proxy err 0.012305029667913914 tr(WHW.T) 4.908621311187744
I0402 09:27:07.167652 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 8 in 1.43735933303833s
I0402 09:27:07.565749 1558010 quantize_finetune_llama.py:159] layer 9 gpu 1
I0402 09:27:09.332510 1565763 finetune.py:45] layer 6_down initial loss 0.0018227110849693418
W0402 09:27:09.332960 1565763 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 09:27:09.471775 1570161 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:27:09.471899 1570161 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:27:09.471956 1570161 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:27:09.647934 1570161 config.py:58] PyTorch version 2.4.0 available.
W0402 09:27:09.841466 1565763 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

6_down proxy err 0.011790185235440731 tr(WHW.T) 6.227620601654053
I0402 09:27:11.709480 1570161 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 09:27:12.122963 1570161 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]I0402 09:27:13.677618 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 9 in 1.1894943714141846s
I0402 09:27:14.160573 1558010 quantize_finetune_llama.py:159] layer 10 gpu 2
  3%|▎         | 1/32 [00:01<00:44,  1.44s/it]  6%|▋         | 2/32 [00:01<00:23,  1.27it/s]  9%|▉         | 3/32 [00:02<00:16,  1.74it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.13it/s]I0402 09:27:15.732472 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 10 in 1.1674010753631592s
 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s]I0402 09:27:16.188148 1570709 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:27:16.188275 1570709 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:27:16.188344 1570709 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:27:16.225596 1558010 quantize_finetune_llama.py:159] layer 11 gpu 3
 19%|█▉        | 6/32 [00:03<00:10,  2.58it/s]I0402 09:27:16.405031 1570709 config.py:58] PyTorch version 2.4.0 available.
 22%|██▏       | 7/32 [00:03<00:09,  2.73it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.84it/s] 28%|██▊       | 9/32 [00:04<00:07,  2.89it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.98it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.05it/s]I0402 09:27:18.085996 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 11 in 1.4223663806915283s
I0402 09:27:18.162305 1570888 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:27:18.162520 1570888 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:27:18.162634 1570888 utils.py:162] NumExpr defaulting to 16 threads.
 38%|███▊      | 12/32 [00:04<00:06,  3.08it/s]I0402 09:27:18.407863 1570888 config.py:58] PyTorch version 2.4.0 available.
 41%|████      | 13/32 [00:05<00:06,  3.06it/s]I0402 09:27:18.537157 1570709 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 09:27:18.588131 1558010 quantize_finetune_llama.py:159] layer 12 gpu 0
 44%|████▍     | 14/32 [00:05<00:05,  3.06it/s]W0402 09:27:18.890201 1570709 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 15/32 [00:05<00:05,  3.06it/s] 50%|█████     | 16/32 [00:06<00:05,  3.05it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.10it/s]  0%|          | 0/32 [00:00<?, ?it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.13it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.15it/s]I0402 09:27:20.655599 1571323 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:27:20.655971 1571323 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:27:20.656230 1571323 utils.py:162] NumExpr defaulting to 16 threads.
 62%|██████▎   | 20/32 [00:07<00:03,  3.14it/s]I0402 09:27:20.936089 1571323 config.py:58] PyTorch version 2.4.0 available.
I0402 09:27:20.974219 1570888 data_utils.py:336] using 256 training seqs, 128 validation seqs
 66%|██████▌   | 21/32 [00:07<00:03,  3.13it/s]W0402 09:27:21.374039 1570888 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:08<00:03,  3.15it/s]  3%|▎         | 1/32 [00:01<00:49,  1.61s/it] 72%|███████▏  | 23/32 [00:08<00:02,  3.12it/s]  6%|▋         | 2/32 [00:01<00:25,  1.16it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.12it/s]  9%|▉         | 3/32 [00:02<00:17,  1.63it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.16it/s]  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:02<00:14,  2.00it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.17it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.18it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.27it/s]I0402 09:27:23.243091 1571323 data_utils.py:336] using 256 training seqs, 128 validation seqs
 88%|████████▊ | 28/32 [00:10<00:01,  3.14it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.43it/s]W0402 09:27:23.586901 1571323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:10<00:00,  3.14it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.11it/s]  3%|▎         | 1/32 [00:01<00:45,  1.46s/it] 25%|██▌       | 8/32 [00:03<00:08,  2.70it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.13it/s]  6%|▋         | 2/32 [00:01<00:24,  1.24it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.79it/s]  0%|          | 0/32 [00:00<?, ?it/s]100%|██████████| 32/32 [00:11<00:00,  3.16it/s]100%|██████████| 32/32 [00:11<00:00,  2.82it/s]
  9%|▉         | 3/32 [00:02<00:17,  1.69it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.87it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.01it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.90it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.94it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.27it/s] 41%|████      | 13/32 [00:05<00:06,  2.91it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.42it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.96it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.61it/s]  3%|▎         | 1/32 [00:01<00:45,  1.47s/it] 47%|████▋     | 15/32 [00:06<00:05,  2.99it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s]  6%|▋         | 2/32 [00:01<00:24,  1.23it/s] 50%|█████     | 16/32 [00:06<00:05,  3.01it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.83it/s]  9%|▉         | 3/32 [00:02<00:17,  1.68it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.03it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.91it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.01it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.98it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.05it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.26it/s]W0402 09:27:27.502000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.502000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.502000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.502000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.502000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.503000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.503000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.528000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.528000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.528000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.528000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.528000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.544000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.544000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.544000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.544000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.544000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:05<00:06,  2.99it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.03it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.44it/s]W0402 09:27:27.865000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.865000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.865000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.865000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:27.865000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  3.02it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.04it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.03it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.06it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.67it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.04it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.00it/s]W0402 09:27:28.737000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:28.737000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:28.738000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:28.738000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:28.738000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:28.738000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:27:28.738000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:28.755000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:28.755000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:28.755000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:28.755000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:28.755000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:04<00:08,  2.74it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.07it/s] 50%|█████     | 16/32 [00:06<00:05,  2.99it/s]W0402 09:27:28.990000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:28.990000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:28.990000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:28.990000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:28.990000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.09it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.97it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.09it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.95it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.85it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.10it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.95it/s]W0402 09:27:30.122000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:30.122000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:30.122000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:30.122000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:30.122000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:30.122000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:30.122000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:27:30.139000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:30.139000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:30.139000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:30.139000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:30.139000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.90it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.10it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.94it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.93it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.09it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.95it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.93it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.07it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.95it/s]W0402 09:27:31.033000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:31.033000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:31.033000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:31.033000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:31.033000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:05,  2.96it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.09it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.94it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.98it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.09it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 75%|███████▌  | 24/32 [00:09<00:02,  2.94it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.99it/s]100%|██████████| 32/32 [00:11<00:00,  3.10it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
 78%|███████▊  | 25/32 [00:09<00:02,  2.93it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.97it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.93it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.93it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.92it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.90it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.90it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.89it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.89it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.88it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.87it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.87it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.86it/s]100%|██████████| 32/32 [00:11<00:00,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
 81%|████████▏ | 26/32 [00:10<00:02,  2.86it/s]W0402 09:27:34.904000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:27:34.904000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:34.905000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:34.905000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:34.905000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:34.905000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:34.905000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:34.933000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:34.933000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:34.933000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:34.933000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:34.933000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:34.950000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:34.950000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:34.950000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:34.950000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:34.951000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  2.85it/s]W0402 09:27:35.289000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:35.289000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:35.289000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:35.289000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:35.289000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  2.85it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.85it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.85it/s]W0402 09:27:36.221000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:36.221000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:36.221000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:36.221000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:27:36.221000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:36.221000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:36.221000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:36.240000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:36.240000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:36.240000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:36.240000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:36.240000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  2.86it/s]W0402 09:27:36.488000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:36.488000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:36.488000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:36.488000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:36.488000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
I0402 09:27:36.820110 1570161 finetune.py:45] layer 8_v initial loss 0.0026052340399473906
W0402 09:27:36.821867 1570161 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:27:37.710000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.710000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.710000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.710000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.710000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.710000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.710000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.729000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.729000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.729000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.729000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.729000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.872000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.872000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.872000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.872000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.872000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.872000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.873000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.899000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.899000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.899000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.899000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.899000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.915000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.915000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.915000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.915000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:37.915000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:38.223632 1570161 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:27:38.233000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:38.233000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:38.234000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:38.234000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:38.234000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:38.635000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:38.635000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:38.635000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:38.635000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:38.636000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.134000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.134000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.134000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.134000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.134000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.134000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.134000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.151000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.152000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.152000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.152000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.152000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
8_v proxy err 0.009063364937901497 tr(WHW.T) 53.42280197143555
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:27:39.383000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.383000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.383000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.383000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.383000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.677000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.678000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.678000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.678000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.678000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.678000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.678000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.703000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.703000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.703000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.703000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.703000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.719000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.719000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.719000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.719000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:39.719000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:40.053000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:40.053000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:40.053000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:40.053000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:40.054000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:29,  1.06it/s]W0402 09:27:40.573000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:40.573000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:40.573000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:40.573000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:40.573000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:40.574000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:40.574000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:27:40.592000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:40.592000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:40.592000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:40.592000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:40.592000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:17,  1.67it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s]W0402 09:27:40.999000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:27:40.999000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.000000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.000000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.000000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.000000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.000000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.018000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.018000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.019000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.019000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.019000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.270000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.270000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.270000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.270000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.270000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:02<00:12,  2.27it/s]W0402 09:27:41.533000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.533000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.533000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.533000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:41.533000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.49it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s]W0402 09:27:42.510000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:42.510000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:42.511000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:27:42.511000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:42.511000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:42.511000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:42.511000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:42.532000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:42.532000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:42.532000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:42.532000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:42.532000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:09,  2.63it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s]W0402 09:27:43.513000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:43.513000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:43.514000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:43.514000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:43.514000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.73it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 38%|███▊      | 12/32 [00:04<00:07,  2.72it/s]I0402 09:27:44.339215 1570709 finetune.py:45] layer 9_v initial loss 0.0031238249503076077
W0402 09:27:44.339618 1570709 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:05<00:06,  2.74it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.76it/s]W0402 09:27:45.660302 1570709 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.79it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.76it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.69it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.69it/s]9_v proxy err 0.009173627011477947 tr(WHW.T) 72.69827270507812
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.68it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.70it/s]  3%|▎         | 1/32 [00:00<00:30,  1.03it/s]I0402 09:27:48.353643 1570888 finetune.py:45] layer 10_v initial loss 0.0020071908365935087
W0402 09:27:48.354148 1570888 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s]  6%|▋         | 2/32 [00:01<00:19,  1.57it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.69it/s]  9%|▉         | 3/32 [00:01<00:15,  1.93it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.15it/s]W0402 09:27:49.525292 1570888 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:27:49.595012 1571323 finetune.py:45] layer 11_v initial loss 0.0026363893412053585
W0402 09:27:49.595393 1571323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 84%|████████▍ | 27/32 [00:10<00:01,  2.67it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.67it/s] 19%|█▉        | 6/32 [00:02<00:11,  2.35it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s]10_v proxy err 0.00860156212002039 tr(WHW.T) 60.08286666870117
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:27:50.633842 1571323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:03<00:10,  2.41it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.64it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.47it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.66it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s]  3%|▎         | 1/32 [00:00<00:29,  1.07it/s]11_v proxy err 0.00707711698487401 tr(WHW.T) 74.2698974609375
  0%|          | 0/32 [00:00<?, ?it/s]100%|██████████| 32/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s]  6%|▋         | 2/32 [00:01<00:17,  1.68it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s]  9%|▉         | 3/32 [00:01<00:14,  2.01it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s]  3%|▎         | 1/32 [00:00<00:29,  1.05it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s]  6%|▋         | 2/32 [00:01<00:18,  1.67it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.27it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 50%|█████     | 16/32 [00:06<00:05,  2.72it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.75it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.70it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.77it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.75it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.68it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.69it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.73it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.70it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.76it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.77it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.73it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.75it/s] 41%|████      | 13/32 [00:05<00:06,  2.74it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s] 50%|█████     | 16/32 [00:06<00:05,  2.74it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.78it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.74it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.79it/s]W0402 09:27:57.782000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.782000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.782000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.782000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.782000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.782000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.783000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.812000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.812000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.813000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.813000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.813000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.828000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.828000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.828000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.828000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.828000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:05,  2.78it/s]W0402 09:27:57.980000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.980000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.980000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.980000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:57.980000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 19/32 [00:07<00:04,  2.79it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.75it/s]W0402 09:27:58.196000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:58.196000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:27:58.196000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:58.196000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:58.196000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:58.196000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:58.196000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:58.218000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:58.218000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:58.218000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:58.218000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:58.218000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:58.281000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:58.281000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:58.281000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:58.281000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:58.281000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:06<00:05,  2.80it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.81it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.74it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.81it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.81it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.76it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.79it/s]W0402 09:27:59.116000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.75it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.80it/s]W0402 09:27:59.418000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:59.418000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:27:59.418000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:59.418000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:59.418000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:59.419000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:27:59.419000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:27:59.440000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:59.440000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:59.441000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:59.441000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:59.441000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.77it/s]W0402 09:27:59.689000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:27:59.690000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:27:59.690000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:27:59.690000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:27:59.690000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:03,  2.80it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.77it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
W0402 09:27:59.942000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.74it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.74it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.75it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.77it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.78it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.80it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
W0402 09:28:05.764000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.764000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.764000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.764000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.765000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.765000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.765000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.792000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.792000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.793000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.793000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.793000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.808000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.808000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.808000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.808000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.808000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.960000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.960000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.960000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.960000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:05.960000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:06.184000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:28:06.184000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:06.184000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:06.184000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:06.184000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:06.184000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:06.184000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:28:06.203000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:06.203000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:06.203000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:06.203000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:06.204000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:06.266000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:06.266000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:06.266000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:06.266000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:06.266000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
I0402 09:28:06.375672 1570161 finetune.py:45] layer 8_q initial loss 0.002608280396088958
W0402 09:28:06.376183 1570161 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:28:07.105000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:28:07.401000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:07.401000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:07.401000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:28:07.401000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:28:07.401000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:07.401000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:07.401000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:07.422000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:07.422000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:07.422000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:07.422000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:07.422000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:28:07.587162 1570161 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:28:07.668000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:07.668000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:07.668000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:07.668000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:07.668000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:28:07.921000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
8_q proxy err 0.001641399459913373 tr(WHW.T) 5514.4345703125
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:28:09.244000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.244000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.244000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.245000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.245000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.245000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.245000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.273000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.273000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.273000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.273000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.273000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.288000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.288000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.288000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.288000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.288000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.437000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.437000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.437000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.437000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.437000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:27,  1.13it/s]W0402 09:28:09.505000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.505000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.505000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.505000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.505000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.505000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.505000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.533000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.533000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.533000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.533000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.533000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.549000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.549000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.549000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.549000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.549000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.665000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.666000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.666000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.666000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.666000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.666000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.666000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.690000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.690000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.690000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.690000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.690000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.713000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.713000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.713000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.713000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.713000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.760000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.760000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.760000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.760000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.760000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:17,  1.73it/s]W0402 09:28:09.950000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.950000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.950000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.950000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.950000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.950000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.950000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.971000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.971000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.971000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.971000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:09.971000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:10.040000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:10.040000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:10.040000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:10.040000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:10.040000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:01<00:14,  2.05it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.25it/s]W0402 09:28:10.672000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:28:10.933000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s]W0402 09:28:11.003000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.003000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.003000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.004000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.004000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.004000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.004000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.025000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.025000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.025000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.026000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.026000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.253000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.254000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.254000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.254000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.254000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.254000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.254000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.277000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.277000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.277000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.277000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.277000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.295000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.295000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.295000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.295000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.296000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:02<00:10,  2.47it/s]W0402 09:28:11.543000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.543000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.543000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.544000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.544000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:28:11.574000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s]W0402 09:28:11.814000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.68it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s]I0402 09:28:13.993067 1570709 finetune.py:45] layer 9_q initial loss 0.0031259970273822546
W0402 09:28:13.993427 1570709 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.72it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s]W0402 09:28:15.217453 1570709 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 17/32 [00:06<00:05,  2.77it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.79it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.79it/s]9_q proxy err 0.0017371790017932653 tr(WHW.T) 5306.75244140625
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.78it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s]  3%|▎         | 1/32 [00:00<00:28,  1.08it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.77it/s]  6%|▋         | 2/32 [00:01<00:17,  1.67it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s]  9%|▉         | 3/32 [00:01<00:14,  2.03it/s]I0402 09:28:18.195598 1571323 finetune.py:45] layer 11_q initial loss 0.0026352861896157265
W0402 09:28:18.195803 1571323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s]I0402 09:28:18.429927 1570888 finetune.py:45] layer 10_q initial loss 0.002009251154959202
W0402 09:28:18.430373 1570888 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 4/32 [00:02<00:12,  2.26it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.77it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s]W0402 09:28:19.241218 1571323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.62it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.79it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.79it/s]W0402 09:28:20.076991 1570888 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

11_q proxy err 0.00186153466347605 tr(WHW.T) 5157.30615234375
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.74it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s]  3%|▎         | 1/32 [00:00<00:26,  1.18it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.86it/s]  6%|▋         | 2/32 [00:01<00:16,  1.82it/s] 41%|████      | 13/32 [00:05<00:06,  2.89it/s]  9%|▉         | 3/32 [00:01<00:13,  2.19it/s]10_q proxy err 0.001849344582296908 tr(WHW.T) 5567.9375
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.87it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.41it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.88it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.58it/s] 50%|█████     | 16/32 [00:06<00:05,  2.87it/s]  3%|▎         | 1/32 [00:00<00:28,  1.07it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.66it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.88it/s]  6%|▋         | 2/32 [00:01<00:17,  1.68it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.72it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.87it/s]  9%|▉         | 3/32 [00:01<00:14,  2.04it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.79it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.89it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.28it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.81it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.87it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.84it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.89it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.88it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.90it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.89it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.65it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.90it/s] 41%|████      | 13/32 [00:04<00:06,  2.91it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.93it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.92it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.76it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.93it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.90it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.91it/s] 50%|█████     | 16/32 [00:06<00:05,  2.90it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.80it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.92it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.83it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.91it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.90it/s] 41%|████      | 13/32 [00:05<00:06,  2.82it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.90it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.91it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.82it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.89it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.91it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.82it/s]I0402 09:28:27.745256 1570161 finetune.py:45] layer 8_k initial loss 0.0026094212662428617
W0402 09:28:27.745544 1570161 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 31/32 [00:11<00:00,  2.90it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.91it/s] 50%|█████     | 16/32 [00:06<00:05,  2.83it/s]100%|██████████| 32/32 [00:11<00:00,  2.88it/s]100%|██████████| 32/32 [00:11<00:00,  2.73it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  2.90it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.85it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s]W0402 09:28:28.955047 1570161 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:08<00:02,  2.84it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.75it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.85it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.77it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.83it/s]8_k proxy err 0.00119362014811486 tr(WHW.T) 4670.25390625
  0%|          | 0/32 [00:00<?, ?it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.76it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.74it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.76it/s]  3%|▎         | 1/32 [00:00<00:23,  1.32it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.73it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.74it/s]  6%|▋         | 2/32 [00:01<00:15,  1.90it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.72it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.73it/s]  9%|▉         | 3/32 [00:01<00:13,  2.19it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.71it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]100%|██████████| 32/32 [00:11<00:00,  2.73it/s]
 12%|█▎        | 4/32 [00:01<00:11,  2.38it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.52it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.69it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.59it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.69it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.65it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.71it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.70it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s]I0402 09:28:34.257857 1570709 finetune.py:45] layer 9_k initial loss 0.0031246638391166925
W0402 09:28:34.258079 1570709 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:04<00:07,  2.73it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.73it/s] 41%|████      | 13/32 [00:05<00:06,  2.72it/s]W0402 09:28:35.387923 1570709 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:05<00:06,  2.72it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.67it/s]9_k proxy err 0.0013064894592389464 tr(WHW.T) 4332.2958984375
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.65it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s]  3%|▎         | 1/32 [00:00<00:25,  1.22it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.72it/s]  6%|▋         | 2/32 [00:01<00:16,  1.80it/s]I0402 09:28:38.130548 1571323 finetune.py:45] layer 11_k initial loss 0.0026366543024778366
W0402 09:28:38.130884 1571323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 66%|██████▌   | 21/32 [00:08<00:04,  2.70it/s]  9%|▉         | 3/32 [00:01<00:13,  2.12it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.32it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.67it/s]W0402 09:28:39.280410 1571323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.60it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.64it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.64it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.70it/s]11_k proxy err 0.0014983233995735645 tr(WHW.T) 4189.68798828125
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.64it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.75it/s]  3%|▎         | 1/32 [00:00<00:23,  1.31it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.65it/s]I0402 09:28:41.424669 1570888 finetune.py:45] layer 10_k initial loss 0.0020092446357011795
W0402 09:28:41.425377 1570888 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:04<00:07,  2.77it/s]  6%|▋         | 2/32 [00:01<00:15,  1.90it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.67it/s] 41%|████      | 13/32 [00:05<00:06,  2.77it/s]  9%|▉         | 3/32 [00:01<00:13,  2.21it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.66it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.77it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.39it/s]100%|██████████| 32/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
W0402 09:28:42.458882 1570888 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 15/32 [00:05<00:06,  2.79it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.54it/s] 50%|█████     | 16/32 [00:06<00:05,  2.81it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.63it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.70it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.83it/s]10_k proxy err 0.001318486756645143 tr(WHW.T) 4710.4462890625
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.77it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.80it/s]  3%|▎         | 1/32 [00:00<00:25,  1.23it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.85it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.81it/s]  6%|▋         | 2/32 [00:01<00:16,  1.79it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.86it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.80it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.83it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.25it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.85it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.81it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.86it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.82it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.87it/s] 50%|█████     | 16/32 [00:06<00:05,  2.82it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.87it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.84it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.86it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.53it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.85it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.87it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.86it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.56it/s]100%|██████████| 32/32 [00:11<00:00,  2.85it/s]100%|██████████| 32/32 [00:11<00:00,  2.70it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  2.82it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.56it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.78it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.76it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.57it/s]I0402 09:28:49.638483 1570161 finetune.py:45] layer 8_o initial loss 0.0027063272427767515
W0402 09:28:49.638888 1570161 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.77it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.80it/s]W0402 09:28:50.584823 1570161 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 17/32 [00:06<00:05,  2.62it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.72it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.67it/s]8_o proxy err 0.010868486016988754 tr(WHW.T) 3.732954978942871
  0%|          | 0/32 [00:00<?, ?it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]100%|██████████| 32/32 [00:11<00:00,  2.69it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  2.70it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.69it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.69it/s]  3%|▎         | 1/32 [00:01<01:00,  1.94s/it] 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.69it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.69it/s]I0402 09:28:54.987093 1570709 finetune.py:45] layer 9_o initial loss 0.0031549532432109118
W0402 09:28:54.987327 1570709 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it] 94%|█████████▍| 30/32 [00:11<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.57it/s]W0402 09:28:56.037174 1570709 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:12<00:00,  2.56it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
  9%|▉         | 3/32 [00:04<00:46,  1.62s/it]9_o proxy err 0.011053637601435184 tr(WHW.T) 4.439431190490723
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it]I0402 09:28:58.885833 1571323 finetune.py:45] layer 11_o initial loss 0.002648388035595417
W0402 09:28:58.886001 1571323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<01:00,  1.96s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it]W0402 09:28:59.996168 1571323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:03<00:50,  1.68s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it]11_o proxy err 0.011281562969088554 tr(WHW.T) 4.454067230224609
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:04<00:45,  1.57s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it]I0402 09:29:03.533632 1570888 finetune.py:45] layer 10_o initial loss 0.0019645055290311575
W0402 09:29:03.534101 1570888 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it]  6%|▋         | 2/32 [00:03<00:50,  1.67s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.48s/it]W0402 09:29:05.128780 1570888 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it]  9%|▉         | 3/32 [00:04<00:45,  1.55s/it]10_o proxy err 0.01089409925043583 tr(WHW.T) 4.188048362731934
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:09<00:37,  1.45s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.48s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.50s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.43s/it]  3%|▎         | 1/32 [00:01<01:00,  1.95s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.47s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.42s/it]  6%|▋         | 2/32 [00:03<00:50,  1.67s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.47s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.45s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.41s/it]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it] 41%|████      | 13/32 [00:19<00:27,  1.47s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.44s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.40s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.55s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.46s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.40s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.52s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.42s/it] 38%|███▊      | 12/32 [00:17<00:27,  1.40s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it] 50%|█████     | 16/32 [00:24<00:23,  1.46s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 41%|████      | 13/32 [00:18<00:26,  1.40s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.41s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.40s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.50s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.39s/it] 41%|████      | 13/32 [00:18<00:26,  1.40s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.46s/it] 50%|█████     | 16/32 [00:23<00:22,  1.39s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.41s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.46s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.39s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.41s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.39s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.46s/it] 50%|█████     | 16/32 [00:23<00:22,  1.41s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.39s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.39s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.39s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.48s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 69%|██████▉   | 22/32 [00:31<00:13,  1.40s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.39s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.39s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.41s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.39s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.39s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.41s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.46s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.49s/it] 84%|████████▍ | 27/32 [00:38<00:06,  1.40s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.41s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.39s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.41s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.46s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.39s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.41s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.39s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.41s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.50s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.39s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.41s/it]100%|██████████| 32/32 [00:45<00:00,  1.39s/it]100%|██████████| 32/32 [00:45<00:00,  1.42s/it]
 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.42s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.51s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.44s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.52s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.44s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.52s/it]100%|██████████| 32/32 [00:45<00:00,  1.46s/it]100%|██████████| 32/32 [00:45<00:00,  1.44s/it]
I0402 09:29:47.761445 1570161 finetune.py:45] layer 8_up initial loss 0.002642255276441574
W0402 09:29:47.761678 1570161 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:29:48.594324 1570161 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it]8_up proxy err 0.009687996469438076 tr(WHW.T) 668.69921875
  0%|          | 0/32 [00:00<?, ?it/s]I0402 09:29:49.967480 1570709 finetune.py:45] layer 9_up initial loss 0.0031263057608157396
W0402 09:29:49.967755 1570709 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:43<00:04,  1.51s/it]W0402 09:29:50.845459 1570709 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:57,  1.84s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.50s/it]9_up proxy err 0.009322119876742363 tr(WHW.T) 723.7196044921875
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:03<00:48,  1.61s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.49s/it]  3%|▎         | 1/32 [00:01<00:59,  1.91s/it]  9%|▉         | 3/32 [00:04<00:44,  1.53s/it]100%|██████████| 32/32 [00:48<00:00,  1.48s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
I0402 09:29:54.669500 1571323 finetune.py:45] layer 11_up initial loss 0.0025811910163611174
W0402 09:29:54.669686 1571323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:03<00:49,  1.66s/it]W0402 09:29:55.493178 1571323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it]11_up proxy err 0.00939381867647171 tr(WHW.T) 789.370361328125
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:04<00:45,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.52s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it]  3%|▎         | 1/32 [00:01<00:56,  1.81s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.50s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it]  6%|▋         | 2/32 [00:03<00:47,  1.59s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.45s/it]  9%|▉         | 3/32 [00:04<00:44,  1.52s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.44s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.48s/it]I0402 09:30:03.588206 1570888 finetune.py:45] layer 10_up initial loss 0.0019407524960115552
W0402 09:30:03.588595 1570888 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it]W0402 09:30:04.900243 1570888 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.45s/it]10_up proxy err 0.00944727472960949 tr(WHW.T) 749.0686645507812
  0%|          | 0/32 [00:00<?, ?it/s] 34%|███▍      | 11/32 [00:16<00:31,  1.48s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.44s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it]  3%|▎         | 1/32 [00:01<00:58,  1.88s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.41s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it]  6%|▋         | 2/32 [00:03<00:49,  1.65s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 41%|████      | 13/32 [00:18<00:26,  1.41s/it]  9%|▉         | 3/32 [00:04<00:45,  1.57s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.47s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.40s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.41s/it] 50%|█████     | 16/32 [00:23<00:23,  1.47s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.40s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.52s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it] 50%|█████     | 16/32 [00:23<00:22,  1.41s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.47s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it] 41%|████      | 13/32 [00:18<00:26,  1.42s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.47s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.40s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.47s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.40s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.40s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.42s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.48s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.40s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.40s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.42s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.46s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.40s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.46s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.40s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.42s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.46s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.41s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.42s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.41s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.42s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.41s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.42s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.40s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.46s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.40s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.42s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.47s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.40s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.42s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
100%|██████████| 32/32 [00:45<00:00,  1.40s/it]100%|██████████| 32/32 [00:45<00:00,  1.42s/it]
 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.42s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.50s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.44s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.50s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.45s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it]100%|██████████| 32/32 [00:46<00:00,  1.47s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 78%|███████▊  | 25/32 [00:37<00:10,  1.51s/it]I0402 09:30:45.118857 1570709 finetune.py:45] layer 9_gate initial loss 0.00306285941042006
W0402 09:30:45.119129 1570709 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:39<00:09,  1.52s/it]I0402 09:30:45.664121 1570161 finetune.py:45] layer 8_gate initial loss 0.0025921049527823925
W0402 09:30:45.664310 1570161 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:30:45.871009 1570709 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:30:46.414999 1570161 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:40<00:07,  1.51s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it]9_gate proxy err 0.003614341141656041 tr(WHW.T) 3179.91943359375
  0%|          | 0/112 [00:00<?, ?it/s]8_gate proxy err 0.0037422131281346083 tr(WHW.T) 2931.24951171875
  0%|          | 0/112 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:43<00:04,  1.59s/it]  1%|          | 1/112 [00:01<02:07,  1.15s/it]  1%|          | 1/112 [00:01<02:06,  1.14s/it]I0402 09:30:50.542833 1571323 finetune.py:45] layer 11_gate initial loss 0.0025512254796922207
W0402 09:30:50.543156 1571323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  2%|▏         | 2/112 [00:01<01:16,  1.44it/s]  2%|▏         | 2/112 [00:01<01:15,  1.46it/s]  3%|▎         | 3/112 [00:01<00:58,  1.85it/s]  3%|▎         | 3/112 [00:01<00:59,  1.83it/s]  4%|▎         | 4/112 [00:02<00:50,  2.15it/s]W0402 09:30:51.325376 1571323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:45<00:03,  1.57s/it]  4%|▍         | 5/112 [00:02<00:45,  2.34it/s]  4%|▎         | 4/112 [00:02<00:51,  2.09it/s]  5%|▌         | 6/112 [00:02<00:42,  2.47it/s]  4%|▍         | 5/112 [00:02<00:46,  2.28it/s]  6%|▋         | 7/112 [00:03<00:41,  2.55it/s]  5%|▌         | 6/112 [00:03<00:44,  2.39it/s]  7%|▋         | 8/112 [00:03<00:40,  2.58it/s]  6%|▋         | 7/112 [00:03<00:42,  2.48it/s]  8%|▊         | 9/112 [00:04<00:39,  2.63it/s] 97%|█████████▋| 31/32 [00:46<00:01,  1.55s/it]  7%|▋         | 8/112 [00:03<00:40,  2.55it/s]  9%|▉         | 10/112 [00:04<00:37,  2.69it/s]  8%|▊         | 9/112 [00:04<00:39,  2.59it/s] 10%|▉         | 11/112 [00:04<00:37,  2.72it/s]  9%|▉         | 10/112 [00:04<00:39,  2.61it/s] 11%|█         | 12/112 [00:05<00:36,  2.76it/s]11_gate proxy err 0.0035410821437835693 tr(WHW.T) 3167.24609375
  0%|          | 0/112 [00:00<?, ?it/s] 10%|▉         | 11/112 [00:04<00:38,  2.64it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.80it/s]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
 11%|█         | 12/112 [00:05<00:37,  2.67it/s] 12%|█▎        | 14/112 [00:05<00:34,  2.83it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.66it/s]  1%|          | 1/112 [00:00<01:29,  1.23it/s] 13%|█▎        | 15/112 [00:06<00:34,  2.80it/s] 12%|█▎        | 14/112 [00:06<00:37,  2.63it/s]  2%|▏         | 2/112 [00:01<01:01,  1.79it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.77it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.61it/s]  3%|▎         | 3/112 [00:01<00:51,  2.11it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.77it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.60it/s]  4%|▎         | 4/112 [00:01<00:46,  2.32it/s] 16%|█▌        | 18/112 [00:07<00:33,  2.78it/s]  4%|▍         | 5/112 [00:02<00:43,  2.46it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.59it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.80it/s]  5%|▌         | 6/112 [00:02<00:41,  2.56it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.61it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.81it/s]  6%|▋         | 7/112 [00:03<00:40,  2.60it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.62it/s] 19%|█▉        | 21/112 [00:08<00:32,  2.80it/s]  7%|▋         | 8/112 [00:03<00:39,  2.66it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.64it/s] 20%|█▉        | 22/112 [00:08<00:31,  2.82it/s]  8%|▊         | 9/112 [00:03<00:38,  2.70it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.66it/s] 21%|██        | 23/112 [00:09<00:31,  2.83it/s]  9%|▉         | 10/112 [00:04<00:37,  2.73it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.84it/s] 20%|█▉        | 22/112 [00:09<00:33,  2.65it/s] 10%|▉         | 11/112 [00:04<00:36,  2.75it/s] 22%|██▏       | 25/112 [00:09<00:30,  2.85it/s] 21%|██        | 23/112 [00:09<00:33,  2.64it/s] 11%|█         | 12/112 [00:04<00:36,  2.73it/s] 23%|██▎       | 26/112 [00:10<00:30,  2.81it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.63it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.73it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.83it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.63it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.76it/s] 25%|██▌       | 28/112 [00:10<00:29,  2.85it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.64it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.76it/s] 26%|██▌       | 29/112 [00:11<00:29,  2.85it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.63it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.76it/s] 27%|██▋       | 30/112 [00:11<00:28,  2.86it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.63it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.75it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.83it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.61it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.75it/s] 29%|██▊       | 32/112 [00:12<00:28,  2.83it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.62it/s] 29%|██▉       | 33/112 [00:12<00:27,  2.84it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.76it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.62it/s] 30%|███       | 34/112 [00:12<00:27,  2.81it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.74it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.63it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.80it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.72it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.63it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.81it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.73it/s] 30%|███       | 34/112 [00:13<00:29,  2.63it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.82it/s] 21%|██        | 23/112 [00:08<00:32,  2.74it/s]I0402 09:31:03.197365 1570888 finetune.py:45] layer 10_gate initial loss 0.001907426049001515
W0402 09:31:03.197729 1570888 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 38/112 [00:14<00:26,  2.82it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.63it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.76it/s] 35%|███▍      | 39/112 [00:14<00:25,  2.83it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.62it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.77it/s] 36%|███▌      | 40/112 [00:15<00:25,  2.84it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.65it/s]W0402 09:31:04.119532 1570888 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 23%|██▎       | 26/112 [00:09<00:30,  2.78it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.84it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.78it/s] 34%|███▍      | 38/112 [00:15<00:27,  2.66it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.85it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.78it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.64it/s] 38%|███▊      | 43/112 [00:16<00:24,  2.84it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.77it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.63it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.82it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.76it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.61it/s] 40%|████      | 45/112 [00:16<00:23,  2.84it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.76it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.60it/s] 41%|████      | 46/112 [00:17<00:23,  2.84it/s] 29%|██▊       | 32/112 [00:12<00:28,  2.76it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.63it/s] 42%|████▏     | 47/112 [00:17<00:22,  2.85it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.77it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.66it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.85it/s] 30%|███       | 34/112 [00:12<00:28,  2.77it/s] 40%|████      | 45/112 [00:17<00:25,  2.67it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.84it/s]10_gate proxy err 0.0036524550523608923 tr(WHW.T) 3045.35498046875
  0%|          | 0/112 [00:00<?, ?it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.76it/s] 41%|████      | 46/112 [00:18<00:24,  2.69it/s] 45%|████▍     | 50/112 [00:18<00:21,  2.85it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.76it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.69it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.83it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.75it/s]  1%|          | 1/112 [00:00<01:32,  1.21it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.70it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.84it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.75it/s]  2%|▏         | 2/112 [00:01<01:01,  1.78it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.69it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.80it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.72it/s]  3%|▎         | 3/112 [00:01<00:51,  2.10it/s] 45%|████▍     | 50/112 [00:19<00:22,  2.70it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.80it/s] 36%|███▌      | 40/112 [00:14<00:26,  2.73it/s]  4%|▎         | 4/112 [00:01<00:47,  2.28it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.70it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.81it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.74it/s]  4%|▍         | 5/112 [00:02<00:44,  2.41it/s] 50%|█████     | 56/112 [00:20<00:19,  2.83it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.70it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.76it/s]  5%|▌         | 6/112 [00:02<00:42,  2.49it/s] 51%|█████     | 57/112 [00:21<00:19,  2.83it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.70it/s] 38%|███▊      | 43/112 [00:16<00:24,  2.77it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.83it/s]  6%|▋         | 7/112 [00:03<00:41,  2.53it/s] 48%|████▊     | 54/112 [00:21<00:21,  2.69it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.79it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.85it/s]  7%|▋         | 8/112 [00:03<00:40,  2.58it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.70it/s] 40%|████      | 45/112 [00:16<00:23,  2.79it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.86it/s]  8%|▊         | 9/112 [00:03<00:39,  2.60it/s] 50%|█████     | 56/112 [00:21<00:20,  2.70it/s] 41%|████      | 46/112 [00:17<00:23,  2.77it/s] 54%|█████▍    | 61/112 [00:22<00:17,  2.84it/s]  9%|▉         | 10/112 [00:04<00:39,  2.61it/s] 51%|█████     | 57/112 [00:22<00:20,  2.69it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.78it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.85it/s] 10%|▉         | 11/112 [00:04<00:38,  2.62it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.69it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.76it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.84it/s] 11%|█         | 12/112 [00:04<00:37,  2.64it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.70it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.77it/s] 57%|█████▋    | 64/112 [00:23<00:16,  2.85it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.64it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.70it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.77it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.85it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.65it/s] 54%|█████▍    | 61/112 [00:23<00:18,  2.70it/s] 46%|████▌     | 51/112 [00:18<00:22,  2.76it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.85it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.65it/s] 55%|█████▌    | 62/112 [00:24<00:18,  2.69it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.77it/s] 60%|█████▉    | 67/112 [00:24<00:15,  2.85it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.66it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.70it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.78it/s] 61%|██████    | 68/112 [00:24<00:15,  2.85it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.66it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.70it/s] 48%|████▊     | 54/112 [00:20<00:20,  2.78it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.85it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.70it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.66it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.78it/s] 62%|██████▎   | 70/112 [00:25<00:14,  2.86it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.70it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.66it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.85it/s] 50%|█████     | 56/112 [00:20<00:20,  2.77it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.67it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.64it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.81it/s] 51%|█████     | 57/112 [00:21<00:20,  2.73it/s] 61%|██████    | 68/112 [00:26<00:16,  2.68it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.83it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.65it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.74it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.84it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.68it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.65it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.75it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.84it/s] 62%|██████▎   | 70/112 [00:27<00:15,  2.67it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.76it/s] 21%|██        | 23/112 [00:09<00:33,  2.65it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.85it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.77it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.68it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.66it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.86it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.78it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.68it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.66it/s] 70%|██████▉   | 78/112 [00:28<00:11,  2.85it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.78it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.65it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.66it/s] 71%|███████   | 79/112 [00:28<00:11,  2.86it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.78it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.67it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.67it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.86it/s] 58%|█████▊    | 65/112 [00:24<00:16,  2.79it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.67it/s] 25%|██▌       | 28/112 [00:10<00:31,  2.66it/s] 72%|███████▏  | 81/112 [00:29<00:10,  2.83it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.75it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.68it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.66it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.84it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.76it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.69it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.66it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.84it/s] 61%|██████    | 68/112 [00:25<00:15,  2.77it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.69it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.67it/s] 75%|███████▌  | 84/112 [00:30<00:09,  2.85it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.79it/s] 71%|███████   | 79/112 [00:30<00:12,  2.70it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.68it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.86it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.80it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.69it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.66it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.86it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.79it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.69it/s] 30%|███       | 34/112 [00:13<00:29,  2.66it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.87it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.80it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.70it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.88it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.67it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.79it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.70it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.87it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.65it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.79it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.70it/s] 80%|████████  | 90/112 [00:32<00:07,  2.87it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.66it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.77it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.82it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.70it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.65it/s] 68%|██████▊   | 76/112 [00:27<00:13,  2.72it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.79it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.69it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.65it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.73it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.82it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.70it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.66it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.75it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.83it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.71it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.66it/s] 71%|███████   | 79/112 [00:29<00:11,  2.76it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.83it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.70it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.65it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.77it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.84it/s] 80%|████████  | 90/112 [00:34<00:08,  2.71it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.66it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.78it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.85it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.72it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.66it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.78it/s] 88%|████████▊ | 98/112 [00:35<00:04,  2.85it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.71it/s] 40%|████      | 45/112 [00:17<00:25,  2.67it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.78it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.86it/s] 83%|████████▎ | 93/112 [00:35<00:06,  2.72it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.77it/s] 41%|████      | 46/112 [00:17<00:25,  2.64it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.84it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.71it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.78it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.64it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.83it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.71it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.78it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.65it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.84it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.72it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.79it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.66it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.84it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.71it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.79it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.84it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.66it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.71it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.79it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.85it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.68it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.72it/s] 80%|████████  | 90/112 [00:33<00:07,  2.80it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.85it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.68it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.72it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.80it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.85it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.68it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.72it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.80it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.86it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.68it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.71it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.77it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.79it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.67it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.71it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.75it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.78it/s] 50%|█████     | 56/112 [00:21<00:20,  2.67it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.71it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.75it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.80it/s] 51%|█████     | 57/112 [00:21<00:20,  2.68it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.70it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.73it/s]100%|██████████| 112/112 [00:40<00:00,  2.78it/s]100%|██████████| 112/112 [00:40<00:00,  2.77it/s]
 52%|█████▏    | 58/112 [00:22<00:20,  2.67it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.70it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.74it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.68it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.71it/s] 88%|████████▊ | 98/112 [00:35<00:05,  2.70it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.66it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.69it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.66it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.67it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.70it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.69it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.68it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.71it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.70it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.69it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.71it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.66it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.68it/s]100%|██████████| 112/112 [00:42<00:00,  2.71it/s]100%|██████████| 112/112 [00:42<00:00,  2.63it/s]
 92%|█████████▏| 103/112 [00:37<00:03,  2.70it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.69it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.73it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.64it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.74it/s] 60%|█████▉    | 67/112 [00:25<00:17,  2.61it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.75it/s] 61%|██████    | 68/112 [00:26<00:17,  2.59it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.75it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.58it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.77it/s] 62%|██████▎   | 70/112 [00:26<00:16,  2.62it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.78it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.61it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.78it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.62it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.77it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.63it/s]100%|██████████| 112/112 [00:41<00:00,  2.76it/s]100%|██████████| 112/112 [00:41<00:00,  2.73it/s]
 66%|██████▌   | 74/112 [00:28<00:14,  2.65it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.65it/s]W0402 09:31:36.104000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.104000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.104000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.104000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.105000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.105000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.105000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.147000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.147000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.147000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.147000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.148000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.164000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.164000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.164000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.164000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.164000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.336000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.337000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.337000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.337000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.337000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 68%|██████▊   | 76/112 [00:29<00:13,  2.66it/s]W0402 09:31:36.653000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.653000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.654000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.654000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.654000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.654000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.654000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.686000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.686000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.686000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.686000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.686000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.753000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.753000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.753000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.753000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:36.753000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 77/112 [00:29<00:13,  2.66it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.66it/s] 71%|███████   | 79/112 [00:30<00:12,  2.67it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.68it/s]W0402 09:31:37.893000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:37.905000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:37.913000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:37.913000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 81/112 [00:30<00:11,  2.68it/s]W0402 09:31:38.348000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.348000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.348000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.348000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.348000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.348000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.348000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.377000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.377000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.377000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.377000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.377000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 73%|███████▎  | 82/112 [00:31<00:11,  2.68it/s]W0402 09:31:38.704000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.704000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.704000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.704000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.704000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.704000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.704000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.704000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.985000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.985000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.985000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.985000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:38.985000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 74%|███████▍  | 83/112 [00:31<00:10,  2.66it/s]W0402 09:31:39.195000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.195000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.195000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.196000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.196000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.196000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.196000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.236000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.236000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.236000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.236000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.236000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.251000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.252000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.252000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.252000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.252000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.304000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.309000 140015097874240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 84/112 [00:32<00:10,  2.68it/s]W0402 09:31:39.415000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.415000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.415000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.415000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.415000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.726000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.726000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.726000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.726000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.726000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.727000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.727000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 76%|███████▌  | 85/112 [00:32<00:10,  2.68it/s]W0402 09:31:39.757000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.758000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.758000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.758000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.758000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.827000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.827000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.827000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.827000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:39.827000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 77%|███████▋  | 86/112 [00:32<00:09,  2.66it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.66it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.66it/s]W0402 09:31:40.974000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:40.986000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:40.994000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:40.994000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 79%|███████▉  | 89/112 [00:33<00:08,  2.63it/s]W0402 09:31:41.430000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:41.430000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:41.430000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:41.430000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:41.430000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:41.430000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:41.430000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:31:41.459000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:41.460000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:41.460000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:41.460000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:41.460000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 80%|████████  | 90/112 [00:34<00:08,  2.67it/s]W0402 09:31:41.794000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:41.794000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:41.794000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:41.794000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:41.794000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:41.794000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:41.794000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:41.794000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 91/112 [00:34<00:07,  2.68it/s]W0402 09:31:42.014000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.014000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.014000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.014000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.015000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.015000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.015000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.057000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.057000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.057000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.057000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.057000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.074000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.074000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.074000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.074000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.074000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.087000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.088000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.088000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.088000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.088000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.248000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.249000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.249000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.249000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.249000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 82%|████████▏ | 92/112 [00:35<00:07,  2.68it/s]W0402 09:31:42.412000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.417000 139648011835200 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.582000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.582000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.582000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.582000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.582000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.582000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.582000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.615000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.615000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.615000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.615000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.615000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.689000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.689000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.689000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.689000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:42.689000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 83%|████████▎ | 93/112 [00:35<00:07,  2.66it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.66it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.69it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.70it/s]W0402 09:31:43.899000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:43.912000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:43.920000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:43.921000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 87%|████████▋ | 97/112 [00:36<00:05,  2.68it/s]W0402 09:31:44.384000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:44.384000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:44.385000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:44.385000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:44.385000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:31:44.385000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:44.385000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:44.417000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:44.417000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:44.417000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:44.417000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:44.417000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 98/112 [00:37<00:05,  2.66it/s]W0402 09:31:44.775000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:44.775000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:44.776000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:44.776000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:44.776000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:44.776000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:31:44.776000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:44.776000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 99/112 [00:37<00:04,  2.64it/s]W0402 09:31:45.083000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:45.083000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:45.083000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:45.083000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:45.083000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 89%|████████▉ | 100/112 [00:38<00:04,  2.63it/s]W0402 09:31:45.431000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:45.437000 139810029299520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 90%|█████████ | 101/112 [00:38<00:04,  2.61it/s]I0402 09:31:45.938041 1570709 finetune.py:45] layer 9_down initial loss 0.0030287564732134342
W0402 09:31:45.938325 1570709 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 102/112 [00:38<00:03,  2.60it/s]W0402 09:31:46.415643 1570709 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 92%|█████████▏| 103/112 [00:39<00:03,  2.60it/s]9_down proxy err 0.012205502018332481 tr(WHW.T) 8.00308609008789
 93%|█████████▎| 104/112 [00:39<00:03,  2.60it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.59it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.59it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.59it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.59it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.59it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.60it/s]I0402 09:31:49.589023 1570161 finetune.py:45] layer 8_down initial loss 0.002544575370848179
W0402 09:31:49.589432 1570161 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 99%|█████████▉| 111/112 [00:42<00:00,  2.60it/s]100%|██████████| 112/112 [00:42<00:00,  2.62it/s]100%|██████████| 112/112 [00:42<00:00,  2.62it/s]
W0402 09:31:50.084277 1570161 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

8_down proxy err 0.012242653407156467 tr(WHW.T) 7.259359359741211
I0402 09:31:52.018155 1571323 finetune.py:45] layer 11_down initial loss 0.002513075014576316
W0402 09:31:52.018378 1571323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:31:52.661270 1571323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

11_down proxy err 0.011821506544947624 tr(WHW.T) 9.07177734375
I0402 09:31:53.948897 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 12 in 1.1793525218963623s
I0402 09:31:54.424476 1558010 quantize_finetune_llama.py:159] layer 13 gpu 1
I0402 09:31:56.428357 1575525 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:31:56.428482 1575525 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:31:56.428540 1575525 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:31:56.770008 1575525 config.py:58] PyTorch version 2.4.0 available.
W0402 09:31:57.355000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.355000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.355000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.355000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.355000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.355000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.356000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.397000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.397000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.397000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.397000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.397000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.413000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.413000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.413000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.413000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.413000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.601000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.601000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.601000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.601000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.601000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.915000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.915000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.915000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.915000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.915000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.915000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.916000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.948000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.948000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.948000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.948000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:57.948000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:58.017000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:58.017000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:58.017000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:58.017000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:58.017000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
I0402 09:31:59.112372 1575525 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 09:31:59.183000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.195000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.203000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.203000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.467429 1575525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:31:59.637000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.637000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.637000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.637000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.637000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.638000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.638000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.668000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.669000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.669000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.669000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.669000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.997000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.997000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.998000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.998000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.998000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.998000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.998000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:31:59.998000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:32:00.292000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:00.292000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:00.292000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:00.292000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:00.292000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:32:00.616000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:00.621000 139935914882880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:46,  1.51s/it]  6%|▋         | 2/32 [00:01<00:24,  1.24it/s]  9%|▉         | 3/32 [00:02<00:16,  1.72it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.11it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 19%|█▉        | 6/32 [00:03<00:09,  2.64it/s] 22%|██▏       | 7/32 [00:03<00:08,  2.80it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.92it/s] 28%|██▊       | 9/32 [00:04<00:07,  3.00it/s] 31%|███▏      | 10/32 [00:04<00:07,  3.04it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.11it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.16it/s] 41%|████      | 13/32 [00:05<00:05,  3.18it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.19it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.21it/s] 50%|█████     | 16/32 [00:06<00:04,  3.20it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.21it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.22it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.21it/s]I0402 09:32:07.613235 1570888 finetune.py:45] layer 10_down initial loss 0.0018897915724664927
W0402 09:32:07.613451 1570888 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 20/32 [00:07<00:03,  3.20it/s]W0402 09:32:08.082675 1570888 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 66%|██████▌   | 21/32 [00:07<00:03,  3.21it/s]10_down proxy err 0.012196337804198265 tr(WHW.T) 8.421324729919434
 69%|██████▉   | 22/32 [00:08<00:03,  3.20it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.18it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.20it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.21it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.22it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.21it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.22it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.19it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.22it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.23it/s]100%|██████████| 32/32 [00:11<00:00,  3.23it/s]100%|██████████| 32/32 [00:11<00:00,  2.87it/s]
I0402 09:32:11.841292 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 13 in 1.223698377609253s
I0402 09:32:12.308092 1558010 quantize_finetune_llama.py:159] layer 14 gpu 2
I0402 09:32:14.110314 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 14 in 1.3911006450653076s
I0402 09:32:14.334866 1576238 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:32:14.335009 1576238 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:32:14.335067 1576238 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:32:14.586117 1576238 config.py:58] PyTorch version 2.4.0 available.
I0402 09:32:14.621883 1558010 quantize_finetune_llama.py:159] layer 15 gpu 3
W0402 09:32:14.663000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:32:14.663000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:14.663000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:14.663000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:14.663000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:14.663000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:14.663000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:14.690000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:14.690000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:14.691000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:14.691000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:14.691000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:14.708000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:14.708000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:14.708000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:14.708000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:14.708000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:15.026000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:15.027000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:15.027000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:15.027000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:15.027000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:15.909000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:15.909000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:32:15.910000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:15.910000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:15.910000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:15.910000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:15.910000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:15.927000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:15.927000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:15.927000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:15.927000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:15.927000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:16.157000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:16.157000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:16.157000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:16.157000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:16.157000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
I0402 09:32:16.212259 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 15 in 1.1893978118896484s
I0402 09:32:16.572680 1576403 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:32:16.572962 1576403 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:32:16.573090 1576403 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:32:16.692690 1558010 quantize_finetune_llama.py:159] layer 16 gpu 0
I0402 09:32:16.697442 1576238 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 09:32:16.953715 1576403 config.py:58] PyTorch version 2.4.0 available.
W0402 09:32:17.021214 1576238 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:32:17.362000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:17.362000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:17.362000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:32:17.362000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:17.362000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:17.362000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:17.362000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:17.380000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:17.381000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:17.381000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:17.381000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:17.381000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:32:18.279000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:18.279000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:18.279000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:18.279000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:18.279000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
I0402 09:32:18.694983 1576773 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:32:18.695134 1576773 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:32:18.695194 1576773 utils.py:162] NumExpr defaulting to 16 threads.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0402 09:32:18.904047 1576773 config.py:58] PyTorch version 2.4.0 available.
I0402 09:32:19.301026 1576403 data_utils.py:336] using 256 training seqs, 128 validation seqs
  3%|▎         | 1/32 [00:01<00:45,  1.45s/it]W0402 09:32:19.646782 1576403 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:01<00:23,  1.25it/s]  9%|▉         | 3/32 [00:02<00:17,  1.69it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.03it/s]  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.47it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.60it/s]I0402 09:32:21.519781 1576773 data_utils.py:336] using 256 training seqs, 128 validation seqs
 25%|██▌       | 8/32 [00:03<00:08,  2.70it/s]W0402 09:32:21.887200 1576773 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:04<00:08,  2.77it/s]  3%|▎         | 1/32 [00:01<00:52,  1.70s/it] 31%|███▏      | 10/32 [00:04<00:07,  2.82it/s]  6%|▋         | 2/32 [00:02<00:27,  1.10it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s]  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:02<00:18,  1.53it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.86it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.87it/s] 41%|████      | 13/32 [00:05<00:06,  2.87it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.16it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.89it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.35it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.90it/s]I0402 09:32:24.351263 1575525 finetune.py:45] layer 12_v initial loss 0.002788012148812413
W0402 09:32:24.351680 1575525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:03<00:10,  2.50it/s] 50%|█████     | 16/32 [00:06<00:05,  2.93it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.61it/s]  3%|▎         | 1/32 [00:01<00:54,  1.75s/it] 53%|█████▎    | 17/32 [00:06<00:05,  2.95it/s]  6%|▋         | 2/32 [00:02<00:27,  1.09it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.71it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.99it/s]W0402 09:32:25.445785 1575525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:02<00:18,  1.54it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.01it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.90it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.00it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.83it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.04it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.20it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.89it/s]12_v proxy err 0.009097253903746605 tr(WHW.T) 68.45219421386719
  0%|          | 0/32 [00:00<?, ?it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.07it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.43it/s] 41%|████      | 13/32 [00:05<00:06,  2.94it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.06it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.60it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.96it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.08it/s] 25%|██▌       | 8/32 [00:04<00:08,  2.73it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.97it/s]  3%|▎         | 1/32 [00:00<00:27,  1.13it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.09it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.83it/s] 50%|█████     | 16/32 [00:06<00:05,  3.00it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.10it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.88it/s] 53%|█████▎    | 17/32 [00:07<00:04,  3.01it/s]  9%|▉         | 3/32 [00:01<00:13,  2.15it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.11it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.94it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.03it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.40it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.12it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.99it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.04it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.55it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.09it/s] 41%|████      | 13/32 [00:05<00:06,  3.00it/s] 62%|██████▎   | 20/32 [00:08<00:03,  3.03it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.66it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.10it/s] 44%|████▍     | 14/32 [00:06<00:05,  3.04it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.06it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.76it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.11it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.06it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.07it/s]100%|██████████| 32/32 [00:11<00:00,  3.11it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
 25%|██▌       | 8/32 [00:03<00:08,  2.79it/s] 50%|█████     | 16/32 [00:06<00:05,  3.03it/s] 72%|███████▏  | 23/32 [00:09<00:02,  3.05it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.77it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.98it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.99it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.80it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.00it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.01it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.95it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.96it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.82it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.94it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.99it/s] 41%|████      | 13/32 [00:05<00:06,  2.86it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.02it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.90it/s] 91%|█████████ | 29/32 [00:11<00:00,  3.03it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.88it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.90it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.04it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.90it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.87it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.06it/s] 50%|█████     | 16/32 [00:06<00:05,  2.91it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.86it/s]W0402 09:32:32.671000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:32:32.671000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:32.672000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:32.672000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:32.672000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:32.672000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:32.672000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:32.697000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:32.697000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:32.697000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:32.697000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:32.697000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:32.712000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:32.713000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:32.713000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:32.713000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:32.713000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  3.06it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
 53%|█████▎    | 17/32 [00:06<00:05,  2.91it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.87it/s]W0402 09:32:33.029000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:33.029000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:33.029000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:33.029000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:33.029000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:06<00:04,  2.88it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.88it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.87it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.87it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s]W0402 09:32:33.947000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:33.947000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:32:33.947000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:33.948000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:33.948000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:33.948000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:33.948000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:33.966000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:33.966000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:33.966000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:33.966000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:33.966000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s]W0402 09:32:34.198000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:34.198000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:34.198000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:34.199000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:34.199000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:07<00:03,  2.87it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.89it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.90it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.90it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.89it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.92it/s]W0402 09:32:35.322000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.322000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.322000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.323000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.323000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.323000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.323000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.340000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.340000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.340000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.340000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.340000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
 78%|███████▊  | 25/32 [00:09<00:02,  2.91it/s]W0402 09:32:35.748000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.749000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.749000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.749000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.749000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.749000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.749000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.775000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.775000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.775000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.775000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.775000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.791000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.791000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.792000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.792000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:35.792000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:09<00:02,  2.89it/s]W0402 09:32:36.119000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:36.119000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:36.119000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:36.119000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:36.119000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:36.243000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:36.243000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:36.243000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:36.243000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:36.243000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:09<00:01,  2.82it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 09:32:37.003000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:37.003000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:37.003000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:32:37.004000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:37.004000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:37.004000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:37.004000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:37.022000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:37.022000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:37.022000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:37.023000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:37.023000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:10<00:01,  2.75it/s]W0402 09:32:37.267000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:37.267000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:37.267000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:37.267000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:37.267000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.76it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.76it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]100%|██████████| 32/32 [00:11<00:00,  2.73it/s]
W0402 09:32:38.464000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:38.464000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:38.464000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:38.464000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:38.464000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:32:38.464000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:38.465000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:38.482000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:38.482000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:38.482000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:38.483000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:38.483000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.050000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.050000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.050000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.051000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.051000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.051000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.051000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.078000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.078000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.078000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.078000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.078000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.099000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.099000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.099000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.099000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.099000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.424000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.424000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.424000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.424000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.424000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.436000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.436000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.436000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.436000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:39.436000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 09:32:40.371000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:40.372000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:40.372000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:32:40.372000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:40.372000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:40.372000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:40.372000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:40.390000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:40.390000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:40.391000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:40.391000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:40.391000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:40.636000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:40.636000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:40.636000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:40.636000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:40.636000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:41.858000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:41.859000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:41.859000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:32:41.859000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:41.859000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:41.859000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:41.859000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:41.877000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:41.877000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:41.877000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:41.877000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:41.878000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
I0402 09:32:42.827883 1576238 finetune.py:45] layer 13_v initial loss 0.003858864540234208
W0402 09:32:42.828783 1576238 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:32:42.834000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:42.835000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:42.835000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:42.835000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:42.835000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 09:32:44.560755 1576238 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:32:44.831000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:32:44.831000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:44.831000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:44.831000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:44.831000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:44.831000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:44.831000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:44.860000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:44.860000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:44.860000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:44.860000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:44.860000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:44.875000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:44.875000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:44.875000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:44.875000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:44.875000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.033000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.034000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.034000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.034000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.034000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.260000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.260000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.260000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.260000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.260000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.261000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.261000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.281000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.281000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.281000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.281000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.281000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.343000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.343000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.343000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.343000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:45.343000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
I0402 09:32:45.410928 1576403 finetune.py:45] layer 14_v initial loss 0.0030982187017798424
W0402 09:32:45.411119 1576403 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

13_v proxy err 0.009518183767795563 tr(WHW.T) 71.04966735839844
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:32:46.197000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:46.470956 1576403 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:32:46.502000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:46.502000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:32:46.502000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:46.502000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:46.503000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:32:46.503000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:46.503000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:46.523000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:46.523000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:46.523000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:46.523000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:46.523000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:29,  1.07it/s]W0402 09:32:46.772000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:32:46.772000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:32:46.772000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:32:46.772000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:32:46.773000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:32:47.022000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:18,  1.64it/s]  9%|▉         | 3/32 [00:01<00:14,  2.00it/s]14_v proxy err 0.008708442561328411 tr(WHW.T) 74.802978515625
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s]  3%|▎         | 1/32 [00:00<00:29,  1.06it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s]  6%|▋         | 2/32 [00:01<00:18,  1.65it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s]I0402 09:32:49.040470 1576773 finetune.py:45] layer 15_v initial loss 0.0037907336372882128
W0402 09:32:49.041032 1576773 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:01<00:14,  2.01it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s]W0402 09:32:50.274212 1576773 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.66it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.55it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.65it/s]15_v proxy err 0.011341140605509281 tr(WHW.T) 67.86465454101562
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 50%|█████     | 16/32 [00:06<00:05,  2.67it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.56it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.67it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s]  3%|▎         | 1/32 [00:00<00:30,  1.02it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s]  6%|▋         | 2/32 [00:01<00:18,  1.61it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.67it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.59it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.67it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.17it/s]I0402 09:32:54.041989 1575525 finetune.py:45] layer 12_q initial loss 0.002784912008792162
W0402 09:32:54.043280 1575525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 66%|██████▌   | 21/32 [00:08<00:04,  2.66it/s] 50%|█████     | 16/32 [00:06<00:06,  2.63it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.66it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.66it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.63it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s]W0402 09:32:55.479577 1575525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.63it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.67it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.68it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.64it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.64it/s]12_q proxy err 0.0013029880356043577 tr(WHW.T) 6419.8271484375
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.71it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.73it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.64it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s]  3%|▎         | 1/32 [00:00<00:27,  1.12it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.75it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.64it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.75it/s]  6%|▋         | 2/32 [00:01<00:17,  1.71it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
 50%|█████     | 16/32 [00:06<00:05,  2.77it/s]  9%|▉         | 3/32 [00:01<00:14,  2.04it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.77it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.30it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.56it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.73it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.70it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.67it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.71it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.75it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.75it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.69it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.76it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.70it/s] 41%|████      | 13/32 [00:05<00:06,  2.74it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.72it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.74it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.71it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.74it/s] 50%|█████     | 16/32 [00:06<00:05,  2.75it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.76it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.78it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.79it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s]W0402 09:33:04.569000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.569000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.569000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.569000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.570000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.570000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.570000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.598000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.598000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.598000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.598000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.598000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s]W0402 09:33:04.613000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.613000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.613000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.613000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.613000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.766000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.766000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.766000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.766000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.767000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s]W0402 09:33:04.982000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.982000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.983000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.983000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.983000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.983000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:04.983000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.002000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.002000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.002000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.002000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.002000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.064000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.064000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.064000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.064000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.064000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.78it/s]W0402 09:33:05.793000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.793000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.794000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.794000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.794000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.794000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.794000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.824000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.824000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.824000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.824000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.824000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.840000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.840000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.840000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.840000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.840000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.915000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.992000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.992000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.992000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.993000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:05.993000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:09<00:02,  2.78it/s]W0402 09:33:06.208000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.208000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.208000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.208000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.208000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.209000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.209000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.217000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.217000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.217000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.218000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.218000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.218000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.218000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.228000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.228000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.228000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.228000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.228000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.239000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.239000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.239000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.239000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.239000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.289000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.290000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.290000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.290000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.290000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s]W0402 09:33:06.490000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.490000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.490000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.490000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.490000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:06.740000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:09<00:02,  2.80it/s]W0402 09:33:07.108000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s]W0402 09:33:07.403000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:07.403000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:07.403000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:07.403000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:33:07.403000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:33:07.403000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:07.403000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:07.423000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:07.423000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:33:07.423000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:07.423000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:07.423000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  2.80it/s]W0402 09:33:07.666000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:07.666000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:33:07.666000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:07.666000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:07.666000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:10<00:01,  2.81it/s]W0402 09:33:07.915000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.78it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
W0402 09:33:10.149000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.149000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.149000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.149000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.149000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.149000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.150000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.180000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.180000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.180000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.181000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.181000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.197000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.197000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.197000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.197000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.197000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.368000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.369000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.369000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.369000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.369000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.612000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.613000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.613000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.613000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.613000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.613000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.613000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.636000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.636000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.636000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.637000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.637000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.707000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.707000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.707000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.707000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:10.707000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:11.623000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:33:11.961000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:11.961000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:33:11.962000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:11.962000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:33:11.962000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:11.962000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:11.962000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:33:11.984000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:11.984000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:11.984000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:11.984000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:33:11.984000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:12.258000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:33:12.258000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:33:12.258000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:33:12.258000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:33:12.258000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:33:12.532000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0402 09:33:13.672755 1576238 finetune.py:45] layer 13_q initial loss 0.003862184938043356
W0402 09:33:13.673452 1576238 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 09:33:14.230422 1576403 finetune.py:45] layer 14_q initial loss 0.0031021274626255035
W0402 09:33:14.230765 1576403 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:33:14.723982 1576238 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:33:15.466202 1576403 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

13_q proxy err 0.002054140903055668 tr(WHW.T) 5299.77734375
  0%|          | 0/32 [00:00<?, ?it/s]I0402 09:33:16.583289 1575525 finetune.py:45] layer 12_k initial loss 0.0027832144405692816
W0402 09:33:16.584331 1575525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:00<00:27,  1.13it/s]14_q proxy err 0.0018487407360225916 tr(WHW.T) 5552.1181640625
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:01<00:17,  1.73it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.29it/s]  3%|▎         | 1/32 [00:00<00:29,  1.06it/s]W0402 09:33:17.897679 1575525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s]  6%|▋         | 2/32 [00:01<00:18,  1.62it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s]  9%|▉         | 3/32 [00:01<00:14,  1.97it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s]I0402 09:33:19.012701 1576773 finetune.py:45] layer 15_q initial loss 0.003787463763728738
W0402 09:33:19.013100 1576773 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s]12_k proxy err 0.0011691219406202435 tr(WHW.T) 4340.4931640625
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.65it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s]W0402 09:33:20.083898 1576773 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:00<00:23,  1.31it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s]  6%|▋         | 2/32 [00:01<00:15,  1.91it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.66it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.64it/s]  9%|▉         | 3/32 [00:01<00:12,  2.23it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s]15_q proxy err 0.0018617017194628716 tr(WHW.T) 6709.19140625
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.41it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.72it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.53it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.75it/s]  3%|▎         | 1/32 [00:00<00:26,  1.15it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.62it/s] 50%|█████     | 16/32 [00:06<00:06,  2.67it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s]  6%|▋         | 2/32 [00:01<00:17,  1.73it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.67it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.68it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s]  9%|▉         | 3/32 [00:01<00:13,  2.12it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.69it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.76it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.36it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.51it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.74it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.69it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.62it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.76it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.77it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.69it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.76it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.69it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s] 41%|████      | 13/32 [00:05<00:06,  2.76it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.79it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.70it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.76it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.77it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.80it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.78it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.69it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.77it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.80it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.81it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.81it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.79it/s] 41%|████      | 13/32 [00:05<00:06,  2.83it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.68it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.77it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.84it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.80it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.83it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.77it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s] 50%|█████     | 16/32 [00:06<00:05,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.69it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.78it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.83it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.80it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.81it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.84it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.76it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
 94%|█████████▍| 30/32 [00:11<00:00,  2.83it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.88it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.84it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.68it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 81%|████████▏ | 26/32 [00:09<00:02,  2.86it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.83it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.58it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.61it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.84it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.61it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.60it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s]100%|██████████| 32/32 [00:11<00:00,  2.81it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
 84%|████████▍ | 27/32 [00:10<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.53it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.55it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.56it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.57it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
I0402 09:33:34.680028 1576238 finetune.py:45] layer 13_k initial loss 0.0038646503817290068
W0402 09:33:34.680364 1576238 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 09:33:35.242101 1576403 finetune.py:45] layer 14_k initial loss 0.0031010310631245375
W0402 09:33:35.242366 1576403 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:33:35.672042 1576238 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:33:36.458370 1576403 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

13_k proxy err 0.001540293567813933 tr(WHW.T) 4513.9775390625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.23it/s]14_k proxy err 0.0013568372232839465 tr(WHW.T) 4950.548828125
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s]  9%|▉         | 3/32 [00:01<00:13,  2.10it/s]  3%|▎         | 1/32 [00:00<00:25,  1.23it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.28it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s]I0402 09:33:39.300111 1575525 finetune.py:45] layer 12_o initial loss 0.002813679398968816
W0402 09:33:39.300604 1575525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:01<00:13,  2.09it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.30it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s]W0402 09:33:40.438446 1575525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.59it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.60it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.62it/s]12_o proxy err 0.010605803690850735 tr(WHW.T) 5.624481201171875
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.63it/s]I0402 09:33:41.703514 1576773 finetune.py:45] layer 15_k initial loss 0.0037839398719370365
W0402 09:33:41.704312 1576773 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:04<00:07,  2.63it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.70it/s]W0402 09:33:42.808151 1576773 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 15/32 [00:06<00:06,  2.63it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s] 50%|█████     | 16/32 [00:06<00:06,  2.64it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.71it/s]  3%|▎         | 1/32 [00:01<00:58,  1.88s/it] 53%|█████▎    | 17/32 [00:06<00:05,  2.65it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.73it/s]15_k proxy err 0.0014395815087482333 tr(WHW.T) 4509.1435546875
  0%|          | 0/32 [00:00<?, ?it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.65it/s] 50%|█████     | 16/32 [00:06<00:05,  2.75it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.66it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.74it/s]  3%|▎         | 1/32 [00:00<00:24,  1.29it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.66it/s]  6%|▋         | 2/32 [00:03<00:47,  1.58s/it] 56%|█████▋    | 18/32 [00:07<00:05,  2.75it/s]  6%|▋         | 2/32 [00:01<00:15,  1.88it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s]  9%|▉         | 3/32 [00:01<00:13,  2.18it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.38it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.77it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.52it/s]  9%|▉         | 3/32 [00:04<00:43,  1.49s/it] 75%|███████▌  | 24/32 [00:09<00:03,  2.65it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.60it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.66it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.67it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.80it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.72it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.66it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.75it/s] 12%|█▎        | 4/32 [00:05<00:40,  1.43s/it] 88%|████████▊ | 28/32 [00:11<00:01,  2.66it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.79it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.78it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.65it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.75it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.66it/s] 16%|█▌        | 5/32 [00:07<00:38,  1.42s/it] 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.75it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 19%|█▉        | 6/32 [00:08<00:36,  1.40s/it] 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.74it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.70it/s] 22%|██▏       | 7/32 [00:10<00:35,  1.40s/it] 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.69it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.66it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.65it/s] 25%|██▌       | 8/32 [00:11<00:34,  1.42s/it] 78%|███████▊  | 25/32 [00:09<00:02,  2.64it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.63it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.60it/s] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 88%|████████▊ | 28/32 [00:10<00:01,  2.54it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.55it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.54it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.54it/s] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it]I0402 09:33:56.142488 1576238 finetune.py:45] layer 13_o initial loss 0.0038600254338234663
W0402 09:33:56.142833 1576238 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:12<00:00,  2.55it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
I0402 09:33:56.463098 1576403 finetune.py:45] layer 14_o initial loss 0.003070040373131633
W0402 09:33:56.463403 1576403 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:33:57.053979 1576238 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:33:57.465885 1576403 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:15<00:30,  1.43s/it]13_o proxy err 0.01060668658465147 tr(WHW.T) 6.745936870574951
  0%|          | 0/32 [00:00<?, ?it/s]14_o proxy err 0.011073713190853596 tr(WHW.T) 6.85344123840332
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it] 41%|████      | 13/32 [00:18<00:26,  1.42s/it]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.41s/it]  6%|▋         | 2/32 [00:03<00:49,  1.65s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.41s/it]  9%|▉         | 3/32 [00:04<00:45,  1.57s/it]I0402 09:34:04.441591 1576773 finetune.py:45] layer 15_o initial loss 0.003736719023436308
W0402 09:34:04.441913 1576773 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 50%|█████     | 16/32 [00:22<00:22,  1.41s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.51s/it]W0402 09:34:05.794512 1576773 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 17/32 [00:24<00:20,  1.39s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.55s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.48s/it]15_o proxy err 0.011811422184109688 tr(WHW.T) 6.766373634338379
  0%|          | 0/32 [00:00<?, ?it/s] 56%|█████▋    | 18/32 [00:25<00:19,  1.38s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.45s/it] 59%|█████▉    | 19/32 [00:26<00:17,  1.37s/it]  3%|▎         | 1/32 [00:01<01:00,  1.97s/it] 22%|██▏       | 7/32 [00:10<00:38,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.37s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.36s/it]  9%|▉         | 3/32 [00:05<00:46,  1.62s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.51s/it] 69%|██████▉   | 22/32 [00:31<00:13,  1.36s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.57s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.36s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.42s/it] 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it] 75%|███████▌  | 24/32 [00:33<00:10,  1.36s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.54s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.50s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.36s/it] 41%|████      | 13/32 [00:18<00:26,  1.41s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.52s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.36s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.41s/it] 84%|████████▍ | 27/32 [00:37<00:06,  1.35s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.50s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.40s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.36s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.50s/it] 50%|█████     | 16/32 [00:23<00:22,  1.41s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.36s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 94%|█████████▍| 30/32 [00:41<00:02,  1.36s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.51s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.41s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.36s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it]100%|██████████| 32/32 [00:44<00:00,  1.36s/it]100%|██████████| 32/32 [00:44<00:00,  1.39s/it]
 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 62%|██████▎   | 20/32 [00:28<00:17,  1.43s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 66%|██████▌   | 21/32 [00:30<00:16,  1.46s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.50s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.50s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.49s/it] 50%|█████     | 16/32 [00:24<00:24,  1.50s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.50s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.50s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.49s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 75%|███████▌  | 24/32 [00:35<00:12,  1.52s/it]I0402 09:34:34.144840 1575525 finetune.py:45] layer 12_up initial loss 0.0027685442473739386
W0402 09:34:34.145240 1575525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▋    | 18/32 [00:27<00:21,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it]W0402 09:34:34.978811 1575525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 78%|███████▊  | 25/32 [00:36<00:10,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.50s/it]12_up proxy err 0.008775413036346436 tr(WHW.T) 855.3870849609375
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:37<00:10,  1.50s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.50s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.50s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.50s/it]  3%|▎         | 1/32 [00:01<00:54,  1.76s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.48s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.50s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.51s/it]  6%|▋         | 2/32 [00:03<00:45,  1.52s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.46s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.50s/it]  9%|▉         | 3/32 [00:04<00:41,  1.45s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.50s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.45s/it] 12%|█▎        | 4/32 [00:05<00:39,  1.42s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.50s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.51s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.45s/it] 16%|█▌        | 5/32 [00:07<00:37,  1.39s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.50s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.44s/it] 19%|█▉        | 6/32 [00:08<00:35,  1.38s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.50s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it]100%|██████████| 32/32 [00:46<00:00,  1.43s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
 22%|██▏       | 7/32 [00:09<00:34,  1.38s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
 25%|██▌       | 8/32 [00:11<00:33,  1.38s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it] 28%|██▊       | 9/32 [00:12<00:32,  1.39s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it] 34%|███▍      | 11/32 [00:15<00:30,  1.44s/it]I0402 09:34:52.726022 1576403 finetune.py:45] layer 14_up initial loss 0.003025642829015851
W0402 09:34:52.726301 1576403 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 94%|█████████▍| 30/32 [00:45<00:03,  1.53s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.44s/it]W0402 09:34:53.591527 1576403 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:47<00:01,  1.50s/it]I0402 09:34:54.246169 1576238 finetune.py:45] layer 13_up initial loss 0.003778855549171567
W0402 09:34:54.246572 1576238 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:18<00:27,  1.43s/it]14_up proxy err 0.009306196123361588 tr(WHW.T) 932.0479125976562
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:34:55.187963 1576238 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:48<00:00,  1.48s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
 44%|████▍     | 14/32 [00:19<00:25,  1.41s/it]13_up proxy err 0.008649068884551525 tr(WHW.T) 916.3327026367188
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:57,  1.86s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.39s/it]  6%|▋         | 2/32 [00:03<00:48,  1.62s/it]  3%|▎         | 1/32 [00:01<01:00,  1.94s/it] 50%|█████     | 16/32 [00:22<00:22,  1.38s/it]  9%|▉         | 3/32 [00:04<00:44,  1.53s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it] 53%|█████▎    | 17/32 [00:23<00:20,  1.37s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.50s/it]  9%|▉         | 3/32 [00:04<00:46,  1.61s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.37s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.48s/it] 59%|█████▉    | 19/32 [00:26<00:17,  1.38s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.47s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.37s/it] 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it] 66%|██████▌   | 21/32 [00:29<00:14,  1.36s/it]I0402 09:35:05.625001 1576773 finetune.py:45] layer 15_up initial loss 0.0037194578908383846
W0402 09:35:05.625513 1576773 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it]W0402 09:35:06.583986 1576773 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:11<00:34,  1.45s/it] 69%|██████▉   | 22/32 [00:30<00:13,  1.36s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it]15_up proxy err 0.009346291422843933 tr(WHW.T) 988.9022827148438
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.36s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it] 75%|███████▌  | 24/32 [00:33<00:10,  1.36s/it]  3%|▎         | 1/32 [00:01<00:58,  1.90s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 78%|███████▊  | 25/32 [00:34<00:09,  1.36s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.44s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.35s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.44s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it] 84%|████████▍ | 27/32 [00:37<00:06,  1.35s/it] 41%|████      | 13/32 [00:19<00:27,  1.43s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.57s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.50s/it] 88%|████████▊ | 28/32 [00:38<00:05,  1.36s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.43s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.36s/it] 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.44s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it] 94%|█████████▍| 30/32 [00:41<00:02,  1.36s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.50s/it] 50%|█████     | 16/32 [00:23<00:22,  1.44s/it] 22%|██▏       | 7/32 [00:10<00:38,  1.52s/it] 97%|█████████▋| 31/32 [00:42<00:01,  1.35s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.50s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.43s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it]100%|██████████| 32/32 [00:44<00:00,  1.35s/it]100%|██████████| 32/32 [00:44<00:00,  1.38s/it]
 50%|█████     | 16/32 [00:24<00:23,  1.50s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.44s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.51s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.49s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.46s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.47s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 66%|██████▌   | 21/32 [00:30<00:16,  1.48s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.50s/it] 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it]I0402 09:35:28.481796 1575525 finetune.py:45] layer 12_gate initial loss 0.002707147039473057
W0402 09:35:28.482094 1575525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 72%|███████▏  | 23/32 [00:33<00:13,  1.51s/it]W0402 09:35:29.176524 1575525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.48s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.50s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.50s/it]12_gate proxy err 0.0033501554280519485 tr(WHW.T) 3184.862548828125
  0%|          | 0/112 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:24<00:23,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it]  1%|          | 1/112 [00:00<01:27,  1.26it/s] 81%|████████▏ | 26/32 [00:38<00:08,  1.48s/it]  2%|▏         | 2/112 [00:01<00:57,  1.90it/s]  3%|▎         | 3/112 [00:01<00:48,  2.25it/s] 53%|█████▎    | 17/32 [00:25<00:22,  1.50s/it]  4%|▎         | 4/112 [00:01<00:43,  2.46it/s] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it]  4%|▍         | 5/112 [00:02<00:41,  2.59it/s] 84%|████████▍ | 27/32 [00:39<00:07,  1.47s/it]  5%|▌         | 6/112 [00:02<00:39,  2.69it/s]  6%|▋         | 7/112 [00:02<00:38,  2.75it/s] 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it]  7%|▋         | 8/112 [00:03<00:37,  2.80it/s] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it]  8%|▊         | 9/112 [00:03<00:36,  2.80it/s] 88%|████████▊ | 28/32 [00:41<00:05,  1.46s/it]  9%|▉         | 10/112 [00:03<00:36,  2.81it/s] 10%|▉         | 11/112 [00:04<00:35,  2.84it/s] 59%|█████▉    | 19/32 [00:28<00:19,  1.50s/it] 11%|█         | 12/112 [00:04<00:34,  2.86it/s] 12%|█▏        | 13/112 [00:04<00:34,  2.84it/s] 84%|████████▍ | 27/32 [00:40<00:07,  1.49s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.45s/it] 12%|█▎        | 14/112 [00:05<00:34,  2.87it/s] 13%|█▎        | 15/112 [00:05<00:33,  2.89it/s] 14%|█▍        | 16/112 [00:05<00:33,  2.89it/s] 62%|██████▎   | 20/32 [00:30<00:17,  1.50s/it] 15%|█▌        | 17/112 [00:06<00:33,  2.87it/s] 88%|████████▊ | 28/32 [00:42<00:05,  1.49s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.45s/it] 16%|█▌        | 18/112 [00:06<00:32,  2.89it/s] 17%|█▋        | 19/112 [00:07<00:32,  2.90it/s] 18%|█▊        | 20/112 [00:07<00:31,  2.91it/s] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it] 19%|█▉        | 21/112 [00:07<00:31,  2.92it/s] 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.44s/it] 20%|█▉        | 22/112 [00:08<00:30,  2.92it/s] 21%|██        | 23/112 [00:08<00:30,  2.92it/s] 21%|██▏       | 24/112 [00:08<00:30,  2.91it/s] 69%|██████▉   | 22/32 [00:33<00:14,  1.50s/it] 22%|██▏       | 25/112 [00:09<00:29,  2.91it/s]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.47s/it]
 23%|██▎       | 26/112 [00:09<00:29,  2.92it/s] 94%|█████████▍| 30/32 [00:45<00:02,  1.49s/it] 24%|██▍       | 27/112 [00:09<00:29,  2.89it/s] 25%|██▌       | 28/112 [00:10<00:29,  2.83it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.80it/s] 72%|███████▏  | 23/32 [00:34<00:13,  1.50s/it] 27%|██▋       | 30/112 [00:10<00:29,  2.81it/s] 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it] 28%|██▊       | 31/112 [00:11<00:28,  2.84it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.82it/s] 29%|██▉       | 33/112 [00:11<00:27,  2.85it/s] 75%|███████▌  | 24/32 [00:36<00:12,  1.50s/it] 30%|███       | 34/112 [00:12<00:27,  2.84it/s]100%|██████████| 32/32 [00:48<00:00,  1.49s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
 31%|███▏      | 35/112 [00:12<00:27,  2.81it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.79it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.77it/s] 78%|███████▊  | 25/32 [00:37<00:10,  1.51s/it] 34%|███▍      | 38/112 [00:13<00:26,  2.75it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.74it/s] 36%|███▌      | 40/112 [00:14<00:26,  2.73it/s] 37%|███▋      | 41/112 [00:14<00:26,  2.72it/s] 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it] 38%|███▊      | 42/112 [00:15<00:25,  2.72it/s] 38%|███▊      | 43/112 [00:15<00:25,  2.72it/s] 39%|███▉      | 44/112 [00:15<00:25,  2.72it/s] 40%|████      | 45/112 [00:16<00:24,  2.72it/s] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it] 41%|████      | 46/112 [00:16<00:24,  2.72it/s]I0402 09:35:49.242846 1576403 finetune.py:45] layer 14_gate initial loss 0.0029325385112315416
W0402 09:35:49.243057 1576403 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 42%|████▏     | 47/112 [00:17<00:23,  2.72it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.76it/s] 44%|████▍     | 49/112 [00:17<00:22,  2.81it/s]W0402 09:35:50.009215 1576403 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 45%|████▍     | 50/112 [00:18<00:22,  2.80it/s] 88%|████████▊ | 28/32 [00:42<00:06,  1.53s/it] 46%|████▌     | 51/112 [00:18<00:21,  2.82it/s] 46%|████▋     | 52/112 [00:18<00:21,  2.82it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.79it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.78it/s] 91%|█████████ | 29/32 [00:44<00:04,  1.54s/it] 49%|████▉     | 55/112 [00:19<00:20,  2.82it/s] 50%|█████     | 56/112 [00:20<00:19,  2.83it/s]I0402 09:35:52.476119 1576238 finetune.py:45] layer 13_gate initial loss 0.003720280947163701
W0402 09:35:52.476531 1576238 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 51%|█████     | 57/112 [00:20<00:19,  2.85it/s]14_gate proxy err 0.003068536752834916 tr(WHW.T) 4249.7998046875
  0%|          | 0/112 [00:00<?, ?it/s] 52%|█████▏    | 58/112 [00:20<00:18,  2.87it/s]W0402 09:35:53.293312 1576238 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 59/112 [00:21<00:18,  2.86it/s] 94%|█████████▍| 30/32 [00:45<00:03,  1.53s/it]  1%|          | 1/112 [00:00<01:30,  1.23it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.87it/s]  2%|▏         | 2/112 [00:01<01:00,  1.83it/s] 54%|█████▍    | 61/112 [00:21<00:17,  2.88it/s]  3%|▎         | 3/112 [00:01<00:50,  2.15it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.88it/s]  4%|▎         | 4/112 [00:01<00:45,  2.36it/s] 56%|█████▋    | 63/112 [00:22<00:16,  2.90it/s] 97%|█████████▋| 31/32 [00:47<00:01,  1.54s/it]  4%|▍         | 5/112 [00:02<00:42,  2.49it/s] 57%|█████▋    | 64/112 [00:22<00:16,  2.90it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.91it/s]  5%|▌         | 6/112 [00:02<00:41,  2.58it/s] 59%|█████▉    | 66/112 [00:23<00:15,  2.89it/s]  6%|▋         | 7/112 [00:02<00:40,  2.62it/s]13_gate proxy err 0.0031574873719364405 tr(WHW.T) 3562.817138671875
  0%|          | 0/112 [00:00<?, ?it/s] 60%|█████▉    | 67/112 [00:24<00:15,  2.82it/s]  7%|▋         | 8/112 [00:03<00:39,  2.63it/s]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
 61%|██████    | 68/112 [00:24<00:15,  2.86it/s]  8%|▊         | 9/112 [00:03<00:38,  2.67it/s] 62%|██████▏   | 69/112 [00:24<00:14,  2.87it/s]  9%|▉         | 10/112 [00:04<00:38,  2.68it/s]  1%|          | 1/112 [00:00<01:34,  1.17it/s] 62%|██████▎   | 70/112 [00:25<00:14,  2.88it/s] 10%|▉         | 11/112 [00:04<00:37,  2.70it/s]  2%|▏         | 2/112 [00:01<01:04,  1.70it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.89it/s] 11%|█         | 12/112 [00:04<00:36,  2.73it/s]  3%|▎         | 3/112 [00:01<00:54,  1.99it/s] 64%|██████▍   | 72/112 [00:25<00:13,  2.90it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.75it/s]  4%|▎         | 4/112 [00:02<00:49,  2.18it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.89it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.75it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.90it/s]  4%|▍         | 5/112 [00:02<00:46,  2.32it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.75it/s] 67%|██████▋   | 75/112 [00:26<00:12,  2.90it/s]  5%|▌         | 6/112 [00:02<00:44,  2.40it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.75it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.90it/s]  6%|▋         | 7/112 [00:03<00:42,  2.47it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.75it/s] 69%|██████▉   | 77/112 [00:27<00:12,  2.90it/s]  7%|▋         | 8/112 [00:03<00:41,  2.50it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.75it/s] 70%|██████▉   | 78/112 [00:27<00:11,  2.91it/s]  8%|▊         | 9/112 [00:03<00:40,  2.52it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.73it/s] 71%|███████   | 79/112 [00:28<00:11,  2.90it/s]  9%|▉         | 10/112 [00:04<00:40,  2.53it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.74it/s] 71%|███████▏  | 80/112 [00:28<00:10,  2.91it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.76it/s] 10%|▉         | 11/112 [00:04<00:39,  2.53it/s] 72%|███████▏  | 81/112 [00:28<00:10,  2.91it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.73it/s] 11%|█         | 12/112 [00:05<00:39,  2.54it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.88it/s] 21%|██        | 23/112 [00:08<00:32,  2.75it/s] 74%|███████▍  | 83/112 [00:29<00:10,  2.90it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.54it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.76it/s] 75%|███████▌  | 84/112 [00:29<00:09,  2.91it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.55it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.90it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.73it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.56it/s] 77%|███████▋  | 86/112 [00:30<00:08,  2.90it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.73it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.57it/s] 78%|███████▊  | 87/112 [00:30<00:08,  2.90it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.73it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.56it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.87it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.72it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.56it/s] 79%|███████▉  | 89/112 [00:31<00:07,  2.90it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.74it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.57it/s] 80%|████████  | 90/112 [00:31<00:07,  2.91it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.74it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.91it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.57it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.75it/s]I0402 09:36:04.785319 1576773 finetune.py:45] layer 15_gate initial loss 0.0036520001012831926
W0402 09:36:04.785635 1576773 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 82%|████████▏ | 92/112 [00:32<00:06,  2.92it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.57it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.76it/s] 83%|████████▎ | 93/112 [00:32<00:06,  2.92it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.56it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.75it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.91it/s]W0402 09:36:05.655376 1576773 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 21%|██        | 23/112 [00:09<00:34,  2.57it/s] 30%|███       | 34/112 [00:12<00:28,  2.76it/s] 85%|████████▍ | 95/112 [00:33<00:05,  2.92it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.59it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.76it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.92it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.57it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.75it/s] 87%|████████▋ | 97/112 [00:34<00:05,  2.91it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.72it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.54it/s] 88%|████████▊ | 98/112 [00:34<00:04,  2.88it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.73it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.53it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.89it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.73it/s] 89%|████████▉ | 100/112 [00:35<00:04,  2.90it/s] 25%|██▌       | 28/112 [00:11<00:33,  2.54it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.74it/s] 90%|█████████ | 101/112 [00:35<00:03,  2.92it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.56it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.92it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.74it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.59it/s] 92%|█████████▏| 103/112 [00:36<00:03,  2.92it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.74it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.60it/s]15_gate proxy err 0.002923478139564395 tr(WHW.T) 5081.4052734375
  0%|          | 0/112 [00:00<?, ?it/s] 93%|█████████▎| 104/112 [00:36<00:02,  2.91it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.74it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.61it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.88it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.70it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.61it/s] 95%|█████████▍| 106/112 [00:37<00:02,  2.89it/s]  1%|          | 1/112 [00:00<01:34,  1.18it/s] 40%|████      | 45/112 [00:16<00:24,  2.72it/s] 30%|███       | 34/112 [00:13<00:29,  2.62it/s] 96%|█████████▌| 107/112 [00:37<00:01,  2.88it/s]  2%|▏         | 2/112 [00:01<01:03,  1.74it/s] 41%|████      | 46/112 [00:17<00:24,  2.72it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.62it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.87it/s]  3%|▎         | 3/112 [00:01<00:53,  2.04it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.71it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.62it/s] 97%|█████████▋| 109/112 [00:38<00:01,  2.88it/s]  4%|▎         | 4/112 [00:02<00:48,  2.23it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.72it/s] 98%|█████████▊| 110/112 [00:38<00:00,  2.89it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.62it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.73it/s]  4%|▍         | 5/112 [00:02<00:45,  2.35it/s] 99%|█████████▉| 111/112 [00:39<00:00,  2.89it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.63it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.74it/s]  5%|▌         | 6/112 [00:02<00:43,  2.43it/s]100%|██████████| 112/112 [00:39<00:00,  2.90it/s]100%|██████████| 112/112 [00:39<00:00,  2.83it/s]
 35%|███▍      | 39/112 [00:15<00:27,  2.63it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.75it/s]  6%|▋         | 7/112 [00:03<00:42,  2.49it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.64it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.72it/s]  7%|▋         | 8/112 [00:03<00:41,  2.53it/s] 37%|███▋      | 41/112 [00:16<00:26,  2.65it/s] 47%|████▋     | 53/112 [00:19<00:22,  2.68it/s]  8%|▊         | 9/112 [00:03<00:40,  2.55it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.64it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.65it/s]  9%|▉         | 10/112 [00:04<00:39,  2.56it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.64it/s] 49%|████▉     | 55/112 [00:20<00:21,  2.64it/s] 10%|▉         | 11/112 [00:04<00:39,  2.57it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.64it/s] 50%|█████     | 56/112 [00:20<00:21,  2.67it/s] 11%|█         | 12/112 [00:05<00:38,  2.57it/s] 40%|████      | 45/112 [00:17<00:25,  2.63it/s] 51%|█████     | 57/112 [00:21<00:20,  2.65it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.58it/s] 41%|████      | 46/112 [00:18<00:25,  2.63it/s] 52%|█████▏    | 58/112 [00:21<00:20,  2.67it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.58it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.63it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.70it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.59it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.62it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.73it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.59it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.62it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.73it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.59it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.62it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.72it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.59it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.63it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.73it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.60it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.63it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.72it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.60it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.63it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.74it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.61it/s] 48%|████▊     | 54/112 [00:21<00:21,  2.64it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.74it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.61it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.74it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.65it/s] 21%|██        | 23/112 [00:09<00:34,  2.61it/s] 61%|██████    | 68/112 [00:25<00:15,  2.75it/s] 50%|█████     | 56/112 [00:22<00:21,  2.66it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.62it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.75it/s] 51%|█████     | 57/112 [00:22<00:20,  2.66it/s]W0402 09:36:18.883000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:36:18.883000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:18.884000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:18.884000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:18.884000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:18.884000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:18.884000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:18.925000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:18.925000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:18.925000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:18.925000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:18.925000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:18.940000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:18.940000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:18.940000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:18.940000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:18.940000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 25/112 [00:10<00:33,  2.62it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.76it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.65it/s]W0402 09:36:19.101000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.101000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.101000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.101000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.101000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 23%|██▎       | 26/112 [00:10<00:32,  2.61it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.76it/s]W0402 09:36:19.409000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.409000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.409000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.409000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.409000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.410000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.410000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 59/112 [00:23<00:20,  2.65it/s]W0402 09:36:19.442000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.443000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.443000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.443000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.443000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.511000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.512000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.512000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.512000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:19.512000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 64%|██████▍   | 72/112 [00:26<00:14,  2.76it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.61it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.65it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.76it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.62it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.65it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.76it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.61it/s] 55%|█████▌    | 62/112 [00:24<00:18,  2.65it/s]W0402 09:36:20.641000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:20.653000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:20.661000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:20.661000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 67%|██████▋   | 75/112 [00:27<00:13,  2.77it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.61it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.65it/s]W0402 09:36:21.093000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.093000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.093000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.093000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.093000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.094000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.094000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.122000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.122000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.122000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.123000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.123000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 68%|██████▊   | 76/112 [00:28<00:12,  2.77it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.60it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.63it/s]W0402 09:36:21.453000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.453000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.453000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.453000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.453000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.453000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.453000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.453000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 77/112 [00:28<00:12,  2.77it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.61it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.64it/s]W0402 09:36:21.737000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.737000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.737000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.737000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:21.737000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 70%|██████▉   | 78/112 [00:28<00:12,  2.77it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.61it/s]W0402 09:36:22.056000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:22.061000 140395902465856 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 66/112 [00:25<00:17,  2.65it/s] 71%|███████   | 79/112 [00:29<00:11,  2.76it/s] 30%|███       | 34/112 [00:13<00:29,  2.61it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.64it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.71it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.61it/s] 61%|██████    | 68/112 [00:26<00:16,  2.65it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.72it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.62it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.65it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.73it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.62it/s] 62%|██████▎   | 70/112 [00:27<00:15,  2.65it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.74it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.62it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.65it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.70it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.63it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.66it/s] 76%|███████▌  | 85/112 [00:31<00:10,  2.66it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.64it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.68it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.64it/s] 37%|███▋      | 41/112 [00:16<00:26,  2.64it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.68it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.62it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.63it/s] 67%|██████▋   | 75/112 [00:29<00:13,  2.66it/s] 79%|███████▊  | 88/112 [00:32<00:09,  2.61it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.67it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.63it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.60it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.67it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.63it/s] 80%|████████  | 90/112 [00:33<00:08,  2.59it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.68it/s] 40%|████      | 45/112 [00:17<00:25,  2.64it/s] 81%|████████▏ | 91/112 [00:33<00:08,  2.59it/s] 71%|███████   | 79/112 [00:30<00:12,  2.67it/s] 41%|████      | 46/112 [00:18<00:25,  2.63it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.58it/s] 71%|███████▏  | 80/112 [00:31<00:11,  2.68it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.63it/s] 83%|████████▎ | 93/112 [00:34<00:07,  2.58it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.69it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.65it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.58it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.68it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.64it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.56it/s] 74%|███████▍  | 83/112 [00:32<00:10,  2.67it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.60it/s] 86%|████████▌ | 96/112 [00:35<00:06,  2.58it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.68it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.61it/s]I0402 09:36:29.028635 1575525 finetune.py:45] layer 12_down initial loss 0.0026717328000813723
W0402 09:36:29.028984 1575525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 87%|████████▋ | 97/112 [00:36<00:05,  2.59it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.66it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.60it/s]W0402 09:36:29.504373 1575525 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 88%|████████▊ | 98/112 [00:36<00:05,  2.62it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.64it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.61it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.66it/s]12_down proxy err 0.011420485563576221 tr(WHW.T) 10.028185844421387
 78%|███████▊  | 87/112 [00:33<00:09,  2.65it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.60it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.70it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.64it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.59it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.72it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.64it/s] 50%|█████     | 56/112 [00:21<00:21,  2.60it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.75it/s] 80%|████████  | 90/112 [00:34<00:08,  2.65it/s] 51%|█████     | 57/112 [00:22<00:21,  2.60it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.77it/s] 81%|████████▏ | 91/112 [00:35<00:07,  2.64it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.60it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.71it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.60it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.60it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.73it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.61it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.61it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.76it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.62it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.61it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.77it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.63it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.62it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.78it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.62it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.78it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.62it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.63it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.80it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.63it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.64it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.81it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.64it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.64it/s]100%|██████████| 112/112 [00:41<00:00,  2.80it/s]100%|██████████| 112/112 [00:41<00:00,  2.69it/s]
 59%|█████▉    | 66/112 [00:25<00:17,  2.64it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.64it/s] 60%|█████▉    | 67/112 [00:26<00:16,  2.65it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.64it/s] 61%|██████    | 68/112 [00:26<00:16,  2.64it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.64it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.64it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.64it/s] 62%|██████▎   | 70/112 [00:27<00:15,  2.65it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.64it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.65it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.65it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.65it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.65it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.65it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.65it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.65it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.65it/s] 67%|██████▋   | 75/112 [00:29<00:13,  2.66it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.65it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.66it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.65it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.65it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.65it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.66it/s]100%|██████████| 112/112 [00:43<00:00,  2.65it/s]100%|██████████| 112/112 [00:43<00:00,  2.59it/s]
 71%|███████   | 79/112 [00:30<00:12,  2.65it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.63it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.59it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.56it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.59it/s]W0402 09:36:41.221000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.221000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.221000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.222000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.222000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.222000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.222000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.261000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.261000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.261000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.261000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.261000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.276000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.276000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.276000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.276000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.276000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.437000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.437000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.437000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.438000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.438000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 84/112 [00:32<00:10,  2.58it/s]W0402 09:36:41.744000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.744000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.744000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.745000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.745000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.745000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.745000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.775000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.775000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.775000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.775000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.775000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.843000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.843000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.843000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.843000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:41.843000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 76%|███████▌  | 85/112 [00:32<00:10,  2.60it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.62it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.62it/s]W0402 09:36:42.989000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 79%|███████▊  | 88/112 [00:34<00:09,  2.63it/s]W0402 09:36:43.000000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.008000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.008000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 79%|███████▉  | 89/112 [00:34<00:08,  2.63it/s]W0402 09:36:43.436000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.436000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.436000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.436000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.436000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.436000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.436000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.464000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.464000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.464000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.465000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.465000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 80%|████████  | 90/112 [00:34<00:08,  2.62it/s]W0402 09:36:43.792000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.792000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.792000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.792000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.792000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.792000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.792000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:43.792000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:44.071000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:44.071000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:44.071000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:44.071000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:44.071000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 91/112 [00:35<00:07,  2.63it/s]W0402 09:36:44.387000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:44.392000 139789098493760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 82%|████████▏ | 92/112 [00:35<00:07,  2.64it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.61it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.62it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.63it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.63it/s]W0402 09:36:46.180000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.180000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.180000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.180000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.180000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.181000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.181000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.224000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.224000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.224000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.224000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.224000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.239000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.239000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.239000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.239000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.240000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.402000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.402000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.402000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.402000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.402000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 87%|████████▋ | 97/112 [00:37<00:05,  2.64it/s]W0402 09:36:46.710000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.710000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.710000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.711000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.711000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.711000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.711000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.745000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.745000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.745000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.745000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.745000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 98/112 [00:37<00:05,  2.65it/s]W0402 09:36:46.814000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.814000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.814000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.815000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:46.815000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 99/112 [00:38<00:04,  2.66it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.67it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.67it/s]W0402 09:36:47.955000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:47.961000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:47.967000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:47.967000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 102/112 [00:39<00:03,  2.67it/s]W0402 09:36:48.397000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:48.397000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:48.397000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:48.397000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:48.398000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:36:48.398000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:48.398000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:48.427000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:48.428000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:48.428000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:48.428000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:48.428000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 92%|█████████▏| 103/112 [00:39<00:03,  2.69it/s]W0402 09:36:48.757000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:48.757000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:48.757000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:48.757000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:48.758000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:48.758000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:36:48.758000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:48.758000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 93%|█████████▎| 104/112 [00:40<00:02,  2.68it/s]W0402 09:36:49.041000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:49.041000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:49.041000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:49.041000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:49.041000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:49.365000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:49.370000 140471788844864 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 105/112 [00:40<00:02,  2.68it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.66it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.67it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.66it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.67it/s]I0402 09:36:51.114544 1576403 finetune.py:45] layer 14_down initial loss 0.002889065071940422
W0402 09:36:51.114855 1576403 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 98%|█████████▊| 110/112 [00:42<00:00,  2.63it/s]W0402 09:36:51.610450 1576403 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 99%|█████████▉| 111/112 [00:42<00:00,  2.61it/s]14_down proxy err 0.012150269001722336 tr(WHW.T) 13.352069854736328
100%|██████████| 112/112 [00:43<00:00,  2.59it/s]100%|██████████| 112/112 [00:43<00:00,  2.59it/s]
I0402 09:36:56.298330 1576238 finetune.py:45] layer 13_down initial loss 0.0036700561176985502
W0402 09:36:56.298605 1576238 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:36:56.777373 1576238 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

13_down proxy err 0.011695792898535728 tr(WHW.T) 11.704399108886719
W0402 09:36:59.356000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.356000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.357000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.357000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.357000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.357000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.357000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.401000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.402000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.402000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.402000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.402000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.418000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.418000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.418000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.418000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.419000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.587000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.587000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.587000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.587000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.587000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.898000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.898000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.898000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.898000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.898000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.898000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.898000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.931000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.931000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.931000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.931000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:36:59.931000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:00.001000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:00.001000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:00.001000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:00.001000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:00.001000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
I0402 09:37:00.578399 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 16 in 1.185612678527832s
I0402 09:37:01.082908 1558010 quantize_finetune_llama.py:159] layer 17 gpu 1
W0402 09:37:01.209000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:01.215000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:01.221000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:01.221000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:01.691000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:01.691000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:01.691000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:01.691000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:01.691000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:01.692000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:01.692000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:01.722000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:01.722000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:01.722000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:01.722000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:01.722000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:02.074000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:02.074000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:02.074000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:02.075000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:02.075000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:02.075000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:02.075000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:02.075000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:02.361000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:02.361000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:02.361000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:02.361000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:02.361000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
I0402 09:37:02.658640 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 17 in 1.1666882038116455s
W0402 09:37:02.694000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:02.700000 139694728320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0402 09:37:03.126188 1581130 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:37:03.126346 1581130 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:37:03.126406 1581130 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:37:03.174374 1558010 quantize_finetune_llama.py:159] layer 18 gpu 2
I0402 09:37:03.472136 1581130 config.py:58] PyTorch version 2.4.0 available.
I0402 09:37:05.205082 1581323 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:37:05.205198 1581323 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:37:05.205254 1581323 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:37:05.375350 1581323 config.py:58] PyTorch version 2.4.0 available.
I0402 09:37:05.745160 1581130 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 09:37:06.078938 1581130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]I0402 09:37:07.542781 1581323 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 09:37:07.877427 1581323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:43,  1.40s/it]  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:01<00:22,  1.32it/s]  9%|▉         | 3/32 [00:02<00:15,  1.81it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.17it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.47it/s]I0402 09:37:10.049692 1576773 finetune.py:45] layer 15_down initial loss 0.0035974017810076475
W0402 09:37:10.050233 1576773 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:02<00:09,  2.69it/s]  3%|▎         | 1/32 [00:01<00:43,  1.42s/it] 22%|██▏       | 7/32 [00:03<00:08,  2.85it/s]  6%|▋         | 2/32 [00:01<00:23,  1.30it/s]W0402 09:37:10.578656 1576773 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 25%|██▌       | 8/32 [00:03<00:08,  2.96it/s]  9%|▉         | 3/32 [00:02<00:16,  1.77it/s]15_down proxy err 0.01217461097985506 tr(WHW.T) 16.894805908203125
 28%|██▊       | 9/32 [00:03<00:07,  3.04it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.15it/s] 31%|███▏      | 10/32 [00:04<00:07,  3.09it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.13it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.64it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.17it/s] 22%|██▏       | 7/32 [00:03<00:08,  2.78it/s] 41%|████      | 13/32 [00:05<00:05,  3.20it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.89it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.21it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.95it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.22it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.98it/s] 50%|█████     | 16/32 [00:06<00:04,  3.22it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.03it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.23it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.05it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.23it/s] 41%|████      | 13/32 [00:05<00:06,  3.08it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.24it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.10it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.24it/s]I0402 09:37:14.517649 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 18 in 1.225135326385498s
 47%|████▋     | 15/32 [00:05<00:05,  3.08it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.20it/s] 50%|█████     | 16/32 [00:06<00:05,  3.05it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.14it/s]I0402 09:37:15.077108 1558010 quantize_finetune_llama.py:159] layer 19 gpu 3
 53%|█████▎    | 17/32 [00:06<00:04,  3.04it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.14it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.16it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.05it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.13it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.00it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.18it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.05it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.22it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.10it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.24it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.13it/s]I0402 09:37:17.017894 1582287 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:37:17.018014 1582287 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:37:17.018073 1582287 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:37:17.203136 1582287 config.py:58] PyTorch version 2.4.0 available.
 91%|█████████ | 29/32 [00:10<00:00,  3.25it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.16it/s]I0402 09:37:17.409643 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 19 in 1.6611089706420898s
 94%|█████████▍| 30/32 [00:10<00:00,  3.23it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.13it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.17it/s]I0402 09:37:17.908420 1558010 quantize_finetune_llama.py:159] layer 20 gpu 0
 78%|███████▊  | 25/32 [00:09<00:02,  3.08it/s]100%|██████████| 32/32 [00:11<00:00,  3.12it/s]100%|██████████| 32/32 [00:11<00:00,  2.89it/s]
 81%|████████▏ | 26/32 [00:09<00:01,  3.07it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.06it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.06it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.03it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.01it/s]I0402 09:37:19.657201 1582287 data_utils.py:336] using 256 training seqs, 128 validation seqs
 97%|█████████▋| 31/32 [00:11<00:00,  3.04it/s]I0402 09:37:19.931613 1582527 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:37:19.931746 1582527 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:37:19.931804 1582527 utils.py:162] NumExpr defaulting to 16 threads.
W0402 09:37:20.037837 1582287 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:37:20.127402 1582527 config.py:58] PyTorch version 2.4.0 available.
100%|██████████| 32/32 [00:11<00:00,  3.05it/s]100%|██████████| 32/32 [00:11<00:00,  2.80it/s]
W0402 09:37:21.149000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.150000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.150000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.150000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.150000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.150000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.150000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.177000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.177000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.177000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.177000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.177000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.194000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.194000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.194000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.194000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.194000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:37:21.532000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.532000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.532000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.532000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:21.532000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
I0402 09:37:22.222347 1582527 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 09:37:22.432000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:22.432000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:22.433000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:22.433000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:22.433000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:22.433000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:22.433000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:22.451000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:22.451000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:22.451000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:22.451000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:22.451000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:22.575382 1582527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:37:22.701000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:22.701000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:22.701000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:22.701000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:22.701000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:51,  1.67s/it]W0402 09:37:23.182000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.182000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.182000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.182000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.182000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.183000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.183000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:26,  1.14it/s]W0402 09:37:23.208000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.208000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.208000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.208000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.208000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.224000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.224000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.224000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.224000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.224000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:02<00:18,  1.60it/s]W0402 09:37:23.533000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.533000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.533000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.533000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.533000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.818000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.818000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.819000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.819000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.819000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.819000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.819000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.836000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.836000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.836000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.836000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:23.836000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:02<00:14,  1.98it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.26it/s]W0402 09:37:24.384000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.384000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.384000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.384000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.385000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.385000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.385000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.401000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.401000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.401000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.402000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.402000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:03<00:10,  2.49it/s]W0402 09:37:24.629000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.629000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.630000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.630000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.630000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.706000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.706000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.706000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.706000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:24.706000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.65it/s]  3%|▎         | 1/32 [00:01<00:44,  1.42s/it] 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
  6%|▋         | 2/32 [00:01<00:23,  1.27it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.83it/s]  9%|▉         | 3/32 [00:02<00:16,  1.74it/s]W0402 09:37:25.756000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:25.756000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:25.756000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:25.756000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:25.756000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:25.756000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:25.756000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:25.773000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:25.773000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:25.773000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:25.773000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:25.773000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:07,  2.91it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.11it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.92it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.90it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.57it/s]W0402 09:37:26.706000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:26.706000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:26.706000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:26.706000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:26.706000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.87it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.73it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.89it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.84it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 28%|██▊       | 9/32 [00:04<00:07,  2.91it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.90it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.96it/s] 50%|█████     | 16/32 [00:06<00:05,  2.90it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.98it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.91it/s] 38%|███▊      | 12/32 [00:05<00:06,  3.00it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.91it/s] 41%|████      | 13/32 [00:05<00:06,  3.01it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.88it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.02it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.84it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.03it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.81it/s] 50%|█████     | 16/32 [00:06<00:05,  3.05it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.06it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.79it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.05it/s]I0402 09:37:30.524836 1581130 finetune.py:45] layer 16_v initial loss 0.004105148371309042
W0402 09:37:30.525919 1581130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.04it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.82it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.05it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.83it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.06it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.07it/s]W0402 09:37:31.984965 1581130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:08<00:02,  3.06it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.84it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.05it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.84it/s]I0402 09:37:32.693991 1581323 finetune.py:45] layer 17_v initial loss 0.00532403914257884
W0402 09:37:32.694332 1581323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 78%|███████▊  | 25/32 [00:09<00:02,  3.07it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.07it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.91it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.08it/s]100%|██████████| 32/32 [00:12<00:00,  2.95it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
16_v proxy err 0.00923982448875904 tr(WHW.T) 68.60269165039062
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:37:33.682569 1581323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 88%|████████▊ | 28/32 [00:10<00:01,  3.09it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.09it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.09it/s]  3%|▎         | 1/32 [00:00<00:29,  1.04it/s]17_v proxy err 0.012142300605773926 tr(WHW.T) 69.1786880493164
  0%|          | 0/32 [00:00<?, ?it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.09it/s]  6%|▋         | 2/32 [00:01<00:18,  1.63it/s]100%|██████████| 32/32 [00:11<00:00,  3.06it/s]100%|██████████| 32/32 [00:11<00:00,  2.77it/s]
  9%|▉         | 3/32 [00:01<00:14,  2.01it/s]  3%|▎         | 1/32 [00:00<00:27,  1.12it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s]  6%|▋         | 2/32 [00:01<00:17,  1.73it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s]  9%|▉         | 3/32 [00:01<00:13,  2.10it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s]W0402 09:37:36.612000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.612000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.612000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.612000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.612000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.612000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.613000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.638000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.638000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.638000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.638000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.638000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:01<00:11,  2.36it/s]W0402 09:37:36.654000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.654000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.654000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.654000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.654000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s]W0402 09:37:36.961000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.961000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.961000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.961000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:36.961000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:10,  2.54it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.65it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.69it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.73it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s]W0402 09:37:37.819000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.820000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.820000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.820000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.820000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.820000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.820000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.837000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.837000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.837000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.838000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.838000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.874000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.874000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.874000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.874000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.874000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.874000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.874000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.900000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.901000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.901000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.901000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.901000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.917000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.917000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.917000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.917000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:37.917000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:08,  2.78it/s]W0402 09:37:38.070000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:38.070000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:38.070000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:38.070000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:38.070000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.72it/s]W0402 09:37:38.226000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:38.226000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:38.226000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:38.226000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:38.226000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:03<00:08,  2.81it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.75it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.82it/s] 41%|████      | 13/32 [00:05<00:06,  2.77it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.84it/s]W0402 09:37:39.087000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.087000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.087000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.087000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.088000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.088000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.088000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.105000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.105000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.105000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.105000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.105000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.187000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.187000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.187000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.187000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.187000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.187000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.187000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.205000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.205000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.205000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.205000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.205000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.78it/s]W0402 09:37:39.338000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.339000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.339000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.339000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:39.339000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.79it/s] 41%|████      | 13/32 [00:05<00:06,  2.87it/s] 50%|█████     | 16/32 [00:06<00:05,  2.80it/s]W0402 09:37:40.078000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:40.078000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:40.078000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:40.078000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:40.078000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.89it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.89it/s]W0402 09:37:40.474000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:40.474000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:40.474000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:40.475000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:40.475000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:40.475000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:40.475000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:40.493000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:40.493000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:40.493000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:40.493000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:40.493000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 56%|█████▋    | 18/32 [00:07<00:05,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.90it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.91it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s]W0402 09:37:41.380000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:41.380000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:41.380000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:41.380000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:41.380000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:06<00:04,  2.86it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.81it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 69%|██████▉   | 22/32 [00:08<00:03,  2.81it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.77it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.83it/s] 66%|██████▌   | 21/32 [00:07<00:04,  2.75it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.78it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.71it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.72it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.71it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.69it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.68it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.68it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.67it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.70it/s]I0402 09:37:45.698314 1582287 finetune.py:45] layer 18_v initial loss 0.004356171004474163
W0402 09:37:45.698605 1582287 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
 94%|█████████▍| 30/32 [00:11<00:00,  2.75it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.76it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
W0402 09:37:46.746172 1582287 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:37:47.006039 1582527 finetune.py:45] layer 19_v initial loss 0.006832814775407314
W0402 09:37:47.006338 1582527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

18_v proxy err 0.010070223361253738 tr(WHW.T) 73.61924743652344
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:37:47.922221 1582527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]19_v proxy err 0.009779744781553745 tr(WHW.T) 87.24554443359375
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:01<00:17,  1.69it/s]  9%|▉         | 3/32 [00:01<00:14,  2.04it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.28it/s]  3%|▎         | 1/32 [00:00<00:28,  1.10it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s]  6%|▋         | 2/32 [00:01<00:17,  1.71it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.33it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.49it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.59it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.66it/s]W0402 09:37:51.986000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:51.986000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:51.986000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:51.986000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:51.987000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:51.987000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:51.987000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.014000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.014000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.015000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.015000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.015000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.029000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.029000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.029000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.029000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.030000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.179000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.179000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.179000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.179000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.179000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s]W0402 09:37:52.319000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.319000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.319000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.319000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.319000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.319000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.319000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.347000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.347000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.347000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.347000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.347000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.362000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.362000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.362000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.362000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.362000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.393000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.394000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.394000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.394000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.394000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.394000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.394000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.414000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.414000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.414000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.415000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.415000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.477000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.477000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.477000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.477000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.477000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.511000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.511000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.511000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.511000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.511000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:04<00:07,  2.79it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.75it/s]W0402 09:37:52.723000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.723000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.723000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.723000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.723000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.723000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.723000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.744000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.744000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.744000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.744000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.744000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.805000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.805000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.805000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.805000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:52.806000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.78it/s]W0402 09:37:53.298000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.80it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.79it/s]W0402 09:37:53.600000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.600000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.600000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.600000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.600000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.600000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.601000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.620000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.620000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.620000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.620000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.620000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.620000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:04<00:07,  2.79it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.80it/s]W0402 09:37:53.869000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.869000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.869000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.869000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.869000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.912000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.912000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.912000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.912000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.912000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.912000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.912000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.933000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.933000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.933000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.933000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:53.933000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 50%|█████     | 16/32 [00:06<00:05,  2.80it/s]W0402 09:37:54.120000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:54.177000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:37:54.177000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:37:54.177000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:37:54.177000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:37:54.177000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:37:54.430000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.78it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.79it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.80it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.75it/s] 50%|█████     | 16/32 [00:06<00:05,  2.80it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.80it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.77it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.81it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.77it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.70it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.74it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.66it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.73it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.63it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.73it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.61it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.56it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.72it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.57it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.69it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.57it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.71it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.58it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s]I0402 09:38:00.389003 1581323 finetune.py:45] layer 17_q initial loss 0.005320048425346613
W0402 09:38:00.389320 1581323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 31/32 [00:11<00:00,  2.69it/s]I0402 09:38:00.789041 1581130 finetune.py:45] layer 16_q initial loss 0.004109187982976437
W0402 09:38:00.789496 1581130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:12<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
W0402 09:38:01.397598 1581323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:38:01.926029 1581130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

17_q proxy err 0.0018124851631000638 tr(WHW.T) 6716.6806640625
  0%|          | 0/32 [00:00<?, ?it/s]16_q proxy err 0.001781624392606318 tr(WHW.T) 6124.5419921875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.17it/s]  6%|▋         | 2/32 [00:01<00:16,  1.81it/s]  3%|▎         | 1/32 [00:00<00:26,  1.16it/s]  9%|▉         | 3/32 [00:01<00:13,  2.20it/s]  6%|▋         | 2/32 [00:01<00:16,  1.79it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.43it/s]  9%|▉         | 3/32 [00:01<00:13,  2.16it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.59it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.39it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.69it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.53it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.77it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.63it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.83it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.70it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.88it/s]W0402 09:38:05.992000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:38:05.992000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:05.993000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:05.993000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:05.993000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:38:05.993000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:38:05.993000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.021000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.021000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.021000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.021000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.021000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.035000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.035000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.036000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.036000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.036000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.187000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.187000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.187000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.187000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.187000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.91it/s]W0402 09:38:06.407000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.407000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.407000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.407000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.407000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.407000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.407000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.426000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.426000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.426000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.426000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.426000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.490000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.491000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.491000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.491000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.491000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.92it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.78it/s]W0402 09:38:06.717000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.718000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.718000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.718000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.718000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.718000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.718000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.746000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.746000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.746000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.746000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.746000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.761000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.761000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.761000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.762000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.762000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.914000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.914000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.914000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.914000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:38:06.914000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:04<00:06,  2.94it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.80it/s]W0402 09:38:07.132000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.133000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.133000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.133000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.133000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.133000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.133000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.152000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.152000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.152000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.152000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.153000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.216000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.216000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.216000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.216000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.216000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:04<00:06,  2.94it/s]W0402 09:38:07.319000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.81it/s]W0402 09:38:07.625000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.625000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.625000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.625000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.625000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.626000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.626000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.647000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.647000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.647000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.647000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.647000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.91it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s]W0402 09:38:07.891000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.891000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.892000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.892000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:07.892000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 15/32 [00:05<00:05,  2.93it/s] 41%|████      | 13/32 [00:05<00:06,  2.83it/s]W0402 09:38:08.047000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:38:08.141000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:05<00:05,  2.91it/s]W0402 09:38:08.348000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:08.348000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:08.348000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:38:08.348000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:38:08.348000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:38:08.348000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:38:08.348000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:08.368000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:08.368000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:08.368000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:38:08.368000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:08.368000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.85it/s]W0402 09:38:08.613000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:38:08.614000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:38:08.614000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:38:08.614000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:38:08.614000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.86it/s]W0402 09:38:08.868000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:06<00:04,  2.94it/s] 50%|█████     | 16/32 [00:06<00:05,  2.83it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.93it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.92it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.86it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.89it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.87it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.85it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.88it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.82it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.86it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.80it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.84it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.79it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.82it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.78it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.81it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.77it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.79it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.77it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.78it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.77it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.78it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.78it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.77it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.76it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]100%|██████████| 32/32 [00:11<00:00,  2.75it/s]
 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s]I0402 09:38:14.121374 1582287 finetune.py:45] layer 18_q initial loss 0.00435589300468564
W0402 09:38:14.121647 1582287 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 31/32 [00:11<00:00,  2.72it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]100%|██████████| 32/32 [00:11<00:00,  2.70it/s]
I0402 09:38:14.877093 1582527 finetune.py:45] layer 19_q initial loss 0.006836192216724157
W0402 09:38:14.877304 1582527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:38:15.243387 1582287 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:38:15.846562 1582527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_q proxy err 0.0022218888625502586 tr(WHW.T) 5733.501953125
  0%|          | 0/32 [00:00<?, ?it/s]19_q proxy err 0.002135638380423188 tr(WHW.T) 6149.1044921875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.05it/s]  6%|▋         | 2/32 [00:01<00:18,  1.62it/s]  3%|▎         | 1/32 [00:00<00:28,  1.10it/s]  9%|▉         | 3/32 [00:01<00:14,  1.97it/s]  6%|▋         | 2/32 [00:01<00:17,  1.70it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s]  9%|▉         | 3/32 [00:01<00:14,  2.05it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.28it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.56it/s]I0402 09:38:20.150663 1581323 finetune.py:45] layer 17_k initial loss 0.00532035157084465
W0402 09:38:20.150981 1581323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:09,  2.63it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.66it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s]W0402 09:38:21.131857 1581323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 38%|███▊      | 12/32 [00:05<00:07,  2.72it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.70it/s]I0402 09:38:21.474663 1581130 finetune.py:45] layer 16_k initial loss 0.004103941842913628
W0402 09:38:21.475049 1581130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:04<00:07,  2.74it/s] 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.77it/s] 41%|████      | 13/32 [00:05<00:06,  2.74it/s]17_k proxy err 0.0016124596586450934 tr(WHW.T) 4246.078125
  0%|          | 0/32 [00:00<?, ?it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.79it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.78it/s]W0402 09:38:22.522147 1581130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 50%|█████     | 16/32 [00:06<00:05,  2.77it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.75it/s]  3%|▎         | 1/32 [00:00<00:23,  1.34it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.80it/s] 50%|█████     | 16/32 [00:06<00:05,  2.79it/s]  6%|▋         | 2/32 [00:01<00:15,  1.97it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.81it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.81it/s]16_k proxy err 0.0012681687949225307 tr(WHW.T) 4878.88427734375
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:01<00:12,  2.30it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.83it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.83it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.51it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.63it/s]  3%|▎         | 1/32 [00:00<00:22,  1.35it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.85it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.71it/s]  6%|▋         | 2/32 [00:01<00:15,  1.95it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.85it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.84it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.75it/s]  9%|▉         | 3/32 [00:01<00:12,  2.27it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.84it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.76it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.45it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.83it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.84it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.80it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.58it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.83it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.66it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.85it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.85it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.71it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.85it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.82it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.87it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.84it/s] 41%|████      | 13/32 [00:04<00:06,  2.88it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.76it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.85it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.89it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.76it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.84it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.89it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.79it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.85it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s] 50%|█████     | 16/32 [00:05<00:05,  2.88it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.79it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
 53%|█████▎    | 17/32 [00:06<00:05,  2.89it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s]100%|██████████| 32/32 [00:11<00:00,  2.86it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
 56%|█████▋    | 18/32 [00:06<00:04,  2.87it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.81it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.83it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.85it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.87it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.77it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.82it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.78it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.75it/s] 66%|██████▌   | 21/32 [00:07<00:04,  2.73it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.73it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.72it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.71it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.72it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.69it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.70it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.69it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.70it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.68it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
 88%|████████▊ | 28/32 [00:10<00:01,  2.71it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.70it/s]I0402 09:38:34.595234 1582287 finetune.py:45] layer 18_k initial loss 0.0043553514406085014
W0402 09:38:34.595597 1582287 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s]I0402 09:38:34.952215 1582527 finetune.py:45] layer 19_k initial loss 0.006831747014075518
W0402 09:38:34.952490 1582527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 31/32 [00:11<00:00,  2.74it/s]100%|██████████| 32/32 [00:11<00:00,  2.75it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
W0402 09:38:35.644157 1582287 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:38:35.869899 1582527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_k proxy err 0.0017474895576015115 tr(WHW.T) 4452.47412109375
  0%|          | 0/32 [00:00<?, ?it/s]19_k proxy err 0.0018543122569099069 tr(WHW.T) 3978.64404296875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.23it/s]  3%|▎         | 1/32 [00:00<00:23,  1.33it/s]  6%|▋         | 2/32 [00:01<00:16,  1.79it/s]  6%|▋         | 2/32 [00:01<00:15,  1.90it/s]  9%|▉         | 3/32 [00:01<00:13,  2.09it/s]  9%|▉         | 3/32 [00:01<00:13,  2.20it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.27it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.38it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.49it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.45it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.56it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.60it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.63it/s]I0402 09:38:40.415192 1581323 finetune.py:45] layer 17_o initial loss 0.005299665033817291
W0402 09:38:40.415364 1581323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:03<00:08,  2.58it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.64it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s]W0402 09:38:41.352272 1581323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.67it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 41%|████      | 13/32 [00:05<00:07,  2.67it/s]I0402 09:38:42.449927 1581130 finetune.py:45] layer 16_o initial loss 0.003936560824513435
W0402 09:38:42.450366 1581130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 44%|████▍     | 14/32 [00:05<00:06,  2.72it/s]17_o proxy err 0.010246685706079006 tr(WHW.T) 6.400020122528076
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.68it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.72it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.71it/s] 50%|█████     | 16/32 [00:06<00:05,  2.72it/s] 50%|█████     | 16/32 [00:06<00:05,  2.72it/s]W0402 09:38:43.436800 1581130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 17/32 [00:06<00:05,  2.74it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.74it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.76it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s]  3%|▎         | 1/32 [00:01<00:56,  1.84s/it]16_o proxy err 0.009853903204202652 tr(WHW.T) 7.369767189025879
  0%|          | 0/32 [00:00<?, ?it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.79it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.78it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.80it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s]  6%|▋         | 2/32 [00:03<00:47,  1.57s/it] 72%|███████▏  | 23/32 [00:08<00:03,  2.81it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.80it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s]  3%|▎         | 1/32 [00:01<00:58,  1.88s/it] 81%|████████▏ | 26/32 [00:10<00:02,  2.78it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.81it/s]  9%|▉         | 3/32 [00:04<00:42,  1.48s/it] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.81it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.79it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.81it/s]  6%|▋         | 2/32 [00:03<00:48,  1.61s/it] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.78it/s] 12%|█▎        | 4/32 [00:05<00:40,  1.44s/it] 97%|█████████▋| 31/32 [00:11<00:00,  2.82it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s]100%|██████████| 32/32 [00:11<00:00,  2.82it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
100%|██████████| 32/32 [00:12<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
  9%|▉         | 3/32 [00:04<00:44,  1.52s/it] 16%|█▌        | 5/32 [00:07<00:38,  1.43s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.43s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.44s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.45s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.48s/it]I0402 09:38:55.308872 1582527 finetune.py:45] layer 19_o initial loss 0.0067126210778951645
W0402 09:38:55.309175 1582527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 09:38:55.413464 1582287 finetune.py:45] layer 18_o initial loss 0.004292245488613844
W0402 09:38:55.413707 1582287 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:13<00:33,  1.45s/it]W0402 09:38:56.205993 1582527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:38:56.355279 1582287 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it]19_o proxy err 0.010462122969329357 tr(WHW.T) 3.939014434814453
  0%|          | 0/32 [00:00<?, ?it/s]18_o proxy err 0.009869259782135487 tr(WHW.T) 4.730301856994629
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:13<00:33,  1.45s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.42s/it]  3%|▎         | 1/32 [00:01<00:58,  1.88s/it]  3%|▎         | 1/32 [00:01<00:59,  1.91s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it]  6%|▋         | 2/32 [00:03<00:48,  1.61s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.43s/it]  6%|▋         | 2/32 [00:03<00:49,  1.64s/it] 41%|████      | 13/32 [00:18<00:26,  1.40s/it]  9%|▉         | 3/32 [00:04<00:44,  1.52s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it]  9%|▉         | 3/32 [00:04<00:44,  1.55s/it] 44%|████▍     | 14/32 [00:20<00:24,  1.39s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.48s/it] 41%|████      | 13/32 [00:19<00:27,  1.43s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.51s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.39s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 50%|█████     | 16/32 [00:22<00:22,  1.39s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.45s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.47s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.38s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.44s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.38s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 59%|█████▉    | 19/32 [00:26<00:17,  1.38s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.45s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.42s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.38s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.38s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.42s/it] 69%|██████▉   | 22/32 [00:31<00:13,  1.37s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.42s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.44s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.37s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.44s/it] 41%|████      | 13/32 [00:18<00:27,  1.42s/it] 75%|███████▌  | 24/32 [00:33<00:10,  1.37s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.42s/it] 41%|████      | 13/32 [00:19<00:27,  1.44s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.37s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.41s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.44s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.37s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.41s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.44s/it] 84%|████████▍ | 27/32 [00:37<00:06,  1.37s/it] 50%|█████     | 16/32 [00:23<00:22,  1.41s/it] 78%|███████▊  | 25/32 [00:36<00:09,  1.41s/it] 50%|█████     | 16/32 [00:23<00:22,  1.43s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.37s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.43s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.38s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.42s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.44s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.38s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.42s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.44s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.38s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.42s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.42s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.44s/it]100%|██████████| 32/32 [00:44<00:00,  1.37s/it]100%|██████████| 32/32 [00:44<00:00,  1.40s/it]
 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.42s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.45s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.42s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.42s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.46s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.43s/it]100%|██████████| 32/32 [00:46<00:00,  1.42s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 72%|███████▏  | 23/32 [00:33<00:13,  1.47s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.43s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.49s/it] 78%|███████▊  | 25/32 [00:35<00:10,  1.43s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.50s/it]I0402 09:39:34.686025 1581323 finetune.py:45] layer 17_up initial loss 0.005215826909989119
W0402 09:39:34.686214 1581323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:37<00:08,  1.44s/it]W0402 09:39:35.511314 1581323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 81%|████████▏ | 26/32 [00:38<00:08,  1.49s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.45s/it]17_up proxy err 0.010092445649206638 tr(WHW.T) 1061.939208984375
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:39<00:07,  1.48s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.46s/it]  3%|▎         | 1/32 [00:01<00:55,  1.79s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it]I0402 09:39:38.679938 1581130 finetune.py:45] layer 16_up initial loss 0.003909295424818993
W0402 09:39:38.680328 1581130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:41<00:04,  1.46s/it]W0402 09:39:39.517411 1581130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:03<00:46,  1.56s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.46s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.45s/it]16_up proxy err 0.010297819972038269 tr(WHW.T) 981.8323974609375
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:04<00:43,  1.49s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.45s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.44s/it]  3%|▎         | 1/32 [00:01<00:55,  1.80s/it] 12%|█▎        | 4/32 [00:05<00:40,  1.45s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.45s/it]100%|██████████| 32/32 [00:46<00:00,  1.43s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
  6%|▋         | 2/32 [00:03<00:47,  1.58s/it] 16%|█▌        | 5/32 [00:07<00:38,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]100%|██████████| 32/32 [00:46<00:00,  1.47s/it]
  9%|▉         | 3/32 [00:04<00:44,  1.52s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.45s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.44s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.45s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it]I0402 09:39:50.922042 1582527 finetune.py:45] layer 19_up initial loss 0.006597853731364012
W0402 09:39:50.922289 1582527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it] 31%|███▏      | 10/32 [00:14<00:32,  1.47s/it]W0402 09:39:51.764449 1582527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:39:51.813570 1582287 finetune.py:45] layer 18_up initial loss 0.004242320079356432
W0402 09:39:51.813880 1582287 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:39:52.656901 1582287 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:11<00:34,  1.46s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it]19_up proxy err 0.011552107520401478 tr(WHW.T) 1067.8548583984375
  0%|          | 0/32 [00:00<?, ?it/s]18_up proxy err 0.010987436398863792 tr(WHW.T) 1055.73193359375
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.44s/it]  3%|▎         | 1/32 [00:01<00:56,  1.81s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it]  3%|▎         | 1/32 [00:01<00:56,  1.84s/it] 41%|████      | 13/32 [00:19<00:27,  1.43s/it]  6%|▋         | 2/32 [00:03<00:47,  1.58s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.43s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.43s/it]  6%|▋         | 2/32 [00:03<00:48,  1.61s/it]  9%|▉         | 3/32 [00:04<00:43,  1.51s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it]  9%|▉         | 3/32 [00:04<00:44,  1.53s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it] 41%|████      | 13/32 [00:19<00:27,  1.43s/it] 50%|█████     | 16/32 [00:23<00:22,  1.41s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.45s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.48s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.41s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.46s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.44s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.45s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.42s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.41s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.45s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.45s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.43s/it] 62%|██████▎   | 20/32 [00:28<00:17,  1.42s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.41s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.45s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.41s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.45s/it] 41%|████      | 13/32 [00:18<00:27,  1.42s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.42s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.41s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.40s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.42s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.44s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 84%|████████▍ | 27/32 [00:38<00:06,  1.39s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.42s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.44s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.39s/it] 78%|███████▊  | 25/32 [00:36<00:09,  1.42s/it] 50%|█████     | 16/32 [00:23<00:23,  1.44s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.38s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.44s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.39s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.42s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.44s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.38s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.42s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.44s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.42s/it]100%|██████████| 32/32 [00:45<00:00,  1.38s/it]100%|██████████| 32/32 [00:45<00:00,  1.42s/it]
 91%|█████████ | 29/32 [00:41<00:04,  1.42s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.42s/it] 66%|██████▌   | 21/32 [00:30<00:16,  1.45s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.42s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.42s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.42s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]100%|██████████| 32/32 [00:45<00:00,  1.44s/it]
 75%|███████▌  | 24/32 [00:34<00:11,  1.43s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.49s/it] 78%|███████▊  | 25/32 [00:35<00:10,  1.43s/it] 75%|███████▌  | 24/32 [00:35<00:12,  1.50s/it]I0402 09:40:29.704213 1581323 finetune.py:45] layer 17_gate initial loss 0.00514315627515316
W0402 09:40:29.704371 1581323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:37<00:08,  1.44s/it]W0402 09:40:30.438760 1581323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 78%|███████▊  | 25/32 [00:36<00:10,  1.50s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.46s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.50s/it]17_gate proxy err 0.003637535497546196 tr(WHW.T) 5241.20556640625
  0%|          | 0/112 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:40<00:05,  1.46s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.49s/it]  1%|          | 1/112 [00:00<01:29,  1.25it/s]  2%|▏         | 2/112 [00:01<00:59,  1.86it/s]  3%|▎         | 3/112 [00:01<00:49,  2.22it/s] 91%|█████████ | 29/32 [00:41<00:04,  1.47s/it]I0402 09:40:34.767102 1581130 finetune.py:45] layer 16_gate initial loss 0.003812454640865326
W0402 09:40:34.767472 1581130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it]  4%|▎         | 4/112 [00:01<00:44,  2.45it/s]  4%|▍         | 5/112 [00:02<00:41,  2.58it/s]W0402 09:40:35.490494 1581130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  5%|▌         | 6/112 [00:02<00:39,  2.68it/s]  6%|▋         | 7/112 [00:02<00:38,  2.75it/s] 94%|█████████▍| 30/32 [00:43<00:02,  1.46s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.45s/it]  7%|▋         | 8/112 [00:03<00:37,  2.78it/s]  8%|▊         | 9/112 [00:03<00:37,  2.76it/s]  9%|▉         | 10/112 [00:03<00:36,  2.80it/s] 10%|▉         | 11/112 [00:04<00:35,  2.82it/s] 97%|█████████▋| 31/32 [00:44<00:01,  1.46s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.46s/it] 11%|█         | 12/112 [00:04<00:35,  2.81it/s] 12%|█▏        | 13/112 [00:05<00:34,  2.84it/s] 12%|█▎        | 14/112 [00:05<00:34,  2.85it/s]16_gate proxy err 0.0036036083474755287 tr(WHW.T) 4852.61962890625
  0%|          | 0/112 [00:00<?, ?it/s] 13%|█▎        | 15/112 [00:05<00:33,  2.86it/s]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 97%|█████████▋| 31/32 [00:45<00:01,  1.44s/it] 14%|█▍        | 16/112 [00:06<00:33,  2.86it/s]  1%|          | 1/112 [00:00<01:31,  1.21it/s] 15%|█▌        | 17/112 [00:06<00:33,  2.87it/s]  2%|▏         | 2/112 [00:01<01:01,  1.78it/s] 16%|█▌        | 18/112 [00:06<00:32,  2.89it/s]  3%|▎         | 3/112 [00:01<00:52,  2.09it/s] 17%|█▋        | 19/112 [00:07<00:32,  2.88it/s]  4%|▎         | 4/112 [00:01<00:46,  2.31it/s] 18%|█▊        | 20/112 [00:07<00:31,  2.88it/s]100%|██████████| 32/32 [00:46<00:00,  1.43s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
  4%|▍         | 5/112 [00:02<00:43,  2.46it/s] 19%|█▉        | 21/112 [00:07<00:31,  2.87it/s]  5%|▌         | 6/112 [00:02<00:41,  2.54it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.80it/s]  6%|▋         | 7/112 [00:03<00:40,  2.61it/s] 21%|██        | 23/112 [00:08<00:32,  2.77it/s]  7%|▋         | 8/112 [00:03<00:39,  2.65it/s] 21%|██▏       | 24/112 [00:08<00:31,  2.80it/s]  8%|▊         | 9/112 [00:03<00:38,  2.66it/s] 22%|██▏       | 25/112 [00:09<00:30,  2.83it/s]  9%|▉         | 10/112 [00:04<00:38,  2.67it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.81it/s] 24%|██▍       | 27/112 [00:09<00:30,  2.82it/s] 10%|▉         | 11/112 [00:04<00:37,  2.67it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.79it/s] 11%|█         | 12/112 [00:04<00:37,  2.68it/s] 26%|██▌       | 29/112 [00:10<00:30,  2.76it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.69it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.74it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.70it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.72it/s] 13%|█▎        | 15/112 [00:06<00:35,  2.70it/s] 29%|██▊       | 32/112 [00:11<00:29,  2.71it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.69it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.70it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.69it/s] 30%|███       | 34/112 [00:12<00:28,  2.70it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.69it/s] 31%|███▏      | 35/112 [00:12<00:28,  2.70it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.69it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.70it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.68it/s]I0402 09:40:46.687947 1582527 finetune.py:45] layer 19_gate initial loss 0.006512751802802086
W0402 09:40:46.688179 1582527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 33%|███▎      | 37/112 [00:13<00:27,  2.69it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.71it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.74it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.70it/s]W0402 09:40:47.434081 1582527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 21%|██        | 23/112 [00:08<00:32,  2.77it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.71it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.77it/s] 36%|███▌      | 40/112 [00:14<00:26,  2.71it/s]I0402 09:40:48.171709 1582287 finetune.py:45] layer 18_gate initial loss 0.004246932454407215
W0402 09:40:48.172080 1582287 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 25/112 [00:09<00:31,  2.76it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.73it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.77it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.72it/s]W0402 09:40:48.884011 1582287 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 38%|███▊      | 43/112 [00:15<00:24,  2.81it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.72it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.80it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.73it/s] 40%|████      | 45/112 [00:16<00:23,  2.81it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.75it/s]19_gate proxy err 0.004959686193615198 tr(WHW.T) 4580.61328125
  0%|          | 0/112 [00:00<?, ?it/s] 41%|████      | 46/112 [00:16<00:23,  2.78it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.77it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.75it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.76it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.78it/s] 29%|██▊       | 32/112 [00:12<00:28,  2.77it/s]  1%|          | 1/112 [00:00<01:29,  1.24it/s] 44%|████▍     | 49/112 [00:17<00:22,  2.78it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.77it/s]  2%|▏         | 2/112 [00:01<00:59,  1.84it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.81it/s] 30%|███       | 34/112 [00:12<00:28,  2.77it/s]  3%|▎         | 3/112 [00:01<00:50,  2.17it/s]18_gate proxy err 0.004493517801165581 tr(WHW.T) 4660.9404296875
  0%|          | 0/112 [00:00<?, ?it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.83it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.78it/s]  4%|▎         | 4/112 [00:01<00:45,  2.39it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.85it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.78it/s]  4%|▍         | 5/112 [00:02<00:42,  2.52it/s]  1%|          | 1/112 [00:00<01:28,  1.25it/s] 47%|████▋     | 53/112 [00:19<00:20,  2.86it/s] 33%|███▎      | 37/112 [00:14<00:26,  2.78it/s]  5%|▌         | 6/112 [00:02<00:40,  2.61it/s]  2%|▏         | 2/112 [00:01<00:58,  1.86it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.88it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.79it/s]  6%|▋         | 7/112 [00:02<00:39,  2.67it/s]  3%|▎         | 3/112 [00:01<00:49,  2.21it/s] 49%|████▉     | 55/112 [00:20<00:19,  2.88it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.79it/s]  7%|▋         | 8/112 [00:03<00:38,  2.71it/s] 50%|█████     | 56/112 [00:20<00:19,  2.88it/s]  4%|▎         | 4/112 [00:01<00:44,  2.41it/s] 36%|███▌      | 40/112 [00:15<00:25,  2.78it/s]  8%|▊         | 9/112 [00:03<00:37,  2.73it/s] 51%|█████     | 57/112 [00:20<00:19,  2.89it/s]  4%|▍         | 5/112 [00:02<00:42,  2.53it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.79it/s]  9%|▉         | 10/112 [00:04<00:36,  2.76it/s] 52%|█████▏    | 58/112 [00:21<00:18,  2.86it/s]  5%|▌         | 6/112 [00:02<00:40,  2.60it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.79it/s] 10%|▉         | 11/112 [00:04<00:36,  2.78it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.85it/s]  6%|▋         | 7/112 [00:02<00:39,  2.65it/s] 38%|███▊      | 43/112 [00:16<00:24,  2.77it/s] 11%|█         | 12/112 [00:04<00:35,  2.78it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.86it/s]  7%|▋         | 8/112 [00:03<00:38,  2.70it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.78it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.80it/s] 54%|█████▍    | 61/112 [00:22<00:17,  2.88it/s]  8%|▊         | 9/112 [00:03<00:37,  2.73it/s] 12%|█▎        | 14/112 [00:05<00:34,  2.80it/s] 40%|████      | 45/112 [00:16<00:24,  2.78it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.88it/s]  9%|▉         | 10/112 [00:04<00:37,  2.75it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.81it/s] 41%|████      | 46/112 [00:17<00:23,  2.78it/s] 56%|█████▋    | 63/112 [00:22<00:16,  2.89it/s] 10%|▉         | 11/112 [00:04<00:36,  2.77it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.81it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.78it/s] 57%|█████▋    | 64/112 [00:23<00:16,  2.88it/s] 11%|█         | 12/112 [00:04<00:36,  2.77it/s] 15%|█▌        | 17/112 [00:06<00:33,  2.80it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.78it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.84it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.73it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.80it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.78it/s] 59%|█████▉    | 66/112 [00:23<00:16,  2.84it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.76it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.81it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.77it/s] 60%|█████▉    | 67/112 [00:24<00:15,  2.86it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.75it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.81it/s] 46%|████▌     | 51/112 [00:19<00:21,  2.78it/s] 61%|██████    | 68/112 [00:24<00:15,  2.86it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.76it/s] 19%|█▉        | 21/112 [00:07<00:32,  2.81it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.78it/s] 62%|██████▏   | 69/112 [00:24<00:15,  2.86it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.77it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.80it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.78it/s] 62%|██████▎   | 70/112 [00:25<00:14,  2.83it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.77it/s] 21%|██        | 23/112 [00:08<00:31,  2.80it/s] 48%|████▊     | 54/112 [00:20<00:20,  2.78it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.85it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.78it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.80it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.79it/s] 64%|██████▍   | 72/112 [00:25<00:13,  2.86it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.80it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.80it/s] 50%|█████     | 56/112 [00:20<00:20,  2.79it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.87it/s] 19%|█▉        | 21/112 [00:07<00:32,  2.79it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.80it/s] 51%|█████     | 57/112 [00:21<00:19,  2.78it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.88it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.80it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.80it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.78it/s] 67%|██████▋   | 75/112 [00:27<00:12,  2.89it/s] 21%|██        | 23/112 [00:08<00:31,  2.80it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.79it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.78it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.89it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.80it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.78it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.78it/s] 69%|██████▉   | 77/112 [00:27<00:12,  2.89it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.79it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.78it/s] 70%|██████▉   | 78/112 [00:28<00:11,  2.89it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.77it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.80it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.79it/s] 71%|███████   | 79/112 [00:28<00:11,  2.89it/s] 55%|█████▌    | 62/112 [00:22<00:18,  2.77it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.79it/s] 71%|███████▏  | 80/112 [00:28<00:11,  2.89it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.77it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.78it/s] 25%|██▌       | 28/112 [00:10<00:29,  2.80it/s] 72%|███████▏  | 81/112 [00:29<00:10,  2.89it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.79it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.78it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.80it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.89it/s] 30%|███       | 34/112 [00:12<00:27,  2.79it/s] 58%|█████▊    | 65/112 [00:24<00:16,  2.78it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.81it/s] 74%|███████▍  | 83/112 [00:29<00:10,  2.89it/s] 31%|███▏      | 35/112 [00:12<00:27,  2.79it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.79it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.79it/s] 75%|███████▌  | 84/112 [00:30<00:09,  2.86it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.79it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.79it/s] 29%|██▊       | 32/112 [00:11<00:29,  2.75it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.83it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.77it/s] 61%|██████    | 68/112 [00:25<00:15,  2.77it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.76it/s] 77%|███████▋  | 86/112 [00:30<00:09,  2.85it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.78it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.78it/s] 30%|███       | 34/112 [00:12<00:28,  2.77it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.85it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.78it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.78it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.77it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.87it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.79it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.79it/s] 79%|███████▉  | 89/112 [00:31<00:07,  2.89it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.79it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.78it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.78it/s] 80%|████████  | 90/112 [00:32<00:07,  2.89it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.79it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.77it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.78it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.88it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.79it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.78it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.78it/s] 82%|████████▏ | 92/112 [00:32<00:06,  2.89it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.80it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.78it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.78it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.90it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.81it/s] 40%|████      | 45/112 [00:16<00:24,  2.79it/s] 68%|██████▊   | 76/112 [00:28<00:12,  2.79it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.90it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.81it/s] 41%|████      | 46/112 [00:16<00:23,  2.78it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.79it/s] 85%|████████▍ | 95/112 [00:33<00:05,  2.92it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.81it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.79it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.79it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.91it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.80it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.80it/s] 71%|███████   | 79/112 [00:29<00:11,  2.79it/s] 87%|████████▋ | 97/112 [00:34<00:05,  2.89it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.80it/s] 44%|████▍     | 49/112 [00:17<00:22,  2.78it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.79it/s] 88%|████████▊ | 98/112 [00:34<00:04,  2.89it/s] 40%|████      | 45/112 [00:16<00:23,  2.81it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.79it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.79it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.90it/s] 41%|████      | 46/112 [00:16<00:23,  2.80it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.79it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.79it/s] 89%|████████▉ | 100/112 [00:35<00:04,  2.87it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.79it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.79it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.79it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.88it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.80it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.79it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.79it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.90it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.80it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.79it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.78it/s] 92%|█████████▏| 103/112 [00:36<00:03,  2.87it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.76it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.78it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.77it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.86it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.78it/s] 50%|█████     | 56/112 [00:20<00:20,  2.78it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.78it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.87it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.78it/s] 51%|█████     | 57/112 [00:20<00:19,  2.79it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.78it/s] 95%|█████████▍| 106/112 [00:37<00:02,  2.87it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.79it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.79it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.88it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.78it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.80it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.79it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.88it/s] 80%|████████  | 90/112 [00:33<00:07,  2.79it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.80it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.80it/s] 97%|█████████▋| 109/112 [00:38<00:01,  2.88it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.79it/s] 50%|█████     | 56/112 [00:20<00:20,  2.79it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.79it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.87it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.78it/s] 51%|█████     | 57/112 [00:20<00:19,  2.79it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.80it/s] 99%|█████████▉| 111/112 [00:39<00:00,  2.87it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.79it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.79it/s]100%|██████████| 112/112 [00:39<00:00,  2.88it/s]100%|██████████| 112/112 [00:39<00:00,  2.81it/s]
 56%|█████▋    | 63/112 [00:23<00:17,  2.80it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.79it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.80it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.79it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.78it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.75it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.80it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.79it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.70it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.80it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.79it/s] 55%|█████▌    | 62/112 [00:22<00:18,  2.67it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.79it/s] 88%|████████▊ | 98/112 [00:35<00:05,  2.78it/s] 56%|█████▋    | 63/112 [00:23<00:18,  2.71it/s] 61%|██████    | 68/112 [00:24<00:15,  2.80it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.79it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.70it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.80it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.77it/s] 58%|█████▊    | 65/112 [00:23<00:17,  2.74it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.79it/s] 90%|█████████ | 101/112 [00:37<00:03,  2.77it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.73it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.79it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.77it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.75it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.79it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.76it/s] 61%|██████    | 68/112 [00:24<00:15,  2.76it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.77it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.75it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.74it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.78it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.76it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.77it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.79it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.74it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.76it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.78it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.75it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.77it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.78it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.75it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.79it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.79it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.77it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.79it/s] 71%|███████   | 79/112 [00:28<00:11,  2.79it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.75it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.79it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.79it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.76it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.79it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.79it/s]100%|██████████| 112/112 [00:40<00:00,  2.77it/s]100%|██████████| 112/112 [00:40<00:00,  2.73it/s]
W0402 09:41:19.802000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:41:19.802000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:19.802000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:19.802000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:19.802000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:19.802000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:19.802000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:19.843000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:19.843000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:19.843000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:19.843000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:19.843000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:19.858000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:19.859000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:19.859000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:19.859000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:19.859000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 77/112 [00:28<00:12,  2.79it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.79it/s]W0402 09:41:20.019000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.019000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.019000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.019000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.019000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 70%|██████▉   | 78/112 [00:28<00:12,  2.80it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.75it/s]W0402 09:41:20.322000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.322000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.322000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.322000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.322000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.322000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.323000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.353000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.354000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.354000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.354000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.354000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.420000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.420000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.420000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.420000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:20.420000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 71%|███████   | 79/112 [00:28<00:11,  2.81it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.72it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.79it/s] 76%|███████▌  | 85/112 [00:30<00:10,  2.69it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.80it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.72it/s]W0402 09:41:21.541000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:21.553000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:21.561000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:21.561000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 73%|███████▎  | 82/112 [00:29<00:10,  2.80it/s] 78%|███████▊  | 87/112 [00:31<00:09,  2.72it/s]W0402 09:41:21.991000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:41:21.992000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:21.992000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:21.992000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:21.992000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:21.992000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:21.992000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:22.020000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:22.021000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:22.021000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:22.021000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:22.021000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 74%|███████▍  | 83/112 [00:30<00:10,  2.80it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.72it/s]W0402 09:41:22.346000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:41:22.346000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:22.346000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:22.346000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:22.347000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:22.347000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:22.347000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:22.347000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 84/112 [00:30<00:09,  2.81it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.75it/s]W0402 09:41:22.626000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:22.626000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:22.626000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:22.626000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:22.627000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 76%|███████▌  | 85/112 [00:30<00:09,  2.81it/s] 80%|████████  | 90/112 [00:32<00:08,  2.74it/s]W0402 09:41:22.950000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:22.955000 139881133426496 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 77%|███████▋  | 86/112 [00:31<00:09,  2.78it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.74it/s] 78%|███████▊  | 87/112 [00:31<00:09,  2.76it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.75it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.76it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.77it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.77it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.76it/s] 80%|████████  | 90/112 [00:32<00:07,  2.78it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.78it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.73it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.79it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.69it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.79it/s] 83%|████████▎ | 93/112 [00:33<00:07,  2.67it/s] 88%|████████▊ | 98/112 [00:35<00:04,  2.82it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.82it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.65it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.81it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.64it/s]W0402 09:41:26.556000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.556000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.557000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.557000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.557000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.557000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.557000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.596000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.596000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.596000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.596000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.597000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.611000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.611000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.611000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.612000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.612000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.771000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.771000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.771000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.771000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:26.771000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 90%|█████████ | 101/112 [00:36<00:03,  2.81it/s] 86%|████████▌ | 96/112 [00:35<00:06,  2.63it/s]W0402 09:41:27.080000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:27.080000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:41:27.080000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:27.081000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:27.081000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:27.081000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:27.081000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:27.112000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:27.112000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:27.112000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:27.112000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:27.112000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 102/112 [00:37<00:03,  2.81it/s]W0402 09:41:27.181000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:27.181000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:27.181000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:27.181000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:27.181000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 87%|████████▋ | 97/112 [00:35<00:05,  2.63it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.81it/s] 88%|████████▊ | 98/112 [00:35<00:05,  2.62it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.81it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.62it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.81it/s]W0402 09:41:28.312000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:28.324000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:28.332000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:28.332000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 89%|████████▉ | 100/112 [00:36<00:04,  2.61it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.80it/s]W0402 09:41:28.763000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:28.763000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:28.763000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:28.763000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:28.763000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:28.763000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:41:28.763000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 90%|█████████ | 101/112 [00:37<00:04,  2.61it/s]W0402 09:41:28.793000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:28.793000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:28.794000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:28.794000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:28.794000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 96%|█████████▌| 107/112 [00:38<00:01,  2.81it/s]W0402 09:41:29.125000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:29.125000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:29.125000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:29.125000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:29.125000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:29.125000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:41:29.125000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:29.125000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 102/112 [00:37<00:03,  2.61it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.81it/s]W0402 09:41:29.411000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:29.411000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:29.411000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:29.411000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:29.411000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 92%|█████████▏| 103/112 [00:37<00:03,  2.61it/s]I0402 09:41:29.618378 1581323 finetune.py:45] layer 17_down initial loss 0.005076378118246794
W0402 09:41:29.618647 1581323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 109/112 [00:39<00:01,  2.80it/s]W0402 09:41:29.731000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:29.736000 140422584497984 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 93%|█████████▎| 104/112 [00:38<00:03,  2.65it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.80it/s]W0402 09:41:30.085739 1581323 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 94%|█████████▍| 105/112 [00:38<00:02,  2.64it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.77it/s]17_down proxy err 0.01263256836682558 tr(WHW.T) 21.307649612426758
 95%|█████████▍| 106/112 [00:38<00:02,  2.66it/s]100%|██████████| 112/112 [00:40<00:00,  2.78it/s]100%|██████████| 112/112 [00:40<00:00,  2.76it/s]
 96%|█████████▌| 107/112 [00:39<00:01,  2.69it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.73it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.75it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.72it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.13it/s]100%|██████████| 112/112 [00:41<00:00,  2.30it/s]100%|██████████| 112/112 [00:41<00:00,  2.70it/s]
I0402 09:41:37.094195 1581130 finetune.py:45] layer 16_down initial loss 0.003771432675421238
W0402 09:41:37.094498 1581130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:41:37.569824 1581130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

16_down proxy err 0.012407403439283371 tr(WHW.T) 17.929277420043945
W0402 09:41:37.957000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:37.957000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:37.958000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:37.958000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:37.958000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:37.958000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:41:37.958000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.002000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.002000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.002000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.002000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.002000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.017000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.018000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.018000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.018000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.018000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.180000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.180000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.180000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.180000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.180000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.490000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.491000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.491000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.491000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.491000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.491000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.491000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.522000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.522000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.522000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.522000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.522000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.590000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.590000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.590000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.590000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:38.590000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.739000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.750000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.758000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.758000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.846000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.847000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.847000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.847000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.847000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.847000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.847000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.888000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.888000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.888000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.888000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.888000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.903000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.904000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.904000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.904000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:39.904000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.069000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.069000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.069000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.069000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.069000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.197000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.197000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.197000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.197000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.197000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.197000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.197000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.228000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.228000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.228000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.228000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.228000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.374000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.375000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.375000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.375000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.375000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.375000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.375000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.404000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.404000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.404000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.404000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.404000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.471000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.471000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.471000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.471000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.472000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.572000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.572000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.573000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.573000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.573000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.573000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.573000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.573000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.861000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.862000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.862000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.862000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:40.862000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:41.192000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:41.197000 140227034195776 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:41.608000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:41.620000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:41.628000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:41.628000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.047000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.047000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.047000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.047000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.047000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.047000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.047000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.078000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.078000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.078000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.078000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.078000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
I0402 09:41:42.349612 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 20 in 1.854978322982788s
W0402 09:41:42.409000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.409000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.409000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.410000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.410000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.410000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.410000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.410000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.716000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.717000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.717000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.717000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:41:42.717000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
I0402 09:41:42.789741 1558010 quantize_finetune_llama.py:159] layer 21 gpu 1
W0402 09:41:43.056000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:41:43.061000 139777195726656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0402 09:41:44.933040 1586459 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:41:44.933201 1586459 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:41:44.933266 1586459 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:41:45.140151 1586459 config.py:58] PyTorch version 2.4.0 available.
I0402 09:41:47.424677 1586459 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 09:41:47.813810 1586459 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:41:48.064511 1582527 finetune.py:45] layer 19_down initial loss 0.006447996944189072
W0402 09:41:48.064933 1582527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:41:48.583255 1582527 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

  0%|          | 0/32 [00:00<?, ?it/s]19_down proxy err 0.012963976711034775 tr(WHW.T) 21.985565185546875
I0402 09:41:49.702516 1582287 finetune.py:45] layer 18_down initial loss 0.004193845205008984
W0402 09:41:49.702788 1582287 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:40,  1.30s/it]W0402 09:41:50.187982 1582287 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

  6%|▋         | 2/32 [00:01<00:21,  1.36it/s]18_down proxy err 0.012929018586874008 tr(WHW.T) 21.0947265625
  9%|▉         | 3/32 [00:01<00:16,  1.81it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.15it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.60it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.73it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.83it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.88it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.95it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.00it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.03it/s] 41%|████      | 13/32 [00:05<00:06,  3.03it/s]I0402 09:41:54.090482 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 21 in 1.4740593433380127s
 44%|████▍     | 14/32 [00:05<00:05,  3.01it/s]I0402 09:41:54.623675 1558010 quantize_finetune_llama.py:159] layer 22 gpu 2
 47%|████▋     | 15/32 [00:05<00:05,  2.97it/s] 50%|█████     | 16/32 [00:06<00:05,  2.95it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.96it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.96it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.97it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.99it/s]I0402 09:41:56.577170 1587085 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:41:56.577293 1587085 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:41:56.577357 1587085 utils.py:162] NumExpr defaulting to 16 threads.
 66%|██████▌   | 21/32 [00:07<00:03,  3.02it/s]I0402 09:41:56.765078 1587085 config.py:58] PyTorch version 2.4.0 available.
I0402 09:41:56.893431 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 22 in 1.6348533630371094s
 69%|██████▉   | 22/32 [00:08<00:03,  3.00it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.96it/s]I0402 09:41:57.397613 1558010 quantize_finetune_llama.py:159] layer 23 gpu 3
 75%|███████▌  | 24/32 [00:08<00:02,  2.94it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.94it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.94it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.97it/s]I0402 09:41:58.962875 1587085 data_utils.py:336] using 256 training seqs, 128 validation seqs
 88%|████████▊ | 28/32 [00:10<00:01,  2.94it/s]W0402 09:41:59.301326 1587085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:41:59.353774 1587269 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:41:59.353886 1587269 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:41:59.353944 1587269 utils.py:162] NumExpr defaulting to 16 threads.
 91%|█████████ | 29/32 [00:10<00:01,  2.96it/s]I0402 09:41:59.380031 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 23 in 1.3648128509521484s
I0402 09:41:59.548787 1587269 config.py:58] PyTorch version 2.4.0 available.
 94%|█████████▍| 30/32 [00:10<00:00,  2.93it/s]I0402 09:41:59.854319 1558010 quantize_finetune_llama.py:159] layer 24 gpu 0
 97%|█████████▋| 31/32 [00:11<00:00,  2.91it/s]  0%|          | 0/32 [00:00<?, ?it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.73it/s]
I0402 09:42:01.813458 1587269 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 09:42:01.821428 1587699 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:42:01.821548 1587699 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:42:01.821611 1587699 utils.py:162] NumExpr defaulting to 16 threads.
  3%|▎         | 1/32 [00:01<00:45,  1.46s/it]I0402 09:42:02.002989 1587699 config.py:58] PyTorch version 2.4.0 available.
W0402 09:42:02.197403 1587269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:01<00:24,  1.23it/s]  9%|▉         | 3/32 [00:02<00:17,  1.66it/s] 12%|█▎        | 4/32 [00:02<00:14,  2.00it/s]  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.25it/s]W0402 09:42:03.348000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.348000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.348000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.349000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.349000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.349000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.349000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.374000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.374000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.374000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.374000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.374000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.390000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.390000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.391000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.391000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.391000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:03<00:10,  2.44it/s]W0402 09:42:03.714000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.714000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.714000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.714000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:03.714000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s]I0402 09:42:04.086813 1587699 data_utils.py:336] using 256 training seqs, 128 validation seqs
 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s]W0402 09:42:04.399196 1587699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:45,  1.46s/it]W0402 09:42:04.605000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:04.605000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:04.605000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:04.605000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:04.605000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:04.605000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:04.605000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:04.623000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:04.623000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:04.623000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:04.623000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:04.623000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:04<00:08,  2.72it/s]W0402 09:42:04.857000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:04.857000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:04.857000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:04.857000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:04.857000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:23,  1.25it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s]  9%|▉         | 3/32 [00:02<00:17,  1.70it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s]  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.04it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.86it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s]W0402 09:42:06.005000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:06.005000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:06.005000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:06.005000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:06.005000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:06.006000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:06.006000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.87it/s]W0402 09:42:06.022000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:06.023000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:06.023000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:06.023000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:06.023000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:03<00:10,  2.49it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.87it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.64it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.90it/s]W0402 09:42:06.909000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:06.910000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:06.910000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:06.910000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:06.910000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s]  3%|▎         | 1/32 [00:01<00:49,  1.58s/it] 50%|█████     | 16/32 [00:06<00:05,  2.93it/s]  6%|▋         | 2/32 [00:01<00:25,  1.19it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.81it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
  9%|▉         | 3/32 [00:02<00:17,  1.66it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.87it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.94it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.05it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.92it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.93it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.90it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.90it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.57it/s] 41%|████      | 13/32 [00:05<00:06,  2.89it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.86it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.75it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.87it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.85it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.86it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.86it/s] 28%|██▊       | 9/32 [00:04<00:07,  2.95it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.84it/s] 50%|█████     | 16/32 [00:06<00:05,  2.86it/s] 31%|███▏      | 10/32 [00:04<00:07,  3.01it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.84it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.85it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.05it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.85it/s] 38%|███▊      | 12/32 [00:05<00:06,  3.05it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.83it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s] 41%|████      | 13/32 [00:05<00:06,  3.08it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.82it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.12it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.85it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.82it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.13it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.85it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.81it/s] 50%|█████     | 16/32 [00:06<00:05,  3.15it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.85it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.17it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.85it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.17it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.81it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.85it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.16it/s]100%|██████████| 32/32 [00:12<00:00,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
 78%|███████▊  | 25/32 [00:09<00:02,  2.85it/s]I0402 09:42:12.902470 1586459 finetune.py:45] layer 20_v initial loss 0.006524056661874056
W0402 09:42:12.902892 1586459 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 20/32 [00:07<00:03,  3.16it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.85it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.17it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.88it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.18it/s]W0402 09:42:13.871716 1586459 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 88%|████████▊ | 28/32 [00:10<00:01,  2.87it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.17it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.18it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.87it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.18it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.92it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.19it/s]20_v proxy err 0.010641125030815601 tr(WHW.T) 90.61302185058594
  0%|          | 0/32 [00:00<?, ?it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.94it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.19it/s]100%|██████████| 32/32 [00:12<00:00,  2.96it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
 88%|████████▊ | 28/32 [00:10<00:01,  3.19it/s]W0402 09:42:15.672000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:15.672000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:15.672000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:15.673000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:15.673000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:15.673000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:15.673000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:15.700000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:15.700000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:15.700000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:15.700000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:15.700000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:15.717000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:15.717000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:15.718000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:15.718000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:15.718000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:10<00:00,  3.18it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s]W0402 09:42:16.041000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:16.041000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:16.042000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:16.042000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:16.042000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:10<00:00,  3.17it/s]  6%|▋         | 2/32 [00:01<00:17,  1.68it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.16it/s]  9%|▉         | 3/32 [00:01<00:14,  2.05it/s]100%|██████████| 32/32 [00:11<00:00,  3.16it/s]100%|██████████| 32/32 [00:11<00:00,  2.81it/s]
 12%|█▎        | 4/32 [00:02<00:12,  2.28it/s]W0402 09:42:16.923000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:16.923000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:16.923000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:16.923000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:16.923000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:16.924000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:16.924000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:16.941000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:16.941000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:16.941000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:16.941000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:16.941000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:17.175000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:17.175000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:17.175000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:17.175000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:17.175000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.57it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s]W0402 09:42:18.090000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.091000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.091000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.091000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.091000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.091000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.091000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.117000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.117000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.117000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.117000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.117000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.133000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.133000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.134000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.134000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.134000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.307000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.307000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.308000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.308000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.308000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.308000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.308000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s]W0402 09:42:18.325000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.325000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.325000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.325000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.325000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.452000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.452000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.452000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.452000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:18.452000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s]W0402 09:42:19.212000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.212000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.212000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.213000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.213000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.313000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.313000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.313000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.313000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.313000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.314000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.314000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.330000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.331000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.331000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.331000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.331000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.76it/s]W0402 09:42:19.552000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.552000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.552000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.553000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.553000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.553000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.553000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.571000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.571000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.571000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.571000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.571000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.579000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.579000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.579000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.579000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.579000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.597000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.598000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.598000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.598000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.598000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 38%|███▊      | 12/32 [00:04<00:07,  2.75it/s]W0402 09:42:19.909000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.909000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.909000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.909000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:19.909000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.74it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s]W0402 09:42:20.726000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.726000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.726000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.726000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.726000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.727000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.727000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.745000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.745000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.745000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.745000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.745000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.760000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.760000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.760000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.760000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.760000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.760000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.760000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.777000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.777000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.777000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.777000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:20.777000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 15/32 [00:05<00:06,  2.73it/s]W0402 09:42:21.007000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:21.007000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:21.007000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:21.007000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:21.007000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:05,  2.71it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.70it/s]W0402 09:42:21.681000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:21.681000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:21.681000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:21.681000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:21.681000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:07<00:05,  2.69it/s]W0402 09:42:22.116000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:22.116000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:22.116000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:22.116000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:22.116000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:22.116000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:22.116000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:22.136000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:22.136000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:22.136000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:22.136000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:22.136000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.67it/s]W0402 09:42:22.995000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:22.995000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:22.995000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:22.995000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:22.995000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:04,  2.66it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.66it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 72%|███████▏  | 23/32 [00:08<00:03,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.67it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.67it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.67it/s]I0402 09:42:25.437309 1587085 finetune.py:45] layer 21_v initial loss 0.00936864409595728
W0402 09:42:25.437824 1587085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 28/32 [00:10<00:01,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.65it/s]W0402 09:42:26.640755 1587085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:11<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
I0402 09:42:27.572890 1587269 finetune.py:45] layer 22_v initial loss 0.006833809427917004
W0402 09:42:27.573282 1587269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

21_v proxy err 0.010498197749257088 tr(WHW.T) 95.41011047363281
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:42:28.531359 1587269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:42:28.591518 1587699 finetune.py:45] layer 23_v initial loss 0.0106764892116189
W0402 09:42:28.591739 1587699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:00<00:30,  1.03it/s]  6%|▋         | 2/32 [00:01<00:18,  1.62it/s]  9%|▉         | 3/32 [00:01<00:14,  2.01it/s]22_v proxy err 0.011455560103058815 tr(WHW.T) 101.29380798339844
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:42:29.529706 1587699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:02<00:12,  2.26it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s]  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]23_v proxy err 0.011988220736384392 tr(WHW.T) 112.7859115600586
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s]  6%|▋         | 2/32 [00:01<00:17,  1.69it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s]  9%|▉         | 3/32 [00:01<00:14,  2.05it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s]  3%|▎         | 1/32 [00:00<00:27,  1.13it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.28it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.68it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s]  9%|▉         | 3/32 [00:01<00:13,  2.15it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.73it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.39it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.75it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.55it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s]W0402 09:42:33.001000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.001000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.001000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.001000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.001000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.001000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.002000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.031000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.032000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.032000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.032000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.032000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.047000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.047000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.048000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.048000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.048000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.75it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.64it/s]W0402 09:42:33.200000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.200000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.201000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.201000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.201000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:03<00:08,  2.66it/s]W0402 09:42:33.416000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.416000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.416000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.416000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.416000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.416000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.416000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s]W0402 09:42:33.437000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.437000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.438000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.438000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.438000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:02<00:09,  2.72it/s]W0402 09:42:33.501000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.501000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.501000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.501000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:33.501000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.73it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.71it/s] 50%|█████     | 16/32 [00:06<00:05,  2.75it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.78it/s]W0402 09:42:34.327000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:04<00:07,  2.71it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.80it/s]W0402 09:42:34.626000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:34.626000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:34.626000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:34.626000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:34.626000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:34.626000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:34.626000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:34.648000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:34.648000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:34.648000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:34.648000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:34.648000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.76it/s]W0402 09:42:34.894000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:34.894000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:34.894000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:34.894000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:34.894000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s]W0402 09:42:35.146000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s] 41%|████      | 13/32 [00:05<00:06,  2.84it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.86it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.77it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.87it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.68it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.87it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.75it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.77it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.69it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.83it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.72it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.86it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.60it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.87it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.68it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.87it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.88it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.65it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.57it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.88it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.62it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.87it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.53it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
 81%|████████▏ | 26/32 [00:09<00:02,  2.88it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.54it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.55it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.57it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.87it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.57it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.88it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.55it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
I0402 09:42:42.200053 1586459 finetune.py:45] layer 20_q initial loss 0.006509359460324049
W0402 09:42:42.200728 1586459 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:11<00:00,  2.88it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
W0402 09:42:43.373044 1586459 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_q proxy err 0.0023143338039517403 tr(WHW.T) 5690.95703125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.15it/s]  6%|▋         | 2/32 [00:01<00:17,  1.74it/s]  9%|▉         | 3/32 [00:01<00:13,  2.10it/s]W0402 09:42:46.193000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.193000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.193000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.193000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.194000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.194000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.194000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.223000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.223000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.223000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.223000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.223000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.239000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.239000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.239000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.239000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.239000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:01<00:12,  2.30it/s]W0402 09:42:46.389000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.390000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.390000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.390000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.390000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.607000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.607000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.607000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.608000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.608000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.608000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.608000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.628000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.628000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.628000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.628000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.628000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.690000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.690000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.690000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.690000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:46.690000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s]W0402 09:42:47.525000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s]W0402 09:42:47.831000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.831000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.831000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.832000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.832000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.832000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.832000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.852000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.852000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.852000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.852000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.852000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.870000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.870000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.870000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.870000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.870000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.870000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.870000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.898000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.898000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.899000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.899000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.899000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.914000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.914000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.914000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.914000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.914000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.960000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.960000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.961000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.961000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.961000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.961000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.961000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.988000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.988000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.988000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.988000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:47.989000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.003000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.003000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.004000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.004000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.004000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.066000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.066000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.066000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.066000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.066000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.106000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.107000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.107000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.107000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.107000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.161000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.161000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.161000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.161000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.161000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:03<00:08,  2.69it/s]W0402 09:42:48.281000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.281000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.281000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.282000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.282000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.282000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.282000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.300000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.300000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.301000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.301000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.301000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.360000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.363000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.363000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.363000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.363000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.363000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.382000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.382000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.382000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.382000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.382000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.382000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.383000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.402000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.402000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.402000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.402000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.402000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.469000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.469000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.469000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.469000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:48.469000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.76it/s]W0402 09:42:49.177000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:04<00:07,  2.79it/s]W0402 09:42:49.309000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.474000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.474000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.474000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.474000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.474000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.475000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.475000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.495000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.495000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.495000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.496000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.496000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.606000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.606000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.606000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.606000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.606000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.606000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.607000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.75it/s]W0402 09:42:49.628000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.628000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.628000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.628000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.629000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.738000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.738000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.738000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.738000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.738000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.874000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.874000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.874000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.874000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:49.874000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.79it/s]W0402 09:42:49.985000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:42:50.125000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 15/32 [00:05<00:06,  2.77it/s] 50%|█████     | 16/32 [00:06<00:05,  2.79it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.85it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.80it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.72it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s]I0402 09:42:54.932610 1587085 finetune.py:45] layer 21_q initial loss 0.009369652718305588
W0402 09:42:54.932893 1587085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 28/32 [00:10<00:01,  2.71it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.71it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s]I0402 09:42:55.844067 1587699 finetune.py:45] layer 23_q initial loss 0.010672462172806263
W0402 09:42:55.844365 1587699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:42:56.132860 1587085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s]I0402 09:42:56.429877 1587269 finetune.py:45] layer 22_q initial loss 0.006828994024544954
W0402 09:42:56.430246 1587269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:12<00:00,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
W0402 09:42:56.794642 1587699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

21_q proxy err 0.0018425163580104709 tr(WHW.T) 6793.6064453125
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:42:57.447089 1587269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

23_q proxy err 0.0020481725223362446 tr(WHW.T) 6408.4541015625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:27,  1.11it/s]22_q proxy err 0.0020816163159906864 tr(WHW.T) 5947.34326171875
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:01<00:17,  1.71it/s]  3%|▎         | 1/32 [00:00<00:26,  1.17it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s]  6%|▋         | 2/32 [00:01<00:16,  1.81it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.32it/s]  9%|▉         | 3/32 [00:01<00:13,  2.19it/s]  3%|▎         | 1/32 [00:00<00:26,  1.16it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.47it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.42it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.59it/s]  9%|▉         | 3/32 [00:01<00:13,  2.09it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.69it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.28it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.75it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.81it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.49it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.85it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.88it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.90it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.66it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.88it/s] 41%|████      | 13/32 [00:05<00:07,  2.67it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 41%|████      | 13/32 [00:04<00:06,  2.90it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.68it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.91it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.91it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.65it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.86it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.70it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.88it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.68it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.69it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.88it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.67it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.67it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.88it/s] 50%|█████     | 16/32 [00:06<00:05,  2.67it/s]I0402 09:43:05.094121 1586459 finetune.py:45] layer 20_k initial loss 0.00651040393859148
W0402 09:43:05.094354 1586459 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 20/32 [00:07<00:04,  2.90it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.68it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.68it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.91it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.90it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.69it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.92it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.70it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.93it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.94it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.76it/s]W0402 09:43:07.112318 1586459 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 81%|████████▏ | 26/32 [00:09<00:02,  2.93it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.79it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.74it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.92it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.74it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.91it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.80it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.92it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.81it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.77it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.91it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s]20_k proxy err 0.0017437328351661563 tr(WHW.T) 4224.9970703125
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.78it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.92it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.82it/s]100%|██████████| 32/32 [00:11<00:00,  2.93it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]
 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
  3%|▎         | 1/32 [00:00<00:23,  1.32it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.79it/s]  6%|▋         | 2/32 [00:01<00:15,  1.90it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s]  9%|▉         | 3/32 [00:01<00:13,  2.19it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.74it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.39it/s]100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
 16%|█▌        | 5/32 [00:02<00:10,  2.53it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.58it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.61it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.63it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.64it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.67it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.68it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.68it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s]I0402 09:43:15.289955 1587699 finetune.py:45] layer 23_k initial loss 0.010666312649846077
W0402 09:43:15.290205 1587699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 53%|█████▎    | 17/32 [00:06<00:05,  2.70it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.76it/s]W0402 09:43:16.219162 1587699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:43:16.326822 1587085 finetune.py:45] layer 21_k initial loss 0.009368709288537502
W0402 09:43:16.327213 1587085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.77it/s]I0402 09:43:17.168024 1587269 finetune.py:45] layer 22_k initial loss 0.006830237805843353
W0402 09:43:17.168291 1587269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

23_k proxy err 0.001799992984160781 tr(WHW.T) 4211.31982421875
  0%|          | 0/32 [00:00<?, ?it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s]W0402 09:43:17.349275 1587085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.80it/s]  3%|▎         | 1/32 [00:00<00:23,  1.34it/s]W0402 09:43:18.137198 1587269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s]  6%|▋         | 2/32 [00:01<00:15,  1.96it/s]21_k proxy err 0.0016456502489745617 tr(WHW.T) 4413.919921875
  0%|          | 0/32 [00:00<?, ?it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.80it/s]  9%|▉         | 3/32 [00:01<00:12,  2.27it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.47it/s]22_k proxy err 0.001686365227214992 tr(WHW.T) 4309.5947265625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:23,  1.32it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.61it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.82it/s]  6%|▋         | 2/32 [00:01<00:15,  1.92it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.67it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.78it/s]  9%|▉         | 3/32 [00:01<00:13,  2.21it/s]  3%|▎         | 1/32 [00:00<00:23,  1.32it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.40it/s]  6%|▋         | 2/32 [00:01<00:15,  1.90it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.77it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.52it/s]  9%|▉         | 3/32 [00:01<00:13,  2.23it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
 19%|█▉        | 6/32 [00:02<00:10,  2.60it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.41it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.81it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.62it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.49it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.83it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.67it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.61it/s] 41%|████      | 13/32 [00:04<00:06,  2.84it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.63it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.84it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.69it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.66it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.84it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 50%|█████     | 16/32 [00:06<00:05,  2.85it/s] 41%|████      | 13/32 [00:05<00:07,  2.71it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.68it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.81it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.80it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.83it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.68it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.83it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.64it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.85it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.66it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.86it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 50%|█████     | 16/32 [00:06<00:06,  2.64it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.87it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.63it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.88it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.64it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.64it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.89it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.89it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.64it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.85it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.64it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.87it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.67it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.89it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.64it/s]I0402 09:43:28.430182 1586459 finetune.py:45] layer 20_o initial loss 0.006593754515051842
W0402 09:43:28.430634 1586459 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.90it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s]100%|██████████| 32/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]
 84%|████████▍ | 27/32 [00:10<00:01,  2.71it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.69it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.71it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.71it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s]W0402 09:43:29.639790 1586459 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:10<00:01,  2.72it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.69it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s]20_o proxy err 0.010947461239993572 tr(WHW.T) 4.07815408706665
  0%|          | 0/32 [00:00<?, ?it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
  3%|▎         | 1/32 [00:02<01:02,  2.02s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it]I0402 09:43:35.229326 1587699 finetune.py:45] layer 23_o initial loss 0.010773011483252048
W0402 09:43:35.229488 1587699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:04<00:46,  1.60s/it]W0402 09:43:36.167754 1587699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

23_o proxy err 0.010030663572251797 tr(WHW.T) 6.165833473205566
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:06<00:42,  1.53s/it]I0402 09:43:37.809430 1587085 finetune.py:45] layer 21_o initial loss 0.009466642513871193
W0402 09:43:37.809764 1587085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 09:43:38.246837 1587269 finetune.py:45] layer 22_o initial loss 0.006851525511592627
W0402 09:43:38.247088 1587269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:07<00:40,  1.48s/it]W0402 09:43:38.888167 1587085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:57,  1.85s/it]W0402 09:43:39.180858 1587269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:09<00:37,  1.45s/it]21_o proxy err 0.007917276583611965 tr(WHW.T) 7.5875244140625
  0%|          | 0/32 [00:00<?, ?it/s]22_o proxy err 0.011781979352235794 tr(WHW.T) 5.007936477661133
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:03<00:47,  1.57s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.44s/it]  9%|▉         | 3/32 [00:04<00:43,  1.49s/it]  3%|▎         | 1/32 [00:01<00:59,  1.90s/it]  3%|▎         | 1/32 [00:01<00:58,  1.88s/it] 25%|██▌       | 8/32 [00:12<00:34,  1.43s/it] 12%|█▎        | 4/32 [00:06<00:40,  1.45s/it]  6%|▋         | 2/32 [00:03<00:48,  1.63s/it]  6%|▋         | 2/32 [00:03<00:48,  1.61s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 16%|█▌        | 5/32 [00:07<00:38,  1.44s/it]  9%|▉         | 3/32 [00:04<00:44,  1.54s/it]  9%|▉         | 3/32 [00:04<00:44,  1.52s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 19%|█▉        | 6/32 [00:08<00:36,  1.42s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.51s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.42s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.41s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.48s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it] 25%|██▌       | 8/32 [00:11<00:33,  1.41s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.47s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.46s/it] 41%|████      | 13/32 [00:19<00:26,  1.41s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.41s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.40s/it] 25%|██▌       | 8/32 [00:12<00:34,  1.46s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.45s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.40s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.45s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 38%|███▊      | 12/32 [00:17<00:27,  1.40s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.45s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 41%|████      | 13/32 [00:18<00:26,  1.40s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.45s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.44s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.41s/it] 44%|████▍     | 14/32 [00:19<00:25,  1.40s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.44s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.45s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.40s/it] 41%|████      | 13/32 [00:19<00:27,  1.44s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 50%|█████     | 16/32 [00:22<00:22,  1.39s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.44s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.45s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.41s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.39s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.44s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.44s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.39s/it] 50%|█████     | 16/32 [00:23<00:22,  1.44s/it] 50%|█████     | 16/32 [00:23<00:23,  1.44s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.42s/it] 59%|█████▉    | 19/32 [00:26<00:18,  1.40s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.44s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.44s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.42s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.40s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.44s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.45s/it] 78%|███████▊  | 25/32 [00:36<00:09,  1.42s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.40s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.44s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.44s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.40s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.44s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.44s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.42s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.40s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.44s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.44s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.42s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.41s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.44s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.45s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.42s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.40s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.44s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.45s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.42s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.40s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.44s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.45s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.41s/it] 84%|████████▍ | 27/32 [00:38<00:06,  1.40s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.44s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.45s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.40s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]100%|██████████| 32/32 [00:45<00:00,  1.44s/it]
 81%|████████▏ | 26/32 [00:37<00:08,  1.45s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.46s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.40s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.46s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.46s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.40s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.46s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.40s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.47s/it]100%|██████████| 32/32 [00:45<00:00,  1.40s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]
 91%|█████████ | 29/32 [00:42<00:04,  1.47s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.47s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.47s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.47s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.47s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.47s/it]I0402 09:44:25.780628 1586459 finetune.py:45] layer 20_up initial loss 0.006510377395898104
W0402 09:44:25.780989 1586459 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:46<00:00,  1.46s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
W0402 09:44:26.912872 1586459 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:46<00:00,  1.46s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
20_up proxy err 0.011813708581030369 tr(WHW.T) 1127.510498046875
  0%|          | 0/32 [00:00<?, ?it/s]I0402 09:44:29.904012 1587699 finetune.py:45] layer 23_up initial loss 0.010685745626688004
W0402 09:44:29.904168 1587699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:59,  1.92s/it]W0402 09:44:30.729054 1587699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:03<00:48,  1.63s/it]23_up proxy err 0.01219997275620699 tr(WHW.T) 1298.444580078125
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:04<00:44,  1.52s/it]  3%|▎         | 1/32 [00:01<00:54,  1.77s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it]I0402 09:44:34.713041 1587269 finetune.py:45] layer 22_up initial loss 0.006768158636987209
W0402 09:44:34.713438 1587269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:03<00:46,  1.55s/it]I0402 09:44:35.170976 1587085 finetune.py:45] layer 21_up initial loss 0.009347626939415932
W0402 09:44:35.171225 1587085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:44:35.570862 1587269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:07<00:39,  1.45s/it]W0402 09:44:36.030930 1587085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:04<00:42,  1.48s/it]22_up proxy err 0.011925896629691124 tr(WHW.T) 1249.8564453125
  0%|          | 0/32 [00:00<?, ?it/s]21_up proxy err 0.011405411176383495 tr(WHW.T) 1237.08935546875
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:08<00:37,  1.43s/it] 12%|█▎        | 4/32 [00:05<00:40,  1.45s/it]  3%|▎         | 1/32 [00:01<00:56,  1.83s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.43s/it]  3%|▎         | 1/32 [00:01<00:56,  1.82s/it] 16%|█▌        | 5/32 [00:07<00:38,  1.43s/it]  6%|▋         | 2/32 [00:03<00:47,  1.60s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.42s/it]  6%|▋         | 2/32 [00:03<00:47,  1.60s/it] 19%|█▉        | 6/32 [00:08<00:36,  1.42s/it]  9%|▉         | 3/32 [00:04<00:44,  1.52s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it]  9%|▉         | 3/32 [00:04<00:44,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.41s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it] 25%|██▌       | 8/32 [00:11<00:33,  1.40s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.43s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it] 28%|██▊       | 9/32 [00:12<00:32,  1.41s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.46s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.40s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.45s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it] 41%|████      | 13/32 [00:18<00:27,  1.42s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.40s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 38%|███▊      | 12/32 [00:17<00:27,  1.39s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 41%|████      | 13/32 [00:18<00:26,  1.40s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 44%|████▍     | 14/32 [00:19<00:25,  1.40s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.44s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.40s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.43s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.42s/it] 50%|█████     | 16/32 [00:22<00:22,  1.40s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it] 41%|████      | 13/32 [00:19<00:27,  1.43s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.39s/it] 41%|████      | 13/32 [00:18<00:27,  1.43s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.43s/it] 62%|██████▎   | 20/32 [00:28<00:17,  1.42s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.40s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.44s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.44s/it] 59%|█████▉    | 19/32 [00:26<00:18,  1.40s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.44s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 50%|█████     | 16/32 [00:23<00:22,  1.44s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.39s/it] 50%|█████     | 16/32 [00:23<00:22,  1.44s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.41s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.44s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.39s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.44s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.41s/it] 69%|██████▉   | 22/32 [00:31<00:13,  1.39s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.44s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.43s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.42s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.39s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.43s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.43s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 75%|███████▌  | 24/32 [00:33<00:11,  1.39s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.44s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.43s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.39s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.42s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.43s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.43s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.39s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.42s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.43s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.43s/it] 84%|████████▍ | 27/32 [00:38<00:06,  1.39s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.42s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.43s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.43s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.40s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.42s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.43s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.43s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.39s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.42s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.43s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.43s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.39s/it]100%|██████████| 32/32 [00:45<00:00,  1.42s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]
 81%|████████▏ | 26/32 [00:37<00:08,  1.43s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.43s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.41s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.43s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.43s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]
 88%|████████▊ | 28/32 [00:40<00:05,  1.43s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.43s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.43s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.42s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.42s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.42s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.42s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.41s/it]I0402 09:45:22.527784 1586459 finetune.py:45] layer 20_gate initial loss 0.006441305857151747
W0402 09:45:22.528115 1586459 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:46<00:00,  1.41s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
100%|██████████| 32/32 [00:46<00:00,  1.42s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
W0402 09:45:23.496298 1586459 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:45:24.493460 1587699 finetune.py:45] layer 23_gate initial loss 0.010585127398371696
W0402 09:45:24.493638 1587699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:45:25.224575 1587699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_gate proxy err 0.0053555662743747234 tr(WHW.T) 4549.56396484375
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:32,  1.19it/s]  2%|▏         | 2/112 [00:01<01:01,  1.80it/s]23_gate proxy err 0.006060384679585695 tr(WHW.T) 4776.345703125
  0%|          | 0/112 [00:00<?, ?it/s]  3%|▎         | 3/112 [00:01<00:50,  2.16it/s]  4%|▎         | 4/112 [00:01<00:45,  2.39it/s]  4%|▍         | 5/112 [00:02<00:42,  2.54it/s]  1%|          | 1/112 [00:00<01:28,  1.25it/s]  5%|▌         | 6/112 [00:02<00:39,  2.65it/s]  2%|▏         | 2/112 [00:01<00:58,  1.88it/s]  6%|▋         | 7/112 [00:02<00:38,  2.72it/s]  3%|▎         | 3/112 [00:01<00:48,  2.23it/s]  7%|▋         | 8/112 [00:03<00:37,  2.76it/s]  4%|▎         | 4/112 [00:01<00:44,  2.44it/s]  8%|▊         | 9/112 [00:03<00:36,  2.79it/s]  4%|▍         | 5/112 [00:02<00:41,  2.58it/s]  9%|▉         | 10/112 [00:04<00:36,  2.76it/s]  5%|▌         | 6/112 [00:02<00:40,  2.62it/s]I0402 09:45:30.732197 1587269 finetune.py:45] layer 22_gate initial loss 0.006721538491547108
W0402 09:45:30.732585 1587269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 10%|▉         | 11/112 [00:04<00:36,  2.78it/s]  6%|▋         | 7/112 [00:02<00:39,  2.68it/s] 11%|█         | 12/112 [00:04<00:35,  2.80it/s]  7%|▋         | 8/112 [00:03<00:38,  2.73it/s]W0402 09:45:31.542846 1587269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▏        | 13/112 [00:05<00:35,  2.81it/s]  8%|▊         | 9/112 [00:03<00:37,  2.77it/s]I0402 09:45:31.692944 1587085 finetune.py:45] layer 21_gate initial loss 0.009251362644135952
W0402 09:45:31.693209 1587085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 14/112 [00:05<00:34,  2.84it/s]  9%|▉         | 10/112 [00:03<00:36,  2.81it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.84it/s] 10%|▉         | 11/112 [00:04<00:35,  2.82it/s]W0402 09:45:32.484486 1587085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 14%|█▍        | 16/112 [00:06<00:33,  2.84it/s] 11%|█         | 12/112 [00:04<00:35,  2.82it/s] 15%|█▌        | 17/112 [00:06<00:33,  2.84it/s] 12%|█▏        | 13/112 [00:05<00:34,  2.84it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.84it/s] 12%|█▎        | 14/112 [00:05<00:34,  2.83it/s] 17%|█▋        | 19/112 [00:07<00:32,  2.83it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.84it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.82it/s] 14%|█▍        | 16/112 [00:06<00:33,  2.82it/s]22_gate proxy err 0.0055962782353162766 tr(WHW.T) 4890.9677734375
  0%|          | 0/112 [00:00<?, ?it/s] 19%|█▉        | 21/112 [00:07<00:32,  2.80it/s] 15%|█▌        | 17/112 [00:06<00:33,  2.81it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.80it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.81it/s] 21%|██        | 23/112 [00:08<00:31,  2.82it/s] 17%|█▋        | 19/112 [00:07<00:32,  2.82it/s]  1%|          | 1/112 [00:00<01:31,  1.21it/s] 21%|██▏       | 24/112 [00:08<00:31,  2.82it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.83it/s]  2%|▏         | 2/112 [00:01<01:01,  1.80it/s]21_gate proxy err 0.005190551746636629 tr(WHW.T) 5024.1337890625
  0%|          | 0/112 [00:00<?, ?it/s] 22%|██▏       | 25/112 [00:09<00:30,  2.82it/s] 19%|█▉        | 21/112 [00:07<00:32,  2.82it/s]  3%|▎         | 3/112 [00:01<00:51,  2.11it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.84it/s] 20%|█▉        | 22/112 [00:08<00:31,  2.84it/s]  4%|▎         | 4/112 [00:01<00:46,  2.32it/s] 24%|██▍       | 27/112 [00:10<00:29,  2.84it/s]  1%|          | 1/112 [00:00<01:31,  1.21it/s] 21%|██        | 23/112 [00:08<00:31,  2.84it/s]  4%|▍         | 5/112 [00:02<00:43,  2.46it/s] 25%|██▌       | 28/112 [00:10<00:29,  2.82it/s]  2%|▏         | 2/112 [00:01<01:00,  1.81it/s] 21%|██▏       | 24/112 [00:08<00:31,  2.83it/s]  5%|▌         | 6/112 [00:02<00:41,  2.54it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.79it/s]  3%|▎         | 3/112 [00:01<00:50,  2.14it/s] 22%|██▏       | 25/112 [00:09<00:30,  2.81it/s]  6%|▋         | 7/112 [00:03<00:40,  2.60it/s]  4%|▎         | 4/112 [00:01<00:45,  2.36it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.77it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.80it/s]  7%|▋         | 8/112 [00:03<00:39,  2.64it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.79it/s]  4%|▍         | 5/112 [00:02<00:43,  2.48it/s] 24%|██▍       | 27/112 [00:09<00:30,  2.81it/s]  8%|▊         | 9/112 [00:03<00:38,  2.67it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.80it/s] 25%|██▌       | 28/112 [00:10<00:29,  2.83it/s]  5%|▌         | 6/112 [00:02<00:41,  2.57it/s]  9%|▉         | 10/112 [00:04<00:37,  2.70it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.81it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.84it/s]  6%|▋         | 7/112 [00:02<00:39,  2.64it/s] 10%|▉         | 11/112 [00:04<00:37,  2.71it/s] 27%|██▋       | 30/112 [00:11<00:28,  2.85it/s] 30%|███       | 34/112 [00:12<00:28,  2.78it/s]  7%|▋         | 8/112 [00:03<00:38,  2.68it/s] 11%|█         | 12/112 [00:04<00:36,  2.72it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.86it/s] 31%|███▏      | 35/112 [00:12<00:27,  2.78it/s]  8%|▊         | 9/112 [00:03<00:38,  2.70it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.74it/s] 29%|██▊       | 32/112 [00:11<00:27,  2.86it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.79it/s]  9%|▉         | 10/112 [00:04<00:37,  2.73it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.74it/s] 29%|██▉       | 33/112 [00:12<00:27,  2.86it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.80it/s] 10%|▉         | 11/112 [00:04<00:36,  2.74it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.73it/s] 30%|███       | 34/112 [00:12<00:27,  2.86it/s] 34%|███▍      | 38/112 [00:13<00:26,  2.81it/s] 11%|█         | 12/112 [00:04<00:36,  2.75it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.74it/s] 31%|███▏      | 35/112 [00:12<00:26,  2.86it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.80it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.75it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.73it/s] 32%|███▏      | 36/112 [00:13<00:26,  2.83it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.78it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.75it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.73it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.83it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.79it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.76it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.73it/s] 34%|███▍      | 38/112 [00:13<00:25,  2.85it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.79it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.76it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.73it/s] 35%|███▍      | 39/112 [00:14<00:25,  2.86it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.80it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.76it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.74it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.87it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.81it/s] 16%|█▌        | 18/112 [00:06<00:34,  2.76it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.74it/s] 37%|███▋      | 41/112 [00:14<00:24,  2.87it/s] 40%|████      | 45/112 [00:16<00:23,  2.81it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.76it/s] 21%|██        | 23/112 [00:08<00:32,  2.74it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.87it/s] 41%|████      | 46/112 [00:16<00:23,  2.81it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.77it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.75it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.86it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.79it/s] 19%|█▉        | 21/112 [00:08<00:32,  2.77it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.74it/s] 39%|███▉      | 44/112 [00:15<00:24,  2.83it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.79it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.77it/s] 40%|████      | 45/112 [00:16<00:23,  2.82it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.74it/s] 44%|████▍     | 49/112 [00:17<00:22,  2.77it/s] 21%|██        | 23/112 [00:08<00:32,  2.77it/s] 41%|████      | 46/112 [00:16<00:23,  2.82it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.74it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.75it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.76it/s] 42%|████▏     | 47/112 [00:17<00:22,  2.83it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.73it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.78it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.76it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.84it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.72it/s] 46%|████▋     | 52/112 [00:18<00:21,  2.79it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.77it/s] 44%|████▍     | 49/112 [00:17<00:22,  2.85it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.73it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.80it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.77it/s] 45%|████▍     | 50/112 [00:18<00:21,  2.86it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.73it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.81it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.77it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.87it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.73it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.81it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.78it/s] 46%|████▋     | 52/112 [00:18<00:20,  2.87it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.72it/s] 50%|█████     | 56/112 [00:20<00:19,  2.81it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.77it/s] 47%|████▋     | 53/112 [00:19<00:20,  2.87it/s] 30%|███       | 34/112 [00:12<00:28,  2.72it/s] 51%|█████     | 57/112 [00:20<00:19,  2.83it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.77it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.88it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.73it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.82it/s] 29%|██▊       | 32/112 [00:12<00:28,  2.78it/s] 49%|████▉     | 55/112 [00:19<00:19,  2.86it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.82it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.72it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.77it/s] 50%|█████     | 56/112 [00:20<00:19,  2.86it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.81it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.72it/s] 30%|███       | 34/112 [00:12<00:28,  2.77it/s] 51%|█████     | 57/112 [00:20<00:19,  2.85it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.81it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.72it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.77it/s] 52%|█████▏    | 58/112 [00:20<00:18,  2.85it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.82it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.73it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.77it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.86it/s] 56%|█████▋    | 63/112 [00:22<00:17,  2.82it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.72it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.78it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.86it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.82it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.72it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.78it/s] 54%|█████▍    | 61/112 [00:21<00:17,  2.86it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.80it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.71it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.86it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.76it/s] 59%|█████▉    | 66/112 [00:23<00:16,  2.78it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.71it/s] 56%|█████▋    | 63/112 [00:22<00:17,  2.82it/s] 36%|███▌      | 40/112 [00:14<00:26,  2.77it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.78it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.71it/s] 57%|█████▋    | 64/112 [00:22<00:17,  2.79it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.77it/s] 61%|██████    | 68/112 [00:24<00:15,  2.79it/s] 40%|████      | 45/112 [00:16<00:24,  2.72it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.81it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.77it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.80it/s] 41%|████      | 46/112 [00:17<00:24,  2.73it/s] 59%|█████▉    | 66/112 [00:23<00:16,  2.81it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.77it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.79it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.73it/s] 60%|█████▉    | 67/112 [00:24<00:15,  2.82it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.77it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.80it/s] 61%|██████    | 68/112 [00:24<00:15,  2.83it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.73it/s] 40%|████      | 45/112 [00:16<00:24,  2.77it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.81it/s] 62%|██████▏   | 69/112 [00:24<00:15,  2.84it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.72it/s] 41%|████      | 46/112 [00:17<00:23,  2.77it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.82it/s] 62%|██████▎   | 70/112 [00:25<00:14,  2.85it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.73it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.78it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.83it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.86it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.73it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.78it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.83it/s] 64%|██████▍   | 72/112 [00:25<00:13,  2.87it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.73it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.78it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.83it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.88it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.72it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.77it/s] 69%|██████▉   | 77/112 [00:27<00:12,  2.80it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.84it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.78it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.72it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.80it/s] 67%|██████▋   | 75/112 [00:26<00:12,  2.85it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.77it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.72it/s] 71%|███████   | 79/112 [00:28<00:11,  2.81it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.85it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.76it/s] 50%|█████     | 56/112 [00:20<00:20,  2.70it/s] 71%|███████▏  | 80/112 [00:28<00:11,  2.81it/s] 69%|██████▉   | 77/112 [00:27<00:12,  2.85it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.76it/s] 51%|█████     | 57/112 [00:21<00:20,  2.71it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.80it/s] 70%|██████▉   | 78/112 [00:27<00:11,  2.86it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.77it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.71it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.81it/s] 71%|███████   | 79/112 [00:28<00:11,  2.86it/s] 50%|█████     | 56/112 [00:20<00:20,  2.76it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.70it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.82it/s] 71%|███████▏  | 80/112 [00:28<00:11,  2.86it/s] 51%|█████     | 57/112 [00:21<00:19,  2.77it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.72it/s] 75%|███████▌  | 84/112 [00:30<00:09,  2.82it/s] 72%|███████▏  | 81/112 [00:28<00:10,  2.86it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.77it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.73it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.78it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.82it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.77it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.72it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.79it/s] 74%|███████▍  | 83/112 [00:29<00:10,  2.83it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.77it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.72it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.78it/s] 75%|███████▌  | 84/112 [00:29<00:09,  2.84it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.78it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.73it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.80it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.86it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.78it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.74it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.80it/s] 77%|███████▋  | 86/112 [00:30<00:09,  2.86it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.77it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.74it/s] 80%|████████  | 90/112 [00:32<00:07,  2.80it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.85it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.77it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.74it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.86it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.81it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.78it/s] 61%|██████    | 68/112 [00:25<00:16,  2.74it/s] 79%|███████▉  | 89/112 [00:31<00:08,  2.86it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.82it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.74it/s] 80%|████████  | 90/112 [00:32<00:07,  2.86it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.75it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.83it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.74it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.87it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.84it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.73it/s] 61%|██████    | 68/112 [00:25<00:15,  2.75it/s] 82%|████████▏ | 92/112 [00:32<00:06,  2.86it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.82it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.73it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.76it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.85it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.79it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.75it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.76it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.86it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.80it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.74it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.77it/s] 85%|████████▍ | 95/112 [00:33<00:05,  2.86it/s] 88%|████████▊ | 98/112 [00:35<00:04,  2.81it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.74it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.77it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.86it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.79it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.75it/s] 87%|████████▋ | 97/112 [00:34<00:05,  2.85it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.77it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.80it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.75it/s] 88%|████████▊ | 98/112 [00:34<00:04,  2.85it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.77it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.80it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.73it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.87it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.77it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.78it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.74it/s] 89%|████████▉ | 100/112 [00:35<00:04,  2.87it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.78it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.78it/s] 71%|███████   | 79/112 [00:29<00:12,  2.74it/s] 90%|█████████ | 101/112 [00:35<00:03,  2.84it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.77it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.74it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.73it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.82it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.77it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.75it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.73it/s] 92%|█████████▏| 103/112 [00:36<00:03,  2.82it/s] 71%|███████   | 79/112 [00:28<00:11,  2.77it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.76it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.73it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.84it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.78it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.78it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.73it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.86it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.77it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.80it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.72it/s] 95%|█████████▍| 106/112 [00:37<00:02,  2.84it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.76it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.77it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.71it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.81it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.76it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.77it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.72it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.83it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.76it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.79it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.72it/s] 97%|█████████▋| 109/112 [00:38<00:01,  2.79it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.76it/s]100%|██████████| 112/112 [00:40<00:00,  2.78it/s]100%|██████████| 112/112 [00:40<00:00,  2.77it/s]
 79%|███████▊  | 88/112 [00:32<00:08,  2.73it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.81it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.77it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.74it/s] 99%|█████████▉| 111/112 [00:39<00:00,  2.79it/s] 78%|███████▊  | 87/112 [00:31<00:09,  2.78it/s] 80%|████████  | 90/112 [00:33<00:08,  2.74it/s]100%|██████████| 112/112 [00:39<00:00,  2.75it/s]100%|██████████| 112/112 [00:39<00:00,  2.81it/s]
 79%|███████▊  | 88/112 [00:32<00:08,  2.78it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.74it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.79it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.74it/s] 80%|████████  | 90/112 [00:32<00:07,  2.80it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.73it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.80it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.74it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.80it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.74it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.81it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.74it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.81it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.73it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.79it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.73it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.79it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.72it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.80it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.73it/s] 88%|████████▊ | 98/112 [00:35<00:05,  2.80it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.74it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.79it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.71it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.79it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.72it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.78it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.72it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.77it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.71it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.77it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.73it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.77it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.73it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.77it/s]W0402 09:46:14.085000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.085000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.085000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.085000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.085000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.085000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.085000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.125000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.125000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.126000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.126000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.126000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.141000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.141000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.141000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.141000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.141000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.299000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.299000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.300000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.300000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.300000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 96%|█████████▋| 108/112 [00:40<00:01,  2.73it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.78it/s]W0402 09:46:14.617000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.618000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.618000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.618000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.618000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.618000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.618000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.651000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.651000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.651000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.651000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.651000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.691000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.692000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.692000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.692000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.692000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.692000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.692000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.718000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.718000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.718000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.719000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.719000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.733000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.733000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.733000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.733000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.733000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.748000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.749000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.749000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.749000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.749000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 109/112 [00:40<00:01,  2.71it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.77it/s]W0402 09:46:14.909000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.909000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.909000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.909000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:14.909000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 98%|█████████▊| 110/112 [00:40<00:00,  2.72it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.73it/s]W0402 09:46:15.216000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:15.216000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:15.216000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:15.216000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:15.217000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:15.217000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:46:15.217000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:15.245000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:15.245000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:15.246000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:15.246000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:15.246000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:15.313000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:15.313000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:15.314000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:15.314000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:15.314000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 99%|█████████▉| 111/112 [00:41<00:00,  2.73it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.75it/s]W0402 09:46:15.865000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
100%|██████████| 112/112 [00:41<00:00,  2.73it/s]100%|██████████| 112/112 [00:41<00:00,  2.70it/s]
W0402 09:46:15.878000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:15.886000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:15.886000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 98%|█████████▊| 110/112 [00:40<00:00,  2.75it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.75it/s]W0402 09:46:16.320000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.321000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.321000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.321000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.321000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.321000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.321000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.351000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.351000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.351000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.351000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.351000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.429000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.441000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.449000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.449000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
100%|██████████| 112/112 [00:40<00:00,  2.69it/s]100%|██████████| 112/112 [00:40<00:00,  2.74it/s]
W0402 09:46:16.687000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.687000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.687000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.687000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.687000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.687000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.688000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.688000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.883000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.883000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.883000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.883000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.883000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.883000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.883000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.913000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.913000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.913000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.913000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.913000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.974000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.974000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.974000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.974000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:16.974000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:17.255000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:17.255000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:46:17.255000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:17.255000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:17.255000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:17.255000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:17.255000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:17.256000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:17.310000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:17.315000 139695150905152 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:17.538000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:17.538000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:17.538000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:17.538000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:17.538000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:17.866000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:17.871000 139858520573760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.523000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.523000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.523000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.523000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.523000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.523000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.523000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.563000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.563000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.563000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.563000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.563000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.578000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.578000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.578000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.578000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.578000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.739000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.739000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.739000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.739000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:22.739000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.050000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.050000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.051000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.051000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.051000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.051000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.051000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.081000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.081000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.082000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.082000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.082000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.150000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.150000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.150000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.150000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.150000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.696000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.696000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.696000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.696000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.696000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.696000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.696000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.738000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.738000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.738000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.738000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.738000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.754000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.754000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.754000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.754000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.754000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.921000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.921000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.921000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.921000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:23.921000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.228000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.228000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.228000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.228000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.228000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.228000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.229000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.261000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.261000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.261000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.261000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.261000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.286000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.292000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.298000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.298000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.331000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.331000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.331000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.331000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.331000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
I0402 09:46:24.413150 1587699 finetune.py:45] layer 23_down initial loss 0.010434293188154697
W0402 09:46:24.413372 1587699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 09:46:24.559063 1586459 finetune.py:45] layer 20_down initial loss 0.006370833609253168
W0402 09:46:24.559488 1586459 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:46:24.726000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.726000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.727000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.727000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.727000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.727000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.727000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.758000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.758000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.758000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.758000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.758000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:24.929140 1587699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0402 09:46:25.074569 1586459 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0402 09:46:25.090000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.091000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.091000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.091000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.091000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.091000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.091000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.091000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
23_down proxy err 0.01230670977383852 tr(WHW.T) 32.75361251831055
W0402 09:46:25.382000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.383000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.383000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.383000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.383000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
20_down proxy err 0.012843886390328407 tr(WHW.T) 23.935514450073242
W0402 09:46:25.468000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.473000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.479000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.479000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.701000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.706000 139720816715584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.907000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.907000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.907000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.908000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.908000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.908000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.908000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.940000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.940000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.940000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.941000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:25.941000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:26.276000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:26.276000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:46:26.276000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:26.276000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:26.276000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:26.277000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:26.277000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:26.277000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:26.563000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:26.563000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:26.563000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:26.563000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:26.563000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:26.892000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:26.897000 140055685896000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0402 09:46:33.020240 1587269 finetune.py:45] layer 22_down initial loss 0.006660526618361473
W0402 09:46:33.020553 1587269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:46:33.617456 1587269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

22_down proxy err 0.012211816385388374 tr(WHW.T) 30.777223587036133
I0402 09:46:34.185929 1587085 finetune.py:45] layer 21_down initial loss 0.00914760958403349
W0402 09:46:34.186261 1587085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:46:34.713981 1587085 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

21_down proxy err 0.012121197767555714 tr(WHW.T) 29.132450103759766
I0402 09:46:38.216272 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 24 in 1.1631255149841309s
I0402 09:46:38.751966 1558010 quantize_finetune_llama.py:159] layer 25 gpu 1
I0402 09:46:40.706849 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 25 in 1.544692039489746s
I0402 09:46:40.819968 1591951 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:46:40.820138 1591951 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:46:40.820200 1591951 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:46:41.044960 1591951 config.py:58] PyTorch version 2.4.0 available.
I0402 09:46:41.239669 1558010 quantize_finetune_llama.py:159] layer 26 gpu 2
I0402 09:46:43.158112 1592130 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:46:43.158244 1592130 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:46:43.158305 1592130 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:46:43.164585 1591951 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 09:46:43.423126 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 26 in 1.586200475692749s
I0402 09:46:43.431176 1592130 config.py:58] PyTorch version 2.4.0 available.
W0402 09:46:43.488922 1591951 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:46:43.895901 1558010 quantize_finetune_llama.py:159] layer 27 gpu 3
  0%|          | 0/32 [00:00<?, ?it/s]I0402 09:46:45.529748 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 27 in 1.194901943206787s
I0402 09:46:45.840980 1592130 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 09:46:45.889862 1592565 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:46:45.890159 1592565 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:46:45.890283 1592565 utils.py:162] NumExpr defaulting to 16 threads.
  3%|▎         | 1/32 [00:01<00:44,  1.44s/it]I0402 09:46:46.022979 1558010 quantize_finetune_llama.py:159] layer 28 gpu 0
W0402 09:46:46.220081 1592130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:46:46.260135 1592565 config.py:58] PyTorch version 2.4.0 available.
  6%|▋         | 2/32 [00:01<00:23,  1.27it/s]  9%|▉         | 3/32 [00:02<00:16,  1.76it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.12it/s]  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 19%|█▉        | 6/32 [00:03<00:09,  2.64it/s] 22%|██▏       | 7/32 [00:03<00:08,  2.83it/s]I0402 09:46:47.964883 1593079 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:46:47.965047 1593079 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:46:47.965105 1593079 utils.py:162] NumExpr defaulting to 16 threads.
 25%|██▌       | 8/32 [00:03<00:08,  2.96it/s]I0402 09:46:48.180983 1593079 config.py:58] PyTorch version 2.4.0 available.
 28%|██▊       | 9/32 [00:03<00:07,  3.01it/s]I0402 09:46:48.553774 1592565 data_utils.py:336] using 256 training seqs, 128 validation seqs
  3%|▎         | 1/32 [00:01<00:45,  1.46s/it] 31%|███▏      | 10/32 [00:04<00:07,  3.07it/s]W0402 09:46:48.925774 1592565 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:01<00:24,  1.23it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.07it/s]  9%|▉         | 3/32 [00:02<00:17,  1.68it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.11it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.03it/s] 41%|████      | 13/32 [00:05<00:06,  3.17it/s]  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.20it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.17it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s]I0402 09:46:50.573047 1593079 data_utils.py:336] using 256 training seqs, 128 validation seqs
 50%|█████     | 16/32 [00:06<00:05,  3.15it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s]W0402 09:46:50.958400 1593079 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 17/32 [00:06<00:04,  3.18it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.14it/s]  3%|▎         | 1/32 [00:01<00:48,  1.56s/it] 28%|██▊       | 9/32 [00:04<00:08,  2.72it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.19it/s]  6%|▋         | 2/32 [00:01<00:25,  1.20it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.23it/s]  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:02<00:17,  1.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.22it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.01it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.86it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.26it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 41%|████      | 13/32 [00:05<00:06,  2.90it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.29it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.51it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.30it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.92it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.66it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.28it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.93it/s]  3%|▎         | 1/32 [00:01<00:49,  1.61s/it] 25%|██▌       | 8/32 [00:03<00:08,  2.78it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.29it/s] 50%|█████     | 16/32 [00:06<00:05,  2.95it/s]  6%|▋         | 2/32 [00:01<00:25,  1.17it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.86it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.28it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.95it/s]  9%|▉         | 3/32 [00:02<00:17,  1.62it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.25it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.89it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.96it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.99it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.28it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.94it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.98it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.24it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.98it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.97it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.51it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.18it/s] 41%|████      | 13/32 [00:05<00:06,  3.00it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.96it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.65it/s]100%|██████████| 32/32 [00:11<00:00,  3.14it/s]100%|██████████| 32/32 [00:11<00:00,  2.88it/s]
 44%|████▍     | 14/32 [00:05<00:05,  3.01it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.97it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.72it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.96it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.96it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.74it/s] 50%|█████     | 16/32 [00:06<00:05,  2.98it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.96it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.81it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.97it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.97it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.86it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.93it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.96it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.90it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.90it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.95it/s] 41%|████      | 13/32 [00:05<00:06,  2.93it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.92it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.97it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.98it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.95it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.98it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.01it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.98it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.98it/s]W0402 09:46:58.595000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.595000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.595000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.595000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.595000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.595000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.596000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:05,  3.02it/s]W0402 09:46:58.620000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.620000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.621000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.621000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.621000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.636000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.636000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.636000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.637000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.637000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:08<00:03,  3.00it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.99it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.05it/s]W0402 09:46:58.942000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.942000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.942000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.942000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:58.942000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:09<00:02,  3.01it/s]100%|██████████| 32/32 [00:12<00:00,  2.98it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
 56%|█████▋    | 18/32 [00:07<00:04,  3.04it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.01it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.06it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.02it/s]W0402 09:46:59.798000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:59.798000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:59.798000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:59.798000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:59.798000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:46:59.798000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:46:59.799000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:46:59.815000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:46:59.815000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:46:59.816000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:46:59.816000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:46:59.816000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:07<00:03,  3.07it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.04it/s]W0402 09:47:00.050000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:00.050000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:00.051000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:00.051000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:00.051000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:03,  3.08it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.04it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.08it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.06it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.09it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.06it/s]W0402 09:47:01.180000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:01.180000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:01.180000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:01.180000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:01.180000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:01.180000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:01.180000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:01.197000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:01.197000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:01.197000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:01.197000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:01.197000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:09<00:02,  3.06it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.05it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.07it/s]100%|██████████| 32/32 [00:11<00:00,  3.07it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
 81%|████████▏ | 26/32 [00:09<00:01,  3.08it/s]W0402 09:47:02.069000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.069000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.069000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.069000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.069000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.069000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.070000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.082000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.082000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.082000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.082000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.082000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.097000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.097000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.097000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.097000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.097000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.113000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.113000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.113000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.113000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.113000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  3.02it/s]W0402 09:47:02.429000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.429000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.429000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.429000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:02.429000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  3.02it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 91%|█████████ | 29/32 [00:10<00:01,  3.00it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.03it/s]W0402 09:47:03.287000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:03.287000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:03.287000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:03.287000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:03.287000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:03.287000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:03.287000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:03.305000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:03.305000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:03.305000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:03.305000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:03.305000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  3.00it/s]W0402 09:47:03.538000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:03.539000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:03.539000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:03.539000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:03.539000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:11<00:00,  2.98it/s]100%|██████████| 32/32 [00:11<00:00,  2.70it/s]
W0402 09:47:04.664000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.664000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.664000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.664000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.665000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.665000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.665000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.672000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.672000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.672000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.672000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.672000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.673000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.673000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.682000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.682000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.682000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.682000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.682000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.700000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.700000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.700000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.700000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.700000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.718000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.718000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.718000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.718000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:04.718000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.054000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.055000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.055000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.055000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.055000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.569000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.570000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.570000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.570000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.570000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.976000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.976000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.976000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.976000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.976000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.976000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.976000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.994000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.994000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.995000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.995000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:05.995000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 09:47:06.249000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:06.249000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:06.249000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:06.249000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:06.249000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:06.990000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:06.991000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:06.991000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:06.991000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:06.991000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:06.991000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:06.991000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.018000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.018000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.018000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.018000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.018000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.035000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.035000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.035000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.036000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.036000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.370000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.370000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.370000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.370000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.370000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.459000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.459000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.459000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.459000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.459000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.459000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.459000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.477000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.477000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.477000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.477000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:07.478000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
I0402 09:47:07.931847 1591951 finetune.py:45] layer 24_v initial loss 0.011651167646050453
W0402 09:47:07.932196 1591951 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:47:08.311000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.312000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.312000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.312000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.312000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.312000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.312000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.331000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.331000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.331000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.331000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.331000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.390000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.390000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.390000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.390000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.391000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.592000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.592000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.592000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.592000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.593000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:08.928801 1591951 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 09:47:09.830000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:09.830000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:09.830000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:09.830000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:09.830000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:09.830000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:09.831000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:09.849000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:09.849000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:09.849000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:09.849000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:09.849000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
24_v proxy err 0.011920065619051456 tr(WHW.T) 136.39785766601562
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:47:10.804000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:10.804000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:10.804000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:10.805000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:10.805000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:29,  1.05it/s]  6%|▋         | 2/32 [00:01<00:18,  1.65it/s]I0402 09:47:11.538684 1592130 finetune.py:45] layer 25_v initial loss 0.013473400846123695
W0402 09:47:11.538904 1592130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s]W0402 09:47:12.443468 1592130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s]25_v proxy err 0.01044139452278614 tr(WHW.T) 164.04367065429688
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.70it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s]I0402 09:47:14.178794 1592565 finetune.py:45] layer 26_v initial loss 0.01805293560028076
W0402 09:47:14.179113 1592565 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:00<00:28,  1.09it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.69it/s]  6%|▋         | 2/32 [00:01<00:17,  1.69it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.72it/s]  9%|▉         | 3/32 [00:01<00:14,  2.05it/s]W0402 09:47:15.240074 1592565 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.27it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.76it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s]26_v proxy err 0.013865821994841099 tr(WHW.T) 123.81900024414062
  0%|          | 0/32 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.75it/s]  3%|▎         | 1/32 [00:00<00:30,  1.03it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.64it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.76it/s]I0402 09:47:17.564338 1593079 finetune.py:45] layer 27_v initial loss 0.02008453570306301
W0402 09:47:17.564807 1593079 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:04<00:08,  2.67it/s]  6%|▋         | 2/32 [00:01<00:18,  1.60it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s]  9%|▉         | 3/32 [00:01<00:14,  1.98it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.84it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.69it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.20it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 41%|████      | 13/32 [00:05<00:07,  2.70it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.85it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.70it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.89it/s]W0402 09:47:19.152341 1593079 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 78%|███████▊  | 25/32 [00:09<00:02,  2.87it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.71it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.87it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.91it/s]27_v proxy err 0.011005000211298466 tr(WHW.T) 203.18115234375
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.65it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.69it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.92it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.94it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.71it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s]  3%|▎         | 1/32 [00:00<00:28,  1.08it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.93it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.71it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.71it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.96it/s]  6%|▋         | 2/32 [00:01<00:18,  1.64it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s]100%|██████████| 32/32 [00:11<00:00,  2.97it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
  9%|▉         | 3/32 [00:01<00:14,  1.98it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.71it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.70it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.72it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.73it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.72it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.72it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.55it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.71it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.71it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.60it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.74it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.72it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.75it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 50%|█████     | 16/32 [00:06<00:06,  2.58it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.57it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.77it/s]W0402 09:47:27.573000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.574000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.574000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.574000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.574000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.574000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.574000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.602000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.602000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.602000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.602000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.602000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.616000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.617000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.617000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.617000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.617000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:07<00:05,  2.55it/s]W0402 09:47:27.766000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.766000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.767000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.767000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.767000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.76it/s]W0402 09:47:27.988000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.989000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.989000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.989000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.989000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.989000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:27.989000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:28.009000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:28.009000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:28.010000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:28.010000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:28.010000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 19/32 [00:07<00:05,  2.58it/s]W0402 09:47:28.073000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:28.073000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:28.073000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:28.074000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:28.074000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  2.77it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s]W0402 09:47:28.900000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  2.64it/s]W0402 09:47:29.218000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:29.218000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:29.218000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:29.218000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:29.219000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:29.219000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:29.219000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:29.240000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:29.240000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:29.240000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:29.240000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:29.240000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:29.491000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:29.491000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:29.491000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:29.491000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:29.491000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s]W0402 09:47:29.745000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:09<00:03,  2.65it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.64it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.63it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.62it/s]W0402 09:47:31.643000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.643000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.643000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.643000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.643000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.644000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.644000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.672000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.672000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.672000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.672000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.672000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.687000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.687000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.687000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.687000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.687000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.838000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.838000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.838000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.838000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:31.838000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.62it/s]W0402 09:47:32.057000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:32.057000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:32.057000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:32.057000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:32.057000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:32.057000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:32.058000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:32.077000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:32.077000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:32.077000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:32.077000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:32.078000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:32.141000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:32.141000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:32.141000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:32.141000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:32.141000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:12<00:00,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.66it/s]W0402 09:47:32.967000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
W0402 09:47:33.269000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:33.269000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:33.269000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:33.269000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:33.269000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:33.269000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:33.269000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:33.290000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:33.291000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:33.291000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:33.291000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:33.291000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:33.541000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:33.541000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:33.541000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:33.542000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:33.542000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:33.793000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.500000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.501000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.501000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.501000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.501000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.501000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.501000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.531000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.532000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.532000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.532000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.532000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.547000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.547000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.548000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.548000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.548000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.708000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.708000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.708000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.708000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.709000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.939000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.940000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.940000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.940000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.940000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.940000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.940000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.960000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.960000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.960000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.961000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:34.961000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:35.027000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:35.027000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:35.027000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:35.027000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:35.028000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:35.905000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0402 09:47:35.911301 1591951 finetune.py:45] layer 24_q initial loss 0.011653407476842403
W0402 09:47:35.911485 1591951 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:47:36.199000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:36.200000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:36.200000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:36.200000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:36.200000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:36.200000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:36.200000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:36.220000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:36.220000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:36.221000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:36.221000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:36.221000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:36.480000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:36.480000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:36.480000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:36.481000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:36.481000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:36.736000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:36.901074 1591951 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_q proxy err 0.0019384176703169942 tr(WHW.T) 6549.0634765625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.17it/s]  6%|▋         | 2/32 [00:01<00:16,  1.79it/s]W0402 09:47:39.235000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.236000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.236000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.236000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.236000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.236000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.236000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.266000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.266000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.266000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.266000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.266000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.281000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.281000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.281000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.282000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.282000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:01<00:13,  2.15it/s]W0402 09:47:39.442000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.442000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.442000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.442000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.442000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.670000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.670000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.670000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.670000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.670000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.671000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.671000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.692000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.692000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.692000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.692000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.693000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.759000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.759000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.759000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.759000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:39.759000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:01<00:11,  2.38it/s]I0402 09:47:40.022162 1592130 finetune.py:45] layer 25_q initial loss 0.01346865389496088
W0402 09:47:40.022348 1592130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:02<00:10,  2.54it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.64it/s]W0402 09:47:40.613000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:02<00:09,  2.71it/s]W0402 09:47:40.918000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:47:40.918000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:47:40.918000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:40.918000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:40.918000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:40.919000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:40.919000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:40.940000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:40.940000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:40.940000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:40.940000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:40.940000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:40.966229 1592130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s]W0402 09:47:41.203000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:47:41.203000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:47:41.203000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:47:41.204000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:47:41.204000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:47:41.462000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:03<00:08,  2.76it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.78it/s]25_q proxy err 0.0016632527112960815 tr(WHW.T) 7679.1796875
  0%|          | 0/32 [00:00<?, ?it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.75it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.76it/s]I0402 09:47:42.775512 1592565 finetune.py:45] layer 26_q initial loss 0.01806115359067917
W0402 09:47:42.775891 1592565 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:00<00:26,  1.15it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s]  6%|▋         | 2/32 [00:01<00:21,  1.39it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.31it/s]  9%|▉         | 3/32 [00:01<00:16,  1.78it/s] 47%|████▋     | 15/32 [00:06<00:07,  2.42it/s]W0402 09:47:44.015612 1592565 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:02<00:13,  2.06it/s] 50%|█████     | 16/32 [00:06<00:06,  2.52it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.27it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.60it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.41it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s]26_q proxy err 0.0019369678338989615 tr(WHW.T) 6098.626953125
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.72it/s]  3%|▎         | 1/32 [00:00<00:28,  1.10it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.65it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s]  6%|▋         | 2/32 [00:01<00:17,  1.71it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.67it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.80it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.69it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.82it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.32it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.71it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.83it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.47it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.85it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.57it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.80it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.62it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.76it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.80it/s]I0402 09:47:48.459093 1593079 finetune.py:45] layer 27_q initial loss 0.02006141096353531
W0402 09:47:48.459443 1593079 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.81it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.71it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.84it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.75it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.76it/s]W0402 09:47:49.480401 1593079 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:04<00:07,  2.79it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.90it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.90it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
 62%|██████▎   | 20/32 [00:08<00:04,  2.76it/s] 41%|████      | 13/32 [00:05<00:06,  2.81it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.75it/s]27_q proxy err 0.0020106935407966375 tr(WHW.T) 6387.7978515625
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.74it/s] 50%|█████     | 16/32 [00:06<00:05,  2.77it/s]  3%|▎         | 1/32 [00:00<00:27,  1.13it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.81it/s]  6%|▋         | 2/32 [00:01<00:17,  1.72it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.77it/s]  9%|▉         | 3/32 [00:01<00:14,  2.05it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.75it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.29it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.77it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.73it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.77it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.77it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.71it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.79it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
 78%|███████▊  | 25/32 [00:09<00:02,  2.71it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.67it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.70it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.68it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.67it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.70it/s]I0402 09:47:56.162786 1591951 finetune.py:45] layer 24_k initial loss 0.011652160435914993
W0402 09:47:56.163198 1591951 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 44%|████▍     | 14/32 [00:05<00:06,  2.68it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.75it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.76it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s]W0402 09:47:57.183318 1591951 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:12<00:00,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
 53%|█████▎    | 17/32 [00:06<00:05,  2.63it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s]24_k proxy err 0.0016566665144637227 tr(WHW.T) 4140.12353515625
  0%|          | 0/32 [00:00<?, ?it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.61it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.61it/s]  3%|▎         | 1/32 [00:00<00:22,  1.36it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.60it/s]  6%|▋         | 2/32 [00:01<00:15,  1.95it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s]  9%|▉         | 3/32 [00:01<00:12,  2.27it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.48it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.61it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.58it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.70it/s]I0402 09:48:00.807408 1592130 finetune.py:45] layer 25_k initial loss 0.013458603993058205
W0402 09:48:00.807598 1592130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.75it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.64it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.65it/s]W0402 09:48:01.763240 1592130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:03<00:08,  2.78it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.80it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]25_k proxy err 0.001659245346672833 tr(WHW.T) 4239.72509765625
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
 41%|████      | 13/32 [00:04<00:06,  2.81it/s]  3%|▎         | 1/32 [00:00<00:23,  1.29it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.82it/s]  6%|▋         | 2/32 [00:01<00:15,  1.88it/s]I0402 09:48:03.966165 1592565 finetune.py:45] layer 26_k initial loss 0.01805947534739971
W0402 09:48:03.967070 1592565 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 50%|█████     | 16/32 [00:06<00:05,  2.84it/s]  9%|▉         | 3/32 [00:01<00:13,  2.19it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.85it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.37it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.89it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.49it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.92it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.54it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.94it/s]W0402 09:48:05.675119 1592565 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:02<00:09,  2.58it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.90it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.89it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.63it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.89it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s]26_k proxy err 0.0015604050131514668 tr(WHW.T) 4395.3486328125
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.86it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.86it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.65it/s]  3%|▎         | 1/32 [00:00<00:23,  1.30it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.88it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.88it/s]  6%|▋         | 2/32 [00:01<00:16,  1.86it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s]  9%|▉         | 3/32 [00:01<00:13,  2.15it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.91it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.69it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.35it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.93it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.46it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.93it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.71it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.54it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.78it/s]
 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s]I0402 09:48:10.225710 1593079 finetune.py:45] layer 27_k initial loss 0.02007046714425087
W0402 09:48:10.226036 1593079 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 59%|█████▉    | 19/32 [00:07<00:04,  2.69it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.69it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.70it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s]W0402 09:48:11.298735 1593079 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.63it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.69it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.66it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s]27_k proxy err 0.0017958009848371148 tr(WHW.T) 4200.8603515625
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.73it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.74it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s]  3%|▎         | 1/32 [00:00<00:23,  1.30it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.74it/s] 50%|█████     | 16/32 [00:06<00:06,  2.61it/s]  6%|▋         | 2/32 [00:01<00:15,  1.88it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.74it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.62it/s]  9%|▉         | 3/32 [00:01<00:13,  2.18it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.74it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.61it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.35it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.76it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.60it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.46it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.74it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.59it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.56it/s]100%|██████████| 32/32 [00:12<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
 66%|██████▌   | 21/32 [00:08<00:04,  2.60it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.59it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.61it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s]I0402 09:48:16.428323 1591951 finetune.py:45] layer 24_o initial loss 0.011608465574681759
W0402 09:48:16.428672 1591951 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.64it/s]W0402 09:48:17.418608 1591951 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:10<00:01,  2.67it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.69it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.62it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.62it/s]24_o proxy err 0.01071228552609682 tr(WHW.T) 6.529373645782471
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s] 50%|█████     | 16/32 [00:06<00:06,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.75it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.59it/s]  3%|▎         | 1/32 [00:01<00:57,  1.85s/it] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.59it/s]  6%|▋         | 2/32 [00:03<00:46,  1.55s/it]I0402 09:48:21.860838 1592130 finetune.py:45] layer 25_o initial loss 0.013547303155064583
W0402 09:48:21.861129 1592130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s]W0402 09:48:22.833763 1592130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:10<00:01,  2.65it/s]  9%|▉         | 3/32 [00:04<00:43,  1.50s/it] 88%|████████▊ | 28/32 [00:11<00:01,  2.64it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.65it/s]25_o proxy err 0.008644293062388897 tr(WHW.T) 8.825087547302246
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.66it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.12it/s] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it]100%|██████████| 32/32 [00:12<00:00,  2.26it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
  3%|▎         | 1/32 [00:02<01:10,  2.28s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it]I0402 09:48:26.491279 1592565 finetune.py:45] layer 26_o initial loss 0.018001116812229156
W0402 09:48:26.491593 1592565 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:48:27.452512 1592565 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it]  6%|▋         | 2/32 [00:03<00:54,  1.81s/it]26_o proxy err 0.006552447564899921 tr(WHW.T) 16.217060089111328
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it]  9%|▉         | 3/32 [00:05<00:48,  1.66s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.42s/it]  3%|▎         | 1/32 [00:01<00:58,  1.88s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.40s/it]  6%|▋         | 2/32 [00:03<00:48,  1.63s/it] 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.39s/it]I0402 09:48:33.186115 1593079 finetune.py:45] layer 27_o initial loss 0.02002619579434395
W0402 09:48:33.186565 1593079 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:04<00:44,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 34%|███▍      | 11/32 [00:15<00:28,  1.38s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.48s/it]W0402 09:48:34.744450 1593079 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:11<00:37,  1.51s/it] 38%|███▊      | 12/32 [00:17<00:27,  1.37s/it]27_o proxy err 0.007764684502035379 tr(WHW.T) 16.17061424255371
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 41%|████      | 13/32 [00:18<00:25,  1.36s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.43s/it]  3%|▎         | 1/32 [00:01<01:00,  1.97s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.48s/it] 44%|████▍     | 14/32 [00:20<00:24,  1.36s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.42s/it]  6%|▋         | 2/32 [00:03<00:50,  1.69s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it] 47%|████▋     | 15/32 [00:21<00:22,  1.35s/it] 25%|██▌       | 8/32 [00:11<00:33,  1.41s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it]  9%|▉         | 3/32 [00:04<00:46,  1.61s/it] 50%|█████     | 16/32 [00:22<00:21,  1.34s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.41s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.46s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.57s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.34s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.40s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it] 56%|█████▋    | 18/32 [00:25<00:18,  1.34s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.55s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.39s/it] 59%|█████▉    | 19/32 [00:26<00:17,  1.34s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.45s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.54s/it] 38%|███▊      | 12/32 [00:17<00:27,  1.40s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.34s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.45s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 41%|████      | 13/32 [00:18<00:26,  1.40s/it] 66%|██████▌   | 21/32 [00:29<00:14,  1.33s/it] 50%|█████     | 16/32 [00:24<00:23,  1.45s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.40s/it] 69%|██████▉   | 22/32 [00:30<00:13,  1.34s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.45s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.40s/it] 72%|███████▏  | 23/32 [00:32<00:11,  1.33s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.45s/it] 50%|█████     | 16/32 [00:22<00:22,  1.40s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 75%|███████▌  | 24/32 [00:33<00:10,  1.33s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.45s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.40s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it] 78%|███████▊  | 25/32 [00:34<00:09,  1.34s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.40s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.52s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.33s/it] 66%|██████▌   | 21/32 [00:31<00:15,  1.45s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.40s/it] 84%|████████▍ | 27/32 [00:37<00:06,  1.33s/it] 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.45s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.40s/it] 88%|████████▊ | 28/32 [00:38<00:05,  1.33s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.45s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.40s/it] 91%|█████████ | 29/32 [00:40<00:03,  1.33s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.45s/it] 69%|██████▉   | 22/32 [00:31<00:13,  1.40s/it] 94%|█████████▍| 30/32 [00:41<00:02,  1.33s/it] 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.45s/it] 97%|█████████▋| 31/32 [00:42<00:01,  1.34s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.40s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it]100%|██████████| 32/32 [00:44<00:00,  1.34s/it]100%|██████████| 32/32 [00:44<00:00,  1.38s/it]
 81%|████████▏ | 26/32 [00:38<00:08,  1.45s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.40s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.45s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.42s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.45s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.43s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.44s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.45s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.44s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.47s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.50s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.45s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.48s/it]I0402 09:49:10.286461 1591951 finetune.py:45] layer 24_up initial loss 0.011435340158641338
W0402 09:49:10.286671 1591951 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:49:11.098251 1591951 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:35<00:13,  1.50s/it]100%|██████████| 32/32 [00:47<00:00,  1.45s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
 94%|█████████▍| 30/32 [00:42<00:02,  1.46s/it]24_up proxy err 0.012605383060872555 tr(WHW.T) 1342.69384765625
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:36<00:12,  1.53s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.45s/it]  3%|▎         | 1/32 [00:01<00:53,  1.72s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it]100%|██████████| 32/32 [00:45<00:00,  1.44s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]
  6%|▋         | 2/32 [00:03<00:45,  1.53s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.54s/it]  9%|▉         | 3/32 [00:04<00:41,  1.45s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.55s/it] 12%|█▎        | 4/32 [00:05<00:39,  1.43s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.55s/it]I0402 09:49:19.150876 1592130 finetune.py:45] layer 25_up initial loss 0.01342211477458477
W0402 09:49:19.151066 1592130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:07<00:38,  1.43s/it]W0402 09:49:19.963161 1592130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:44<00:04,  1.54s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.43s/it]25_up proxy err 0.01266726665198803 tr(WHW.T) 1428.771728515625
  0%|          | 0/32 [00:00<?, ?it/s]I0402 09:49:21.693856 1592565 finetune.py:45] layer 26_up initial loss 0.017811523750424385
W0402 09:49:21.694172 1592565 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 94%|█████████▍| 30/32 [00:45<00:03,  1.53s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.42s/it]W0402 09:49:22.504471 1592565 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:57,  1.86s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.52s/it]26_up proxy err 0.012432817369699478 tr(WHW.T) 1584.181640625
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:11<00:33,  1.39s/it]  6%|▋         | 2/32 [00:03<00:48,  1.62s/it] 28%|██▊       | 9/32 [00:12<00:31,  1.37s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]
  3%|▎         | 1/32 [00:01<00:56,  1.81s/it]  9%|▉         | 3/32 [00:04<00:45,  1.57s/it] 31%|███▏      | 10/32 [00:14<00:29,  1.36s/it]  6%|▋         | 2/32 [00:03<00:47,  1.57s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it] 34%|███▍      | 11/32 [00:15<00:28,  1.35s/it]  9%|▉         | 3/32 [00:04<00:43,  1.51s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 38%|███▊      | 12/32 [00:16<00:26,  1.34s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it] 41%|████      | 13/32 [00:18<00:25,  1.33s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.45s/it] 44%|████▍     | 14/32 [00:19<00:23,  1.33s/it] 22%|██▏       | 7/32 [00:10<00:38,  1.53s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it] 47%|████▋     | 15/32 [00:20<00:22,  1.32s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it]I0402 09:49:33.578415 1593079 finetune.py:45] layer 27_up initial loss 0.019786717370152473
W0402 09:49:33.578716 1593079 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:10<00:35,  1.42s/it] 50%|█████     | 16/32 [00:22<00:21,  1.33s/it]W0402 09:49:34.675265 1593079 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:13<00:34,  1.52s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.42s/it] 53%|█████▎    | 17/32 [00:23<00:19,  1.33s/it]27_up proxy err 0.01159120537340641 tr(WHW.T) 1872.387451171875
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:15<00:33,  1.50s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.42s/it] 56%|█████▋    | 18/32 [00:24<00:18,  1.33s/it]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 59%|█████▉    | 19/32 [00:26<00:17,  1.33s/it]  6%|▋         | 2/32 [00:03<00:50,  1.67s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 62%|██████▎   | 20/32 [00:27<00:15,  1.33s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.42s/it]  9%|▉         | 3/32 [00:04<00:45,  1.59s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 66%|██████▌   | 21/32 [00:28<00:14,  1.33s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it] 69%|██████▉   | 22/32 [00:30<00:13,  1.33s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.46s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.55s/it] 41%|████      | 13/32 [00:18<00:27,  1.42s/it] 72%|███████▏  | 23/32 [00:31<00:11,  1.33s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 75%|███████▌  | 24/32 [00:32<00:10,  1.33s/it] 50%|█████     | 16/32 [00:24<00:23,  1.45s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it] 78%|███████▊  | 25/32 [00:34<00:09,  1.33s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.45s/it] 50%|█████     | 16/32 [00:23<00:22,  1.41s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it] 81%|████████▏ | 26/32 [00:35<00:07,  1.33s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.45s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.50s/it] 84%|████████▍ | 27/32 [00:36<00:06,  1.33s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.42s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.45s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it] 88%|████████▊ | 28/32 [00:38<00:05,  1.33s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.44s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 91%|█████████ | 29/32 [00:39<00:03,  1.33s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 66%|██████▌   | 21/32 [00:31<00:15,  1.44s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 94%|█████████▍| 30/32 [00:40<00:02,  1.33s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.41s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.44s/it] 97%|█████████▋| 31/32 [00:41<00:01,  1.33s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 72%|███████▏  | 23/32 [00:34<00:12,  1.44s/it]100%|██████████| 32/32 [00:43<00:00,  1.33s/it]100%|██████████| 32/32 [00:43<00:00,  1.35s/it]
 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.43s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.44s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.43s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.44s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 78%|███████▊  | 25/32 [00:35<00:10,  1.46s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.44s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.44s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.47s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.44s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.48s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it]I0402 09:50:03.312533 1591951 finetune.py:45] layer 24_gate initial loss 0.011359354481101036
W0402 09:50:03.312712 1591951 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:42<00:04,  1.44s/it]W0402 09:50:04.010817 1591951 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 88%|████████▊ | 28/32 [00:40<00:05,  1.47s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.44s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.47s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.44s/it]24_gate proxy err 0.006504200864583254 tr(WHW.T) 4752.3671875
  0%|          | 0/112 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:43<00:02,  1.46s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it]  1%|          | 1/112 [00:00<01:27,  1.26it/s]  2%|▏         | 2/112 [00:01<00:58,  1.88it/s]100%|██████████| 32/32 [00:47<00:00,  1.44s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]
  3%|▎         | 3/112 [00:01<00:48,  2.26it/s] 97%|█████████▋| 31/32 [00:44<00:01,  1.45s/it]  4%|▎         | 4/112 [00:01<00:43,  2.49it/s]  4%|▍         | 5/112 [00:02<00:40,  2.62it/s] 69%|██████▉   | 22/32 [00:33<00:15,  1.50s/it]  5%|▌         | 6/112 [00:02<00:39,  2.71it/s]  6%|▋         | 7/112 [00:02<00:37,  2.76it/s]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
  7%|▋         | 8/112 [00:03<00:36,  2.83it/s]  8%|▊         | 9/112 [00:03<00:36,  2.81it/s] 72%|███████▏  | 23/32 [00:34<00:13,  1.51s/it]  9%|▉         | 10/112 [00:03<00:36,  2.79it/s] 10%|▉         | 11/112 [00:04<00:35,  2.81it/s] 11%|█         | 12/112 [00:04<00:35,  2.85it/s] 12%|█▏        | 13/112 [00:04<00:34,  2.84it/s] 12%|█▎        | 14/112 [00:05<00:34,  2.88it/s] 75%|███████▌  | 24/32 [00:36<00:12,  1.53s/it] 13%|█▎        | 15/112 [00:05<00:33,  2.88it/s] 14%|█▍        | 16/112 [00:06<00:33,  2.83it/s] 15%|█▌        | 17/112 [00:06<00:33,  2.81it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.79it/s] 78%|███████▊  | 25/32 [00:37<00:10,  1.55s/it] 17%|█▋        | 19/112 [00:07<00:33,  2.78it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.77it/s] 19%|█▉        | 21/112 [00:07<00:32,  2.77it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.77it/s] 21%|██        | 23/112 [00:08<00:32,  2.75it/s] 81%|████████▏ | 26/32 [00:39<00:09,  1.56s/it] 21%|██▏       | 24/112 [00:08<00:31,  2.76it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.76it/s]I0402 09:50:16.256969 1592130 finetune.py:45] layer 25_gate initial loss 0.013390074484050274
W0402 09:50:16.257266 1592130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 23%|██▎       | 26/112 [00:09<00:31,  2.77it/s] 24%|██▍       | 27/112 [00:09<00:30,  2.76it/s] 84%|████████▍ | 27/32 [00:40<00:07,  1.55s/it]W0402 09:50:16.968157 1592130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 28/112 [00:10<00:30,  2.77it/s]I0402 09:50:17.261506 1592565 finetune.py:45] layer 26_gate initial loss 0.01750754751265049
W0402 09:50:17.261802 1592565 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 26%|██▌       | 29/112 [00:10<00:29,  2.82it/s] 27%|██▋       | 30/112 [00:11<00:28,  2.86it/s]W0402 09:50:17.974972 1592565 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 31/112 [00:11<00:28,  2.88it/s] 88%|████████▊ | 28/32 [00:42<00:06,  1.56s/it] 29%|██▊       | 32/112 [00:11<00:27,  2.87it/s] 29%|██▉       | 33/112 [00:12<00:27,  2.88it/s] 30%|███       | 34/112 [00:12<00:27,  2.85it/s] 31%|███▏      | 35/112 [00:12<00:27,  2.83it/s] 32%|███▏      | 36/112 [00:13<00:26,  2.87it/s] 91%|█████████ | 29/32 [00:44<00:04,  1.55s/it]25_gate proxy err 0.006472693756222725 tr(WHW.T) 5091.62109375
  0%|          | 0/112 [00:00<?, ?it/s] 33%|███▎      | 37/112 [00:13<00:25,  2.89it/s] 34%|███▍      | 38/112 [00:13<00:25,  2.92it/s]26_gate proxy err 0.0059727029874920845 tr(WHW.T) 5988.38623046875
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:31,  1.21it/s] 35%|███▍      | 39/112 [00:14<00:25,  2.91it/s]  2%|▏         | 2/112 [00:01<01:01,  1.79it/s] 36%|███▌      | 40/112 [00:14<00:24,  2.91it/s] 94%|█████████▍| 30/32 [00:45<00:03,  1.54s/it]  1%|          | 1/112 [00:00<01:30,  1.23it/s] 37%|███▋      | 41/112 [00:14<00:24,  2.92it/s]  3%|▎         | 3/112 [00:01<00:51,  2.11it/s] 38%|███▊      | 42/112 [00:15<00:23,  2.93it/s]  2%|▏         | 2/112 [00:01<00:59,  1.84it/s]  4%|▎         | 4/112 [00:01<00:46,  2.31it/s] 38%|███▊      | 43/112 [00:15<00:23,  2.94it/s]  3%|▎         | 3/112 [00:01<00:49,  2.19it/s]  4%|▍         | 5/112 [00:02<00:43,  2.45it/s] 39%|███▉      | 44/112 [00:15<00:23,  2.94it/s]  4%|▎         | 4/112 [00:01<00:45,  2.38it/s]  5%|▌         | 6/112 [00:02<00:41,  2.53it/s] 40%|████      | 45/112 [00:16<00:22,  2.93it/s] 97%|█████████▋| 31/32 [00:47<00:01,  1.53s/it]  4%|▍         | 5/112 [00:02<00:42,  2.51it/s]  6%|▋         | 7/112 [00:03<00:40,  2.58it/s] 41%|████      | 46/112 [00:16<00:22,  2.94it/s]  5%|▌         | 6/112 [00:02<00:40,  2.61it/s]  7%|▋         | 8/112 [00:03<00:39,  2.62it/s] 42%|████▏     | 47/112 [00:16<00:22,  2.93it/s]  6%|▋         | 7/112 [00:02<00:39,  2.66it/s]  8%|▊         | 9/112 [00:03<00:38,  2.65it/s] 43%|████▎     | 48/112 [00:17<00:21,  2.94it/s]  7%|▋         | 8/112 [00:03<00:38,  2.70it/s]  9%|▉         | 10/112 [00:04<00:38,  2.66it/s] 44%|████▍     | 49/112 [00:17<00:21,  2.95it/s]  8%|▊         | 9/112 [00:03<00:37,  2.74it/s]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
 10%|▉         | 11/112 [00:04<00:37,  2.68it/s] 45%|████▍     | 50/112 [00:17<00:20,  2.95it/s]  9%|▉         | 10/112 [00:04<00:37,  2.75it/s] 11%|█         | 12/112 [00:04<00:37,  2.66it/s] 46%|████▌     | 51/112 [00:18<00:20,  2.95it/s] 10%|▉         | 11/112 [00:04<00:36,  2.77it/s] 46%|████▋     | 52/112 [00:18<00:20,  2.97it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.62it/s] 11%|█         | 12/112 [00:04<00:35,  2.79it/s] 47%|████▋     | 53/112 [00:18<00:19,  2.98it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.60it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.79it/s] 48%|████▊     | 54/112 [00:19<00:19,  2.97it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.59it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.79it/s] 49%|████▉     | 55/112 [00:19<00:19,  2.96it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.62it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.79it/s] 50%|█████     | 56/112 [00:19<00:18,  2.96it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.64it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.79it/s] 51%|█████     | 57/112 [00:20<00:18,  2.96it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.63it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.79it/s] 52%|█████▏    | 58/112 [00:20<00:18,  2.94it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.64it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.78it/s] 53%|█████▎    | 59/112 [00:20<00:18,  2.92it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.78it/s] 18%|█▊        | 20/112 [00:07<00:35,  2.62it/s] 54%|█████▎    | 60/112 [00:21<00:17,  2.92it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.80it/s] 54%|█████▍    | 61/112 [00:21<00:17,  2.93it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.61it/s] 55%|█████▌    | 62/112 [00:21<00:16,  2.95it/s] 19%|█▉        | 21/112 [00:07<00:32,  2.80it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.61it/s] 56%|█████▋    | 63/112 [00:22<00:16,  2.96it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.80it/s] 21%|██        | 23/112 [00:09<00:34,  2.61it/s] 57%|█████▋    | 64/112 [00:22<00:16,  2.95it/s] 21%|██        | 23/112 [00:08<00:31,  2.79it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.60it/s] 58%|█████▊    | 65/112 [00:22<00:15,  2.95it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.79it/s] 22%|██▏       | 25/112 [00:09<00:33,  2.61it/s] 59%|█████▉    | 66/112 [00:23<00:15,  2.95it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.80it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.60it/s] 60%|█████▉    | 67/112 [00:23<00:15,  2.97it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.82it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.60it/s] 61%|██████    | 68/112 [00:23<00:14,  2.95it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.81it/s] 62%|██████▏   | 69/112 [00:24<00:14,  2.95it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.59it/s] 25%|██▌       | 28/112 [00:10<00:29,  2.81it/s] 62%|██████▎   | 70/112 [00:24<00:14,  2.96it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.58it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.82it/s] 63%|██████▎   | 71/112 [00:24<00:13,  2.96it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.59it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.81it/s] 64%|██████▍   | 72/112 [00:25<00:13,  2.97it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.58it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.82it/s] 65%|██████▌   | 73/112 [00:25<00:13,  2.97it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.81it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.58it/s] 66%|██████▌   | 74/112 [00:26<00:12,  2.95it/s]I0402 09:50:32.958976 1593079 finetune.py:45] layer 27_gate initial loss 0.01964711770415306
W0402 09:50:32.959404 1593079 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 29%|██▉       | 33/112 [00:12<00:28,  2.79it/s] 29%|██▉       | 33/112 [00:12<00:30,  2.59it/s] 67%|██████▋   | 75/112 [00:26<00:12,  2.91it/s] 30%|███       | 34/112 [00:12<00:27,  2.80it/s] 30%|███       | 34/112 [00:13<00:30,  2.58it/s] 68%|██████▊   | 76/112 [00:26<00:12,  2.93it/s] 31%|███▏      | 35/112 [00:12<00:27,  2.80it/s]W0402 09:50:33.841137 1593079 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 77/112 [00:27<00:11,  2.94it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.62it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.80it/s] 70%|██████▉   | 78/112 [00:27<00:11,  2.93it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.64it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.75it/s] 71%|███████   | 79/112 [00:27<00:11,  2.88it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.62it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.76it/s] 71%|███████▏  | 80/112 [00:28<00:11,  2.90it/s] 34%|███▍      | 38/112 [00:14<00:28,  2.62it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.77it/s] 72%|███████▏  | 81/112 [00:28<00:10,  2.92it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.60it/s] 73%|███████▎  | 82/112 [00:28<00:10,  2.93it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.78it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.59it/s] 74%|███████▍  | 83/112 [00:29<00:09,  2.93it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.79it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.62it/s] 75%|███████▌  | 84/112 [00:29<00:09,  2.92it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.78it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.63it/s] 76%|███████▌  | 85/112 [00:29<00:09,  2.94it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.78it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.63it/s] 77%|███████▋  | 86/112 [00:30<00:08,  2.93it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.78it/s]27_gate proxy err 0.005389703903347254 tr(WHW.T) 7248.01416015625
  0%|          | 0/112 [00:00<?, ?it/s] 78%|███████▊  | 87/112 [00:30<00:08,  2.94it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.66it/s] 40%|████      | 45/112 [00:16<00:23,  2.79it/s] 79%|███████▊  | 88/112 [00:30<00:08,  2.95it/s] 40%|████      | 45/112 [00:17<00:25,  2.67it/s] 41%|████      | 46/112 [00:16<00:23,  2.77it/s]  1%|          | 1/112 [00:00<01:31,  1.21it/s] 79%|███████▉  | 89/112 [00:31<00:07,  2.93it/s] 41%|████      | 46/112 [00:17<00:24,  2.67it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.77it/s] 80%|████████  | 90/112 [00:31<00:07,  2.92it/s]  2%|▏         | 2/112 [00:01<01:01,  1.78it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.69it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.79it/s] 81%|████████▏ | 91/112 [00:31<00:07,  2.94it/s]  3%|▎         | 3/112 [00:01<00:52,  2.08it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.69it/s] 44%|████▍     | 49/112 [00:17<00:22,  2.81it/s] 82%|████████▏ | 92/112 [00:32<00:06,  2.94it/s]  4%|▎         | 4/112 [00:01<00:47,  2.26it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.70it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.80it/s] 83%|████████▎ | 93/112 [00:32<00:06,  2.95it/s]  4%|▍         | 5/112 [00:02<00:44,  2.39it/s] 45%|████▍     | 50/112 [00:19<00:22,  2.70it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.81it/s] 84%|████████▍ | 94/112 [00:32<00:06,  2.94it/s]  5%|▌         | 6/112 [00:02<00:43,  2.46it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.70it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.81it/s] 85%|████████▍ | 95/112 [00:33<00:05,  2.93it/s]  6%|▋         | 7/112 [00:03<00:41,  2.52it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.70it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.80it/s] 86%|████████▌ | 96/112 [00:33<00:05,  2.94it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.81it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.71it/s]  7%|▋         | 8/112 [00:03<00:40,  2.54it/s] 87%|████████▋ | 97/112 [00:33<00:05,  2.95it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.80it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.70it/s]  8%|▊         | 9/112 [00:03<00:40,  2.56it/s] 88%|████████▊ | 98/112 [00:34<00:04,  2.93it/s] 50%|█████     | 56/112 [00:20<00:20,  2.75it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.69it/s]  9%|▉         | 10/112 [00:04<00:39,  2.58it/s] 88%|████████▊ | 99/112 [00:34<00:04,  2.90it/s] 51%|█████     | 57/112 [00:20<00:19,  2.77it/s] 89%|████████▉ | 100/112 [00:34<00:04,  2.92it/s] 50%|█████     | 56/112 [00:21<00:20,  2.70it/s] 10%|▉         | 11/112 [00:04<00:38,  2.59it/s] 90%|█████████ | 101/112 [00:35<00:03,  2.94it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.78it/s] 51%|█████     | 57/112 [00:21<00:20,  2.70it/s] 11%|█         | 12/112 [00:05<00:38,  2.59it/s] 91%|█████████ | 102/112 [00:35<00:03,  2.94it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.79it/s] 52%|█████▏    | 58/112 [00:22<00:19,  2.71it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.61it/s] 92%|█████████▏| 103/112 [00:35<00:03,  2.95it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.79it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.72it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.61it/s] 93%|█████████▎| 104/112 [00:36<00:02,  2.94it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.78it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.71it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.62it/s] 94%|█████████▍| 105/112 [00:36<00:02,  2.95it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.80it/s] 54%|█████▍    | 61/112 [00:23<00:18,  2.71it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.63it/s] 95%|█████████▍| 106/112 [00:36<00:02,  2.95it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.80it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.72it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.63it/s] 96%|█████████▌| 107/112 [00:37<00:01,  2.95it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.80it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.69it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.64it/s] 96%|█████████▋| 108/112 [00:37<00:01,  2.95it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.81it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.70it/s] 97%|█████████▋| 109/112 [00:37<00:01,  2.94it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.64it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.79it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.70it/s] 98%|█████████▊| 110/112 [00:38<00:00,  2.94it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.64it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.80it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.70it/s] 99%|█████████▉| 111/112 [00:38<00:00,  2.95it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.63it/s] 61%|██████    | 68/112 [00:24<00:15,  2.80it/s]100%|██████████| 112/112 [00:38<00:00,  2.95it/s]100%|██████████| 112/112 [00:38<00:00,  2.87it/s]
 60%|█████▉    | 67/112 [00:25<00:16,  2.70it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.64it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.80it/s] 61%|██████    | 68/112 [00:26<00:16,  2.70it/s] 21%|██        | 23/112 [00:09<00:33,  2.63it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.77it/s] 62%|██████▏   | 69/112 [00:26<00:15,  2.70it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.64it/s] 63%|██████▎   | 71/112 [00:25<00:15,  2.73it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.70it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.64it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.71it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.71it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.65it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.74it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.70it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.63it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.73it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.70it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.74it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.63it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.71it/s] 68%|██████▊   | 76/112 [00:27<00:13,  2.75it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.64it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.71it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.77it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.63it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.71it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.78it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.64it/s] 69%|██████▉   | 77/112 [00:29<00:12,  2.72it/s] 71%|███████   | 79/112 [00:28<00:11,  2.79it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.61it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.72it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.78it/s] 29%|██▉       | 33/112 [00:12<00:30,  2.62it/s] 71%|███████   | 79/112 [00:30<00:12,  2.72it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.79it/s] 30%|███       | 34/112 [00:13<00:29,  2.63it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.73it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.80it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.63it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.73it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.80it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.63it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.72it/s] 75%|███████▌  | 84/112 [00:30<00:09,  2.81it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.64it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.73it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.81it/s] 34%|███▍      | 38/112 [00:14<00:28,  2.62it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.70it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.80it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.63it/s] 76%|███████▌  | 85/112 [00:32<00:09,  2.71it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.81it/s]W0402 09:50:52.518000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.518000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.519000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.519000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.519000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.519000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.519000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.558000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.558000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.558000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.558000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.558000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.573000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.573000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.573000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.573000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.573000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.733000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.733000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.733000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.733000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:50:52.734000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 36%|███▌      | 40/112 [00:15<00:27,  2.64it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.72it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.82it/s]W0402 09:50:53.043000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:50:53.043000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:50:53.043000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:50:53.043000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:50:53.046000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:50:53.046000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:50:53.046000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:50:53.078000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:50:53.078000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:50:53.078000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:50:53.078000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:50:53.079000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 37%|███▋      | 41/112 [00:16<00:26,  2.64it/s]W0402 09:50:53.146000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:50:53.146000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:50:53.147000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:50:53.147000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:50:53.147000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 87/112 [00:33<00:09,  2.72it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.81it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.65it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.72it/s] 80%|████████  | 90/112 [00:32<00:07,  2.81it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.65it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.72it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.81it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.82it/s]W0402 09:50:54.259000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:50:54.264000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 80%|████████  | 90/112 [00:34<00:08,  2.71it/s]W0402 09:50:54.270000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:50:54.270000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 39%|███▉      | 44/112 [00:17<00:25,  2.64it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.78it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.71it/s] 40%|████      | 45/112 [00:17<00:25,  2.64it/s]W0402 09:50:54.713000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:50:54.713000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:50:54.713000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:50:54.713000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:50:54.713000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:50:54.713000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:50:54.713000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:50:54.741000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:50:54.741000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:50:54.741000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:50:54.741000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:50:54.741000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 94/112 [00:34<00:06,  2.80it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.71it/s] 41%|████      | 46/112 [00:17<00:24,  2.65it/s]W0402 09:50:55.069000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:50:55.069000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:50:55.069000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:50:55.069000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:50:55.069000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:50:55.070000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:50:55.070000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:50:55.070000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 85%|████████▍ | 95/112 [00:34<00:06,  2.80it/s]W0402 09:50:55.351000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:50:55.351000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:50:55.351000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:50:55.351000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:50:55.351000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 83%|████████▎ | 93/112 [00:35<00:07,  2.70it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.64it/s]W0402 09:50:55.673000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:50:55.677000 140661346772800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 86%|████████▌ | 96/112 [00:34<00:05,  2.80it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.71it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.64it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.77it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.69it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.63it/s] 88%|████████▊ | 98/112 [00:35<00:05,  2.78it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.70it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.63it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.78it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.71it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.65it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.79it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.71it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.64it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.77it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.72it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.65it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.73it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.73it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.66it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.66it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.73it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.64it/s] 93%|█████████▎| 104/112 [00:37<00:03,  2.65it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.73it/s] 50%|█████     | 56/112 [00:21<00:21,  2.65it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.64it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.72it/s] 51%|█████     | 57/112 [00:22<00:20,  2.66it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.63it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.68it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.64it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.62it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.69it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.67it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.69it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.62it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.66it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.69it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.62it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.67it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.70it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.62it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.69it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.70it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.62it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.68it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.69it/s]100%|██████████| 112/112 [00:40<00:00,  2.62it/s]100%|██████████| 112/112 [00:40<00:00,  2.74it/s]
 57%|█████▋    | 64/112 [00:24<00:17,  2.68it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.70it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.67it/s]100%|██████████| 112/112 [00:42<00:00,  2.70it/s]100%|██████████| 112/112 [00:42<00:00,  2.65it/s]
 59%|█████▉    | 66/112 [00:25<00:17,  2.67it/s]I0402 09:51:02.579309 1591951 finetune.py:45] layer 24_down initial loss 0.011251618154346943
W0402 09:51:02.579486 1591951 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 60%|█████▉    | 67/112 [00:25<00:17,  2.64it/s]W0402 09:51:03.037942 1591951 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 61%|██████    | 68/112 [00:26<00:16,  2.59it/s]24_down proxy err 0.012326152995228767 tr(WHW.T) 34.95817565917969
 62%|██████▏   | 69/112 [00:26<00:16,  2.57it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.56it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.58it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.57it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.58it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.59it/s] 67%|██████▋   | 75/112 [00:28<00:14,  2.61it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.60it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.62it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.62it/s] 71%|███████   | 79/112 [00:30<00:12,  2.63it/s] 71%|███████▏  | 80/112 [00:30<00:12,  2.60it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.60it/s]W0402 09:51:08.417000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.417000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.417000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.417000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.417000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.417000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.418000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.458000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.458000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.458000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.458000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.458000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.473000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.473000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.473000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.473000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.473000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.633000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.633000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.633000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.633000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.633000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 73%|███████▎  | 82/112 [00:31<00:11,  2.62it/s]W0402 09:51:08.939000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.939000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.939000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.939000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.939000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.939000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.939000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.968000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.968000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.968000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.968000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:08.968000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.036000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.036000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.036000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.036000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.036000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 74%|███████▍  | 83/112 [00:31<00:11,  2.62it/s]W0402 09:51:09.436000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.436000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.436000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.437000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.437000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.437000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.437000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.477000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.477000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.477000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.477000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.477000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 84/112 [00:32<00:10,  2.63it/s]W0402 09:51:09.492000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.492000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.492000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.492000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.492000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.655000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.655000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.655000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.655000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.655000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 76%|███████▌  | 85/112 [00:32<00:10,  2.63it/s]W0402 09:51:09.961000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.961000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.961000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.961000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.961000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.961000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.962000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.991000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.991000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.991000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.991000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:09.991000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.061000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.061000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.061000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.061000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.061000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.168000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.181000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.189000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.189000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 77%|███████▋  | 86/112 [00:33<00:09,  2.63it/s]W0402 09:51:10.614000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.614000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.614000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.614000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.614000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.614000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.614000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 87/112 [00:33<00:09,  2.63it/s]W0402 09:51:10.644000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.644000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.644000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.644000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.644000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.970000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.970000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.971000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.971000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.971000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.971000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.971000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:51:10.971000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 79%|███████▊  | 88/112 [00:33<00:09,  2.64it/s]W0402 09:51:11.195000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.209000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.217000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.217000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.254000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.254000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.254000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.255000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.255000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 79%|███████▉  | 89/112 [00:34<00:08,  2.64it/s]W0402 09:51:11.570000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.575000 139791294424896 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.645000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.645000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.646000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.646000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.646000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.646000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.646000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.676000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.676000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.676000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.676000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:11.676000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 80%|████████  | 90/112 [00:34<00:08,  2.63it/s]W0402 09:51:12.014000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:51:12.015000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:12.015000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:12.015000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:12.015000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:12.015000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:12.015000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:12.015000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 91/112 [00:35<00:07,  2.63it/s]W0402 09:51:12.300000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:12.300000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:12.300000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:12.301000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:12.301000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 82%|████████▏ | 92/112 [00:35<00:07,  2.64it/s]W0402 09:51:12.621000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:12.626000 140411265591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 83%|████████▎ | 93/112 [00:35<00:07,  2.63it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.63it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.66it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.68it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.67it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.63it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.61it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.59it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.59it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.52it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.54it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.56it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.56it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.57it/s]I0402 09:51:18.197880 1592565 finetune.py:45] layer 26_down initial loss 0.017307456582784653
W0402 09:51:18.198077 1592565 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 96%|█████████▌| 107/112 [00:41<00:01,  2.60it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.66it/s]W0402 09:51:18.684859 1592565 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 97%|█████████▋| 109/112 [00:41<00:01,  2.69it/s]26_down proxy err 0.012589285150170326 tr(WHW.T) 43.263397216796875
 98%|█████████▊| 110/112 [00:42<00:00,  2.66it/s]I0402 09:51:19.582558 1592130 finetune.py:45] layer 25_down initial loss 0.013281772844493389
W0402 09:51:19.582846 1592130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 99%|█████████▉| 111/112 [00:42<00:00,  2.71it/s]W0402 09:51:20.066420 1592130 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

100%|██████████| 112/112 [00:43<00:00,  2.71it/s]100%|██████████| 112/112 [00:43<00:00,  2.60it/s]
25_down proxy err 0.012433793395757675 tr(WHW.T) 37.92062759399414
I0402 09:51:23.667589 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 28 in 1.129462718963623s
I0402 09:51:24.188297 1558010 quantize_finetune_llama.py:159] layer 29 gpu 1
I0402 09:51:25.800199 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 29 in 1.2021286487579346s
I0402 09:51:26.277496 1597292 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:51:26.277641 1597292 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:51:26.277701 1597292 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:51:26.302941 1558010 quantize_finetune_llama.py:159] layer 30 gpu 2
I0402 09:51:26.607356 1597292 config.py:58] PyTorch version 2.4.0 available.
W0402 09:51:27.934000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:27.935000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:27.935000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:27.935000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:51:27.935000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:27.935000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:27.935000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:27.978000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:27.979000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:27.979000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:27.979000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:27.979000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:27.994000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:27.994000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:27.994000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:27.994000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:27.994000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.162000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.162000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.162000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.162000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.162000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
I0402 09:51:28.284408 1597453 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:51:28.284536 1597453 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:51:28.284596 1597453 utils.py:162] NumExpr defaulting to 16 threads.
W0402 09:51:28.471000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.472000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.472000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.472000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.472000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.472000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.472000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
I0402 09:51:28.483846 1597453 config.py:58] PyTorch version 2.4.0 available.
W0402 09:51:28.518000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.518000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.518000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.518000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.518000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.588000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.588000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.588000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.588000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:28.588000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
I0402 09:51:28.951877 1597292 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 09:51:29.318404 1597292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:51:29.774000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:29.787000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:29.794000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:29.794000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.219000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.219000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.219000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.219000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.219000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.219000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.219000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.251000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.251000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.251000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.251000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.251000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:51:30.583000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.584000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.584000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.584000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.584000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.584000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.584000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.585000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.887000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.887000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.887000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.887000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:30.887000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
I0402 09:51:30.959907 1597453 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 09:51:31.223000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:31.228000 140405243930432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:31.365059 1597453 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:44,  1.42s/it]  6%|▋         | 2/32 [00:01<00:23,  1.30it/s]  9%|▉         | 3/32 [00:02<00:16,  1.79it/s]  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.17it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.47it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.68it/s] 22%|██▏       | 7/32 [00:03<00:08,  2.84it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.95it/s]  3%|▎         | 1/32 [00:01<00:46,  1.49s/it] 28%|██▊       | 9/32 [00:03<00:07,  3.04it/s]  6%|▋         | 2/32 [00:01<00:24,  1.24it/s] 31%|███▏      | 10/32 [00:04<00:07,  3.08it/s]  9%|▉         | 3/32 [00:02<00:16,  1.71it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.12it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.09it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.17it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 41%|████      | 13/32 [00:05<00:05,  3.18it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.21it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.75it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.22it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.86it/s] 50%|█████     | 16/32 [00:06<00:04,  3.22it/s] 28%|██▊       | 9/32 [00:04<00:07,  2.93it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.20it/s] 31%|███▏      | 10/32 [00:04<00:07,  3.00it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.22it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.06it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.22it/s] 38%|███▊      | 12/32 [00:05<00:06,  3.05it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.22it/s] 41%|████      | 13/32 [00:05<00:06,  3.06it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.23it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.08it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.24it/s]I0402 09:51:38.334197 1593079 finetune.py:45] layer 27_down initial loss 0.019502747803926468
W0402 09:51:38.334665 1593079 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 47%|████▋     | 15/32 [00:05<00:05,  3.07it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.21it/s] 50%|█████     | 16/32 [00:06<00:05,  3.08it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.21it/s]W0402 09:51:38.921477 1593079 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 53%|█████▎    | 17/32 [00:06<00:04,  3.11it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.22it/s]27_down proxy err 0.010576522909104824 tr(WHW.T) 61.202301025390625
 56%|█████▋    | 18/32 [00:06<00:04,  3.12it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.22it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.13it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.22it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.13it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.23it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.13it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.24it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.13it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.25it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.23it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.11it/s]100%|██████████| 32/32 [00:11<00:00,  3.18it/s]100%|██████████| 32/32 [00:11<00:00,  2.90it/s]
 75%|███████▌  | 24/32 [00:08<00:02,  3.06it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.03it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.03it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.02it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.01it/s]I0402 09:51:42.759437 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 30 in 1.2699377536773682s
 91%|█████████ | 29/32 [00:10<00:01,  2.92it/s]I0402 09:51:43.276617 1558010 quantize_finetune_llama.py:159] layer 31 gpu 3
 94%|█████████▍| 30/32 [00:10<00:00,  2.91it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.96it/s]100%|██████████| 32/32 [00:11<00:00,  2.95it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]
W0402 09:51:44.466000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.466000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.467000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.467000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.467000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.467000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.467000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.494000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.494000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.494000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.494000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.494000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.511000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.511000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.512000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.512000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.512000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.841000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.841000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.841000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.841000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:44.842000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
I0402 09:51:45.096721 1558010 quantize_finetune_llama.py:190] computed original embedding for layer 31 in 1.4232285022735596s
I0402 09:51:45.205897 1598501 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:51:45.206233 1598501 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:51:45.206402 1598501 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:51:45.622216 1598501 config.py:58] PyTorch version 2.4.0 available.
W0402 09:51:45.767000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:45.767000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:45.767000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:45.767000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:51:45.767000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:45.767000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:45.767000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:45.786000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:45.786000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:45.786000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:45.786000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:45.787000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:46.023000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:46.023000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:46.023000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:46.023000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:46.023000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.178000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.178000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.179000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.179000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.179000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.179000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.179000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.196000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.196000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.196000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.196000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.196000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.299000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.299000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.299000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.299000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.299000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.299000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.299000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.324000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.324000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.324000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.324000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.324000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.340000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.340000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.340000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.340000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.340000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
I0402 09:51:47.607523 1598673 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:51:47.607693 1598673 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:51:47.607754 1598673 utils.py:162] NumExpr defaulting to 16 threads.
W0402 09:51:47.654000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.654000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.654000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.654000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:47.655000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
I0402 09:51:47.988543 1598673 config.py:58] PyTorch version 2.4.0 available.
I0402 09:51:48.012008 1598501 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 09:51:48.106000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.107000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.107000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.107000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.107000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.363098 1598501 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:51:48.564000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.564000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.564000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.564000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.565000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.565000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.565000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.583000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.583000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.583000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.583000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.583000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 09:51:48.834000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.834000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.834000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.834000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:48.834000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:51:50.010000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:50.010000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:51:50.010000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:50.010000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:51:50.010000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:50.010000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:50.010000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:51:50.029000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:50.029000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:50.029000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:50.030000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:50.030000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
I0402 09:51:50.243302 1598673 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 09:51:50.572724 1598673 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:47,  1.52s/it]W0402 09:51:50.976000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:51:50.976000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:51:50.976000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:51:50.977000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:51:50.977000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:24,  1.21it/s]  9%|▉         | 3/32 [00:02<00:17,  1.64it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 12%|█▎        | 4/32 [00:02<00:14,  1.98it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.18it/s]  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.34it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.61it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s]  3%|▎         | 1/32 [00:01<00:53,  1.73s/it] 34%|███▍      | 11/32 [00:05<00:07,  2.69it/s]I0402 09:51:54.427952 1597292 finetune.py:45] layer 28_v initial loss 0.026240995153784752
W0402 09:51:54.428319 1597292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:02<00:27,  1.08it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.71it/s]  9%|▉         | 3/32 [00:02<00:19,  1.51it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.85it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.75it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.12it/s]W0402 09:51:55.605543 1597292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 15/32 [00:06<00:06,  2.76it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s] 50%|█████     | 16/32 [00:06<00:05,  2.77it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.78it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.57it/s]28_v proxy err 0.013195355422794819 tr(WHW.T) 175.40756225585938
  0%|          | 0/32 [00:00<?, ?it/s]I0402 09:51:56.923353 1597453 finetune.py:45] layer 29_v initial loss 0.033886197954416275
W0402 09:51:56.924355 1597453 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▋    | 18/32 [00:07<00:05,  2.78it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.67it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.85it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.83it/s]  3%|▎         | 1/32 [00:00<00:28,  1.11it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.90it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.91it/s]  6%|▋         | 2/32 [00:01<00:17,  1.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.95it/s] 41%|████      | 13/32 [00:05<00:06,  2.98it/s]  9%|▉         | 3/32 [00:01<00:13,  2.17it/s] 44%|████▍     | 14/32 [00:06<00:05,  3.02it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.98it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.43it/s]W0402 09:51:58.841155 1597453 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 15/32 [00:06<00:05,  3.04it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.98it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.55it/s] 50%|█████     | 16/32 [00:06<00:05,  3.01it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.96it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.66it/s] 53%|█████▎    | 17/32 [00:07<00:04,  3.03it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.98it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.76it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.05it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.00it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.81it/s]29_v proxy err 0.011003940366208553 tr(WHW.T) 249.68582153320312
  0%|          | 0/32 [00:00<?, ?it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.06it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.01it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.86it/s] 62%|██████▎   | 20/32 [00:08<00:03,  3.06it/s] 91%|█████████ | 29/32 [00:11<00:00,  3.01it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.90it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.08it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.03it/s]  3%|▎         | 1/32 [00:00<00:29,  1.05it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.92it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.06it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.01it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.88it/s]  6%|▋         | 2/32 [00:01<00:18,  1.61it/s] 72%|███████▏  | 23/32 [00:09<00:02,  3.01it/s]100%|██████████| 32/32 [00:12<00:00,  2.98it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
 41%|████      | 13/32 [00:04<00:06,  2.89it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.02it/s]  9%|▉         | 3/32 [00:01<00:14,  1.95it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.00it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.84it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.99it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.99it/s] 50%|█████     | 16/32 [00:06<00:05,  2.85it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.02it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.89it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 91%|█████████ | 29/32 [00:11<00:00,  3.02it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.88it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.05it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.91it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.57it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.07it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.92it/s]W0402 09:52:04.525000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.525000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.525000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.525000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.525000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.525000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.526000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.99it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s]W0402 09:52:04.551000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.552000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.552000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.552000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.552000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.568000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.568000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.568000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.568000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.568000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:07<00:03,  2.93it/s]W0402 09:52:04.902000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.902000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.902000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.902000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:04.902000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:08,  2.57it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.89it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.86it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.84it/s]W0402 09:52:05.804000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:05.804000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:05.804000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:05.804000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:52:05.804000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:05.805000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:05.805000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:05.822000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:05.822000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:05.822000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:05.822000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:05.822000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:09<00:02,  2.85it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s]W0402 09:52:06.060000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:06.060000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:06.061000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:06.061000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:06.061000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:09<00:02,  2.89it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.90it/s] 50%|█████     | 16/32 [00:06<00:06,  2.64it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.92it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.66it/s]W0402 09:52:07.189000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.189000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.189000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.189000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.189000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.189000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.189000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.206000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.206000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.206000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.207000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.207000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:10<00:01,  2.93it/s]W0402 09:52:07.546000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.546000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.546000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.546000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.546000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.546000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.546000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s]W0402 09:52:07.572000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.572000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.573000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.573000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.573000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.589000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.589000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.589000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.589000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.589000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:10<00:00,  2.93it/s]W0402 09:52:07.913000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.913000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.914000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.914000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:07.914000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.91it/s]W0402 09:52:08.099000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:08.099000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:08.100000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:08.100000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:08.100000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s]100%|██████████| 32/32 [00:11<00:00,  2.92it/s]100%|██████████| 32/32 [00:11<00:00,  2.77it/s]
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 66%|██████▌   | 21/32 [00:08<00:04,  2.63it/s]W0402 09:52:08.824000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:08.824000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:08.824000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:52:08.825000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:08.825000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:08.825000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:08.825000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:08.844000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:08.844000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:08.844000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:08.844000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:08.844000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  2.64it/s]W0402 09:52:09.102000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:09.102000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:09.102000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:09.102000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:09.102000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:09<00:03,  2.66it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.68it/s]W0402 09:52:10.290000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:10.290000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:10.290000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:52:10.290000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:10.290000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:10.290000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:10.291000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:10.309000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:10.310000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:10.310000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:10.310000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:10.310000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.69it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.65it/s]W0402 09:52:11.334000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:11.334000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:11.334000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:11.334000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:11.335000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 94%|█████████▍| 30/32 [00:11<00:00,  2.67it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
I0402 09:52:13.813240 1598501 finetune.py:45] layer 30_v initial loss 0.0544372983276844
W0402 09:52:13.813409 1598501 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:52:14.567000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.567000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.567000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.567000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.567000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.568000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.568000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.597000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.598000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.598000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.598000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.598000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.614000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.614000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.614000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.614000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.614000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.779000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.779000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.779000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.779000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.779000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:14.893783 1598501 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:52:15.016000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:15.016000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:15.016000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:15.017000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:15.017000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:52:15.017000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:15.017000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:15.039000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:15.040000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:15.040000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:15.040000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:15.040000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:15.106000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:15.107000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:15.107000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:15.107000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:15.107000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
30_v proxy err 0.013652844354510307 tr(WHW.T) 252.81201171875
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:52:16.002000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:16.327000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:16.327000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:52:16.327000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:16.328000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:16.328000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:16.328000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:16.328000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:16.351000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:16.351000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:16.351000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:16.351000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:16.351000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:16.617000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:16.617000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:16.617000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:16.617000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:16.617000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:16.897000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:30,  1.01it/s]  6%|▋         | 2/32 [00:01<00:19,  1.57it/s]I0402 09:52:17.464828 1598673 finetune.py:45] layer 31_v initial loss 0.02958616241812706
W0402 09:52:17.466671 1598673 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:01<00:14,  1.94it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s]W0402 09:52:18.794000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.794000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.794000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.794000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.794000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.795000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.795000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:02<00:10,  2.47it/s]W0402 09:52:18.824000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.824000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.824000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.824000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.824000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.839000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.839000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.839000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.839000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.839000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.902730 1598673 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:52:18.991000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.991000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.991000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.991000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:18.991000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s]W0402 09:52:19.221000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:52:19.221000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:19.221000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:19.221000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:19.221000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:19.221000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:19.221000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:19.242000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:19.242000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:19.242000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:19.242000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:19.242000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:19.308000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:19.308000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:19.308000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:19.308000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:19.308000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s]31_v proxy err 0.005586537998169661 tr(WHW.T) 365.68487548828125
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.54it/s]W0402 09:52:20.141000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s]W0402 09:52:20.443000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:20.443000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:20.443000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:52:20.443000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:20.443000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:20.443000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:20.443000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:20.463000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:20.463000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:20.463000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:20.463000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:20.464000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:20.715000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:20.715000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:20.715000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:20.715000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:20.715000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:08,  2.56it/s]  3%|▎         | 1/32 [00:00<00:29,  1.05it/s]W0402 09:52:20.970000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:05<00:07,  2.57it/s]  6%|▋         | 2/32 [00:01<00:18,  1.62it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s]  9%|▉         | 3/32 [00:01<00:14,  1.97it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.58it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.20it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 50%|█████     | 16/32 [00:06<00:06,  2.58it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.59it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s]I0402 09:52:23.486221 1597292 finetune.py:45] layer 28_q initial loss 0.026250923052430153
W0402 09:52:23.486592 1597292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.63it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.49it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.63it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.52it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s]W0402 09:52:24.599455 1597292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:04<00:08,  2.53it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.53it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s] 41%|████      | 13/32 [00:05<00:07,  2.55it/s]28_q proxy err 0.0018284362740814686 tr(WHW.T) 6755.15625
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.73it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.56it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.74it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.56it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.76it/s]  3%|▎         | 1/32 [00:00<00:26,  1.17it/s] 50%|█████     | 16/32 [00:06<00:06,  2.56it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.75it/s]  6%|▋         | 2/32 [00:01<00:16,  1.82it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.56it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.73it/s]  9%|▉         | 3/32 [00:01<00:13,  2.18it/s]I0402 09:52:27.372568 1597453 finetune.py:45] layer 29_q initial loss 0.033849965780973434
W0402 09:52:27.373074 1597453 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:11<00:01,  2.75it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.44it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.57it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.60it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.75it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.69it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.74it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.56it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.77it/s]100%|██████████| 32/32 [00:12<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
W0402 09:52:28.640851 1597453 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.82it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.58it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.81it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.86it/s]29_q proxy err 0.002198538277298212 tr(WHW.T) 6063.78369140625
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.85it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.64it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.88it/s]  3%|▎         | 1/32 [00:00<00:27,  1.12it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.60it/s] 41%|████      | 13/32 [00:04<00:06,  2.89it/s]  6%|▋         | 2/32 [00:01<00:17,  1.69it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.90it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.60it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.92it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 50%|█████     | 16/32 [00:05<00:05,  2.94it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.25it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.62it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.62it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.95it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.62it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.96it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
 62%|██████▎   | 20/32 [00:07<00:04,  2.96it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.94it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.89it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.91it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s]W0402 09:52:34.306000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.306000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.306000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.306000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.306000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.306000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.307000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:08<00:02,  2.93it/s]W0402 09:52:34.335000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.335000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.335000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.335000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.335000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.351000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.351000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.351000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.352000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.352000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.514000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.515000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.515000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.515000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.515000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:04<00:07,  2.67it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.90it/s]W0402 09:52:34.742000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.742000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.742000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.742000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.742000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.742000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.742000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.762000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.762000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.762000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.762000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.762000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.827000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.827000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.827000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.827000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:34.827000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:09<00:02,  2.90it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.93it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s]W0402 09:52:35.654000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  2.95it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s]W0402 09:52:35.954000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:35.955000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:35.955000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:35.955000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:35.955000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:52:35.955000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:35.955000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:35.974000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:35.974000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:35.975000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:35.975000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:35.975000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:10<00:01,  2.96it/s] 50%|█████     | 16/32 [00:06<00:05,  2.67it/s]W0402 09:52:36.219000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:36.219000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:36.219000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:36.219000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:36.219000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:10<00:00,  2.96it/s]W0402 09:52:36.467000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:06<00:05,  2.66it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.97it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s]100%|██████████| 32/32 [00:11<00:00,  2.93it/s]100%|██████████| 32/32 [00:11<00:00,  2.80it/s]
 59%|█████▉    | 19/32 [00:07<00:04,  2.67it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.70it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.73it/s]W0402 09:52:39.012000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.012000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.012000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.013000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.013000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.013000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.013000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.044000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.044000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.044000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.044000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.044000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.059000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.060000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.060000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.060000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.060000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:09<00:02,  2.74it/s]W0402 09:52:39.212000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.212000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.213000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.213000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.213000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.431000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.432000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.432000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.432000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.432000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.432000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.432000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.453000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.453000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.453000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.453000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.454000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s]W0402 09:52:39.518000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.518000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.518000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.518000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:39.518000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:10<00:02,  2.74it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s]W0402 09:52:40.358000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  2.72it/s]W0402 09:52:40.655000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:40.655000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:52:40.655000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:40.655000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:40.656000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:52:40.656000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:40.656000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:40.677000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:40.677000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:40.677000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:40.677000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:40.677000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:52:40.927000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:52:40.927000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:52:40.927000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:52:40.927000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:52:40.927000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s]W0402 09:52:41.181000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
I0402 09:52:42.480548 1598501 finetune.py:45] layer 30_q initial loss 0.054370004683732986
W0402 09:52:42.480723 1598501 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:52:43.534396 1598501 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:52:43.949369 1597292 finetune.py:45] layer 28_k initial loss 0.026256272569298744
W0402 09:52:43.949687 1597292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

30_q proxy err 0.0015036624390631914 tr(WHW.T) 7045.0302734375
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:52:45.015451 1597292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:00<00:27,  1.12it/s]  6%|▋         | 2/32 [00:01<00:17,  1.74it/s]28_k proxy err 0.001505597960203886 tr(WHW.T) 4372.6787109375
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:01<00:13,  2.11it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.34it/s]  3%|▎         | 1/32 [00:00<00:22,  1.36it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.49it/s]  6%|▋         | 2/32 [00:01<00:15,  1.99it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.58it/s]  9%|▉         | 3/32 [00:01<00:12,  2.34it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.65it/s]I0402 09:52:47.837241 1598673 finetune.py:45] layer 31_q initial loss 0.029566951096057892
W0402 09:52:47.837505 1598673 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 4/32 [00:01<00:11,  2.54it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.70it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.65it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.71it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.75it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s]I0402 09:52:48.792224 1597453 finetune.py:45] layer 29_k initial loss 0.03385768085718155
W0402 09:52:48.792575 1597453 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:02<00:08,  2.82it/s]W0402 09:52:48.986532 1598673 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.88it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.79it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.91it/s] 41%|████      | 13/32 [00:05<00:06,  2.81it/s]W0402 09:52:49.868315 1597453 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:03<00:07,  2.93it/s]31_q proxy err 0.0010198559612035751 tr(WHW.T) 9335.720703125
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.81it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.95it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.81it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.95it/s] 50%|█████     | 16/32 [00:06<00:05,  2.81it/s] 41%|████      | 13/32 [00:04<00:06,  2.96it/s]  3%|▎         | 1/32 [00:00<00:27,  1.11it/s]29_k proxy err 0.00163553177844733 tr(WHW.T) 4804.9951171875
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.81it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.97it/s]  6%|▋         | 2/32 [00:01<00:17,  1.70it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.82it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.96it/s]  9%|▉         | 3/32 [00:01<00:14,  2.05it/s]  3%|▎         | 1/32 [00:00<00:24,  1.29it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.81it/s] 50%|█████     | 16/32 [00:05<00:05,  2.93it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.26it/s]  6%|▋         | 2/32 [00:01<00:16,  1.84it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.82it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.94it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s]  9%|▉         | 3/32 [00:01<00:13,  2.14it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.81it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.94it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.31it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.82it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.93it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.95it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.83it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.96it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.83it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.64it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.93it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.93it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.81it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.68it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.94it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.80it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.68it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.91it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.92it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.79it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.62it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.93it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.63it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.93it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.95it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s]100%|██████████| 32/32 [00:11<00:00,  2.85it/s]100%|██████████| 32/32 [00:11<00:00,  2.69it/s]
 44%|████▍     | 14/32 [00:05<00:06,  2.64it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.95it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.70it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.89it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s]100%|██████████| 32/32 [00:11<00:00,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  2.83it/s]
 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.67it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.71it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.67it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.72it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.70it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.74it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.75it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.75it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.71it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.75it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.68it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.75it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
 94%|█████████▍| 30/32 [00:11<00:00,  2.68it/s]I0402 09:53:02.607810 1598501 finetune.py:45] layer 30_k initial loss 0.05436742678284645
W0402 09:53:02.608062 1598501 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
W0402 09:53:03.770617 1598501 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:53:04.499058 1597292 finetune.py:45] layer 28_o initial loss 0.026489419862627983
W0402 09:53:04.499358 1597292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

30_k proxy err 0.0012907757190987468 tr(WHW.T) 4109.9091796875
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:53:05.543459 1597292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:00<00:23,  1.31it/s]  6%|▋         | 2/32 [00:01<00:15,  1.89it/s]  9%|▉         | 3/32 [00:01<00:13,  2.22it/s]28_o proxy err 0.007501998450607061 tr(WHW.T) 24.542434692382812
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.43it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.56it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.61it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.67it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.67it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.71it/s]  3%|▎         | 1/32 [00:01<00:57,  1.84s/it]I0402 09:53:08.698076 1598673 finetune.py:45] layer 31_k initial loss 0.029548995196819305
W0402 09:53:08.698286 1598673 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.72it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.75it/s]W0402 09:53:09.743474 1598673 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:03<00:46,  1.55s/it] 41%|████      | 13/32 [00:05<00:06,  2.77it/s]I0402 09:53:10.175752 1597453 finetune.py:45] layer 29_o initial loss 0.033426713198423386
W0402 09:53:10.176108 1597453 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 44%|████▍     | 14/32 [00:05<00:06,  2.78it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.80it/s]31_k proxy err 0.0011286856606602669 tr(WHW.T) 4138.115234375
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:53:11.099901 1597453 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 50%|█████     | 16/32 [00:06<00:05,  2.80it/s]  9%|▉         | 3/32 [00:04<00:42,  1.46s/it] 53%|█████▎    | 17/32 [00:06<00:05,  2.81it/s]  3%|▎         | 1/32 [00:00<00:24,  1.28it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.81it/s]  6%|▋         | 2/32 [00:01<00:16,  1.85it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.80it/s]29_o proxy err 0.004669212736189365 tr(WHW.T) 36.68419647216797
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:01<00:13,  2.15it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.81it/s] 12%|█▎        | 4/32 [00:05<00:39,  1.42s/it] 12%|█▎        | 4/32 [00:01<00:12,  2.31it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.79it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 16%|█▌        | 5/32 [00:07<00:37,  1.40s/it] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s]  3%|▎         | 1/32 [00:01<01:01,  1.97s/it] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.77it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.62it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.77it/s] 19%|█▉        | 6/32 [00:08<00:36,  1.40s/it] 91%|█████████ | 29/32 [00:10<00:01,  2.78it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.63it/s]  6%|▋         | 2/32 [00:03<00:51,  1.70s/it] 94%|█████████▍| 30/32 [00:11<00:00,  2.79it/s] 41%|████      | 13/32 [00:05<00:07,  2.63it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 22%|██▏       | 7/32 [00:10<00:34,  1.39s/it]100%|██████████| 32/32 [00:11<00:00,  2.78it/s]100%|██████████| 32/32 [00:11<00:00,  2.69it/s]
 47%|████▋     | 15/32 [00:06<00:06,  2.63it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s]  9%|▉         | 3/32 [00:04<00:46,  1.62s/it] 53%|█████▎    | 17/32 [00:06<00:05,  2.65it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.65it/s] 25%|██▌       | 8/32 [00:11<00:33,  1.40s/it] 59%|█████▉    | 19/32 [00:07<00:04,  2.65it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.66it/s] 12%|█▎        | 4/32 [00:06<00:43,  1.57s/it] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s] 28%|██▊       | 9/32 [00:12<00:32,  1.40s/it] 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.67it/s] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it] 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 84%|████████▍ | 27/32 [00:10<00:01,  2.67it/s] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 88%|████████▊ | 28/32 [00:10<00:01,  2.69it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.70it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.68it/s] 34%|███▍      | 11/32 [00:15<00:30,  1.43s/it] 97%|█████████▋| 31/32 [00:12<00:00,  2.69it/s]I0402 09:53:23.246560 1598501 finetune.py:45] layer 30_o initial loss 0.05214079096913338
W0402 09:53:23.246795 1598501 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it]100%|██████████| 32/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it]W0402 09:53:24.155336 1598501 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it]30_o proxy err 0.004206132609397173 tr(WHW.T) 84.08126831054688
  0%|          | 0/32 [00:00<?, ?it/s] 41%|████      | 13/32 [00:18<00:26,  1.40s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 44%|████▍     | 14/32 [00:19<00:24,  1.38s/it]  3%|▎         | 1/32 [00:01<00:58,  1.87s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.53s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.37s/it]  6%|▋         | 2/32 [00:03<00:48,  1.61s/it] 50%|█████     | 16/32 [00:22<00:21,  1.36s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it]  9%|▉         | 3/32 [00:04<00:44,  1.52s/it]I0402 09:53:30.148739 1598673 finetune.py:45] layer 31_o initial loss 0.029978511855006218
W0402 09:53:30.148994 1598673 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 53%|█████▎    | 17/32 [00:23<00:20,  1.35s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.54s/it]W0402 09:53:31.266286 1598673 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:06<00:41,  1.48s/it] 56%|█████▋    | 18/32 [00:25<00:18,  1.35s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it]31_o proxy err 0.0027750551234930754 tr(WHW.T) 182.20681762695312
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it] 59%|█████▉    | 19/32 [00:26<00:17,  1.35s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.45s/it]  3%|▎         | 1/32 [00:01<01:01,  1.97s/it] 62%|██████▎   | 20/32 [00:27<00:16,  1.35s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.44s/it] 66%|██████▌   | 21/32 [00:29<00:14,  1.35s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it] 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 69%|██████▉   | 22/32 [00:30<00:13,  1.34s/it]  9%|▉         | 3/32 [00:05<00:46,  1.62s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 72%|███████▏  | 23/32 [00:31<00:12,  1.35s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.57s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.50s/it] 75%|███████▌  | 24/32 [00:33<00:10,  1.35s/it] 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.43s/it] 78%|███████▊  | 25/32 [00:34<00:09,  1.35s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.54s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.35s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.50s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 84%|████████▍ | 27/32 [00:37<00:06,  1.35s/it] 41%|████      | 13/32 [00:18<00:26,  1.42s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 88%|████████▊ | 28/32 [00:38<00:05,  1.35s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.35s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.50s/it] 94%|█████████▍| 30/32 [00:41<00:02,  1.35s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.50s/it] 97%|█████████▋| 31/32 [00:42<00:01,  1.34s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.50s/it]100%|██████████| 32/32 [00:44<00:00,  1.34s/it]100%|██████████| 32/32 [00:44<00:00,  1.38s/it]
 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.42s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.50s/it] 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.44s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.50s/it] 62%|██████▎   | 20/32 [00:28<00:17,  1.44s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.49s/it] 66%|██████▌   | 21/32 [00:30<00:16,  1.46s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.49s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.48s/it] 50%|█████     | 16/32 [00:24<00:24,  1.50s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.48s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.50s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.49s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.48s/it]I0402 09:53:59.481638 1597292 finetune.py:45] layer 28_up initial loss 0.026020828634500504
W0402 09:53:59.482130 1597292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 75%|███████▌  | 24/32 [00:34<00:11,  1.48s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it]W0402 09:54:00.600837 1597292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:48<00:00,  1.48s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
 78%|███████▊  | 25/32 [00:36<00:10,  1.47s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it]28_up proxy err 0.00973548088222742 tr(WHW.T) 2478.37158203125
  0%|          | 0/32 [00:00<?, ?it/s] 81%|████████▏ | 26/32 [00:37<00:08,  1.46s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.52s/it]  3%|▎         | 1/32 [00:01<00:54,  1.76s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.45s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it]  6%|▋         | 2/32 [00:03<00:45,  1.52s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.44s/it]  9%|▉         | 3/32 [00:04<00:41,  1.44s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.54s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.43s/it] 12%|█▎        | 4/32 [00:05<00:39,  1.41s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.54s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.43s/it]I0402 09:54:09.013411 1597453 finetune.py:45] layer 29_up initial loss 0.03316695615649223
W0402 09:54:09.013711 1597453 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:07<00:37,  1.39s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.55s/it]W0402 09:54:10.042413 1597453 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:44<00:01,  1.43s/it] 19%|█▉        | 6/32 [00:08<00:36,  1.39s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.54s/it]29_up proxy err 0.007956069894134998 tr(WHW.T) 3248.7060546875
  0%|          | 0/32 [00:00<?, ?it/s]100%|██████████| 32/32 [00:46<00:00,  1.43s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]
 22%|██▏       | 7/32 [00:09<00:34,  1.38s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.54s/it]  3%|▎         | 1/32 [00:01<00:58,  1.90s/it] 25%|██▌       | 8/32 [00:11<00:33,  1.39s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it] 28%|██▊       | 9/32 [00:12<00:32,  1.40s/it]  6%|▋         | 2/32 [00:03<00:49,  1.66s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.52s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.41s/it]  9%|▉         | 3/32 [00:04<00:45,  1.58s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.42s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.55s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.51s/it]I0402 09:54:18.974107 1598501 finetune.py:45] layer 30_up initial loss 0.052226945757865906
W0402 09:54:18.974303 1598501 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:17<00:28,  1.44s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it]W0402 09:54:19.825334 1598501 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it] 41%|████      | 13/32 [00:18<00:27,  1.42s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it]30_up proxy err 0.0048965346068143845 tr(WHW.T) 5513.9267578125
  0%|          | 0/32 [00:00<?, ?it/s]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]
 44%|████▍     | 14/32 [00:19<00:25,  1.40s/it] 22%|██▏       | 7/32 [00:10<00:38,  1.53s/it]  3%|▎         | 1/32 [00:01<00:56,  1.83s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.39s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.54s/it]  6%|▋         | 2/32 [00:03<00:47,  1.59s/it] 50%|█████     | 16/32 [00:22<00:22,  1.38s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it]  9%|▉         | 3/32 [00:04<00:43,  1.51s/it] 53%|█████▎    | 17/32 [00:23<00:20,  1.37s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.36s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.45s/it] 59%|█████▉    | 19/32 [00:26<00:17,  1.36s/it]I0402 09:54:29.739625 1598673 finetune.py:45] layer 31_up initial loss 0.029878946021199226
W0402 09:54:29.739922 1598673 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 20/32 [00:27<00:16,  1.35s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.54s/it]W0402 09:54:30.767213 1598673 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 66%|██████▌   | 21/32 [00:29<00:14,  1.35s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.43s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it]31_up proxy err 0.0019350938964635134 tr(WHW.T) 12277.7177734375
  0%|          | 0/32 [00:00<?, ?it/s] 69%|██████▉   | 22/32 [00:30<00:13,  1.35s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.42s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 72%|███████▏  | 23/32 [00:31<00:12,  1.35s/it]  3%|▎         | 1/32 [00:01<00:58,  1.89s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.42s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 75%|███████▌  | 24/32 [00:33<00:10,  1.35s/it]  6%|▋         | 2/32 [00:03<00:49,  1.66s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 78%|███████▊  | 25/32 [00:34<00:09,  1.35s/it]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.42s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.35s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.55s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 84%|████████▍ | 27/32 [00:37<00:06,  1.35s/it] 41%|████      | 13/32 [00:18<00:27,  1.42s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 88%|████████▊ | 28/32 [00:38<00:05,  1.35s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.36s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 94%|█████████▍| 30/32 [00:41<00:02,  1.35s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 97%|█████████▋| 31/32 [00:42<00:01,  1.35s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it]100%|██████████| 32/32 [00:44<00:00,  1.35s/it]100%|██████████| 32/32 [00:44<00:00,  1.38s/it]
 72%|███████▏  | 23/32 [00:35<00:13,  1.50s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.43s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.50s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.44s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.50s/it] 62%|██████▎   | 20/32 [00:28<00:17,  1.46s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it] 66%|██████▌   | 21/32 [00:30<00:16,  1.47s/it] 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.49s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.49s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.48s/it]I0402 09:54:54.339541 1597292 finetune.py:45] layer 28_gate initial loss 0.025666493922472
W0402 09:54:54.339952 1597292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 72%|███████▏  | 23/32 [00:33<00:13,  1.49s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it]W0402 09:54:55.142036 1597292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:44<00:04,  1.49s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.49s/it] 50%|█████     | 16/32 [00:24<00:23,  1.50s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.49s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.49s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.49s/it]28_gate proxy err 0.004743177909404039 tr(WHW.T) 8688.7451171875
  0%|          | 0/112 [00:00<?, ?it/s] 81%|████████▏ | 26/32 [00:37<00:08,  1.47s/it]  1%|          | 1/112 [00:00<01:29,  1.25it/s] 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it]  2%|▏         | 2/112 [00:01<00:58,  1.89it/s]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
  3%|▎         | 3/112 [00:01<00:48,  2.25it/s] 84%|████████▍ | 27/32 [00:39<00:07,  1.45s/it]  4%|▎         | 4/112 [00:01<00:43,  2.47it/s]  4%|▍         | 5/112 [00:02<00:40,  2.63it/s] 59%|█████▉    | 19/32 [00:28<00:19,  1.50s/it]  5%|▌         | 6/112 [00:02<00:38,  2.72it/s]  6%|▋         | 7/112 [00:02<00:37,  2.78it/s] 88%|████████▊ | 28/32 [00:40<00:05,  1.44s/it]  7%|▋         | 8/112 [00:03<00:37,  2.80it/s]  8%|▊         | 9/112 [00:03<00:36,  2.83it/s]  9%|▉         | 10/112 [00:03<00:35,  2.85it/s] 62%|██████▎   | 20/32 [00:30<00:18,  1.52s/it] 10%|▉         | 11/112 [00:04<00:35,  2.85it/s] 91%|█████████ | 29/32 [00:42<00:04,  1.44s/it] 11%|█         | 12/112 [00:04<00:34,  2.87it/s] 12%|█▏        | 13/112 [00:04<00:34,  2.89it/s] 12%|█▎        | 14/112 [00:05<00:33,  2.90it/s] 66%|██████▌   | 21/32 [00:31<00:16,  1.52s/it] 13%|█▎        | 15/112 [00:05<00:33,  2.91it/s] 14%|█▍        | 16/112 [00:05<00:32,  2.93it/s] 94%|█████████▍| 30/32 [00:43<00:02,  1.43s/it] 15%|█▌        | 17/112 [00:06<00:32,  2.94it/s] 16%|█▌        | 18/112 [00:06<00:31,  2.94it/s] 17%|█▋        | 19/112 [00:06<00:31,  2.94it/s] 69%|██████▉   | 22/32 [00:33<00:15,  1.53s/it] 18%|█▊        | 20/112 [00:07<00:31,  2.92it/s] 97%|█████████▋| 31/32 [00:44<00:01,  1.43s/it] 19%|█▉        | 21/112 [00:07<00:31,  2.91it/s] 20%|█▉        | 22/112 [00:07<00:30,  2.93it/s] 21%|██        | 23/112 [00:08<00:30,  2.93it/s] 72%|███████▏  | 23/32 [00:35<00:13,  1.54s/it] 21%|██▏       | 24/112 [00:08<00:30,  2.93it/s]100%|██████████| 32/32 [00:46<00:00,  1.42s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]
 22%|██▏       | 25/112 [00:09<00:29,  2.93it/s] 23%|██▎       | 26/112 [00:09<00:29,  2.89it/s]I0402 09:55:07.978177 1597453 finetune.py:45] layer 29_gate initial loss 0.03272479027509689
W0402 09:55:07.978410 1597453 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 24%|██▍       | 27/112 [00:09<00:29,  2.84it/s] 75%|███████▌  | 24/32 [00:36<00:12,  1.54s/it] 25%|██▌       | 28/112 [00:10<00:29,  2.81it/s]W0402 09:55:08.703629 1597453 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 26%|██▌       | 29/112 [00:10<00:29,  2.84it/s] 27%|██▋       | 30/112 [00:10<00:28,  2.84it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.82it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.82it/s] 78%|███████▊  | 25/32 [00:38<00:10,  1.54s/it] 29%|██▉       | 33/112 [00:11<00:28,  2.80it/s] 30%|███       | 34/112 [00:12<00:28,  2.78it/s] 31%|███▏      | 35/112 [00:12<00:27,  2.76it/s] 32%|███▏      | 36/112 [00:12<00:27,  2.75it/s] 81%|████████▏ | 26/32 [00:39<00:09,  1.53s/it] 33%|███▎      | 37/112 [00:13<00:27,  2.73it/s]29_gate proxy err 0.004288113676011562 tr(WHW.T) 9685.263671875
  0%|          | 0/112 [00:00<?, ?it/s] 34%|███▍      | 38/112 [00:13<00:27,  2.73it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.73it/s]  1%|          | 1/112 [00:00<01:33,  1.19it/s] 36%|███▌      | 40/112 [00:14<00:26,  2.71it/s]  2%|▏         | 2/112 [00:01<01:02,  1.75it/s] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it] 37%|███▋      | 41/112 [00:14<00:26,  2.72it/s]  3%|▎         | 3/112 [00:01<00:52,  2.09it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.73it/s]  4%|▎         | 4/112 [00:01<00:47,  2.29it/s] 38%|███▊      | 43/112 [00:15<00:25,  2.72it/s]  4%|▍         | 5/112 [00:02<00:44,  2.40it/s] 39%|███▉      | 44/112 [00:15<00:25,  2.71it/s]  5%|▌         | 6/112 [00:02<00:42,  2.49it/s] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it] 40%|████      | 45/112 [00:16<00:24,  2.72it/s]I0402 09:55:14.919885 1598501 finetune.py:45] layer 30_gate initial loss 0.05228482559323311
W0402 09:55:14.920329 1598501 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 7/112 [00:03<00:41,  2.53it/s] 41%|████      | 46/112 [00:16<00:24,  2.72it/s]  7%|▋         | 8/112 [00:03<00:40,  2.54it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.77it/s]W0402 09:55:15.677365 1598501 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  8%|▊         | 9/112 [00:03<00:40,  2.57it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.82it/s]  9%|▉         | 10/112 [00:04<00:39,  2.59it/s] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it] 44%|████▍     | 49/112 [00:17<00:22,  2.80it/s] 10%|▉         | 11/112 [00:04<00:38,  2.59it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.81it/s] 11%|█         | 12/112 [00:05<00:38,  2.60it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.79it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.60it/s] 46%|████▋     | 52/112 [00:18<00:21,  2.77it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.63it/s] 94%|█████████▍| 30/32 [00:45<00:03,  1.51s/it] 47%|████▋     | 53/112 [00:19<00:20,  2.81it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.85it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.64it/s] 49%|████▉     | 55/112 [00:19<00:19,  2.87it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.65it/s] 50%|█████     | 56/112 [00:20<00:19,  2.88it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.65it/s] 51%|█████     | 57/112 [00:20<00:19,  2.89it/s] 97%|█████████▋| 31/32 [00:47<00:01,  1.50s/it] 16%|█▌        | 18/112 [00:07<00:35,  2.65it/s] 52%|█████▏    | 58/112 [00:20<00:18,  2.89it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.64it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.88it/s]30_gate proxy err 0.0033539910800755024 tr(WHW.T) 13194.71484375
  0%|          | 0/112 [00:00<?, ?it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.65it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.88it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.63it/s] 54%|█████▍    | 61/112 [00:21<00:17,  2.89it/s]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
 20%|█▉        | 22/112 [00:08<00:34,  2.62it/s]  1%|          | 1/112 [00:00<01:33,  1.18it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.90it/s] 21%|██        | 23/112 [00:09<00:34,  2.61it/s]  2%|▏         | 2/112 [00:01<01:02,  1.76it/s] 56%|█████▋    | 63/112 [00:22<00:16,  2.91it/s] 57%|█████▋    | 64/112 [00:22<00:16,  2.92it/s]  3%|▎         | 3/112 [00:01<00:51,  2.10it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.58it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.90it/s]  4%|▎         | 4/112 [00:01<00:47,  2.28it/s] 22%|██▏       | 25/112 [00:09<00:34,  2.54it/s] 59%|█████▉    | 66/112 [00:23<00:15,  2.89it/s]  4%|▍         | 5/112 [00:02<00:44,  2.41it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.53it/s] 60%|█████▉    | 67/112 [00:23<00:15,  2.90it/s]  5%|▌         | 6/112 [00:02<00:42,  2.51it/s] 24%|██▍       | 27/112 [00:10<00:33,  2.56it/s] 61%|██████    | 68/112 [00:24<00:15,  2.92it/s]  6%|▋         | 7/112 [00:03<00:40,  2.58it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.56it/s] 62%|██████▏   | 69/112 [00:24<00:14,  2.91it/s]  7%|▋         | 8/112 [00:03<00:39,  2.60it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.57it/s] 62%|██████▎   | 70/112 [00:24<00:14,  2.91it/s]  8%|▊         | 9/112 [00:03<00:39,  2.63it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.58it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.91it/s]  9%|▉         | 10/112 [00:04<00:38,  2.65it/s] 64%|██████▍   | 72/112 [00:25<00:13,  2.91it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.57it/s] 10%|▉         | 11/112 [00:04<00:38,  2.65it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.92it/s] 29%|██▊       | 32/112 [00:12<00:31,  2.56it/s] 11%|█         | 12/112 [00:04<00:37,  2.68it/s] 66%|██████▌   | 74/112 [00:26<00:12,  2.93it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.56it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.69it/s] 67%|██████▋   | 75/112 [00:26<00:12,  2.93it/s] 30%|███       | 34/112 [00:13<00:30,  2.56it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.70it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.94it/s] 31%|███▏      | 35/112 [00:13<00:30,  2.56it/s] 13%|█▎        | 15/112 [00:06<00:35,  2.72it/s] 69%|██████▉   | 77/112 [00:27<00:11,  2.94it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.57it/s] 70%|██████▉   | 78/112 [00:27<00:11,  2.93it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.72it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.57it/s] 71%|███████   | 79/112 [00:28<00:11,  2.92it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.72it/s] 71%|███████▏  | 80/112 [00:28<00:10,  2.92it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.56it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.71it/s] 72%|███████▏  | 81/112 [00:28<00:10,  2.92it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.56it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.72it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.93it/s] 36%|███▌      | 40/112 [00:15<00:28,  2.56it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.73it/s] 74%|███████▍  | 83/112 [00:29<00:09,  2.93it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.73it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.56it/s] 75%|███████▌  | 84/112 [00:29<00:09,  2.93it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.71it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.55it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.91it/s]I0402 09:55:28.719344 1598673 finetune.py:45] layer 31_gate initial loss 0.029848195612430573
W0402 09:55:28.719633 1598673 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 21%|██        | 23/112 [00:08<00:32,  2.72it/s] 38%|███▊      | 43/112 [00:17<00:27,  2.55it/s] 77%|███████▋  | 86/112 [00:30<00:08,  2.91it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.72it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.57it/s] 78%|███████▊  | 87/112 [00:30<00:08,  2.91it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.73it/s]W0402 09:55:29.558476 1598673 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 40%|████      | 45/112 [00:17<00:25,  2.59it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.91it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.73it/s] 79%|███████▉  | 89/112 [00:31<00:07,  2.90it/s] 41%|████      | 46/112 [00:18<00:25,  2.59it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.73it/s] 80%|████████  | 90/112 [00:31<00:07,  2.89it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.57it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.73it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.90it/s] 43%|████▎     | 48/112 [00:18<00:25,  2.55it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.74it/s] 82%|████████▏ | 92/112 [00:32<00:06,  2.86it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.53it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.75it/s] 83%|████████▎ | 93/112 [00:32<00:06,  2.88it/s] 45%|████▍     | 50/112 [00:19<00:24,  2.54it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.89it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.75it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.58it/s] 85%|████████▍ | 95/112 [00:33<00:05,  2.89it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.75it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.59it/s] 86%|████████▌ | 96/112 [00:33<00:05,  2.90it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.72it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.60it/s] 87%|████████▋ | 97/112 [00:34<00:05,  2.91it/s] 30%|███       | 34/112 [00:12<00:28,  2.74it/s]31_gate proxy err 0.001462055603042245 tr(WHW.T) 25696.34765625
  0%|          | 0/112 [00:00<?, ?it/s] 88%|████████▊ | 98/112 [00:34<00:04,  2.90it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.62it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.73it/s] 88%|████████▊ | 99/112 [00:34<00:04,  2.89it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.62it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.73it/s]  1%|          | 1/112 [00:00<01:32,  1.20it/s] 89%|████████▉ | 100/112 [00:35<00:04,  2.90it/s] 50%|█████     | 56/112 [00:22<00:21,  2.62it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.73it/s]  2%|▏         | 2/112 [00:01<01:02,  1.76it/s] 90%|█████████ | 101/112 [00:35<00:03,  2.91it/s] 51%|█████     | 57/112 [00:22<00:20,  2.63it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.73it/s]  3%|▎         | 3/112 [00:01<00:52,  2.08it/s] 91%|█████████ | 102/112 [00:35<00:03,  2.92it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.63it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.75it/s] 92%|█████████▏| 103/112 [00:36<00:03,  2.93it/s]  4%|▎         | 4/112 [00:01<00:47,  2.27it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.74it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.63it/s] 93%|█████████▎| 104/112 [00:36<00:02,  2.90it/s]  4%|▍         | 5/112 [00:02<00:44,  2.38it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.72it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.62it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.90it/s]  5%|▌         | 6/112 [00:02<00:43,  2.46it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.75it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.62it/s] 95%|█████████▍| 106/112 [00:37<00:02,  2.91it/s]  6%|▋         | 7/112 [00:03<00:41,  2.51it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.76it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.63it/s] 96%|█████████▌| 107/112 [00:37<00:01,  2.91it/s]  7%|▋         | 8/112 [00:03<00:40,  2.55it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.76it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.91it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.63it/s]  8%|▊         | 9/112 [00:03<00:39,  2.58it/s] 40%|████      | 45/112 [00:16<00:24,  2.75it/s] 97%|█████████▋| 109/112 [00:38<00:01,  2.91it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.62it/s]  9%|▉         | 10/112 [00:04<00:39,  2.60it/s] 41%|████      | 46/112 [00:17<00:23,  2.75it/s] 98%|█████████▊| 110/112 [00:38<00:00,  2.92it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.63it/s] 10%|▉         | 11/112 [00:04<00:38,  2.60it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.74it/s] 99%|█████████▉| 111/112 [00:39<00:00,  2.91it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.63it/s] 11%|█         | 12/112 [00:05<00:38,  2.61it/s]100%|██████████| 112/112 [00:39<00:00,  2.92it/s]100%|██████████| 112/112 [00:39<00:00,  2.84it/s]
 43%|████▎     | 48/112 [00:18<00:23,  2.76it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.62it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.62it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.74it/s] 61%|██████    | 68/112 [00:26<00:16,  2.63it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.63it/s] 45%|████▍     | 50/112 [00:18<00:23,  2.67it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.64it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.62it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.64it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.60it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.63it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.62it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.62it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.63it/s] 47%|████▋     | 53/112 [00:19<00:22,  2.66it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.62it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.64it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.65it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.62it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.62it/s] 49%|████▉     | 55/112 [00:20<00:21,  2.65it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.61it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.62it/s] 50%|█████     | 56/112 [00:21<00:20,  2.68it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.62it/s] 51%|█████     | 57/112 [00:21<00:20,  2.70it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.62it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.63it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.72it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.63it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.63it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.71it/s] 21%|██        | 23/112 [00:09<00:33,  2.62it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.63it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.72it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.63it/s] 71%|███████   | 79/112 [00:30<00:12,  2.64it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.74it/s] 22%|██▏       | 25/112 [00:09<00:33,  2.63it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.64it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.75it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.63it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.64it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.74it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.63it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.65it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.74it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.64it/s] 74%|███████▍  | 83/112 [00:32<00:10,  2.65it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.75it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.64it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.65it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.74it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.64it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.65it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.74it/s]W0402 09:55:44.980000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:55:44.980000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:55:44.980000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:55:44.980000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:55:44.980000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:55:44.981000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:55:44.981000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.020000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.020000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.020000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.020000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.021000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.036000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.036000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.036000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.036000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.036000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 31/112 [00:12<00:30,  2.64it/s]W0402 09:55:45.199000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.199000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.199000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.200000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.200000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 77%|███████▋  | 86/112 [00:33<00:09,  2.64it/s] 61%|██████    | 68/112 [00:25<00:16,  2.74it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.63it/s]W0402 09:55:45.508000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.508000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.508000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.508000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.508000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.508000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.508000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.540000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.540000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.540000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.540000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.540000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.607000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.607000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.607000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.607000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:55:45.607000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 87/112 [00:33<00:09,  2.64it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.75it/s] 29%|██▉       | 33/112 [00:12<00:30,  2.63it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.64it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.72it/s] 30%|███       | 34/112 [00:13<00:29,  2.63it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.73it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.64it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.62it/s]W0402 09:55:46.743000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:55:46.756000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:55:46.765000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:55:46.765000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 64%|██████▍   | 72/112 [00:26<00:14,  2.73it/s] 80%|████████  | 90/112 [00:34<00:08,  2.64it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.63it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.73it/s] 81%|████████▏ | 91/112 [00:35<00:07,  2.64it/s]W0402 09:55:47.199000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.199000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.199000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.199000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.200000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.200000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.200000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.230000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.230000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.230000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.230000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.230000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 33%|███▎      | 37/112 [00:14<00:28,  2.63it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.75it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.64it/s]W0402 09:55:47.559000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.559000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.560000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.560000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.560000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.560000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.560000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.560000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 38/112 [00:14<00:28,  2.63it/s]W0402 09:55:47.844000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.844000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.844000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.844000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:55:47.844000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 67%|██████▋   | 75/112 [00:28<00:13,  2.74it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.64it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.62it/s]W0402 09:55:48.175000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:55:48.180000 139875781199680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 68%|██████▊   | 76/112 [00:28<00:13,  2.73it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.64it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.62it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.70it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.63it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.62it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.71it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.63it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.62it/s] 71%|███████   | 79/112 [00:29<00:12,  2.72it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.63it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.62it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.74it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.64it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.63it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.72it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.64it/s] 40%|████      | 45/112 [00:17<00:25,  2.63it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.68it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.65it/s] 41%|████      | 46/112 [00:17<00:24,  2.64it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.65it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.66it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.66it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.63it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.66it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.63it/s] 76%|███████▌  | 85/112 [00:31<00:10,  2.61it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.65it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.64it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.60it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.64it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.64it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.60it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.65it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.65it/s] 79%|███████▊  | 88/112 [00:32<00:09,  2.61it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.65it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.65it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.62it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.66it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.67it/s] 80%|████████  | 90/112 [00:33<00:08,  2.63it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.67it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.67it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.63it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.67it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.66it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.63it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.67it/s] 50%|█████     | 56/112 [00:21<00:20,  2.67it/s] 83%|████████▎ | 93/112 [00:34<00:07,  2.64it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.68it/s] 51%|█████     | 57/112 [00:22<00:20,  2.66it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.64it/s]100%|██████████| 112/112 [00:43<00:00,  2.67it/s]100%|██████████| 112/112 [00:43<00:00,  2.59it/s]
I0402 09:55:55.176546 1597292 finetune.py:45] layer 28_down initial loss 0.025288494303822517
W0402 09:55:55.176906 1597292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 52%|█████▏    | 58/112 [00:22<00:20,  2.64it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.65it/s] 53%|█████▎    | 59/112 [00:22<00:20,  2.60it/s]W0402 09:55:55.755878 1597292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 86%|████████▌ | 96/112 [00:35<00:05,  2.69it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.55it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.73it/s]28_down proxy err 0.010123632848262787 tr(WHW.T) 82.2515869140625
 88%|████████▊ | 98/112 [00:36<00:05,  2.75it/s] 54%|█████▍    | 61/112 [00:23<00:20,  2.53it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.76it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.54it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.76it/s] 56%|█████▋    | 63/112 [00:24<00:19,  2.56it/s] 90%|█████████ | 101/112 [00:37<00:03,  2.77it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.56it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.75it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.57it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.75it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.58it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.76it/s] 60%|█████▉    | 67/112 [00:25<00:17,  2.60it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.77it/s] 61%|██████    | 68/112 [00:26<00:16,  2.61it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.78it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.62it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.79it/s] 62%|██████▎   | 70/112 [00:27<00:15,  2.63it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.80it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.62it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.79it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.63it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.80it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.64it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.80it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.63it/s]100%|██████████| 112/112 [00:41<00:00,  2.80it/s]100%|██████████| 112/112 [00:41<00:00,  2.69it/s]
 67%|██████▋   | 75/112 [00:29<00:14,  2.63it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.63it/s]W0402 09:56:02.325000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.325000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.325000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.325000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.325000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.325000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.326000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.367000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.367000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.368000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.368000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.368000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.383000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.383000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.383000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.383000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.383000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.544000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.544000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.544000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.544000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.544000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 77/112 [00:29<00:13,  2.63it/s]W0402 09:56:02.848000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.848000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.848000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.848000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.848000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.848000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.849000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.879000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.879000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.880000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.880000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.880000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.947000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.947000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.947000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.947000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:02.947000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 70%|██████▉   | 78/112 [00:30<00:12,  2.64it/s] 71%|███████   | 79/112 [00:30<00:12,  2.63it/s] 71%|███████▏  | 80/112 [00:30<00:12,  2.64it/s]W0402 09:56:04.077000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.090000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.098000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.098000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 81/112 [00:31<00:11,  2.65it/s]W0402 09:56:04.533000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.533000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.533000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.533000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.533000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.533000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.533000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 73%|███████▎  | 82/112 [00:31<00:11,  2.63it/s]W0402 09:56:04.563000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.563000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.563000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.563000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.563000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.888000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.888000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.888000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.888000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.888000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.888000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.888000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:04.888000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 74%|███████▍  | 83/112 [00:32<00:10,  2.64it/s]W0402 09:56:05.170000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:05.170000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:05.170000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:05.170000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:05.170000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 84/112 [00:32<00:10,  2.64it/s]W0402 09:56:05.501000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:05.506000 140490299680576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 76%|███████▌  | 85/112 [00:32<00:10,  2.62it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.63it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.63it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.63it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.64it/s] 80%|████████  | 90/112 [00:34<00:08,  2.61it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.59it/s]W0402 09:56:08.298000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.298000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.298000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.298000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.298000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.299000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.299000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.336000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.336000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.336000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.336000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.336000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.351000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.351000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.351000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.351000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.351000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 82%|████████▏ | 92/112 [00:35<00:07,  2.58it/s]W0402 09:56:08.515000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.515000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.515000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.515000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.515000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 83%|████████▎ | 93/112 [00:35<00:07,  2.57it/s]W0402 09:56:08.827000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.827000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.827000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.828000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.828000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.828000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.828000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.859000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.859000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.859000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.859000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.859000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.926000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.926000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.926000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.926000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:08.926000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 94/112 [00:36<00:07,  2.56it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.56it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.56it/s]W0402 09:56:10.049000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.054000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.061000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.061000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 87%|████████▋ | 97/112 [00:37<00:05,  2.55it/s]W0402 09:56:10.496000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.497000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.497000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.497000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.497000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.497000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.497000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.527000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.527000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.527000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.527000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.527000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 98/112 [00:37<00:05,  2.55it/s]W0402 09:56:10.854000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.855000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.855000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.855000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.855000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.855000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.855000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:10.855000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 99/112 [00:38<00:05,  2.54it/s]W0402 09:56:11.136000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:11.136000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:11.136000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:11.136000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:11.137000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:11.461000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:11.466000 140008402630464 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 89%|████████▉ | 100/112 [00:38<00:04,  2.54it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.55it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.53it/s]I0402 09:56:12.513036 1597453 finetune.py:45] layer 29_down initial loss 0.03224613517522812
W0402 09:56:12.513816 1597453 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 92%|█████████▏| 103/112 [00:39<00:03,  2.53it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.58it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.62it/s]W0402 09:56:13.469896 1597453 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 95%|█████████▍| 106/112 [00:40<00:02,  2.63it/s]29_down proxy err 0.008494860492646694 tr(WHW.T) 132.4029083251953
 96%|█████████▌| 107/112 [00:41<00:01,  2.66it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.67it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.67it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.66it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.67it/s]100%|██████████| 112/112 [00:43<00:00,  2.67it/s]100%|██████████| 112/112 [00:43<00:00,  2.59it/s]
I0402 09:56:18.130697 1598501 finetune.py:45] layer 30_down initial loss 0.05177374929189682
W0402 09:56:18.130969 1598501 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:56:18.581472 1598501 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

30_down proxy err 0.004593616351485252 tr(WHW.T) 368.5620422363281
W0402 09:56:22.976000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:22.976000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:22.976000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:56:22.976000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:22.976000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:22.976000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:22.976000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.017000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.017000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.017000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.017000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.017000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.032000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.032000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.032000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.032000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.032000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.198000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.198000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.198000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.198000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.198000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.504000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.504000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.504000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.504000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.505000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.505000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.505000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.536000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.537000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.537000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.537000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.537000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.605000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.605000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.605000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.605000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:23.605000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:24.734000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:24.740000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:24.746000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:24.746000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.177000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.178000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.178000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.178000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.178000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.178000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.178000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.210000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.210000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.210000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.210000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.210000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.541000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.541000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.542000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.542000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.542000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.542000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.542000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.542000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.821000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.821000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.821000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.822000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:56:25.822000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:26.140000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:56:26.145000 140219100112704 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0402 09:56:33.066746 1598673 finetune.py:45] layer 31_down initial loss 0.029588844627141953
W0402 09:56:33.067046 1598673 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:56:33.588596 1598673 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

31_down proxy err 0.0014152623480185866 tr(WHW.T) 2762.897216796875
I0402 09:56:45.474778 1603019 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:56:45.474921 1603019 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:56:45.474961 1603019 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:56:45.707506 1603019 config.py:58] PyTorch version 2.4.0 available.
W0402 09:56:47.669490 1603019 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0402 09:56:47.669970 1603019 hfize_llama.py:25] LlamaConfig {
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {
    "K": 3,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  4.23it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  4.58it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  4.65it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  4.69it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  4.83it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  3.92it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  3.52it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  4.00it/s]
Some weights of the model checkpoint at ../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B and are newly initialized: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.tp_rank', 'model.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  3.39it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  3.51it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:01,  3.61it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:01<00:00,  3.78it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  3.97it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  4.02it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  4.12it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  3.91it/s]
W0402 09:56:53.486790 1603019 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(f'{args.quantized_path}/{ii}_layernorm.pt',

W0402 09:56:53.487931 1603019 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',

W0402 09:56:53.507886 1603019 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_k.pt',

W0402 09:56:53.518540 1603019 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_v.pt',

W0402 09:56:53.523409 1603019 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_o.pt',

W0402 09:56:53.539448 1603019 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',

W0402 09:56:53.559365 1603019 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_gate.pt',

W0402 09:56:53.579018 1603019 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_down.pt',

I0402 09:56:53.593742 1603019 hfize_llama.py:113] loaded layer 0
I0402 09:56:53.708048 1603019 hfize_llama.py:113] loaded layer 1
I0402 09:56:53.831171 1603019 hfize_llama.py:113] loaded layer 2
I0402 09:56:53.952651 1603019 hfize_llama.py:113] loaded layer 3
I0402 09:56:54.044548 1603019 hfize_llama.py:113] loaded layer 4
I0402 09:56:54.132364 1603019 hfize_llama.py:113] loaded layer 5
I0402 09:56:54.272146 1603019 hfize_llama.py:113] loaded layer 6
I0402 09:56:54.346709 1603019 hfize_llama.py:113] loaded layer 7
I0402 09:56:54.417047 1603019 hfize_llama.py:113] loaded layer 8
I0402 09:56:54.510793 1603019 hfize_llama.py:113] loaded layer 9
I0402 09:56:54.615384 1603019 hfize_llama.py:113] loaded layer 10
I0402 09:56:54.715136 1603019 hfize_llama.py:113] loaded layer 11
I0402 09:56:54.792147 1603019 hfize_llama.py:113] loaded layer 12
I0402 09:56:54.934845 1603019 hfize_llama.py:113] loaded layer 13
I0402 09:56:55.049518 1603019 hfize_llama.py:113] loaded layer 14
I0402 09:56:55.146461 1603019 hfize_llama.py:113] loaded layer 15
I0402 09:56:55.220762 1603019 hfize_llama.py:113] loaded layer 16
I0402 09:56:55.305346 1603019 hfize_llama.py:113] loaded layer 17
I0402 09:56:55.375085 1603019 hfize_llama.py:113] loaded layer 18
I0402 09:56:55.471970 1603019 hfize_llama.py:113] loaded layer 19
I0402 09:56:55.567143 1603019 hfize_llama.py:113] loaded layer 20
I0402 09:56:55.686590 1603019 hfize_llama.py:113] loaded layer 21
I0402 09:56:55.827085 1603019 hfize_llama.py:113] loaded layer 22
I0402 09:56:55.941970 1603019 hfize_llama.py:113] loaded layer 23
I0402 09:56:56.048893 1603019 hfize_llama.py:113] loaded layer 24
I0402 09:56:56.135370 1603019 hfize_llama.py:113] loaded layer 25
I0402 09:56:56.232661 1603019 hfize_llama.py:113] loaded layer 26
I0402 09:56:56.342744 1603019 hfize_llama.py:113] loaded layer 27
I0402 09:56:56.417685 1603019 hfize_llama.py:113] loaded layer 28
I0402 09:56:56.534934 1603019 hfize_llama.py:113] loaded layer 29
I0402 09:56:56.629461 1603019 hfize_llama.py:113] loaded layer 30
I0402 09:56:56.747172 1603019 hfize_llama.py:113] loaded layer 31
I0402 09:56:56.747317 1603019 hfize_llama.py:115] saving model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.36it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.11it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]
I0402 09:57:09.388972 1603019 hfize_llama.py:122] successfully loaded hfized model
