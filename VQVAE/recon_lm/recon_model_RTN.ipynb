{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    CLIPVisionModelWithProjection,\n",
    "    ViTForImageClassification,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "std = 0.012528747320175171"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.52it/s]\n"
     ]
    }
   ],
   "source": [
    "def latest_version_path(cache_dir, model_name, branch=\"main\"):\n",
    "    model_name_dir = \"models--\" + model_name.replace(\"/\", \"--\")\n",
    "    path = os.path.join(cache_dir, model_name_dir)\n",
    "    if not os.path.isdir(os.path.join(path, \"snapshots\")):\n",
    "        return None\n",
    "    branch_file = os.path.join(path, \"refs\", branch)\n",
    "    with open(branch_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, \"snapshots\", revision)\n",
    "\n",
    "\n",
    "cache_directory = \"../Wparam_dataset_v0/model_zoo/huggingface\"\n",
    "ckpt_path = latest_version_path(cache_directory, \"meta-llama/Meta-Llama-3-8B\")\n",
    "net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "ckpt_path = \"/home/jgryu/Weight_compression/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path, local_files_only=True)\n",
    "state_dict = net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_quantize_tensor(\n",
    "    w, n_bit=8, zero_point=True, q_group_size=-1, inplace=False, get_scale_zp=False\n",
    "):\n",
    "    org_w_shape = w.shape\n",
    "    if q_group_size > 0:\n",
    "        assert org_w_shape[-1] % q_group_size == 0\n",
    "        w = w.reshape(-1, q_group_size)\n",
    "    assert w.dim() == 2\n",
    "    if zero_point:\n",
    "        max_val = w.amax(dim=1, keepdim=True)\n",
    "        min_val = w.amin(dim=1, keepdim=True)\n",
    "        max_int = 2**n_bit - 1\n",
    "        min_int = 0\n",
    "        scales = (max_val - min_val).clamp(min=1e-5) / max_int\n",
    "        zeros = (-torch.round(min_val / scales)).clamp_(min_int, max_int)\n",
    "    else:  # we actually never used this\n",
    "        assert min_val is None\n",
    "        max_val = w.abs().amax(dim=1, keepdim=True)\n",
    "        max_val = max_val.clamp(min=1e-5)\n",
    "        max_int = 2 ** (n_bit - 1) - 1\n",
    "        min_int = -(2 ** (n_bit - 1))\n",
    "        scales = max_val / max_int\n",
    "        zeros = 0\n",
    "\n",
    "    assert torch.isnan(scales).sum() == 0\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    if inplace:\n",
    "        (\n",
    "            (w.div_(scales).round_().add_(zeros)).clamp_(min_int, max_int).sub_(zeros)\n",
    "        ).mul_(scales)\n",
    "    else:\n",
    "        w = (\n",
    "            torch.clamp(torch.round(w / scales) + zeros, min_int, max_int) - zeros\n",
    "        ) * scales\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    w = w.reshape(org_w_shape)\n",
    "\n",
    "    if get_scale_zp:\n",
    "        return w, scales.view(w.shape[0], -1), zeros.view(w.shape[0], -1)\n",
    "    else:\n",
    "        return w\n",
    "    \n",
    "def get_named_linears(module):\n",
    "    return {name: m for name, m in module.named_modules() if isinstance(m, nn.Linear)}\n",
    "\n",
    "def get_blocks(model):\n",
    "    if model.__class__.__name__ in (\"LlamaForCausalLM\", \"Qwen2ForCausalLM\"):\n",
    "        layers = model.model.layers\n",
    "    elif model.__class__.__name__ == \"LlavaLlamaForCausalLM\":\n",
    "        # layers = [model.model.layers, model.model.vision_tower.vision_tower.vision_model.encoder.layers]\n",
    "        layers = model.model.layers\n",
    "    elif isinstance(model, OPTForCausalLM):\n",
    "        layers = model.model.decoder.layers\n",
    "    elif isinstance(model, BloomForCausalLM):\n",
    "        layers = model.transformer.h\n",
    "    elif \"mpt\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.blocks\n",
    "    elif \"falcon\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.h\n",
    "    elif \"bigcode\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.h\n",
    "    elif \"neox\" in str(model.__class__).lower():\n",
    "        layers = model.gpt_neox.layers\n",
    "    elif model.__class__.__name__ == \"LlavaLlamaModel\":\n",
    "        layers = model.llm.model.layers\n",
    "    else:\n",
    "        raise NotImplementedError(type(model))\n",
    "    return layers\n",
    "\n",
    "@torch.no_grad()\n",
    "def pseudo_quantize_model_weight(\n",
    "    model,\n",
    "    w_bit,\n",
    "    q_config,\n",
    "):\n",
    "\n",
    "    layers = get_blocks(model)\n",
    "    for i in tqdm(range(len(layers)), desc=\"pseudo weight quantization...\"):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        for n, m in named_linears.items():\n",
    "            m.cuda()\n",
    "            m.weight.data = pseudo_quantize_tensor(\n",
    "                m.weight.data, n_bit=w_bit, **q_config\n",
    "            )\n",
    "            m.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pseudo weight quantization...: 100%|██████████| 32/32 [00:13<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strart saving\n",
      "End saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pseudo weight quantization...: 100%|██████████| 32/32 [00:06<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strart saving\n",
      "End saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pseudo weight quantization...: 100%|██████████| 32/32 [00:06<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strart saving\n",
      "End saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pseudo weight quantization...: 100%|██████████| 32/32 [00:06<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strart saving\n",
      "End saving\n"
     ]
    }
   ],
   "source": [
    "for q_group_size in [128, -1]:\n",
    "    for b in [5, 7]:\n",
    "        net.load_state_dict(state_dict)\n",
    "        pseudo_quantize_model_weight(net, w_bit = b, q_config= {'q_group_size': q_group_size})\n",
    "\n",
    "        save_directory = (\n",
    "            f\"/home/jgryu/Weight_compression/model_reconstructed/rtn/b{b}_g{q_group_size}\"\n",
    "        )\n",
    "        net = net.to(dtype=torch.bfloat16)\n",
    "\n",
    "        print('Strart saving')\n",
    "        net.save_pretrained(save_directory)\n",
    "        tokenizer.save_pretrained(save_directory)\n",
    "        print('End saving')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
